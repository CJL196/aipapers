# Long-Sequence Recommendation Models Need Decoupled Embeddings

Ningya Feng1  Junwei Pan2; Jialong $\mathbf { W } \mathbf { u } ^ { 1 }$ •Baixu Chen1, Ximei Wang2, Qian $\mathbf { L i } ^ { 2 }$ , Xian $\mathbf { H } \mathbf { u } ^ { 2 }$   
Jie Jiang2, Mingsheng Long1   
1School of Software, BNRist, Tsinghua University, China 2Tencent Inc, China   
fny21@mails.tsinghua.edu.cn,jonaspan@tencent.com,wujialong0229@gmail.com   
mingsheng@tsinghua.edu.cn

# ABSTRACT

Lifelong user behavior sequences are crucial for capturing user interests and predicting user responses in modern recommendation systems. A two-stage paradigm is typically adopted to handle these long sequences: a subset of relevant behaviors is first searched from the original long sequences via an attention mechanism in the first stage and then aggregated with the target item to construct a discriminative representation for prediction in the second stage. In this work, we identify and characterize, for the first time, a neglected deficiency in existing long-sequence recommendation models: a single set of embeddings struggles with learning both attention and representation, leading to interference between these two processes. Initial attempts to address this issue with some common methods (e.g., linear projections—a technique borrowed from language processing) proved ineffective, shedding light on the unique challenges of recommendation models. To overcome this, we propose the Decoupled Attention and Representation Embeddings (DARE) model, where two distinct embedding tables are initialized and learned separately to fully decouple attention and representation. Extensive experiments and analysis demonstrate that DARE provides more accurate searches of correlated behaviors and outperforms baselines with AUC gains up to $9 \text{‰}$ on public datasets and notable improvements on Tencent's advertising platform. Furthermore, decoupling embedding spaces allows us to reduce the attention embedding dimension and accelerate the search procedure by $50 \%$ without significant performance impact, enabling more efficient, high-performance online serving. Code in PyTorch for experiments, including model analysis, is available athttps://github.com/thuml/DARE.

# 1 INTRODUCTION

In recommendation systems, content providers must deliver well-suited items to diverse users. To enhance user engagement, the provided items should align with user interests, as evidenced by their clicking behaviors. Thus, the Click-Through Rate (CTR) prediction for target items has become a fundamental task. Accurate predictions rely heavily on effectively capturing user interests as reflected in their history behaviors. Previous research has shown that longer user histories facilitate more accurate predictions (Pi et al., 2020). Consequently, long-sequence recommendation models have attracted significant research interest in recent years (Chen et al., 2021; Cao et al., 2022). In online services, system response delays can severely disrupt the user experience, making efficient handling of long sequences within a limited time crucial. A general paradigm employs a two-stage process (Pi et al., 2020): search (a.k.a. General Search Unit) and sequence modeling (a.k.a. Exact Search Unit). This method relies on two core modules: the attention module1, which measures the target-behavior correlation, and the representation module, which generates discriminative representations of behaviors. The search stage uses the attention module to retrieve top-k relevant behaviors, constructing a shorter sub-sequence from the original long behavior sequence2. The sequence modeling stage relies on both modules to predict user responses by aggregating behavior representations in the sub-sequence based on their attention, thus extracting a discriminative representation. Exising works widely adopt this paradigm (Pi et al., 2020; Chang et al., 2023; Si et al., 2024).

![](images/1.jpg)  

Figure 1: Overview of our work. During search, only a limited number of important behaviors are retrieved according to their attention scores. During sequence modeling, the selected behaviors are aggregated into a discriminative representation for prediction. Our DARE model decouples the embeddings used in attention calculation and representation aggregation, effectively resolving their conflict and leading to improved performance and faster inference speed.

Attention is critical in the long-sequence recommendation, as it not only models the importance of each behavior for sequence modeling but, more importantly, determines which behaviors are selected in the search stage. However, in most existing works, the attention and representation modules share the same embeddings despite serving distinct functions—one learning correlation scores, the other learning discriminative representations. We analyze these two modules, for the first time, in the perspective of Multi-Task Learning (MTL). (Caruana, 1997). Adopting gradient analysis commonly used in MTL (Yu et al., 2020; Liu et al., 2021), we reveal that, unfortunately, gradients of these shared embeddings are dominated by representation, and more concerning, gradient directions from two modules tend to conflict with each other. Domination and conflict of gradients are two typical phenomena of interference between tasks, influencing the model's performance on both tasks. Our experimental results are consistent with the theoretical insight: attention fails to capture behavior importance accurately, causing key behaviors to be mistakenly filtered out during the search stage (as shown in Sec. 4.3). Furthermore, gradient conflicts also degrade the discriminability of the representations (as shown in Sec. 4.4). Inspired by the use of separate query, key (for attention), and value (for representation) projection matrices in the original self-attention mechanism (Vaswani et al., 2017), we experimented with attention- and representation-specific projections in recommendation models, aiming to resolve conflicts between these two modules. However, this approach did not yield positive results. We also tried three other kinds of candidate methods, but unfortunately, none of them worked effectively. Through insightful empirical analysis, we hypothesize that the failure is due to the significantly lower capacity (i.e., fewer parameters) of the projection matrices in recommendation models compared to those in natural language processing (NLP). This limitation is diffcult to overcome, as it stems from the low embedding dimension imposed by interaction collapse theory (Guo et al., 2024). To address these issues, we propose the Decoupled Attention and Representation Embeddings (DARE) model, which completely decouples these two modules at the embedding level by using two independent embedding tables—one for attention and the other for representation. This decoupling allows us to fully optimize attention to capture correlation and representation to enhance discriminability. Furthermore, by separating the embeddings, we can accelerate the search stage by $50 \%$ by reducing the attention embedding dimension to half, with minimal impact on performance. On the public Taobao and Tmall long-sequence datasets, DARE outperforms the state-of-the-art TWIN model across all embedding dimensions, achieving AUC improvements of up to $9 \text{‰}$ Online evaluation on Tencent's advertising platform, one of the world's largest platforms, achieves a $1 . 4 7 \%$ lift in GMV (Gross Merchandise Value). Our contribution can be summarized as follows: •We identify the issue of interference between attention and representation learning in existing long-sequence recommendation models and demonstrate that common methods (e.g., linear projections borrowed from NLP) fail to decouple these two modules effectively. •We propose the DARE model, which uses module-specific embeddings to fully decouple attention and representation. Our comprehensive analysis shows that our model significantly improves attention accuracy and representation discriminability. •Our model achieves state-of-the-art on two public datasets and gets a $1 . 4 7 \%$ GMV lift in one of the world's largest recommendation systems. Additionally, our method can largely accelerate the search stage by reducing decoupled attention embedding size.

# 2 An In-Depth Analysis Into ATtention and Representation

In this section, we first review the general formulation for long-sequence recommendation. Then, we analyze the training of shared embeddings, highlighting the domination and conflict of gradients from the attention and representation modules. Finally, we explore why straightforward approaches (e.g., using module-specific projection matrices) fail to address the issue.

# 2.1 PRELIMINARIES

Problem formulation. We consider the fundamental task, Click-Through Rate (CTR) prediction, which aims to predict whether a user will click a specific target item based on the user's behavior history. This is typically formulated as binary classification, learning a predictor $f : \mathcal { X } \mapsto [ 0 , 1 ]$ given a training dataset $\mathcal { D } = \{ ( \mathbf { x } _ { 1 } , y _ { 1 } ) , \dotsc , ( \mathbf { x } _ { | \mathcal { D } | } , y _ { | \mathcal { D } | } ) \}$ , where $\mathbf { x }$ contains a sequence of items representing behavior history and another single item representing the target.

Long-sequence recommendation model. To satisfy the strictly limited inference time in online services, current long-sequence recommendation models generally construct a short sequence first by retrieving top- $\mathbf { \nabla } \cdot \mathbf { k }$ correlated behaviors. The attention scores are measured by the scaled dot product of behavior and target embedding. Formally, the $i$ -th history behavior and target $t$ is embedded into $e _ { i }$ and $\boldsymbol { v } _ { t } \in \mathbb { R } ^ { \breve { d } }$ , and without loss of generality, $1 , 2 , \dotsc , K = \mathrm { T o p - K } ( \langle e _ { i } , { \pmb v } _ { t } \rangle , i \in [ 1 , N ] )$ , where $\langle \cdot , \cdot \rangle$ stands for dot product. Then the weight of each behavior $w _ { i }$ is calculated using softmax function: $\begin{array} { r } { w _ { i } = \frac { e ^ { \langle { \pmb { e } _ { i } } , { \pmb { v } _ { t } } \rangle / \sqrt { d } } } { \sum _ { j = 1 } ^ { K } e ^ { \langle { \pmb { e } _ { j } } , { \pmb { v } _ { t } } \rangle / \sqrt { d } } } } \end{array}$   
into performance through exquisite industrial optimization.

# 2.2 GRadient Analysis oF Domination and Conflict

The attention and representation modules can be seen as two tasks: the former focuses on learning correlation scores for behaviors, while the latter focuses on learning discriminative (i.e., separable) representations in a high-dimensional space. However, current methods use a shared embedding for both tasks, which may cause a similar phenomenon to "task conflict" in Multi-Task Learning (MTL) (Yu et al., 2020; Liu et al., 2021) and prevent either from being fully achieved. To validate this assumption, we analyze the gradients from both modules on the shared embeddings. Experimental validation. Following the methods in MTL, we empirically observe the gradients back propagated to the embeddings from the attention and representation modules. Comparing their gradient norms, we find that gradients from the representation are five times larger, dominating those from attention, as demonstrated in Fig. 2. Observing their gradient directions, we further find that in nearly two-thirds of cases, the cosine of the gradient angles is negative, indicating the conflict between them, as shown in Fig. 3. Domination and conflict are two typical phenomena of task interference, suggesting challenges in learning them well.

![](images/2.jpg)  

Figure 2: The magnitude of embedding gradients from the attention and representation modules.

In summary, the attention module and representation modules optimize the embedding table towards different directions with varying intensities during training, causing attention to lose correlation accuracy and representation to lose its discriminability. Notably, due to domination, such influence is more severe to attention, as indicated by the poor learned correlation between categories in Sec. 4.3. While some commonly used techniques in MTL may ease the conflict, we tend to seek an optimized model structure that further resolves the conflict.

![](images/3.jpg)  

Figure 3: Cosine angles of gradients.

Finding 1. In sequential recommenders, gradients from the representation module tend to conflict with that from the attention module, and typically dominate the embedding gradients.

![](images/4.jpg)  

Figure 4: Illustration and evaluation for adopting linear projections. (a-b) The attention module in the original TWIN and after adopting linear projections. (c) Performance of TWIN variants. Adopting linear projections causes an AUC drop of nearly $2 \%$ on Taobao.

# 2.3 Recommendation Models CalL for More Powerful Decoupling Methods

Normal decoupling methods fail to resolve conflicts. To address such conflict, a straightforward approach is to use separate projections for attention and representation, mapping the original embeddings into two new decoupled spaces. This is adopted in the standard self-attention mechanism (Vaswani et al., 2017), which introduces query, key (for attention), and value projection matrices (for representation). Inspired by this, we propose a variant of TWIN that utilizes linear projections to decouple attention and representation modules, named TWIN (w/ proj.). The comparison with the original TWIN structure is shown in Fig. 4a and 4b. Surprisingly, linear projection, which works well in NLP, loses efficacy in recommendation systems, leading to negative performance impact, as shown in Tab. 4c. We also tried three kinds of other candidate methods (MLP-based projection, strengthening the capacity of linear projection, and gradient normalization), resulting in a total of eight models, but none of them resolved the conflict effectively. For the structure of these models and more details, refer to Appendix C.

Larger embedding dimension makes linear projection effective in NLP. The failure of introducing projection matrices makes us wonder why it works well in NLP but not in recommendation. One possible reason is that the relative capacity of projection matrices regarding the token numbers in NLP is usually strong, e.g., with an embedding dimension of 4096 in LLaMA3.1 (Dubey et al., 2024), there are around 16 million parameters $( 4 0 9 6 \times 4 0 9 6 = 1 6 , 7 7 7 , 2 1 6 )$ in each projection matrix to map only 128,000 tokens in the vocabulary. To validate our hypothesis, we conduct a synthetic experiment in NLP using nanoGPT (Andrej) with the Shakespeare dataset. In particular, we decrease its embedding dimension from 128 to 2 and check the performance gap between the two models with/without projection matrices. As shown in Fig. 5, we observe that when the matrix has enough capacity, i.e., the embedding dimension is larger than 16, projection leads to significantly less loss. However, when the matrix capacity is further reduced, the gap vanishes. Our experiment indicates that using projection matrices only works with enough capacity.

![](images/5.jpg)  

Figure 5: The influence of linear projections with different embedding dimensions in NLP.

![](images/6.jpg)  

Figure 6: Architecture of the proposed DARE model. One embedding is responsible for attention, learning the correlation between the target and history behaviors, while another embedding is responsible for representation, learning discriminative representations for prediction. Decoupling these two embeddings allows us to resolve the conflict between the two modules.

Limited embedding dimension makes linear projections fail in recommendation. In contrast, due to the interaction collapse theory (Guo et al., 2024), the embedding dimension in recommendation is usually no larger than 200, leading to only up to 40000 parameters for each matrix to map millions to billions of IDs. Therefore, the projection matrices in recommendation never get enough capacity, making them unable to decouple attention and representation. In this case, other normal decoupling methods mentioned in Appendix C also suffer from weak capacity. Finding 2. Normal methods like linear projection fail to decouple attention and representation in sequential recommendation models due to limited capacity caused by embedding dimension.

# 3 DaRE: DEcoupLeD ATtEntion and REPreseNtation EmBEDdingS

With al eight normal decoupling models shown in Appendix C failed, based on our analysis, we seek methods with enough capacity, hoping to completely resolve the conflict. To this end, we propose to decouple these two modules at the embedding level. That is, we employ two embedding tables, one for attention $( E ^ { \mathrm { A t t } } )$ and another for representation $( E ^ { \mathrm { R e p r } } )$ With gradient back propagated to different embedding tables, our method has the potential to fully resolve the gradient domination and conflict between these two modules. We introduce our model specifically in this section and demonstrate its advantage by experiments in the next section.

# 3.1 ATTENTIoN EmBEDDiNG

Attention measures the correlation between history behaviors and the target (Zhou et al., 2018). Following the common practice, we use the scaled dot-product function (Vaswani et al., 2017). Mathematically, the $i$ history behavior $i$ and target $t$ ae embeded into $e _ { i } ^ { \mathrm { A t t } } , v _ { t } ^ { \mathrm { A t t } } \sim E ^ { \mathrm { A t t } }$ where $E ^ { \mathrm { A t t } }$ is the attention embedding table. After retrieval $1 , 2 , \ldots , K = \mathrm { T o p } { \mathrm { - } } \dot { \mathrm { K } } ( \langle e _ { i } , { \pmb v } _ { t } \rangle , i \in [ 1 , N ] )$ their weight $w _ { i }$ is formalized as:

$$
w _ { i } = \frac { { e ^ { \langle { { e _ { i } ^ { \mathrm { A t t } } } , { \bf { v } } _ { t } ^ { \mathrm { A t t } } } \rangle / \sqrt { \lvert { \cal E } ^ { \mathrm { A t t } } } \rvert } } } { { \sum _ { j = 1 } ^ { K } { e ^ { \langle { { e _ { j } ^ { \mathrm { A t t } } } , { \bf { v } } _ { t } ^ { \mathrm { A t t } } } \rangle / \sqrt { \lvert { \cal E } ^ { \mathrm { A t t } } } \rvert } } } } ,
$$

where $\langle \cdot , \cdot \rangle$ stands for dot product and $| E ^ { \mathrm { A t t } } |$ stands for the embedding dimension.

# 3.2 RePresentation EmBEDding

In the representation part, another embedding table $E ^ { \mathrm { R e p r } }$ is used, where $i$ and $t$ is embedded into x of each retrieved behavior and then concatenate it with the embedding of the target as the input of Multi-Layer Perceptron (MLP): $\textstyle \left[ \sum _ { i } w _ { i } e _ { i } , { \boldsymbol { v } } _ { t } \right]$ .However, it has been proved that MLP struggles to effectively learn explicit interactions (Rendle et al., 2020; Zhai et al., 2023). To enhance the discrim$e _ { i } ^ { \mathrm { R e p r } } \odot v _ { t } ^ { \mathrm { R e p r } }$ , denoted as TR in our following paper (refer to Sec. 4.4 for empirical evaluation of discriminability). The overall structure of our model is shown in Fig. 6. Formally, user history $h$ is compressed into: $\begin{array} { r } { \pmb { h } = \sum _ { i = 1 } ^ { K } w _ { i } \cdot ( \pmb { e } _ { i } ^ { \mathrm { R e p r } } \odot \pmb { v } _ { t } ^ { \mathrm { R e p r } } ) } \end{array}$

# 3.3 INFERENCE ACCELERATION

By decoupling the attention and representation embedding tables, the dimension of attention embeddings $E ^ { \mathrm { A t t } }$ and the dimension of representation embeddings $E ^ { \mathrm { R e p r } }$ have more flexibility. In particular, we can reduce $E ^ { \mathrm { A t t } }$ while keeping $E ^ { \mathrm { R e p r } }$ to acelerate he searching over the riginal lon sequence whilst not affecting the model's performance. Empirical experiments in Sec. 4.5 show that our model has the potential to speed up searching by $50 \%$ with quite little influence on performance and even by $7 5 \%$ with an acceptable performance loss.

# 3.4 Discussion

Considering the superiority of decoupling the attention and representation embeddings, one may naturally raise an idea: we can further decouple the embeddings of history and target within the attention (and representation) module, i.e. forming a TWIN with 4 Embeddings method, or TWIN-4E in short, consisting of attention-history (named keys in NLP) $e _ { i } ^ { \mathrm { A t t } } \ \in$ $E ^ { \mathrm { A t t - h } }$ , atteio-target (name querys in NLP) $\pmb { v } _ { t } ^ { \mathrm { A t t } } \in \pmb { E } ^ { \mathrm { A t t - t } }$ , rentatin-history named values  NLP) $e _ { i } ^ { \mathrm { R e p r } } \in E ^ { \mathrm { R e p r - h } }$ and representationtarget vRepr $v _ { t } ^ { \mathrm { R e p r } } \in E ^ { \mathrm { R e p r - t } }$ . The structure of TWIN-4E is shown in Fig. 7. Compared to our DARE model, TWIN-4E further decouples the behaviors and the target, meaning that the same category or item has two totally independent embeddings as behavior and target. This is strongly against two prior knowledge in recommendation system. 1. The correlation of two behaviors is similar no matter which is the target and which is from history. 2. Behaviors with the same category should be more correlated, which is natural in DARE since a vector's dot product with itself tends to be bigger.

![](images/7.jpg)  

Figure 7: Illustration of the TWIN-4E model.

# 4 EXPERIMENTS

# 4.1 SETUP

Datasets and task. We use the publicly available Taobao (Zhu et al., 2018; 2019; Zhuo et al., 2020) and Tmall (Tianchi, 2018) datasets, which provide users' behavior data over specific time periods on their platforms. Each dataset includes the items users clicked, represented by item IDs and their corresponding category IDs. Thus, a user's history is modeled as a sequence of item and category IDs. The model's input consists of a recent, continuous sub-sequence of the user's lifelong history, along with a target item. For positive samples, the target items are the actual items users clicked next, and the model is expected to output "Yes." For negative samples, the target items are randomly sampled, and the model should output "No." In addition to these public datasets, we validated our performance on one of the world's largest online advertising platforms. More details on datasets and training/validation/test splits are shown in Appendix B.

Baselines. We compare against a variety of recommendation models, including ETA (Chen et al., 2021), SDIM (Cao et al., 2022), DIN (Zhou et al., 2018), TWIN (Chang et al., 2023) and its variants, as well as TWIN-V2 (Si et al., 2024). As discussed in Sec.3.2, the target-aware representation by crossing e $e _ { i } ^ { \mathrm { R e p r } } \odot v _ { t } ^ { \mathrm { R e p r } }$ significantly improves representation discriminability, so we include it in our baselines for fairness. TWIN-4E refers to the model introduced in Sec. 3.4, while TWIN (w/ proj.) refers to the model described in Sec. 2.3. TWIN (hard) represents a variant using "hard search" in the search stage, meaning it only retrieves behaviors from the same category as the target. TWIN (w/o TR) refers to the original TWIN model without target-aware representation, i.e., representing user history as $\begin{array} { r } { \pmb { h } = \sum _ { i } { w _ { i } } \cdot \pmb { e } _ { i } } \end{array}$ instead of $\begin{array} { r } { \pmb { h } = \sum _ { i } w _ { i } ( \pmb { e } _ { i } \hat { \odot } \pmb { v } _ { t } ) } \end{array}$ .

Table 1: Overall comparison reported by the means and standard deviations of AUC. The best results are highlighted in bold, while the previous best model is underlined. Our model outperforms all existing methods with obvious advantages, especially with small embedding dimensions.   

<table><tr><td>Setup</td><td colspan="2">Embedding Dim. = 16</td><td colspan="2">Embedding Dim. = 64</td><td colspan="2">Embedding Dim. = 128</td></tr><tr><td>Dataset</td><td>Taobao</td><td>Tmall</td><td>Taobao</td><td>Tmall</td><td>Taobao</td><td>Tmall</td></tr><tr><td>ETA (2021)</td><td>0.91326 (0.00338)</td><td>0.95744</td><td>0.92300 0.00079)</td><td>0.96658 (0.00042)</td><td>0.92480 (0.00032)</td><td>0.96956 (0.00039)</td></tr><tr><td>SDIM (2022)</td><td>0.90430</td><td>0.00108) 0.93516</td><td>0.90854</td><td>0.94110</td><td>0.91108</td><td>0.94298</td></tr><tr><td>DIN (2018)</td><td>(0.0103) 0.90442</td><td>(0.00069) 0.95894</td><td>(0.00085) 0.90912</td><td>(0.00093) 0.96194</td><td>(0.00119) 0.91078</td><td>(0.00081) 0.96428</td></tr><tr><td>TWIN (2023)</td><td>(0.00060) 0.91688</td><td>0.0037) 0.95812</td><td>(0.00092) 0.92636</td><td>(0.00033) 0.96684</td><td>(0.00054) 0.93116</td><td>(0.00013) 0.97060</td></tr><tr><td>TWIN (hard)</td><td>(0.00211) 0.91002</td><td>(0.00073) 0.96026</td><td>(0.00052) 0.91984</td><td>(0.00039) 0.96448</td><td>(0.00056) 0.91446</td><td>(0.00005) 0.96712</td></tr><tr><td>TWIN (w/ proj.)</td><td>(0.00053) 0.89642</td><td>(0.00024) 0.96152</td><td>(0.00048) 0.87176</td><td>(0.00042) 0.95570</td><td>(0.00055) 0.87990</td><td>(0.00019) 0.95724</td></tr><tr><td>TWIN (w/o TR)</td><td>(0.00351) 0.90732</td><td>(0.00088) 0.96170</td><td>(0.00437) 0.91590</td><td>(0.00403) 0.96320</td><td>(0.02022) 0.92060</td><td>(0.00194) 0.96366</td></tr><tr><td>TWIN-V2 (2024)</td><td>(0.00063) 0.89434</td><td>(0.00057) 0.94714</td><td>(0.00083) 0.90170</td><td>(0.00032) 0.95378</td><td>(0.00084) 0.90586</td><td>(0.00103) 0.95732</td></tr><tr><td>TWIN-4E</td><td>(0.00077) 0.90414</td><td>(0.00110) 0.96124</td><td>(0.00063) 0.90356</td><td>(0.00037) 0.96372</td><td>(0.00059) 0.90946</td><td>(0.00045) 0.96016</td></tr><tr><td>DARE (Ours)</td><td>(0.01329) 0.92568 (0.00025)</td><td>(0.00026) 0.96800 (0.00024)</td><td>(0.01505) 0.92992 (0.00046)</td><td>(0.0004) 0.97074 (0.00012)</td><td>(0.01508) 0.93242 (0.00045)</td><td>(0.01048) 0.97254 (0.00016)</td></tr></table>

# 4.2 OVERALL PERFoRMaNCE

In recommendation systems, it is well-recognized that even increasing AUC by $1 \text{‰}$ to $2 \text{‰}$ is more than enough to bring online profit. As shown in Tab. 1, our model achieves AUC improvements of $1 \text{‰}$ and $9 \text{‰}$ compared to current state-of-the-art methods across all settings with various embedding sizes. In particular, significant AUC lifts of $9 \text{‰}$ and $6 \text{‰}$ are witnessed with an embedding dimension of 16 on Taobao and Tmall datasets, respectively. There are also some notable findings. TWIN outperforms TWIN (w/o TR) in most cases, proving tha tarewa eion $e _ { i } ^ { \mathrm { R e p r } } \odot v _ { t } ^ { \mathrm { R e p r } }$ is shown in Sec. 4.4). Our DARÉ model has an obvious advantage over TWIN-4E, confirming that the prior knowledge discussed in Sec. 3.4 is well-suited for the recommendation system. ETA and SDM, which are based on TWIN and focus on accelerating the search stage at the expense of performance, understandably show lower AUC scores. TWIN-V2, a domain-specific method optimized for video recommendations, is less effective in our settings.

# 4.3 ATTENTION ACCURACY

Mutual information, which captures the shared information between two variables, is a powerful tool for understanding relationships in data. We calculate the mutual information between behaviors and the target as the ground truth correlation, following (Zhou et al., 2024). The learned attention score reflects the model's measurement of the importance of each behavior. Therefore, we compare the attention distribution with mutual information in Fig. 8. In particular, Fig. 8a presents the mutual information between a target category and behaviors with top-10 categories and their target-relative positions (i.e., how close to the target is the behavior across time). We observe a strong semantic-temporal correlation: behaviors from the same category as the target (5th row) are generally more correlated, with a noticeable temporal decay pattern. Fig. 8b presents TWIN's learned attention scores, which show a decent temporal decay pattern but overestimate the semantic correlation of behaviors across different categories, making it too sensitive to recent behaviors, even those from unrelated categories. In contrast, our proposed DARE can effectively capture both the temporal decaying and semantic patterns. The retrieval in the search stage relies entirely on attention scores. Thus, we further investigate the retrieval on the test dataset, which provides a more intuitive reflection of attention quality. Behaviors with top-k mutual information are considered the optimal retrieval, and we evaluate model performance using normalized discounted cumulative gain (NDCG) (Järvelin & Kekäläinen, 2002). The results, along with case studies, are presented in Fig. 9 (more examples in Appendix E.4). We find:

![](images/8.jpg)  

Figure 8: The ground truth (GT) and learned correlation between history behaviors of top-10 frequent categories (y-axis) at various positions ( $\mathbf { \dot { X } }$ -axis), with category 15 as the target. Our correlation scores are noticeably closer to the ground truth.

![](images/9.jpg)  

Figure 9: Retrieval in the search stage. (a) Our model can retrieve more correlated behaviors. (b-c) Two showcases where the $\mathbf { X }$ -axis is the categories of the recent ten behaviors.

• DARE achieves significantly better retrieval. As shown in Fig. 9a, the NDCG of our model is substantially higher than all baselines, with a $4 6 . 5 \%$ increase (0.8124 vs. 0.5545) compared to TWIN and a $2 7 . 3 \%$ increase (0.8124 vs. 0.6382) compared to DIN. •TWIN is overly sensitive to temporal information. As discussed, TWIN tends to select recent behaviors regardless of their categories, against the ground truth, due to overestimated correlations between different categories, as shown in Fig. 9b and 9c. •Other methods perform unstably. For the other methods, they filter out some important behaviors and retrieve unrelated ones in many cases, which explains their bad performance. Result 1. DARE succeeds in capturing the semantic-temporal correlation between behaviors and the target, retaining more correlated behaviors during the search stage.

# 4.4 REPREsENTATIoN DISCRIMinABiLITY

We then analyze the discriminability of learned representation. On test datasets, we take the com$\begin{array} { r } { \pmb { h } = \sum _ { i = 1 } ^ { K } \pmb { w } _ { i } \cdot \left( \pmb { e } _ { i } \odot \pmb { v } _ { t } \right) } \end{array}$ sample. Using K-means, we quantize these vectors, mapping each to a cluster $Q ( h )$ . The mutual information (MI) between the discrete variable $Q ( h )$ and label $Y$ (whether the target was clicked) can then reflect the representation's discriminability: Discriminability $( h , Y ) = { \bf M I } \bar { ( } Q ( h ) , Y )$ . As shown in Fig. 10a, across various numbers of clusters, our DARE model outperforms the state-ofthe-art TWIN model, demonstrating that decoupling improves representation discriminability. There are also other notable findings. Although DIN achieves more accurate retrieval in the search stage (as evidenced by a higher NDCG in Fig. 9a), its representation discriminability is obviously lower than TWIN, especially on Taobao dataset, which explains its lower overall performance. TWIN-4E shows comparable discriminability to our DARE model, further confirming that its poorer performance is due to inaccurate attention caused by the lack of recommendation-specific prior knowledge. To fully demonstrate the effectiveness of $e _ { i } \odot v _ { t }$ , we compare it with the classical concatenation $[ \Sigma _ { i } . e _ { i } , \dot { \mathbf { v } } _ { t } ]$ . As shown in Fig. 10c, a huge gap (in orange) is caused by the target-aware representation, while smaller gaps (in blue and green) result from decoupling. Notably, our DARE model also outperforms TWIN even when using concatenation.

![](images/10.jpg)  

Figure 10: Representation discriminability of different models, measured by the mutual information between the quantized representations and labels.

![](images/11.jpg)  

Figure 11: Efficiency during training and inference. (a-b) Our model performs obviously better with fewer training data. (c-d) Reducing the search embedding dimension, a key factor of online inference speed, has little influence on our model, while TWIN suffers an obvious performance loss.

Result 2. In the DARE model, the form of target-aware representation and embedding decou pling both improve the discriminability of representation significantly.

# 4.5 COnvergence and EfFiciency

Faster convergence during training. In recommendation systems, faster learning speed means the model can achieve strong performance with less training data, which is especially crucial for online services. We track accuracy on the validation dataset during training, shown in Fig. 11a. Our DARE model converges significantly faster. For example, on the Tmall dataset, TWIN reaches $90 \%$ accuracy after more than 1300 iterations. In contrast, our DARE model achieves comparable performance in only about 450 iterations—one-third of the time required by TWIN.

Efficient search during inference. By decoupling the attention embedding space $e _ { i } , v _ { t } \in \mathbb { R } ^ { K _ { A } }$ and representation embedding space $e _ { i } , \mathbf { \boldsymbol { v } } _ { t } \in \dot { \mathbb { R } } ^ { K _ { R } }$ , we can assign different dimensions for these two spaces. Empirically, we find that the attention module performs comparably well with smaller embedding dimensions, allowing us to reduce the size of the attention space $( K _ { A } ~ \ll ~ K _ { R } )$ and significantly accelerate the search stage, as its complexity is $O ( K _ { A } N )$ where $N$ is the length of the user history. Using $K _ { A } \ = \ 1 2 8$ as a baseline ("1"), we normalize the complexity of smaller embedding dimensions. Fig. 11c shows that our model can accelerate the searching speed by $50 \%$ with quite little influence on performance and even by $7 5 \%$ with an acceptable performance loss, offering more flexible options for practical use. In contrast, TWIN experiences a significant AUC drop when reducing the embedding dimension. Result 3. Embedding decoupling leads to faster model training convergence and at least $50 \%$ inference acceleration without significantly influencing the AUC by reducing the dimension of attention embeddings.

# 4.6 ONLinE A/B TEsting AND DEpLoyMENTS

We apply our methods to Tencent's advertising platform. Since users' behaviors on ads are sparse, which makes the sequence length relatively shorter than the content recommendation scenario, we involve the user's behavior sequence from our article and the micro-video recommendation scenario. Specifically, the user's ad and content behaviors in the last two years are introduced. Before the search, the maximal length of the ads and content sequence is 4000 and 6000, respectively, with 170 and 1500 on average. After searching with DARE, the sequence length is reduced to less than 500. Regarding sequence features (side info), we choose the category ID, behavior type ID, scenario ID, and two target-aware temporal encodings, i.e., position relative to the target, and time interval relative to the target (with discretization). There are about 1.0 billion training samples per day. During the 5-day online A/B test in September 2024, the proposed DARE method achieves $0 . 5 7 \%$ cost, and $1 . 4 7 \%$ GMV (Gross Merchandize Value) lift over the production baseline of TWIN. This would lead to hundreds of millions of dollars in revenue lift per year.

# 4.7 SUPpLEMENTARY EXPERIMENT RESULTS IN APPENDIX

Retrieval number in the search stage. DARE's advantage is more obvious with less retrieval number, proving once again that DARE selects important behaviors more accurately (Appendix D.1). Sequence length and short-sequence modeling. DARE can consistently benefit from longer sequences, while it delivers marginal advantages in short-sequence modeling (Appendix D.2). GAUC and Logloss. Besides AUC, we also evaluate DARE and all the baselines under GAUC and Logloss. DARE shows consistent superiority, proving the solidity of our results (Appendix E.1).

# 5 RELATED WORK

Click-through rate prediction and long-sequence modeling. CTR prediction is fundamental in recommendation systems, as user interest is often reflected in their clicking behaviors. Deep Interest Network (DIN) (Zhou et al., 2018) introduces target-aware attention, using an MLP to learn attentive weights of each history behavior regarding a specific target. This framework has been extended by models like DIEN (Zhou et al., 2019), DSIN (Feng et al., 2019), and BST (Chen et al., 2019) to capture user interests better. Research has proved that longer user histories lead to more accurate predictions, bringing long-sequence modeling under the spotlight. SIM (Pi et al., 2020) introduces a search stage (GSU), greatly accelerating the sequence modeling stage (ESU). Models like ETA (Chen et al., 2021) and SDIM (Cao et al., 2022) further improve this framework. Notably, TWIN (Chang et al., 2023) and TWIN-V2 (Si et al., 2024) unify the target-aware attention metrics used in both stages, significantly improving search quality. However, as pointed out in Sec. 2.2, in all these methods, attention learning is often dominated by representation learning, creating a significant gap between the learned and actual behavior correlations. Attention. The attention mechanism, most well-known in Transformers (Vaswani et al., 2017), has proven highly effective and is widely used for correlation measurement. Transformers employ Q, K (attention projection), and V (representation projection) matrices to generate queries, keys, and values for each item. The scaled dot product of query and key serves as the correlation score, while the value serves as the representation. This structure is widely used in many domains, including natural language processing (Brown et al., 2020) and computer vision (Dosovitskiy et al., 2021). However, in recommendation systems, due to the interaction-collapse theory pointed out by Guo et al. (2024), the small embedding dimension would make linear projections completely lose effectiveness, as discussed in Sec. 2.3. Thus, proper adjustment is needed in this specific domain.

# 6 CONCLUSION

This paper focuses on long-sequence recommendation, starting with an analysis of gradient domination and conflict on the embeddings. We then propose a novel Decoupled Attention and Representation Embeddings (DARE) model, which fully decouples attention and representation using separate embedding tables. Both offline and online experiments demonstrate DARE's potential, with comprehensive analysis highlighting its advantages in attention accuracy, representation discriminability, and faster inference speed.

# REPRODUCIBILITY STATEMENT

To ensure reproducibility, we provide the hyperparameters and baseline implementation details in Appendix A, along with dataset details in Appendix B. We have released the full code, including dataset processing, model training, and analysis experiments, at https : / /github. com/ thuml/DARE.

# ACKNOWLEDGEMENTS

This work was supported by the National Natural Science Foundation of China (62021002), the BNRist Project, the Tencent Innovation Fund, and the National Engineering Research Center for Big Data Software.