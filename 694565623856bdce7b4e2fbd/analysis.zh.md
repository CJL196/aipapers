# 1. 论文基本信息

## 1.1. 标题
**Deformable 3D Gaussians for High-Fidelity Monocular Dynamic Scene Reconstruction**

中文翻译：**用于高保真单目动态场景重建的可变形 3D 高斯**

论文标题清晰地揭示了其核心内容：
*   **核心技术:** `Deformable 3D Gaussians` (可变形 3D 高斯)，表明本文是对 `3D Gaussian Splatting` 这一前沿技术进行扩展，使其能够处理形变。
*   **核心任务:** `High-Fidelity Monocular Dynamic Scene Reconstruction` (高保真单目动态场景重建)，指出了研究的具体问题域——从单个移动的摄像机（单目）拍摄的视频中，重建出随时间变化的场景（动态场景），并追求极高的视觉真实感（高保真）。

## 1.2. 作者
*   **作者列表:** Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing Zhang, Xiaogang Jin。
*   **隶属机构:**
    *   浙江大学 CAD&CG 国家重点实验室 (State Key Laboratory of CAD&CG, Zhejiang University)
    *   字节跳动公司 (ByteDance Inc.)
*   **背景分析:** 作者团队来自顶尖学术机构和业界巨头，这种产学研结合的研究背景通常意味着研究工作既有学术深度，又紧密结合实际应用需求。

## 1.3. 发表期刊/会议
本文是一篇预印本 (Preprint) 论文，发表于 **arXiv**。arXiv 是一个开放获取的学术论文预印本库，允许研究者在正式同行评审之前分享他们的研究成果。这使得最新的研究能够被快速传播。

## 1.4. 发表年份
2023年9月22日。

## 1.5. 摘要
隐式神经表示（如 NeRF）为动态场景的重建与渲染开辟了新道路，但目前最先进的方法在捕捉场景细节方面仍有困难，并且难以实现实时渲染。为了解决这些问题，本文提出了一种<strong>可变形 3D 高斯泼洒 (Deformable 3D Gaussian Splatting)</strong> 方法。该方法在<strong>规范空间 (canonical space)</strong> 中学习一组 3D 高斯基元，并利用一个<strong>形变场 (deformation field)</strong> 来为单目动态场景建模。此外，作者还引入了一种无额外开销的<strong>退火平滑训练机制 (annealing smoothing training mechanism)</strong>，以减轻真实世界数据集中不精确相机位姿对时间插值任务平滑性的影响。通过可微分的高斯光栅化器，该方法不仅实现了更高的渲染质量，还达到了实时渲染速度。实验证明，该方法在渲染质量和速度上均显著优于现有方法。

## 1.6. 原文链接
*   **官方来源:** [https://arxiv.org/abs/2309.13101](https://arxiv.org/abs/2309.13101)
*   **PDF 链接:** [https://arxiv.org/pdf/2309.13101v2.pdf](https://arxiv.org/pdf/2309.13101v2.pdf)
*   **发布状态:** 预印本 (Preprint)。

# 2. 整体概括

## 2.1. 研究背景与动机
*   **核心问题:** 如何从单目视频中高质量、高效率地重建动态场景？
*   **重要性:** 高质量、实时的动态场景三维重建是增强现实/虚拟现实 (AR/VR)、3D 内容创作和娱乐等应用的关键技术。
*   <strong>现有挑战 (Gap):</strong>
    1.  **质量瓶颈:** 基于隐式神经表示（如 `NeRF`）的动态场景方法虽然取得了成功，但它们通常难以捕捉精细的几何和纹理细节，渲染结果容易出现模糊或伪影。
    2.  **效率瓶颈:** 这些隐式方法依赖于耗时的光线投射 (ray-casting) 和体积渲染 (volume rendering)，导致训练和渲染速度缓慢，难以满足实时应用的需求。
    3.  **静态方法的局限性:** 近期出现的 `3D Gaussian Splatting` (3D-GS) 在静态场景上实现了前所未有的实时渲染速度和高保真度，但其设计初衷是针对静态场景，其高度定制化的 CUDA 光栅化管线使其难以直接扩展到动态场景。
*   **本文切入点:** 既然 `3D-GS` 在静态场景上如此成功，能否将其强大的显式表示能力与动态场景建模的常用策略相结合？本文的创新思路是：**将 `3D-GS` 的显式高斯表示与 `D-NeRF` 等工作中的“规范空间+形变场”解耦思想相结合**。即，不在每个时间帧都重建一套全新的高斯点云，而是在一个标准的、静态的“规范空间”中学习一套 3D 高斯，然后用一个小型神经网络（形变场）来学习如何将这些高斯点在不同时间点 $t$ 移动和变形到正确的位置和形态。

## 2.2. 核心贡献/主要发现
本文的主要贡献可以总结为以下三点：
1.  **提出了一个用于单目动态场景建模的可变形 3D-GS 框架:** 这是首次将 `3D-GS` 通过形变场扩展到动态场景的工作。该框架通过在规范空间中学习 3D 高斯，并使用一个 MLP 预测其随时间的变化，成功地将 `3D-GS` 的高效率和高保真度带入了动态场景领域。
2.  <strong>设计了一种新颖的退火平滑训练 (AST) 机制:</strong> 针对真实世界数据中由 `Structure-from-Motion` (SfM) 算法估计的相机位姿不准确导致的时间抖动问题，AST 在训练初期向时间输入中添加随训练进程衰减的噪声。这既能保证时间上的平滑性，又能在训练后期保留动态细节，且不增加任何计算复杂度。
3.  **实现了顶尖的性能:** 实验结果表明，该方法在渲染质量（尤其在 `SSIM` 和 `LPIPS` 等感知指标上）和渲染速度（可达实时）方面均显著超越了当前最先进的动态场景神经渲染方法。

# 3. 预备知识与相关工作

## 3.1. 基础概念
*   <strong>神经辐射场 (Neural Radiance Fields, NeRF):</strong> `NeRF` 是一种用神经网络来表示 3D 场景的革命性技术。它使用一个多层感知机 (MLP) 来学习一个连续函数，这个函数将一个 5D 坐标（3D 空间位置 `(x, y, z)` 和 2D 观察方向 $(\theta, \phi)$）映射到该点的体积密度 (volume density) 和颜色 (color)。通过沿着从相机出发的每条光线进行密集采样，并使用经典的体积渲染方程将采样点的颜色和密度积分起来，就可以合成任意新视角的图像。`NeRF` 的渲染质量非常高，但因为每渲染一个像素都需要多次查询 MLP，所以速度非常慢。

*   <strong>3D 高斯泼洒 (3D Gaussian Splatting, 3D-GS):</strong> `3D-GS` 是一种与 `NeRF` 不同的场景表示方法。它不使用隐式的 MLP，而是用一堆显式的、可微的 3D 高斯分布来表示场景。每个 3D 高斯都具有以下属性：
    *   <strong>位置 (Position):</strong> $\pmb{x}$，高斯中心在 3D 空间中的坐标。
    *   <strong>协方差 (Covariance):</strong> $\Sigma$，一个 3x3 矩阵，描述了高斯的形状、大小和朝向。它通常由一个旋转（四元数 $\pmb{r}$）和一个缩放（向量 $\pmb{s}$）参数化得到。
    *   <strong>颜色 (Color):</strong> 通常用球谐函数 (Spherical Harmonics, SH) 表示，使其具有视图依赖性。
    *   <strong>不透明度 (Opacity):</strong> $\sigma$，控制高斯的透明度。
        渲染时，这些 3D 高斯被快速投影（或称为“泼洒”，Splatting）到 2D 图像平面上，形成 2D 高斯。然后，这些 2D 高斯按照深度排序，并使用 alpha-blending 技术从后到前混合起来，得到最终的像素颜色。由于整个过程（特别是光栅化）可以在 CUDA 中高度优化，`3D-GS` 实现了极高的渲染速度（实时）和与 `NeRF` 相当甚至更好的质量。

*   <strong>规范空间 (Canonical Space):</strong> 在计算机图形学和视觉中，这是一个非常重要的概念。对于一个动态或可变形的物体（如一个行走的人），规范空间指的是一个“标准”或“模板”的 3D 空间，物体在该空间中处于一个静止的、预定义的姿态（例如，T-pose）。然后，物体的运动或形变被建模为一个从规范空间到每个时间点 $t$ 的观察空间的映射函数。这种方法的好处是<strong>解耦 (decouple)</strong> 了物体的静态几何结构（在规范空间中定义）和其动态行为（由形变函数定义），使得学习和控制更加容易。

## 3.2. 前人工作
*   **动态 NeRF 方法:**
    *   <strong>纠缠式方法 (Entangled Methods):</strong> 如 `NeRF-T`，直接将时间戳 $t$ 作为 `NeRF` 网络的额外输入。这种方式将场景的几何、外观和运动全部耦合在同一个 MLP 中，难以学习复杂的运动，并且泛化能力较差。
    *   <strong>解耦式方法 (Disentangled Methods):</strong> 这是更主流的方法。代表作有 `D-NeRF`、`Nerfies` 和 `HyperNeRF`。它们引入一个独立的<strong>形变网络 (deformation network)</strong>。这个网络将观察空间中的点 `(x, y, z)` 和时间 $t$作为输入，输出该点在规范空间中的对应位置。然后，再用一个标准的 `NeRF` 网络在规范空间中查询颜色和密度。本文正是借鉴了这种解耦思想。

*   **NeRF 加速方法:**
    *   <strong>离散结构方法 (Grid-based Methods):</strong> 如 `Instant-NGP`、`TensoRF`、`K-Planes` 等，它们使用显式的体素网格 (voxel grids) 或平面来存储特征，并用小型 MLP 解码这些特征。这极大地减少了对大型 MLP 的依赖，从而加速了训练和渲染。然而，本文指出，这些方法通常基于低秩假设 (low-rank assumption)，而动态场景的“秩”更高，这可能限制了它们能达到的质量上限。
    *   <strong>显式表示方法 (Explicit Representation):</strong> `3D-GS` 是这一路线的杰出代表，它完全抛弃了 `NeRF` 的体积渲染范式，转向了基于点（高斯）的渲染，实现了速度和质量的双重突破。

## 3.3. 技术演进
该领域的技术演进路线大致如下：
1.  **静态场景:** 从慢而精的 `NeRF` -> 结合离散结构加速的 `Plenoxels`、`Instant-NGP` -> 快且精的 `3D-GS`。
2.  **动态场景:** 从基础的 `NeRF-T` -> 采用解耦思想的 `D-NeRF`、`Nerfies` -> 尝试用离散结构加速的 `TiNeuVox`、`HexPlane` -> **本文，将最快的静态场景表示 `3D-GS` 引入动态场景领域**。

## 3.4. 差异化分析
*   <strong>与动态 NeRF 方法 (如 D-NeRF) 的对比:</strong>
    *   **表示方式:** `D-NeRF` 使用隐式的 MLP 表示规范空间，而本文使用显式的 3D 高斯集合。
    *   **渲染方式:** `D-NeRF` 依赖于耗时的光线投射和体积渲染，而本文使用高效的可微高斯光栅化。这使得本文在速度上具有碾压性优势。
    *   **细节表现:** 显式的点云表示通常比隐式场更容易优化以捕捉高频细节。
*   **与静态 3D-GS 的对比:**
    *   **核心扩展:** 本文引入了规范空间和 MLP 形变场，使静态的 3D 高斯能够表示随时间变化的动态。
    *   **学习目标:** 静态 `3D-GS` 直接优化世界坐标系下的高斯参数。本文则优化规范空间下的高斯参数以及形变网络的参数。
*   **与其他动态加速方法的对比:**
    *   **与 `TiNeuVox`, `HexPlane` 等对比:** 这些方法依然在 `NeRF` 的体积渲染框架内，通过引入时空体素网格或特征平面来加速。而本文采用了完全不同的、更快的基于点光栅化的渲染管线。

# 4. 方法论

本文方法的核心是将动态场景的建模过程分解为两部分：一个在规范空间中静态存在的 3D 高斯集合，以及一个学习如何随时间移动和变形这些高斯的神经网络。

下图（原文 Figure 2）展示了该方法的整体流程：

![该图像是示意图，展示了可变形3D高斯用于动态场景重建的过程。图中包含了时间、初始化、3D高斯和图像生成等多个步骤，以及不同的流向与参数调整，体现了动态场景重建中的关键机制和流动关系。](images/2.jpg)
*该图像是示意图，展示了可变形3D高斯用于动态场景重建的过程。图中包含了时间、初始化、3D高斯和图像生成等多个步骤，以及不同的流向与参数调整，体现了动态场景重建中的关键机制和流动关系。*

## 4.1. 方法原理
整体流程如下：
1.  **初始化:** 使用 `Structure-from-Motion` (SfM) 工具（如 COLMAP）处理输入的单目视频帧，得到一个稀疏的点云和每帧的相机位姿。基于这个稀疏点云，初始化一组 3D 高斯。这些高斯被认为是存在于一个与时间无关的<strong>规范空间 (canonical space)</strong> 中。
2.  **形变:** 对于任意给定的时间 $t$ 和一个规范空间中的高斯（以其中心位置 $\pmb{x}$ 为代表），一个形变网络 $\mathcal{F}_{\theta}$ 会预测出该高斯在此刻的位移、旋转和缩放变化量。
3.  **渲染:** 将规范高斯根据预测的变化量进行变换，得到当前时刻 $t$ 的动态高斯。然后，使用 `3D-GS` 中高效的可微高斯光栅化器将这些动态高斯渲染成 2D 图像。
4.  **优化:** 计算渲染图像与真实图像之间的损失（如 L1 损失和 D-SSIM 损失），然后通过反向传播，**同时优化**规范空间中 3D 高斯的参数（位置、旋转、缩放、不透明度等）和形变网络 $\mathcal{F}_{\theta}$ 的权重。
5.  **自适应密度控制:** 在训练过程中，周期性地对高斯集合进行调整（增加、分裂、剪枝），以更好地拟合场景的几何细节。

## 4.2. 核心方法详解 (逐层深入)

### 4.2.1. 可微渲染与规范空间 (Differentiable Rendering in Canonical Space)
这部分继承自 `3D-GS`，但应用在了形变后的高斯上。

*   **3D 到 2D 投影:** 一个 3D 高斯由其协方差矩阵 $\Sigma$ 定义。当它被投影到 2D 图像平面时，其对应的 2D 协方差矩阵 $\Sigma'$ 计算如下：
    $$
    \Sigma ^ { \prime } = J V \Sigma V ^ { T } J ^ { T }
    $$
    *   $V$: 视图矩阵，将点从世界坐标系转换到相机坐标系。
    *   $J$: 投影变换的仿射近似的雅可比矩阵。
    *   $\Sigma$: 3D 高斯的协方差矩阵。

*   **协方差参数化:** 为了便于优化，3D 协方差矩阵 $\Sigma$ 被分解为两个可学习的组件：一个表示旋转的四元数 $\pmb{r}$ 和一个表示缩放的 3D 向量 $\pmb{s}$。它们分别被转换成旋转矩阵 $R$ 和缩放矩阵 $S$。
    $$
    \Sigma = R S S ^ { T } R ^ { T }
    $$

*   **颜色合成:** 渲染图像上某个像素 $\mathbf{p}$ 的最终颜色 $C(\mathbf{p})$ 是通过 alpha-blending 将所有覆盖该像素的、按深度排序的高斯颜色 $c_i$ 混合得到的：
    $$
    C ( { \bf p } ) = \sum _ { i \in N } T _ { i } \alpha _ { i } c _ { i }
    $$
    其中，$T_i = \Pi_{j=1}^{i-1}(1-\alpha_j)$ 是透射率 (transmittance)，表示光线穿过前面 `i-1` 个高斯后剩余的能量。$\alpha_i$ 是第 $i$ 个高斯对该像素的贡献度，由其不透明度 $\sigma_i$ 和 2D 高斯分布在该像素位置的密度值决定。**注意：** 原文提供的 $\alpha_i$ 公式 $\alpha_{i} = \sigma_{i} e ^ { - \frac 1 2 ( { \bf p } - { \boldsymbol \mu } _ { i } ) ^ { T } } \sum ^ { \prime } ( { \bf p } - { \boldsymbol \mu } _ { i } )$ 存在明显错误，应为高斯函数的标准形式。根据其 PDF 截图及高斯分布定义，正确的公式应为：
    $$
    \alpha_{i} = \sigma_{i} \exp\left( - \frac 1 2 ( { \bf p } - { \boldsymbol \mu } _ { i } ) ^ { T } (\Sigma')^{-1} ( { \bf p } - { \boldsymbol \mu } _ { i } ) \right)
    $$
    *   $\mu_i$: 第 $i$ 个 3D 高斯中心投影到 2D 图像上的坐标。
    *   $\sigma_i$: 第 $i$ 个高斯的不透明度。
    *   $(\Sigma')^{-1}$: 2D 协方差矩阵的逆。

### 4.2.2. 可变形 3D 高斯 (Deformable 3D Gaussians)
这是本文的核心创新，用于为静态的 3D 高斯赋予动态能力。

*   **形变网络:** 作者设计了一个 MLP 网络 $\mathcal{F}_{\theta}$ 作为形变场。该网络输入规范空间中高斯的中心位置 $\pmb{x}$ 和当前时间 $t$，输出该高斯在位置、旋转和尺度上的偏移量 $(\delta\pmb{x}, \delta\pmb{r}, \delta\pmb{s})$。
    $$
    ( \delta \mathbf { } x , \delta \pmb { r } , \delta \pmb { s } ) = \mathcal { F } _ { \boldsymbol { \theta } } ( \gamma ( \mathrm { s g } ( \pmb { x } ) ) , \gamma ( t ) )
    $$
    *   $\gamma(\cdot)$: <strong>位置编码 (Positional Encoding)</strong> 函数。它将一个标量（如坐标值或时间）映射到一个高维向量，有助于 MLP 学习高频变化。其公式为：
        $$
        \gamma ( p ) = ( \sin ( 2 ^ { k } \pi p ) , \cos ( 2 ^ { k } \pi p ) ) _ { k = 0 } ^ { L - 1 }
        $$
    *   $\mathrm{sg}(\pmb{x})$: <strong>停止梯度 (stop-gradient)</strong> 操作。这是一个至关重要的细节。它意味着在反向传播时，从最终损失传回的梯度不会通过形变网络 $\mathcal{F}_{\theta}$ 的输入端流向规范位置 $\pmb{x}$。换言之，形变网络只根据规范位置来 *预测* 形变，但不能 *改变* 规范位置本身。规范位置 $\pmb{x}$ 的梯度直接来自于光栅化器。这确保了形变场和规范几何的学习是分离的。

*   **应用形变:** 得到偏移量后，将它们应用到规范高斯的参数上，得到在时间 $t$ 的动态高斯：
    *   位置: $\pmb{x}_{dynamic} = \pmb{x}_{canonical} + \delta\pmb{x}$
    *   旋转: $\pmb{r}_{dynamic} = \pmb{r}_{canonical} + \delta\pmb{r}$ (四元数加法)
    *   尺度: $\pmb{s}_{dynamic} = \pmb{s}_{canonical} + \delta\pmb{s}$

### 4.2.3. 退火平滑训练 (Annealing Smooth Training, AST)
这是为解决真实世界数据中相机位姿不准而设计的训练策略。不准确的位姿会导致视频帧之间存在微小的空间抖动，使得模型学习到的时间序列不平滑。

*   **带噪声的训练:** 在训练的第 $i$ 次迭代中，AST 不直接使用时间 $t$，而是在其位置编码后添加一个随迭代次数衰减的高斯噪声 $\mathcal{X}(i)$。
    $$
    \Delta = \mathcal { F } _ { \theta } \left( \gamma ( \mathrm { s g } ( \pmb { x } ) ) , \gamma ( t ) + \mathcal { X } ( i ) \right)
    $$
    其中，噪声 $\mathcal{X}(i)$ 的计算方式为：
    $$
    \mathcal { X } ( i ) = \mathbb { N } ( 0 , 1 ) \cdot \beta \cdot \Delta t \cdot ( 1 - i / \tau )
    $$
    *   $\mathbb{N}(0, 1)$: 标准正态分布，即随机采样一个噪声。
    *   $i$: 当前的训练迭代次数。
    *   $\tau$: 噪声衰减的阈值迭代次数（例如 20k 次）。
    *   $(1 - i / \tau)$: 线性衰减项。当 $i=0$ 时，噪声最大；当 $i \ge \tau$ 时，噪声为 0。
    *   $\beta, \Delta t$: 缩放因子。

*   **工作直觉:** 在训练早期 ($i \ll \tau$)，较大的噪声迫使形变网络 $\mathcal{F}_{\theta}$ 对时间输入的微小变化不那么敏感，从而学习到一个更平滑的时间函数，有效抵抗由位姿不准带来的时间抖动。随着训练的进行 ($i \to \tau$)，噪声逐渐消失，网络可以开始学习更精细的、真实的动态细节，而不会被过度平滑。这种方法相比于引入额外的平滑损失项，没有任何计算开销。

# 5. 实验设置

## 5.1. 数据集
*   <strong>合成数据集 (Synthetic Dataset):</strong>
    *   **来源:** `D-NeRF` [34] 数据集。
    *   **特点:** 该数据集包含多个具有复杂非刚性运动的场景，如 `Hell Warrior`, `Mutant`, `Bouncing Balls`, `Lego` 等。相机轨迹是平滑的合成轨迹，背景通常为纯色。
    *   **作用:** 用于验证方法在理想条件下的性能上限和对复杂动态的建模能力。
*   <strong>真实世界数据集 (Real-world Dataset):</strong>
    *   **来源:** `HyperNeRF` [31] 和 `NeRF-DS` [50] 数据集。
    *   **特点:** 这些数据集由手持手机拍摄，包含了真实的日常场景和物体运动。通过 `COLMAP` 工具估计相机位姿，这些位姿可能存在误差和抖动。
    *   **作用:** 用于验证方法在真实、复杂且带有噪声条件下的鲁棒性和泛化能力。
*   **图像分辨率:** 所有实验均在 800x800 的全分辨率下进行。

## 5.2. 评估指标
论文使用了三个标准的图像质量评估指标来衡量渲染结果与真实图像的差异。

*   <strong>峰值信噪比 (Peak Signal-to-Noise Ratio, PSNR):</strong>
    1.  **概念定义:** PSNR 是衡量图像重建质量最常用的指标之一。它通过计算原始图像与重建图像之间像素级别的均方误差 (MSE) 来衡量失真程度。PSNR 值越高，表示图像失真越小，质量越好。它对像素级别的精确对应非常敏感。
    2.  **数学公式:**
        $$
        \text{PSNR} = 10 \cdot \log_{10} \left( \frac{MAX_I^2}{\text{MSE}} \right)
        $$
        其中，均方误差 (MSE) 的计算公式为：
        $$
        \text{MSE} = \frac{1}{m \times n} \sum_{i=0}^{m-1} \sum_{j=0}^{n-1} [I(i,j) - K(i,j)]^2
        $$
    3.  **符号解释:**
        *   $MAX_I$: 图像像素值的最大可能值（对于 8-bit 图像，为 255）。
        *   `m, n`: 图像的高度和宽度。
        *   `I(i,j)`: 真实图像在像素 `(i,j)` 处的值。
        *   `K(i,j)`: 渲染图像在像素 `(i,j)` 处的值。

*   <strong>结构相似性指数 (Structural Similarity Index Measure, SSIM):</strong>
    1.  **概念定义:** 与 PSNR 不同，SSIM 旨在从人类视觉感知的角度来评估图像质量。它综合考量两张图像在亮度、对比度和结构上的相似性。SSIM 的值域为 [-1, 1]，越接近 1 表示两张图像在结构上越相似，质量越好。
    2.  **数学公式:**
        $$
        \text{SSIM}(x,y) = \frac{(2\mu_x\mu_y + c_1)(2\sigma_{xy} + c_2)}{(\mu_x^2 + \mu_y^2 + c_1)(\sigma_x^2 + \sigma_y^2 + c_2)}
        $$
    3.  **符号解释:**
        *   $\mu_x, \mu_y$: 图像块 $x$ 和 $y$ 的平均亮度。
        *   $\sigma_x^2, \sigma_y^2$: 图像块 $x$ 和 $y$ 的方差（衡量对比度）。
        *   $\sigma_{xy}$: 图像块 $x$ 和 $y$ 的协方差（衡量结构相似度）。
        *   $c_1, c_2$: 用于避免分母为零的稳定常数。

*   <strong>学习感知图像块相似度 (Learned Perceptual Image Patch Similarity, LPIPS):</strong>
    1.  **概念定义:** LPIPS 是一种更先进的感知度量指标。它通过一个预训练的深度神经网络（如 VGG, AlexNet）提取两张图像在不同网络层的特征图，然后计算这些特征图之间的加权距离。LPIPS 被认为比 PSNR 和 SSIM 更能反映人类对图像相似性的主观判断。LPIPS 值越低，表示两张图像在感知上越相似。
    2.  **数学公式:**
        $$
        d(x, x_0) = \sum_l \frac{1}{H_l W_l} \sum_{h,w} \| w_l \odot ( \hat{y}_{hw}^l - \hat{y}_{0hw}^l ) \|_2^2
        $$
    3.  **符号解释:**
        *   $d(x, x_0)$: 图像 $x$ 和 $x_0$ 之间的 LPIPS 距离。
        *   $l$: 神经网络的第 $l$ 个卷积层。
        *   $\hat{y}^l, \hat{y}_{0}^l$: 从图像 $x, x_0$ 中提取的第 $l$ 层的特征图。
        *   $w_l$: 用于校准不同通道重要性的权重向量。
        *   $H_l, W_l$: 第 $l$ 层特征图的高度和宽度。

## 5.3. 对比基线
本文将自己的方法与一系列当前最先进的动态场景神经渲染方法进行了比较，这些基线具有很强的代表性：
*   `3D-GS`: 静态 3D 高斯泼洒方法。作为基线，展示了如果不处理动态，直接在动态数据集上训练的效果。
*   `D-NeRF`: 经典的解耦式动态 NeRF 方法。
*   `TiNeuVox`: 基于时空体素网格加速的动态 NeRF 方法。
*   `Tensor4D`: 基于 4D 张量分解的动态场景表示方法。
*   `K-Planes`: 基于时空特征平面的高效动态场景表示方法。
*   `HyperNeRF`, `NeRF-DS`: 针对真实世界动态场景（特别是拓扑变化和镜面反射）的先进 NeRF 方法。

# 6. 实验结果与分析

## 6.1. 核心结果分析
### 6.1.1. 合成数据集上的性能
在 `D-NeRF` 合成数据集上的定量比较结果如下表所示。

**以下是原文 Table 1 的结果：**

<table>
<thead>
<tr>
<th rowspan="2">Method</th>
<th colspan="3">Hell Warrior</th>
<th colspan="3">Mutant</th>
<th colspan="3">Hook</th>
<th colspan="3">Bouncing Balls</th>
</tr>
<tr>
<th>PSNR↑</th>
<th>SSIM↑</th>
<th>LPIPS↓</th>
<th>PSNR↑</th>
<th>SSIM↑</th>
<th>LPIPS↓</th>
<th>PSNR↑</th>
<th>SSIM↑</th>
<th>LPIPS↓</th>
<th>PSNR↑</th>
<th>SSIM↑</th>
<th>LPIPS↓</th>
</tr>
</thead>
<tbody>
<tr>
<td>3D-GS</td>
<td>29.89</td>
<td>0.9155</td>
<td>0.1056</td>
<td>24.53</td>
<td>0.9336</td>
<td>0.0580</td>
<td>21.71</td>
<td>0.8876</td>
<td>0.1034</td>
<td>23.20</td>
<td>0.9591</td>
<td>0.0600</td>
</tr>
<tr>
<td>D-NeRF</td>
<td>24.06</td>
<td>0.9440</td>
<td>0.0707</td>
<td>30.31</td>
<td>0.9672</td>
<td>0.0392</td>
<td>29.02</td>
<td>0.9595</td>
<td>0.0546</td>
<td>38.17</td>
<td>0.9891</td>
<td>0.0323</td>
</tr>
<tr>
<td>TiNeuVox</td>
<td>27.10</td>
<td>0.9638</td>
<td>0.0768</td>
<td>31.87</td>
<td>0.9607</td>
<td>0.0474</td>
<td>30.61</td>
<td>0.9599</td>
<td>0.0592</td>
<td>40.23</td>
<td>0.9926</td>
<td>0.0416</td>
</tr>
<tr>
<td>Tensor4D</td>
<td>31.26</td>
<td>0.9254</td>
<td>0.0735</td>
<td>29.11</td>
<td>0.9451</td>
<td>0.0601</td>
<td>28.63</td>
<td>0.9433</td>
<td>0.0636</td>
<td>24.47</td>
<td>0.9622</td>
<td>0.0437</td>
</tr>
<tr>
<td>K-Planes</td>
<td>24.58</td>
<td>0.9520</td>
<td>0.0824</td>
<td>32.50</td>
<td>0.9713</td>
<td>0.0362</td>
<td>28.12</td>
<td>0.9489</td>
<td>0.0662</td>
<td>40.05</td>
<td>0.9934</td>
<td>0.0322</td>
</tr>
<tr>
<td><strong>Ours</strong></td>
<td><strong>41.54</strong></td>
<td><strong>0.9873</strong></td>
<td><strong>0.0234</strong></td>
<td><strong>42.63</strong></td>
<td><strong>0.9951</strong></td>
<td><strong>0.0052</strong></td>
<td><strong>37.42</strong></td>
<td><strong>0.9867</strong></td>
<td><strong>0.0144</strong></td>
<td><strong>41.01</strong></td>
<td><strong>0.9953</strong></td>
<td><strong>0.0093</strong></td>
</tr>
</tbody>
<thead>
<tr>
<th rowspan="2">Method</th>
<th colspan="3">Lego</th>
<th colspan="3">T-Rex</th>
<th colspan="3">Stand Up</th>
<th colspan="3">Jumping Jacks</th>
</tr>
<tr>
<th>PSNR↑</th>
<th>SSIM↑</th>
<th>LPIPS↓</th>
<th>PSNR↑</th>
<th>SSIM↑</th>
<th>LPIPS↓</th>
<th>PSNR↑</th>
<th>SSIM↑</th>
<th>LPIPS↓</th>
<th>PSNR↑</th>
<th>SSIM↑</th>
<th>LPIPS↓</th>
</tr>
</thead>
<tbody>
<tr>
<td>3D-GS</td>
<td>22.10</td>
<td>0.9384</td>
<td>0.0607</td>
<td>21.93</td>
<td>0.9539</td>
<td>0.0487</td>
<td>21.91</td>
<td>0.9301</td>
<td>0.0785</td>
<td>20.64</td>
<td>0.9297</td>
<td>0.0828</td>
</tr>
<tr>
<td>D-NeRF</td>
<td>25.56</td>
<td>0.9363</td>
<td>0.0821</td>
<td>30.61</td>
<td>0.9671</td>
<td>0.0535</td>
<td>33.13</td>
<td>0.9781</td>
<td>0.0355</td>
<td>32.70</td>
<td>0.9779</td>
<td>0.0388</td>
</tr>
<tr>
<td>TiNeuVox</td>
<td>26.64</td>
<td>0.9258</td>
<td>0.0877</td>
<td>31.25</td>
<td>0.9666</td>
<td>0.0478</td>
<td>34.61</td>
<td>0.9797</td>
<td>0.0326</td>
<td>33.49</td>
<td>0.9771</td>
<td>0.0408</td>
</tr>
<tr>
<td>Tensor4D</td>
<td>23.24</td>
<td>0.9183</td>
<td>0.0721</td>
<td>23.86</td>
<td>0.9351</td>
<td>0.0544</td>
<td>30.56</td>
<td>0.9581</td>
<td>0.0363</td>
<td>24.20</td>
<td>0.9253</td>
<td>0.0667</td>
</tr>
<tr>
<td>K-Planes</td>
<td>28.91</td>
<td>0.9695</td>
<td>0.0331</td>
<td>30.43</td>
<td>0.9737</td>
<td>0.0343</td>
<td>33.10</td>
<td>0.9793</td>
<td>0.0310</td>
<td>31.11</td>
<td>0.9708</td>
<td>0.0468</td>
</tr>
<tr>
<td><strong>Ours</strong></td>
<td><strong>33.07</strong></td>
<td><strong>0.9794</strong></td>
<td><strong>0.0183</strong></td>
<td><strong>38.10</strong></td>
<td><strong>0.9933</strong></td>
<td><strong>0.0098</strong></td>
<td><strong>44.62</strong></td>
<td><strong>0.9951</strong></td>
<td><strong>0.0063</strong></td>
<td><strong>37.72</strong></td>
<td><strong>0.9897</strong></td>
<td><strong>0.0126</strong></td>
</tr>
</tbody>
</table>

*   **分析:** 表格数据清晰地表明，本文方法 (`Ours`) 在所有场景和所有指标上都**大幅度领先**于所有基线方法。尤其是在 `LPIPS` 指标上，本文方法的数值通常比次优方法低数倍（例如，`Mutant` 场景中，`Ours` 的 LPIPS 为 0.0052，而次优的 `K-Planes` 为 0.0362），这证明其渲染结果在感知质量上与真实图像极为接近，细节保留得非常好。
*   <strong>定性结果 (原文 Figure 3):</strong>

    ![该图像是插图，展示了不同方法在动态场景重建中的效果比较。每一行代表一个场景，包括“恐龙”、“站立”、“突变体”、“跳远”等，通过不同的方法如“GT”、“Ours”、“TiNeuVox”、“K-Planes”等进行重建。图中包含场景的高质量重建结果，并显示了每种方法在细节表现和渲染质量上的差异。](images/3.jpg)
    *该图像是插图，展示了不同方法在动态场景重建中的效果比较。每一行代表一个场景，包括“恐龙”、“站立”、“突变体”、“跳远”等，通过不同的方法如“GT”、“Ours”、“TiNeuVox”、“K-Planes”等进行重建。图中包含场景的高质量重建结果，并显示了每种方法在细节表现和渲染质量上的差异。*

    上图直观地展示了这种优势。与其他方法（如 `TiNeuVox`, `K-Planes`）相比，本文方法渲染的图像（`Ours`）明显更清晰，细节更丰富，伪影更少，几乎与真实图像（`GT`）无法区分。

### 6.1.2. 真实世界数据集上的性能
由于真实世界数据集的相机位姿不准，PSNR 这种像素级指标可能会惩罚那些虽然清晰但有微小位移的重建结果。因此，作者主要在 `NeRF-DS` 数据集上进行了定量评估。

**以下是原文 Table 2 的结果：**

<table>
<tr>
<td></td>
<td>PSNR ↑</td>
<td>SSIM ↑</td>
<td>LPIPS ↓</td>
</tr>
<tr>
<td>3D-GS</td>
<td>20.29</td>
<td>0.7816</td>
<td>0.2920</td>
</tr>
<tr>
<td>TiNeuVox</td>
<td>21.61</td>
<td>0.8234</td>
<td>0.2766</td>
</tr>
<tr>
<td>HyperNeRF</td>
<td>23.45</td>
<td>0.8488</td>
<td>0.1990</td>
</tr>
<tr>
<td>NeRF-DS</td>
<td>23.60</td>
<td>0.8494</td>
<td>0.1816</td>
</tr>
<tr>
<td>Ours (w/o AST)</td>
<td>23.97</td>
<td>0.8346</td>
<td>0.2037</td>
</tr>
<tr>
<td><strong>Ours</strong></td>
<td><strong>24.11</strong></td>
<td><strong>0.8525</strong></td>
<td><strong>0.1769</strong></td>
</tr>
</table>

*   **分析:** 在 `NeRF-DS` 数据集的平均指标上，本文方法 (`Ours`) 取得了最好的 `PSNR`、`SSIM` 和 `LPIPS`。这证明了即使在有噪声的真实数据上，该方法依然具有强大的性能和鲁棒性。
*   <strong>定性结果 (原文 Figure 5):</strong>

    ![该图像是一个示意图，展示了不同方法在动态场景重建中的表现，包括 GT、Ours、TiNeuVox、HyperNeRF、NeRF-DS 和 3D-GS。每列展示了在不同测试场景（as、bell 和 sieve）下的重建效果，对比了各方法的准确性和渲染质量。](images/5.jpg)
    *该图像是一个示意图，展示了不同方法在动态场景重建中的表现，包括 GT、Ours、TiNeuVox、HyperNeRF、NeRF-DS 和 3D-GS。每列展示了在不同测试场景（as、bell 和 sieve）下的重建效果，对比了各方法的准确性和渲染质量。*

    上图展示了在 `NeRF-DS` 数据集上的视觉对比。可以看出，本文方法 (`Ours`) 的渲染结果在清晰度和真实感上优于其他方法，例如在 `sieve` 场景中，金属筛网的细节和光泽都得到了很好的还原。

### 6.1.3. 渲染效率
论文指出，当 3D 高斯的数量在 25 万以下时，该方法可以在单张 NVIDIA RTX 3090 显卡上达到超过 30 FPS 的实时渲染速度。这远超所有基于 NeRF 的方法，是该工作的一大亮点。

### 6.1.4. 深度图可视化

![Figure 6. Depth Visualization. We visualized the depth map of the D-NeRF dataset. The first row includes bouncing-balls, hellwarrior, hook, and jumping-jacks, while the second row includes lego, mutant, standup, and trex.](images/6.jpg)
*该图像是深度可视化图表，展示了D-NeRF数据集中的深度图。第一行包括反弹球、地狱战士、钩子和跳跃者，第二行则包括乐高、突变体、站立和霸王龙。*

原文 Figure 6 展示了在 `D-NeRF` 数据集上渲染出的深度图。这些深度图边缘清晰，几何结构准确。这有力地证明了模型学习到的是真实的场景三维几何，而不是简单地通过颜色匹配来“作弊”，这对于新视角合成等任务至关重要。

## 6.2. 消融实验/参数分析
消融实验用于验证模型中各个组件的有效性。

### 6.2.1. 退火平滑训练 (AST) 的作用
*   <strong>定量分析 (Table 2):</strong> 从上文的 Table 2 中可以看到，在 `NeRF-DS` 数据集上，不使用 AST 的版本 (`Ours (w/o AST)`) 在所有指标上都劣于使用 AST 的完整版本 (`Ours`)。例如，`LPIPS` 从 0.2037 降低到 0.1769，`SSIM` 从 0.8346 提升到 0.8525。这证明了 AST 机制能有效提升在真实数据上的重建质量。
*   <strong>定性分析 (原文 Figure 4):</strong>

    ![该图像是图表，展示了三种不同方法在动态场景重建中的效果对比。左侧为使用6个采样点且未应用退火平滑训练（6pe w/o AST），中间为使用10个采样点但同样未应用退火平滑训练（10pe w/o AST），右侧为本文提出的方法（ours）。每种方法的细节表现有所不同，特别是在动态效果和清晰度方面，强调了提出方法的优势。](images/4.jpg)
    *该图像是图表，展示了三种不同方法在动态场景重建中的效果对比。左侧为使用6个采样点且未应用退火平滑训练（6pe w/o AST），中间为使用10个采样点但同样未应用退火平滑训练（10pe w/o AST），右侧为本文提出的方法（ours）。每种方法的细节表现有所不同，特别是在动态效果和清晰度方面，强调了提出方法的优势。*

    上图对比了不同设置下的效果。右侧的完整方法 (`ours`) 渲染出的水流细节最丰富、最清晰，而左侧未采用 AST 的版本 (`w/o AST`) 则出现了更多的模糊和伪影。这说明 AST 不仅提升了时间平滑性，还有助于模型收敛到更精细的细节。

### 6.2.2. 形变网络输出组件的作用
<strong>以下是原文 Table 7 的部分结果 (以 Hell Warrior 和 Mutant 场景为例):</strong>

<table>
<thead>
<tr>
<th rowspan="2"></th>
<th colspan="3">Hell Warrior</th>
<th colspan="3">Mutant</th>
</tr>
<tr>
<th>PSNR↑</th>
<th>SSIM↑</th>
<th>LPIPS↓</th>
<th>PSNR↑</th>
<th>SSIM↑</th>
<th>LPIPS↓</th>
</tr>
</thead>
<tbody>
<tr>
<td>w/o δs (无尺度形变)</td>
<td>41.55</td>
<td>0.9878</td>
<td>0.0223</td>
<td>42.15</td>
<td>0.9949</td>
<td>0.0053</td>
</tr>
<tr>
<td>w/o δr (无旋转形变)</td>
<td>41.17</td>
<td>0.9866</td>
<td>0.0256</td>
<td>42.51</td>
<td>0.9950</td>
<td>0.0054</td>
</tr>
<tr>
<td>w/o r&s (无旋转和尺度形变)</td>
<td>40.39</td>
<td>0.9833</td>
<td>0.0323</td>
<td>41.30</td>
<td>0.9934</td>
<td>0.0075</td>
</tr>
<tr>
<td><strong>ours (完整模型)</strong></td>
<td><strong>41.54</strong></td>
<td><strong>0.9873</strong></td>
<td><strong>0.0234</strong></td>
<td><strong>42.63</strong></td>
<td><strong>0.9951</strong></td>
<td><strong>0.0052</strong></td>
</tr>
</tbody>
</table>

*   **分析:** 该消融实验验证了形变网络预测位置 ($\delta\pmb{x}$)、旋转 ($\delta\pmb{r}$) 和尺度 ($\delta\pmb{s}$) 三种偏移量的必要性。从表中可以看出，只预测位置偏移 (`w/o r&s`) 的效果最差。完整模型 (`ours`) 综合性能最佳，说明同时对高斯的位置、旋转和尺度进行动态调整，对于精确建模动态场景至关重要。

# 7. 总结与思考

## 7.1. 结论总结
本文成功地将 `3D Gaussian Splatting` 这一强大的静态场景表示方法扩展到了具有挑战性的单目动态场景重建任务中。通过以下创新，实现了在质量和速度上对现有方法的全面超越：
1.  **提出可变形 3D 高斯框架:** 核心思想是在规范空间中学习一套静态 3D 高斯，并用一个 MLP 形变场来建模其随时间的动态变化。这巧妙地结合了显式表示的高效率和解耦建模的灵活性。
2.  <strong>引入退火平滑训练 (AST):</strong> 提出了一种简单而有效的训练策略，通过在训练初期对时间输入添加衰减噪声，有效缓解了真实数据中相机位姿不准带来的时间抖动问题，提升了模型的鲁棒性和最终质量，且无额外计算开销。
3.  **SOTA 性能:** 实验证明，该方法不仅能够生成比以往方法更清晰、细节更丰富的渲染结果，还能达到实时渲染的帧率，为动态场景的实时应用铺平了道路。

## 7.2. 局限性与未来工作
作者在论文中坦诚地指出了当前方法的几个局限性：
*   **对视点覆盖率的敏感性:** 与许多三维重建方法一样，该方法的效果依赖于输入视频中视点的多样性。如果视点过于稀疏或覆盖范围有限，模型可能会过拟合。
*   **对位姿精度的依赖:** 尽管 AST 机制可以缓解部分位姿噪声，但过于不准确的相机位姿仍然会导致模型收敛失败。
*   **计算成本随高斯数量增加:** 场景的复杂性越高，所需的高斯数量就越多，这会直接导致训练时间和内存消耗的增加，并降低渲染速度。
*   **对复杂运动的泛化能力:** 论文主要在具有中等运动幅度的场景上进行了验证。该方法能否处理更精细、更复杂的运动（如人类微妙的面部表情）仍是一个有待探索的问题。

## 7.3. 个人启发与批判
*   **启发:**
    1.  **范式迁移的成功典范:** 这篇论文是“将一个领域的成功范式迁移到另一个领域”的绝佳案例。它敏锐地抓住了 `3D-GS` 在静态场景上的巨大成功，并找到了一个优雅的方案（规范空间+形变场）将其能力释放到动态场景中。这启发我们，在研究中应时刻关注相关领域的突破性进展，并思考如何将其思想借鉴和应用到自己的问题上。
    2.  **简单而有效的问题解决方案:** AST 机制是一个非常漂亮的设计。它没有引入复杂的网络结构或损失函数，而是通过一个简单的“加噪声并退火”的技巧，就有效解决了真实数据中的一个痛点。这种追求简洁、高效的工程智慧值得学习。
    3.  **显式表示的回归:** `NeRF` 开启了神经渲染的“隐式”时代，而 `3D-GS` 及其动态扩展则标志着“显式”表示的强势回归。显式表示（如点、网格）由于其易于编辑、符合传统图形管线以及巨大的速度优势，在未来可能会扮演越来越重要的角色。

*   **批判与展望:**
    1.  **位姿优化问题:** 本文最大的局限性之一是对输入位姿的依赖。未来的一个重要研究方向是将相机位姿的联合优化整合到框架中，类似于 `BARF` 或 `NeRF--` 对 `NeRF`所做的工作，从而实现真正的端到端动态场景重建，摆脱对 `COLMAP` 等外部工具的依赖。
    2.  **高斯数量的控制:** 如何在保证质量的同时，用尽可能少的高斯来表示场景，是该方法走向更广泛应用的关键。未来的工作可以探索多分辨率或层级式的高斯表示，或者引入更智能的自适应控制策略，实现高斯数量和场景复杂度的动态平衡。
    3.  **更强的形变模型:** 目前的 MLP 形变场对于拓扑变化或极端变形可能仍然力不从心。探索更强大的形变模型，例如结合骨骼动画、物理仿真或非刚性配准领域的先进技术，有望处理更具挑战性的动态场景。