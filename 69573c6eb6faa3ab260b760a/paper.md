# Pretraining Frame Preservation in Autoregressive Video Memory Compression

Lvmin Zhang1 Shengqu Cai1 Muyang Li2 Chong Zeng1 Beijia Lu3 Anyi Rao4 Song Han2 Gordon Wetzstein1 Maneesh Agrawala1 1Stanford University 2MIT 3Carnegie Mellon University 4HKUST

![](images/1.jpg)  
The $^ { 2 0 + }$ second history was compressed into a ${ \sim } 5 \mathrm { k }$ context length and processed by an RTX 4070 12GB.

# Abstract

We present a neural network structure to compress long videos into short contexts, with an explicit pretraining objective to preserve the high-frequency details of single frames at arbitrary temporal positions. The baseline model can compress a 20-second video into a context at about $5 k$ length, where random frames can be retrieved at perceptually preserved appearances. Such pretrained models can be directly fine-tuned as memory encoders for autoregressive video models, enabling long history memory with low context cost and relatively low fidelity loss. We evaluate the framework with ablative settings and discuss the trade-offs of possible neural architecture designs.

# 1. Introduction

Storytelling capability, narrative coherence, and context quality are increasingly important in the video generation community. The latest commercial models like Sora2 [46], Veo3.1 [20], and Kling2.5 [37] enable storyboard creationbased, scene planning, and dynamic camera shots (e.g., in the style of TikTok or YouTube Shorts). Recent academic models have also focused on long video streaming [27, 72], scene planning [22, 26, 83, 86, 87], and video context consistency [28, 31, 42]. Among these approaches, autoregressive models are a key paradigm for video storytelling, supporting native video continuation and storyboard streaming. Autoregressive models view video history as context, and they face unique challenges when handling long-form content like in-the-wild videos and movies.

The trade-off between context quality and length is a typical and critical problem in video context modeling. A naive sliding window, which cuts off all distant-enough frames, maintains a fixed context length but loses long-range history. Various compression approaches have been extensively studied; e.g., highly compressed VAEs like LTXV [23] and DC-AE [9] show that it is possible to compress pixelspace videos into more compact contexts, and hybrid approaches [1, 32, 88] like FramePack [78] show that compression can happen at multiple levels, albeit at the cost of high-frequency image details. Another strategy is to preserve the context length but reduce computation (e.g., using sparse [64, 65, 77, 80] or linear [5, 11, 33, 58, 68, 74] attention), though the linear layer cost remains an overhead for training and bidirectional inference. More specialized methods like token merging [2, 4] also demonstrate that a higher merging rate leads to greater detail loss. These research efforts implicitly converge on the observation that context quality and length are governed by a fundamental trade-off where the optimal balance has not yet been reached. This observation leads to a wide range of open questions. Is there an explicit correlation between context length and quality, and how can we model it? Should context quality be measured by object, scene, or facial consistency, or potentially, by full-frame reconstruction quality? To achieve a perceptually acceptable quality from a human perspective, what is the maximum level of compression, and where is the sweet spot? How can such a balance be found? Do different tasks and objectives pose varied requirements for this trade-off? Do spatial and temporal compression have different levels of importance in various tasks? Is there a marginal effect of context length that makes a sufficiently long context actually redundant? To study these questions, this paper targets an explicit objective: a learning task to directly compress video history frames into a minimized context length while maximizing frame retrieval quality. This setting allows us to explicitly study the correlation between context length and reconstruction quality using both quantitative metrics and human perceptual evaluations. We can thereby independently study the quality, influence, and cost of the compression model before moving on to the actual autoregressive video generation. This white-box feedback eases cost management and contributes to the explainability of separate encoding components in autoregressive video models. To this end, one challenge is that learning to compress and transform a long video context causes computational bottlenecks. We observe that a strong indicator of a video compression mechanism's ability to preserve contextual details is its capacity for high-quality frame reconstruction at arbitrary temporal locations. This leads to a randomized approach: we first pretrain a memory encoder to reconstruct frames at random time positions, and then finetune an autoregressive video model using the compression model as a history memory encoder. Pretraining with millions of videos, we show that this method reduces the video model training cost by a large margin and facilitates a stronger neural representation for detailed frame retrieval. Experiments show that the pretrained memory encoder can, by itself, reconstruct history frames at high quality (measured by PSNR/SSIM [60]). When connected to video diffusion models for autoregressive finetuning, the encoder provides long-range consistency. This framework achieves a balanced and optimized point in the context length-quality trade-off, leading to a practical autoregressive video model with a relatively short context length and perceptually high consistency based on its long-range history. In summary, we (1) discuss the trade-off between context length and quality in autoregressive video generation; (2) propose to pretrain an explicit compression task for frame retrieval for improved autoregressive video memory; (3) establish an autoregressive system to fine-tune the pretrained compression model as a history memory encoder for DiTs; (4) conduct extensive experiments, including quantitative, qualitative, and ablation studies, to analyze the influence of different compression designs; and (5) provide pretrained models for practical video generation and editing applications.

# 2. Related Work

Autoregressive video diffusion and long videos. Diffusion has become the driving force behind video synthesis, where a central challenge is length scaling. Trainingfree length-extension methods [43, 44, 48, 49, 84] reschedule noise or re-balance temporal frequency to stretch pretrained models beyond their training horizon. A complementary thread blends diffusion with causal prediction: Diffusion Forcing [7] and HistoryGuidance [52] enable variablehorizon conditioning and stable long rollouts by noise injection. These approaches are adapted in industrial systems such as SkyReels-V2 [8] and Magi-1 [55]. Additionally, StreamingT2V [24] augments existing models with shortand long-term memory with randomized blending to generate longer videos. FAR [21] studies long-context AR with flexible RoPE [53] decay and mixed short/long windows, while CausVid [72] distills a bidirectional teacher to a few-step causal generator. StreamDiT [35] combines multi-step distillation with a moving frame buffer and mixed partition training to generate results in real-time. To mitigate error accumulation during AR generation, Self-Forcing [27] simulates AR rollout during training, while its extensions [12, 41, 69] further improve length generalization. Context learning and compression. Long-context persistence is crucial as we extend video generation beyond a few seconds. One option is retrieval, which grounds generation in a persistent state. WorldMem [67] and Context-as-Memory [73] augment AR video world models with FoV-based history retrieval, while VMem [39] indexes past views by surfels to retrieve the most relevant viewpoints. Memory Forcing [25] pairs geometry-indexed spatial memory with tailored training regimes to balance exploration and revisits. Beyond fixed retrieval rules, Mixture-ofContexts [6] learns a dynamic sparse attention context router so that tokens attend only to the most salient chunks. Similarly, MoGA [30] and Holocine [45] also propose token-level sparse attending policies. Pack-and-Force [63] instead proposes a learnable context semantic retriever. Orthogonal to retrieval, compression of history turns an unbounded context into a compact state. FramePack [78] compresses prior frames into a fixed-size latent "packed" context. Captain Cinema [66] uses a similar compression for keyframes. StateSpaceDiffuser [50] and Po et al. [47] swap quadratic attention for recurrent states to maintain long-term memory. TTTVideo [13] and LaCT [81] use light MLP test-time training layers as a learned context representation.

Efficient video diffusion designs. As we scale video generation to long horizons, large context windows are bottlenecked, driving a wave of efficient computational designs. Kernel advances such as FlashAttention [14, 15] improve throughput. Static or hardware-friendly sparse patterns include sliding/tiling 3D windows [80], radial spatiotemporal masks [40], and training-free head/pruning heuristics [64, 70]. Dynamic or learned pruning/routing further select salient token pairs or blocks [62], coarseto-fine sparse token selection [79], Sage/SpargeAttention families [7577], blockified routing with cached search [65], and progressive block carving [82]. Another line of work leverages compressing the latent space or token sequence: token merging and patch scaling [3, 38], compact/variablerate tokenizers [1], highly compressed latent space [23], or multiscale pyramids with re-noising [32]. SANA [68] introduced a linear-attention Diffusion Transformer for images, while SANA-Video [10] extends this with block-linear attention and a constant-memory KV cache. Besides, this approach presents distinctions with VAEs: VAEs have bottlenecks with 4, 16, or 64 channels, whereas this method directly works at DiT inner channels like 3072 or 5120 with a different feature scope; VAEs have training overheads to process full contexts, while this method works with lightweight partial overheads (retrieval objective).

# 3. Method

We pretrain a video compression model with the objective to retrieve frames at arbitrary time positions, and simultaneously preserve frame details (Section 3.1). The pretrained compression model is then finetuned as a memory encoder for autoregressive video diffusion models (Section 3.2).

Preliminaries. We clarify the notation of a typical autoregressive video diffusion model. Unless otherwise noted, all frames", "pixels", etc., refer to latent concepts. We consider the typical Diffusion Transformers (DiTs) like Wan [56] and HunyuanVideo [36] with flow match scheduling. Rectifiedflow models map noisy latents $\boldsymbol { X } _ { t _ { i } } \in \mathbb { R } ^ { T \times H \times \bar { W } \times C }$ with from clean latents $X _ { 0 }$ where $t _ { i } ~ \in ~ ( 0 , 1 ]$ is the diffusion timestep. The autoregressive model typically takes the history $\bar { \pmb { H } } \in \mathbb { R } ^ { T _ { h } \times H \times \bar { W } \times C }$ as conditioning. To learn the toregressive generator $G _ { \theta } ( \cdot )$ , the learning objective is where $^ c$ is a set of conditions such as text prompts (a single prompt or streamed from a storyboard), and $t _ { i } \sim \mathcal { L } ( 0 , 1 )$ is the shifted logit-normal distribution [18] for flow matching.

$$
\begin{array} { r } { \mathbf { } \mathbf { } X _ { t _ { i } } = ( 1 - t _ { i } ) \mathbf { } X _ { 0 } + t _ { i } \mathbf { \epsilon } , \quad \mathbf { \epsilon } \in \mathcal { N } \mathbf { ( 0 , I ) } , } \end{array}
$$

![](images/2.jpg)  
Figure 2. Pretraining of memory compression models. The memory compression model has to compress long videos (e.g., 20 seconds) into short contexts (e.g., of length 5k). The objective of the pretraining is to retrieve frames with high-frequency details in arbitrary history time positions.

$$
\begin{array} { r } { \mathbb { E } _ { X _ { 0 } , H , c , \epsilon , t _ { i } \sim \mathcal { L } ( 0 , 1 ) } \bigg | \bigg | \big ( \epsilon - X _ { 0 } \big ) - G _ { \theta } \big ( X _ { t _ { i } } , t _ { i } , c , H \big ) \bigg | \bigg | _ { 2 } ^ { 2 } , } \end{array}
$$

# 3.1. Pretraining Memory Compression Model

Consider a typical case where we have a history of a 60- second video and want to generate the next 1 second, in a typical setting of 480p, 24fps using standard Hunyuan/Wan VAE and patchifying. The history context length becomes $8 3 2 / 1 6 \times 4 8 0 / 1 6 \times 6 0 \times 2 4 / 4 = 5 6 1 , 6 0$ 0, which creates an inference bottleneck for consumer-level devices that have limited GPU memory (and a training bottleneck for clusters even with causal computation or KV-cache designs). A naive sliding window will fail to model long-range consistency. Typical overhead reduction like token merging, downsampling, larger patch sizes, compact VAEs, etc., come with costs of losing image detail at various levels. We observe that a well-suited indicator of a video compression mechanism's ability to preserve contextual details is its capacity for high-quality frame retrieval at arbitrary time positions. For high compression rates, perfect retrieval becomes impractical, leading to a goal to maximize the retrieval quality of arbitrary frames. We formulate this goal in a technical form. Considering that a compression process $\phi ( \cdot )$ receives a history $\pmb { H }$ of

![](images/3.jpg)  
Figure 3. Architecture of memory compression model. We use 3D convolution, SiLU, and attention to establish a lightweight neural structure as the baseline compression model. Different alternative architectures (e.g., various channels, full transformer, etc.) are possible and will be discussed in ablation.

$T _ { h }$ frames and compresses it into a context $\phi ( H )$ , given an arbitrary set of frame indices $\Omega \subseteq \mathbb { N }$ ,and denoting a indexed retrieval process as $\phi ^ { - 1 } ( \cdot , \Omega )$ , we want to minimize the feature distance $\| H _ { \Omega } - \phi ^ { - 1 } ( \phi ( H ) , \Omega ) \| _ { 2 } ^ { 2 }$ for optimized reconstruction.

As shown in Fig. 2, the frame selection $\pmb { \Omega }$ and retrieval $\phi ^ { - 1 } ( \cdot , \Omega )$ can be established as an autoregressive video diffusion framework. We sample random videos as the history, and then pick random frames $\pmb { \Omega }$ : in particular, we randomly keep a subset of frames at indices $\pmb { \Omega }$ unchanged and then mask all remaining frames. Various types of masks can be utilized like range scaling, shifting, etc. Herein, we use a noise-as-mask method: add latent noise levels from $\mathcal { L } ( 0 . 2 , 1 )$ . After that, we clone the picked clean frames as the diffusion target, so that the diffusion system will try to reconstruct the target frames at arbitrary positions. This process can also be written in line with Eq. (2) as where $G _ { \theta } ( \cdot )$ can be a trainable video diffusion model like WAN or HunyuanVideo with LoRAs.

$$
{ { \mathbb E } _ { H , \Omega , c , \epsilon , t _ { i } } } \bigg | \bigg | \big ( \epsilon - H _ { \Omega } \big ) - { { G } _ { \theta } } \big ( \big ( H _ { \Omega } \big ) _ { t _ { i } } , t _ { i } , c , \phi ( H ) \big ) \bigg | \bigg | _ { 2 } ^ { 2 } ,
$$

When the output length of the compression model $\phi ( \cdot )$ is short, e.g., about only $5 \mathrm { k }$ length for every 20 seconds, the retrieval task becomes highly difficult, making it well-suited for pretraining strong context compression models using millions of in-the-wild videos. We also point out the importance of the randomness in frame indices $\pmb { \Omega }$ selection: if the frames are not randomly selected, e.g., only training with fixed starting or ending positions, the framework will learn a cheating solution, e.g., using all $5 \mathrm { k }$ context to encode a few ending frames while ignoring all other frames. Besides, note that directly training to retrieve all frames is impractical with minute-level data, even for highly efficient infrastructures.

Network architecture. As shown in Fig. 3, we present a lightweight baseline architecture for the compression model $\phi ( \cdot )$ . Instead of building the compressed representation from scratch, we reuse the DiT's context representation: we first downsample a high-resolution high-fps video into a lowresolution low-fps one and process it with the VAEs and DiT's patchifiers and first-layer projections. After that, we encode the high-resolution original video into a residualenhancing vector and add it to the context vector. Note that the feature addition happens after the DiT's first-layer projection, which means the encoder does not go through the VAE's 16 channel bottleneck but directly outputs at inner channels (like 3072 for WAN-5B) for best fidelity. The baseline encoder uses 3D convolutions as starting layers and ends with attention layers. Besides, it is also possible to connect the DiT with cross-attentions from the last hidden states of the encoder for another refinement pass, but the quality gain is marginal and will be covered in ablations.

![](images/4.jpg)  
Figure 4. Finetuning autoregressive video models. We illustrate the finetuning and inference of the final autoregressive video models. The pretraining of the memory compression model is finished before the finetuning.

# 3.2. Finetuning Video Diffusion Model

With the pretrained memory compression model $\phi ( \cdot )$ , we can form autoregressive video generation systems by finetuning video diffusion models like WAN (with LoRAs) and the pretrained compression model as history memory encoders. This will lead to an autoregressive video model with long history windows $( e . g . , > 2 0 \mathrm { s } )$ , short history context length (e.g., about $5 \mathrm { k }$ ), and explicitly optimized frame retrieval quality. The diffusion can be written in line with Eq (2) as where the process is as shown in Fig. 4-(a). The inference follows autoregressive generation by repeatedly concatenating the generated sections into the history as shown in Fig. 4-(b). Note that since the baseline encoder (Fig. 3) is almost fully convolutional, the compressed history can also be concatenated on-the-fly without recomputation.

$$
\begin{array} { r } { \mathbb { E } _ { X _ { 0 } , H , c , \epsilon , t _ { i } } \bigg \| ( \epsilon - X _ { 0 } ) - G _ { \theta } \big ( X _ { t _ { i } } , t _ { i } , c , \phi ( H ) \big ) \bigg \| _ { 2 } ^ { 2 } , } \end{array}
$$

# 4. Experiment

# 4.1. Experimental Details

Implementation details. We conduct pretraining with $8 \times \mathrm { H 1 0 0 }$ GPU clusters and LoRAs finetunings with $1 \times$

![](images/5.jpg)  
t   8.

H100s or A100s. This method's context length compression enables efficient training without specialized infrastructure. We use pure PyTorch with Accelerate, with precomputed latents and conditions, and without gradient accumulation. The method achieves a batch size of about 64 on a single $8 \times \mathrm { A 1 0 0 } – 8 0 \mathrm { G }$ node with the 12.8B HunyuanVideo model at 480p resolution LoRA training with window size 2 or 3 (or batch size 32 of window size 4 or 5). This is suitable for personal or laboratory-scale training and experimentation. For the LoRAs in the DiTs, we always use rank 128.

Hyper-parameters. When constructing the encoders of the memory compression models, we use the denotation $H \times$ $W \times T$ to mark the compression rates, e.g., $4 \times 4 \times 2$ means compressing the latent width 4 times, height 4 times, and temporal length 2 times. Note that this compression rate is on top of the latent space and can be multiplied by the latent rate and patchifier rate to compute the final compression rate against the pixel space. When building the 3D convolution layers, we follow the rule to always first reduce the temporal dimension and then the spatial dimension (using strides in convolutions). The hidden channels of the encoder follow the pattern $6 4 \to 1 2 8 \to 2 5 6 \to 5 1 2$ and are then always kept at 512 (e.g., in the final attention layer) before the last $1 \times 1$ convolution, which projects the feature to the same dimensions (like 3072, 5120, etc.) as the DiT context. Base models. All experiments are conducted on base models of HunyuanVideo [36] and Wan [56] family. For Wan 2.2, the involved models are the 5B model and the 14B high noise model. Only the high noise model is used to simplify experiments (similar to the design choice of WanAnimate). Data preparation. The dataset is prepared with about 5 million internet videos from a range of websites. About half of the data is vertical Short-style videos and the rest are common horizontal videos. The data is cleaned for quality, and then the high-quality part is captioned with Gemini-2.5-flash VLM and the remainder is processed with local VLMs like QwenVL [57]. The generated captions are in storyboard format with timestamps. When training the autoregressive models, only the prompt with the nearest timestamp is received by the model. Storyboards. If not noticed, all storyboard prompts in this paper are written by Gemini-2.5-pro. We also tested open source alternatives like Qwen [57] and the quality difference is negligible. See also the supplement for instructions. Test samples. The test set includes 1000 storyboard prompts written by Gemini-2.5-pro and 4096 unseen videos independent from the training dataset.

# 4.2. Ablation Study

Compression model architecture. As shown in Fig. 5, we compare the image reconstruction details of different architectures under various compression rates, focusing on the performance of Only LR, Without LR, Large Patchifier, and our proposed method. The Only LR variant keeps only the low-resolution branch in Fig. 3, while Without LR removes it, leaving only the high-resolution branch. The Large Patchifier method increases the compression rate by enlarging the DiT's patchifying projection kernel without introducing new layers, equivalent to FramePack [78] and we directly use its implementation. The results indicate that the Only LR and Without $L R$ variants produce reconstructions that differ significantly from the original images. The Large Patchifier method yields substantial structural changes. The proposed method, even at a high compression rate such as $4 \times 4 \times 2$ can still preserve much of the image appearance. A lower compression rate, such as $2 \times 2 \times 1$ , provides the best detail preservation, albeit at the cost of a longer context length.

Table 1. Quantitative results on compression structures. We show numerical tests with different ablative compression architectures. \* The "Large Patchifier" is technically equivalent to [78].   

<table><tr><td>Method</td><td>PSNR ↑</td><td>SSIM↑</td><td>LPIPS ↓</td></tr><tr><td>Large Pachifier* (4×4×2)</td><td>12.93</td><td>0.412</td><td>0.365</td></tr><tr><td>Only LR (4×4×2)</td><td>15.21</td><td>0.472</td><td>0.212</td></tr><tr><td>Without LR (4×4×2)</td><td>15.73</td><td>0.423</td><td>0.198</td></tr><tr><td>Proposed (4×4×2)</td><td>17.41</td><td>0.596</td><td>0.171</td></tr><tr><td>Proposed (2×2×2)</td><td>19.12</td><td>0.683</td><td>0.152</td></tr><tr><td>Proposed (2×2×4)</td><td>18.63</td><td>0.637</td><td>0.153</td></tr><tr><td>Proposed (2×2×1)</td><td>20.19</td><td>0.705</td><td>0.121</td></tr></table>

The quantitative results are presented in Table 1. These results show that this method achieves relatively higher performance on metrics like PSNR and SSIM. Furthermore, even at a higher compression rate of $4 \times 4 \times 2$ , this method effectively preserves the original image structure. Influence of pretraining. We demonstrate in Fig. 6 the impact of pre-training by comparing models trained with and without it. We keep the model architecture and the 20- second history input identical. One model is trained from randomly initialized encoder weights, while the other is finetuned using the pretrained memory encoder. Both models are trained with $1 0 0 \mathrm { k }$ sufficient steps. The results show a marked difference. With pre-training, the model maintains strong temporal consistency, leading to notable improvements in facial features, clothing, global video style, storytelling plots, and the coordination of camera movements. In contrast, the model trained without pre-training sometimes fails to attend to relevant frames in the history, resulting in inconsistencies. Error accumulation (drifting). The problem of error accumulation highly depends on the training dataset. Surprisingly, we find that when trained on videos with dense camera shot shifts, like Short-style videos, the error accumulation problem seems to be absent or invisible. For other types of videos requiring long continuation over single shots, multiple design choices [7, 27, 78] are discussed in the supplement.

Table 2. Quantitative results on video content consistency. We present the performance of different possible architectures on content consistency. The "1p" in Qwen means using 1 image as an image model input. Methods with severe artifacts are excluded from human ELO scores.   

<table><tr><td rowspan="2">Method</td><td colspan="2">Human</td><td rowspan="2">Object</td><td rowspan="2">User Study ELO ↑</td></tr><tr><td>Cloth ↑</td><td>Identity ↑</td></tr><tr><td>WanI2V [56] + QwenEdit [61] (1p)</td><td>94.10</td><td>68.45</td><td>85.21</td><td>I</td></tr><tr><td>WanI2V [56] + QwenEdit [61] (2p)</td><td>95.09</td><td>68.22</td><td>91.19</td><td>1198</td></tr><tr><td>WanI2V [56] + QwenEdit [61] (3p)</td><td>94.28</td><td>67.30</td><td>80.34</td><td>1</td></tr><tr><td>Only LR (4×4×2)</td><td>91.98</td><td>69.22</td><td>85.32</td><td>1194</td></tr><tr><td>Without LR (4×4×2)</td><td>89.64</td><td>67.41</td><td>82.85</td><td>,</td></tr><tr><td>Without Pretrain (4×4×2)</td><td>87.12</td><td>66.99</td><td>81.13</td><td>I</td></tr><tr><td>Proposed (4×4×2)</td><td>96.12</td><td>70.73</td><td>89.89</td><td>1216</td></tr><tr><td>Proposed (2×2×2)</td><td>96.71</td><td>72.12</td><td>90.27</td><td>1218</td></tr></table>

# 4.3. Qualitative Results

Storyboard results. As shown in Fig. 7, we demonstrate that our model can handle a diverse range of prompts and storyboards while maintaining consistency across characters, scenes, objects, and plotlines.

# 4.4. Quantitative Results

Metrics. We include several metrics for video evaluations from VBench [29], VBench2 [85], etc., with some modifications. Cloth: The cloth consistency by asking VLM questions. We upgrade VBench2 [85]'s LLaVA to Gemini2.5-pro; Instance: The object-level quality by asking VLM questions about unnatural or inconsistent instances. We upgrade VBench2 [85]'s Qwen to Gemini-2.5-pro; Identity: The facial similarity using ArcFace [16] with face detection RetinaFace [17]; Clarity: The MUSIQ [34] image quality predictor trained on SPAQ [19]. This metric evaluates artifacts like noises; Aesthetic: The LAION aesthetic predictor [51], measuring the aesthetic values perceived by a CLIP estimator; Dynamic: The RAFT [54] modified by VBench to estimate the degree of dynamics. Note that multi-shot videos intuitively have a special RAFT flow between shots, but in practice, RAFT seems to output near-zero values between shots, making the difference negligible from regular videos; Semantic: The video-text score computed by ViCLIP [59]. This metric measures the overall semantic consistency and prompt alignments. Consistency analysis. We compare various architectures of our method, including different compression rates and ablations with or without pre-training and the LR branch. We also compare against a common baseline that combines Wan-2.2-I2V [56] with Qwen-Image-Edit-2509 [61]. This baseline first generates a series of initial frames with an image model, animates them into video clips, and then concatenates the clips. We test this baseline with 1, 2, and 3 images for Qwen-Edit as history contexts (since the model supports at most 3 inputs). Our evaluation focuses on the consistency of clothing, identity, and objects. For several practical and well-performed models, we also conducted a user study and evaluated their ELO scores.

![](images/6.jpg)

![](images/7.jpg)  
whe  pot coverteamTheboar t tel gus.

As shown in Table 2, our proposed method reports rational scores across several consistency metrics. The Wan+Qwen combo appears to have a leading score in instance score, and this is likely because the image model does not significantly alter or move objects, thus avoiding artifacts that are detected by the VLM question answering. Our method demonstrates comparable scores in object consistency. Furthermore, the user study and ELO scores validate our proposed architecture, confirming that it achieves an effective trade-off between compression and quality. Different base models. As shown in Table 3, we also evaluate the performance on different base models, focusing on single-frame quality, temporal dynamics, and overall semantic alignment. The results indicate that the Wan-family models achieve high quality on several metrics. Concurrently, larger models, such as the 14B and 12.8B variants, outperform the 5B model in several metrics. The user scores confirm that the 14B Wan model is a well-suited model.

![](images/8.jpg)  
Figure 8. Adding sliding window. We show that the framework can be combined with a small sliding window to facilitate a continuous shot covering multiple generations.

Table 3. Quantitative results on different base models. We discuss the results using different alternative models as base models. All models are trained as LoRA at rank 128. \* The 14B Wan 2.2 is the high noise model.   

<table><tr><td></td><td colspan="2">Frame</td><td>Temporal</td><td colspan="2">Alignment</td></tr><tr><td>Method</td><td>Aesthetic ↑</td><td>Clarity ↑</td><td>Dynamics ↑</td><td>Semantic ↑</td><td>ELO ↑</td></tr><tr><td>HunyuanVideo [36] 12.8B (4×4×2)</td><td>61.27</td><td>67.49</td><td>71.22</td><td>26.29</td><td>1189</td></tr><tr><td>Wan [56] 2.2 14B* (4×4×2)</td><td>67.22</td><td>69.37</td><td>69.81</td><td>27.12</td><td>1231</td></tr><tr><td>Wan [56] 2.2 5B (4×4×2)</td><td>66.25</td><td>69.01</td><td>65.13</td><td>25.99</td><td>1215</td></tr><tr><td>Wan [56] 2.2 5B (2×2×2)</td><td>66.37</td><td>68.95</td><td>66.29</td><td>26.13</td><td>1224</td></tr></table>

# 4.5. Alternative Architecture

Adding sliding window. As shown in Fig. 8, this method can be combined with a small sliding window (like 3 latent frames) to adjust the occurrence of camera shot shifts. With a sliding window, the frequency of shot changes is reduced in certain cases, allowing the auto-regressive model to generate a continuous shot over multiple generation iterations, resulting in more desirable videos in certain scenarios. For generations without such sliding windows, we recommend generating a complete shot in each generation. Incorporating a sliding window slightly increases the context length. Cross attention enhancement. As shown in Fig. 9, we demonstrate that connecting the features from the penultimate layer of the encoder to each block of the DiT via cross-attention (like IP-Adapter [71]) can further enhance the model's consistency. This is effective in highly challenging scenarios, e.g., maintaining the arrangement order of items on supermarket shelves. In such detail-oriented situations, adding cross-attention provides a rational consistency boost, at the cost of additional computational costs. Combining multiple compression models. As shown in Fig. 10, another enhancement is to use multiple memory encoders simultaneously. For instance, a standard $4 \times 4 \times 2$ encoder can be paired with a $2 \times 2 \times 8$ encoder, which prioritizes temporal compression while preserving more spatial details. Using multiple encoders allows the model to capture different aspects of the history. This architecture offers advantages such as preserving fine-grained details, like text on newspapers, magazines, or billboards, at the cost of an increased context length.

![](images/9.jpg)  
Figure 9. Adding cross-attention enhancing. We show that connecting the compression model and the DiT with cross-attention layers can improve consistency in difficult cases.

![](images/10.jpg)  
Figure 10. Using multiple memory compression models. We show that using multiple compression models at the same time with different compression patterns, e.g., higher temporal and higher spatial, can facilitate detail consistency in difficult cases, at the cost of doubling the context length.

# 5. Conclusion

In this work, we discuss the fundamental trade-off between context length and quality in autoregressive video generation as a critical challenge for creating long, coherent narratives. We introduced a framework pretraining a memory compression model with an explicit objective: high-fidelity retrieval of arbitrary frames from a long video history. This pretraining phase imposes the model to learn a compact yet detailrich representation of the video context, effectively preserving high-frequency details. By subsequently fine-tuning this pretrained encoder within an autoregressive video diffusion system, we developed a practical and efficient model capable of handling long-term dependencies, such as maintaining character identity and scene consistency.

References   
[1] Roman Bachmann, Jesse Allardice, David Mizrahi, Enrico Fini, Ouzhan Fatih Kar, Elmira Amirloo, Alaaeldin ElNouby, Amir Zamir, and Afshin Dehghan. Flextok: Resampling images into 1d token sequences of flexible length. In arXiv, 2025. 1, 3   
[2] Daniel Bolya and Judy Hoffman. Token merging for fast stable diffusion. CVPR Workshop on Efficient Deep Learning for Computer Vision, 2023. 2   
[3] Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Csh  uy Hon T Your vit but faster. In arXiv, 2022. 3   
[4] Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your ViT but faster. In International Conference on Learning Representations, 2023. 2   
[5] Han Cai, Junyan Li, Muyan Hu, Chuang Gan, and Song Han. Efficientvit: Lightweight multi-scale attention for highresolution dense prediction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 17302 17313, 2023. 2   
[6] Shengqu Cai, Ceyuan Yang, Lvmin Zhang, Yuwei Guo, Junfei Xiao, Ziyan Yang, Yinghao Xu, Zhenheng Yang, Alan Yuille, Leonidas Guibas, Maneesh Agrawala, Lu Jiang, and Gordon Wetzstein. Mixture of contexts for long video generation. In arXiv, 2025. 2   
[7] Boyuan Chen, Diego Martí Monsó, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion forcing: Next-token prediction meets full-sequence diffusion. Advances in Neural Information Processing Systems, 37:2408124125, 2025. 2, 6   
[8] Guibin Chen, Dixuan Lin, Jiangping Yang, Chunze Lin, Junchen Zhu, Mingyuan Fan, Hao Zhang, Sheng Chen, Zheng Chen, Chengcheng Ma, Weiming Xiong, Wei Wang, Nuo Pang, Kang Kang, Zhiheng Xu, Yuzhe Jin, Yupeng Liang, Yubing Song, Peng Zhao, Boyuan Xu, Di Qiu, Debang Li, Zhengcong Fei, Yang Li, and Yahui Zhou. Skyreels-v2: Infinite-length film generative model, 2025. 2   
[9] Junyu Chen, Han Cai, Junsong Chen, Enze Xie, Shang Yang, Haotian Tang, Muyang Li, Yao Lu, and Song Han. Deep compression autoencoder for efficient high-resolution diffusion models. arXiv preprint arXiv:2410.10733, 2024. 1   
10] Junsong Chen, Yuyang Zhao, Jincheng Yu, Ruihang Chu, Junyu Chen, Shuai Yang, Xianbang Wang, Yicheng Pan, Daquan Zhou, Huan Ling, et al. Sana-video: Efficient video generation with block linear diffusion transformer. In arXiv, 2025. 3   
11] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020. 2   
12] Justin Cui, Jie Wu, Ming Li, Tao Yang, Xiaojie Li, Rui Wang, Andrew Bai, Yuanhao Ban, and Cho-Jui Hsieh. Selfforcing $^ { + + }$ Towards minute-scale high-quality video generation. In arXiv, 2025. 2 [13] Karan Dalal, Daniel Koceja, Gashon Hussein, Jiarui Xu, Yue Zhao, Youjin Song, Shihao Han, Ka Chun Cheung, Jan Kautz, Carlos Guestrin, Tatsunori Hashimoto, Sanmi Koyejo, Yejin Choi, Yu Sun, and Xiaolong Wang. One-minute video generation with test-time training, 2025. 3 [14] Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. In ICLR, 2024. 3 [15] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness, 2022. 3 [16] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 46904699, 2019.   
6 [17] Jiankang Deng, Jia Guo, Yuxiang Zhou, Jinke Yu, Irene Kotsia, and Stefanos Zafeiriou. Retinaface: Single-stage dense face localisation in the wild, 2019. 6 [18] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. 3 [19] Yuming Fang, Hanwei Zhu, Yan Zeng, Kede Ma, and Zhou Wang. Perceptual quality assessment of smartphone photography. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 36743683, 2020. 6 [20] Google. Ve03.1, 2025. Accessed on November 9, 2025. 1 [21] Yuchao Gu, Weijia Mao, and Mike Zheng Shou. Long-context autoregressive video modeling with next-frame prediction,   
2025.2 [22] Yuwei Guo, Ceyuan Yang, Ziyan Yang, Zhibei Ma, Zhijie Lin, Zhenheng Yang, Dahua Lin, and Lu Jiang. Long context tuning for video generation. In ICCV, 2025. 1 [23] Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, et al. Ltx-video: Realtime video latent diffusion. arXiv preprint arXiv:2501.00103, 2024.   
1, 3 [24] Roberto Henschel, Levon Khachatryan, Daniil Hayrapetyan, Hayk Poghosyan, Vahram Tadevosyan, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Streamingt2v: Consistent, dynamic, and extendable long video generation from text. arXiv preprint arXiv:2403.14773, 2024. 2 [25] Junchao Huang, Xinting Hu, Boyao Han, Shaoshuai Shi, Zhuotao Tian, Tianyu He, and Li Jiang. Memory forcing: Spatio-temporal memory for consistent scene generation on minecraft. In arXiv, 2025. 2 [26] Lianghua Huang, Wei Wang, Zhi-Fan Wu, Yupeng Shi, Huanzhang Dou, Chen Liang, Yutong Feng, Yu Liu, and Jingren Zhou. In-context lora for diffusion transformers. arXiv preprint arXiv:2410.23775, 2024. 1 [27] Xun Huang, Zhengqi Li, Guande He, Mingyuan Zhou, and Eli Shechtman. Self forcing: Bridging the train-test gap in autoregressive video diffusion. In arXiv, 2025. 1, 2, 6 [28] Yuzhou Huang, Ziyang Yuan, Quande Liu, Qiulin Wang, Xintao Wang, Ruimao Zhang, Pengfei Wan, Di Zhang, and Kun Gai. Conceptmaster: Multi-concept video customization on diffusion transformer models without test-time tuning. arXiv preprint arXiv:2501.04698, 2025. 1   
[29] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. VBench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 6   
[30] Weinan Jia, Yuning Lu, Mengqi Huang, Hualiang Wang, Binyuan Huang, Nan Chen, Mu Liu, Jidong Jiang, and Zhendong Mao. Moga: Mixture-of-groups attention for end-to-end long video generation. In arXiv, 2025. 2   
[31] Yuming Jiang, Tianxing Wu, Shuai Yang, Chenyang Si, Dahua Lin, Yu Qiao, Chen Change Loy, and Ziwei Liu. Videobooth: Diffusion-based video generation with image prompts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 66896700, 2024. 1   
[32] Yang Jin, Zhicheng Sun, Ningyuan Li, Kun Xu, Kun Xu, Hao Jiang, Nan Zhuang, Quzhe Huang, Yang Song, Yadong Mu, and Zhouchen Lin. Pyramidal flow matching for efficient video generative modeling, 2024. 1, 3   
[33] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International conference on machine learning, pages 51565165. PMLR, 2020. 2   
[34] Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. Musiq: Multi-scale image quality transformer, 2021.6   
[35] Akio Kodaira, Tingbo Hou, Ji Hou, Masayoshi Tomizuka, and Yue Zhao. Streamdit: Real-time streaming text-to-video generation. In arXiv, 2025. 2   
[36] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiag Xiong, Xin Li Bo Wu, Jia Zha, et al. Hunyuanvideo: A systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 3, 5, 8   
[37] Kuaishou. Kling2.5, 2025. Accessed on November 10, 2025.   
[38] Seon-Ho Lee, Jue Wang, Zhikang Zhang, David Fan, and Xinyu Li. Video token merging for long-form video understanding. In arXiv, 2024. 3   
[39] Runjia Li, Philip Torr, Andrea Vedaldi, and Tomas Jakab. Vmem: Consistent interactive video scene generation with surfel-indexed view memory. In ICCV, 2025. 2   
[40] Xingyang Li, Muyang Li, Tianle Cai, Haocheng Xi, Shuo Yang, Yujun Lin, Lvmin Zhang, Songlin Yang, Jinbo Hu, Kelly Peng, Maneesh Agrawala, Ion Stoica, Kurt Keutzer, and Song Han. Radial attention: ${ \mathcal { O } } ( n \log n )$ sparse attention with energy decay for long video generation. In arXiv, 2025. 3   
[41] Kunhao Liu, Wenbo Hu, Jiale Xu, Ying Shan, and Shijian Lu. Rolling forcing: Autoregressive long video diffusion in real time. In arXiv, 2025. 2   
[42] Fuchen Long, Zhaofan Qiu, Ting Yao, and Tao Mei. Videostudio: Generating consistent-content and multi-scene videos, 2024. 1 [43] Yu Lu and Yi Yang. Freelong $^ { + + }$ : Training-free long video generation via multi-band spectralfusion. In arXiv, 2025. 2 [44] Yu Lu, Yuanzhi Liang, Linchao Zhu, and Yi Yang. Freelong: Training-free long video generation with spectralblend temporal attention. Advances in Neural Information Processing Systems, 37:131434131455, 2025. 2 [45] Yihao Meng, Hao Ouyang, Yue Yu, Qiuyu Wang, Wen Wang, Ka Leong Cheng, Hanlin Wang, Yixuan Li, Cheng Chen, Yanhong Zeng, Yujun Shen, and Huamin Qu. Holocine: Holistic generation of cinematic multi-shot long video narratives. In arXiv, 2025. 2 [46] OpenAI. Sora2, 2025. Accessed on November 8, 2025. 1 [47] Ryan Po, Yotam Nitzan, Richard Zhang, Berlin Chen, Tri Dao, Eli Shechtman, Gordon Wetzstein, and Xun Huang. Long-context state-space video world models. In ICCV, 2025.   
3 [48] Haonan Qiu, Menghan Xia, Yong Zhang, Yingqing He, Xintao Wang, Ying Shan, and Ziwei Liu. Freenoise: Tuning-free longer video diffusion via noise rescheduling. arXiv preprint arXiv:2310.15169, 2023. 2 [49] David Ruhe, Jonathan Heek, Tim Salimans, and Emiel Hoogeboom. Rolling diffusion models, 2024. 2 [50] Nedko Savov, Naser Kazemi, Deheng Zhang, Danda Pani Paudel, Xi Wang, and Luc Van Gool. Statespacediffuser: Bringing long context to diffusion world models. In NeurIPS,   
2025. 3 [51] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:2527825294, 2022. 6 [52] Kiwhan Song, Boyuan Chen, Max Simchowitz, Yilun Du, Russ Tedrake, and Vincent Sitzmann. History-guided video diffusion. arXiv preprint arXiv:2502.06764, 2025. 2 [53] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063,   
2024. 2 [54] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow, 2020. 6 [55] Hansi Teng, Hongyu Jia, Lei Sun, Lingzhi Li, Maolin Li, Mingqiu Tang, Shuai Han, Tianning Zhang, Weifeng Luo, Yuchen Sun, Yue Cao, Yunpeng Huang, Yutong Lin, Yuxin Fang, Zewei Tao, Zheng Zhang, et al. Magi-1: Autoregressive video generation at scale. In arXiv, 2025. 2 [56] Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced largescale video generative models. In arXiv, 2025. 3, 5, 6, 8   
[57] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution. arXiv preint arXiv:2409.12191, 2024. 5   
[58] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020. 2   
[59] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, et al. Internvid: A large-scale video-text dataset for multimodal understanding and generation. In The Twelfth International Conference on Learning Representations, 2023. 6   
[60] Zhou Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli Image quality assessment: From error visibility to structural similarity. IEEE Transactions on Image Processing, 13(4): 600612, 2004. 2   
[61] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, Yuxiang Chen, Zecheng Tang, Zekai Zhang, Zhengyi Wang, An Yang, Bowen Yu, Chen Cheng, Dayiheng Liu, Deqing Li, Hang Zhang, Hao Meng, Hu Wei, Jingyuan Ni, Kai Chen, Kuan Cao, Liang Peng, Lin Qu, Minggang Wu, Peng Wang, Shuting Yu, Tingkun Wen, Wensen Feng, Xiaoxiao Xu, Yi Wang, Yichang Zhang, Yongqiang Zhu, Yujia Wu, Yuxuan Cai, and Zenan Liu. Qwen-image technical report, 2025. 6   
[62] Jianzong Wu, Liang Hou, Haotian Yang, Xin Tao, Ye Tian, Pengfei Wan, Di Zhang, and Yunhai Tong. Vmoba: Mixtureof-block attention for video diffusion models. In arXiv, 2025. 3   
[63] Xiaofei Wu, Guozhen Zhang, Zhiyong Xu, Yuan Zhou, Qinglin Lu, and Xuming He. Pack and force your memory: Long-form and consistent video generation. In arXiv, 2025. 2   
[64] Haocheng Xi, Shuo Yang, Yilong Zhao, Chenfeng Xu, Muyang Li, Xiuyu Li, Yujun Lin, Han Cai, Jintao Zhang, Dacheng Li, et al. Sparse videogen: Accelerating video diffusion transformers with spatial-temporal sparsity. arXiv preprint arXiv:2502.01776, 2025. 2, 3   
[65] Yifei Xia, Suhan Ling, Fangcheng Fu, Yujie Wang, Huixia Li, Xuefeng Xiao, and Bin Cui. Training-free and adaptive sparse attention for effcient long video generation, 2025. 2, 3   
[66] Junfei Xiao, Ceyuan Yang, Lvmin Zhang, Shengqu Cai, Yang Zhao, Yuwei Guo, Gordon Wetzstein, Maneesh Agrawala, Alan Yuille, and Lu Jiang. Captain cinema: Towards short movie generation. In arXiv, 2025. 3   
[67] Zeqi Xiao, Yushi Lan, Yifan Zhou, Wenqi Ouyang, Shuai Yang, Yanhong Zeng, and Xingang Pan. Worldmem: Longterm consistent world simulation with memory. In arXiv, 2025.2   
[68] Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang Li, Ligeng Zhu, Yao Lu, et al. Sana: Efficient high-resolution image synthesis with linear diffusion transformers. arXiv preprint arXiv:2410.10629, 2024. 2, 3   
[69] Shuai Yang, Wei Huang, Ruihang Chu, Yicheng Xiao, Yuyang Zhao, Xianbang Wang, Muyang Li, Enze Xie, Yingcong Chen, Yao Lu, Song Han, and Yukang Chen. Longlive: Realtime interactive long video generation. In arXiv, 2025. 2   
[70] Shuo Yang, Haocheng Xi, Yilong Zhao, Muyang Li, Jintao Zhang, Han Cai, Yujun Lin, Xiuyu Li, Chenfeng Xu, Kelly Peng, et al. Sparse videogen2: Accelerate video generation with sparse attention via semantic-aware permutation. In NeurIPS, 2025. 3   
[71] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arXiv:2308.06721, 2023.8   
[72] Tianwei Yin, Qiang Zhang, Richard Zhang, William T Freeman, Fredo Durand, Eli Shechtman, and Xun Huang. From slow bidirectional to fast causal video generators. arXiv preprint arXiv:2412.07772, 2024. 1, 2   
[73] Jiwen Yu, Jianhong Bai, Yiran Qin, Quande Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Xihui Liu. Context as memory: Scene-consistent interactive long video generation with memory retrieval. In arXiv, 2025. 2   
[74] Weihao Yu, Mi Luo, Pan Zhou, Chenyang Si, Yichen Zhou, Xinchao Wang, Jiashi Feng, and Shuicheng Yan. Metaformer is actually what you need for vision. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1081910829, 2022. 2   
[75] Jintao Zhang, Haofeng Huang, Pengle Zhang, Jia Wei, Jun Zhu, and Jianfei Chen. Sageattention2: Efficient attention with thorough outlier smoothing and per-thread int4 quantization. In ICML, 2025. 3   
[76] Jintao Zhang, Jia Wei, Pengle Zhang, Jun Zhu, and Jianfei Chen. Sageattention: Accurate 8-bit attention for plug-andplay inference acceleration. In International Conference on Learning Representations (ICLR), 2025.   
[77] Jintao Zhang, Chendong Xiang, Haofeng Huang, Jia Wei, Haocheng Xi, Jun Zhu, and Jianfei Chen. Spargeattn: Accurate sparse attention accelerating any model inference. In ICML, 2025. 2, 3   
[78] Lvmin Zhang and Maneesh Agrawala. Packing input frame contexts in next-frame prediction models for video generation. In arXiv, 2025. 1, 3, 5, 6   
[79] Peiyuan Zhang, Yongqi Chen, Haofeng Huang, Will Lin, Zhengzhong Liu, Ion Stoica, Eric Xing, and Hao Zhang. Vsa: Faster video diffusion with trainable sparse attention. In arXiv, 2025. 3   
[80] Peiyuan Zhang, Yongqi Chen, Runlong Su, Hangliang Ding, Ion Stoica, Zhengzhong Liu, and Hao Zhang. Fast video generation with sliding tile attention. In arXiv, 2025. 2, 3   
[81] Tianyuan Zhang, Sai Bi, Yicong Hong, Kai Zhang, Fujun Luan, Songlin Yang, Kalyan Sunkavalli, William T Freeman, and Hao Tan. Test-time training done right. In arXiv, 2025. 3   
[82] Yuechen Zhang, Jinbo Xing, Bin Xia, Shaoteng Liu, Bohao Peng, Xin Tao, Pengei Wan, Eric Lo, and Jiaya Jia. Trainingfree efficient video generation via dynamic token carving. In arXiv. 2025. 3   
[83] Canyu Zhao, Mingyu Liu, Wen Wang, Weihua Chen, Fan Wang, Hao Chen, Bo Zhang, and Chunhua Shen. Moviedreamer: Hierarchical generation for coherent long visual sequence. arXiv preprint arXiv:2407.16655, 2024. 1   
[84] Min Zhao, Guande He, Yixiao Chen, Hongzhou Zhu, Chongxuan Li, and Jun Zhu. Riflex: A free lunch for length extrapolation in video diffusion transformers. In arXiv, 2025. 2   
[85] Dian Zheng, Ziqi Huang, Hongbo Liu, Kai Zou, Yinan He, Fan Zhang, Yuanhan Zhang, Jingwen He, Wei-Shi Zheng, Yu Qiao, and Ziwei Liu. VBench-2.0: Advancing video generation benchmark suite for intrinsic faithfulness. arXiv preprint arXiv:2503.21755, 2025. 6   
[86] Mingzhe Zheng, Yongqi Xu, Haojian Huang, Xuran Ma, Yexin Liu, Wenjie Shu, Yatian Pang, Feilong Tang, Qifeng Chen, Harry Yang, et al. Videogen-of-thought: A collaborative framework for multi-shot video generation. arXiv preprint arXiv:2412.02259, 2024. 1   
[87] Yupeng Zhou, Daquan Zhou, Ming-Ming Cheng, Jiashi Feng, and Qibin Hou. Storydiffusion: Consistent self-attention for long-range image and video generation. Advances in Neural Information Processing Systems, 37:110315110340, 2024. 1   
[88] Ziqin Zhou, Yifan Yang, Yuqing Yang, Tianyu He, Houwen Peng, Kai Qiu, Qi Dai, Lili Qiu, Chong Luo, and Lingqiao Liu. Hitvideo: Hierarchical tokenizers for enhancing text-tovideo generation with autoregressive large language models. arXiv preprint arXiv:2503.11513, 2025. 1