# Dream to Recall: Imagination-Guided Experience Retrieval for Memory-Persistent Vision-and-Language Navigation

Yunzhe Xu, Yiyuan Pan, Zhe Liu A— a va iae improvements across all scenarios, with $5 . 4 \%$ SPL gains on IR2R over the best memory-persistent baseline, accompanied by $8 . 3 \times$ training speedup and $74 \%$ inference memory reduction. The results validate that predictive retrieval of both environmental and behavioral memories enables more effective navigation, with analysis indicating substantial headroom $7 3 . 3 \%$ vS $9 3 . 4 \%$ upper bound) for this imagination-guided paradigm. Code is available at https://github.com/xyz9911/Memoir. Index Terms—Vision-and-language navigation, embodied intelligence, memory mechanisms, world models.

# 1 INTRODUCTION a cornerstone challenge in embodied AI, requiring agents to interpret natural language instructions and navigate through environments to reach specified goals. The fundamental episodic nature of traditional VLN tasks [1], [2], [3], where agents operate independently across episodes without retaining experiential knowledge, limits their capacity for progressive improvement and environmental adaptation, constraining real-world applicability where sustained operation is essential. This limitation has motivated the development of memory-persistent navigation tasks [4], [5], [6] that evaluate agents' ability to accumulate and leverage experience across multiple navigation episodes. These tasks more accurately reflect practical application scenarios where robotic agents must continuously improve their navigation capabilities through environmental familiarity and learned behavioral patterns.

Recent advances in memory-persistent VLN have primarily focused on long-term memory mechanisms for progressive scene knowledge accumulation. Early approaches employed strategies such as episodic history stacking [5], but simply extending history suffers from redundancyinduced performance degradation. Subsequent work [8] addressed this by augmenting visual representations with broader spatial horizons rather than incorporating navigation histories, while OVER-NAV [7] leverages openvocabulary detection to construct multimodal topological graphs that strengthen keyword-observation correspondence. Most recently, GR-DUET [6] enhanced the DUET architecture [9] with retained topological observation memory, achieving strong performance in VLN scene adaptation. Despite these advances, existing approaches exhibit two critical limitations. First, current approaches lack effective memory access mechanisms, instead relying on either complete memory incorporation (leading to irrelevant information integration and computational overhead) or fixedhorizon spatial lookup (risking valuable experience loss). Second, navigation behavioral histories contain valuable decision-making patterns regarding how agents interpreted instructions and selected actions across different scenarios. However, existing memory-persistent VLN methods either ignore them entirely or, when attempted [5], fail to effectively leverage this information.

How can agents effectively determine which memories to access in order to leverage navigation experiences? Human navigators naturally engage in mental imagination of navigation routes [10] and future travel events [11], consulting experiences to finalize decisions based on mental simulations [12], highlighting that imagination serves as a query mechanism—agents can predict where they might navigate and retrieve relevant past experiences matching those predicted states. This paradigm differs from traditional imagine-planning approaches [13] that generate trajectories in isolation; instead, imagination is grounded by querying explicit long-term memory, ensuring retrieved experiences directly inform decision-making while avoiding hallucination. To this end, we propose Model-based Hybrid Viewpoint-Level Memory for Experience Retrieval (Memoir), an agent that employs predictive world modeling for memory retrieval at viewpoint granularity. Our approach addresses the aforementioned limitations through a unified framework. First, adaptive retrieval is achieved by using imagined future states as queries, avoiding both complete memory incorporation and fixed-horizon lookup. Second, behavioral pattern preservation is enabled by encoding navigation histories into latent states that capture decisionmaking strategies with viewpoint-level anchoring. Figure 1 illustrates Memoir's workflow.

![](images/1.jpg)  
. approach adaptively retrieves both observation and histories for navigation planning through imagination.

Realizing this imagination-guided paradigm requires addressing three key challenges: how to generate predictive queries, what to store and retrieve, and how to integrate retrieved knowledge for navigation. Memoir tackles these through a unified framework. 1) A languageconditioned world model learns to imagine future navigation states conditioned on instructions. These imagined states serve dual purposes: encoding current navigation experiences into latent representations for storage, and generating queries to retrieve similar past experiences through compatibility matching. 2) Hybrid Viewpoint-Level Memory (HVM) maintains this accumulated knowledge by anchoring both environmental observations and behavioral patterns to viewpoints, enabling retrieval of not just what agents saw, but how they navigated. 3) The navigation model then processes current observations alongside retrieved experiences through specialized encoders to make informed decisions. This enables adaptive memory access that preserves strategic knowledge across episodes. We implement Memoir on various VLN methods, validating its effectiveness across memory-persistent benchmarks with 10 distinctive testing scenarios. Memoir demonstrates consistent improvements, achieving $5 . 4 \%$ improvement in SPL on IR2R [5], accompanied by $8 . 3 \times$ training speedup and $7 4 \%$ inference memory reduction. We also reveal substantial headroom $( 7 3 . 3 \%$ vs $9 3 . 4 \%$ upper bound) for this paradigm and illuminate future directions. Our key contributions include: We propose a novel paradigm where imagination serves as a retrieval mechanism grounded by explicit memory, addressing two fundamental limitations in memory-persistent VLN: ineffective memory access via complete incorporation or fixed-horizon lookup, and exclusive focus on environmental observations while neglecting behavioral histories that encode decision-making patterns. This enables adaptive filtering of both environmental and behavioral experiences based on navigation intent.   
We develop Memoir, a unified framework where a language-conditioned world model encodes navigation histories for storage and generates imagined trajectories as retrieval queries, Hybrid Viewpoint-Level Memory (HVM) maintains both observations and behavioral patterns at viewpoint granularity, and an experience-augmented navigation model integrates retrieved information for robust decision-making. Through extensive evaluation across 10 diverse memory-persistent scenarios, we demonstrate consistent improvements with significant efficiency benefits, while oracle analysis reveals substantial headroom, illuminating promising directions for advancing this paradigm.

# 2 RELAtED WOrk

# 2.1 Vision-and-Language Navigation

Vision-and-Language Navigation (VLN) [1], [2], [3] requires agents to follow natural language instructions while navigating toward target destinations. Single-episode VLN research has evolved through data augmentation approaches from speaker models [14], [15], [16] to synthetic data [17], [18] using Large Language Models (LLMs), and memory architectures progressing from historical representations [19], [20] to structured spatial systems, particularly topological observation memory [9], [21] which has been widely adopted. However, these single-episode approaches cannot accumulate knowledge across episodes. Memorypersistent VLN benchmarks [5], [6] address real-world requirements where agents should operate continuously and improve through accumulated experience. TourHAMT [5] extends historical memory by stacking complete navigation sequences, but suffers performance degradation from excessive redundancy. ESceme [8] enhances environmental observations with broader spatial contexts, while OVER-NAV [7] constructs omni-graphs with fixed-distance retrieval. MAPCMA [5] builds global semantic maps augmented with fixed-horizon egocentric perception. GR-DUET [6] retains complete topological memory, achieving performance gains at computational cost. These approaches share fundamental limitations: reliance on complete memory incorporation or fixed-horizon lookup, and exclusive focus on environmental observations while neglecting navigation behavioral patterns that encode decision-making strategies across contexts. Our work addresses these limitations through imaginationguided memory retrieval that selectively accesses both environmental and behavioral histories.

# 2.2 Memory Mechanism

Memory mechanisms in navigation systems encompass two primary types that serve complementary roles in spatial reasoning. Navigation history memory captures temporal decision-making patterns and behavioral context through sequence representations [19], [22] or natural language expression [23], [24], preserving how agents make decisions across different scenarios. Environmental observation memory preserves spatial information through structured representations such as occupancy maps [25], [26], semantic maps [27], [28], bird's-eye view representations [29], [30], and topological memory [31], [32], [33], maintaining spatial layouts and visual features for scene understanding. In memory-persistent scenarios where agents must accumulate knowledge across diverse experiences, current approaches [5], [6] treat these information sources separately. Environmental observations alone cannot encode the behavioral reasoning underlying navigation decisions, while navigation histories without spatial anchoring cannot disambiguate similar patterns across different environments. This separation limits knowledge transfer across navigation scenarios, motivating our unified memory that leverage both spatial and temporal historical information.

# 2.3 World Model

Predictive world models have demonstrated significant impact in model-based reinforcement learning through POMDP solutions via latent dynamics modeling [34], [35]. The Recurrent State-Space Model (RSSM) [34] represents the dominant architecture, with extensions to language conditioning [36] and large-scale pretraining [37]. Contrastive world models [38], [39] offer computational efficiency without observation reconstruction. In navigation domains, world models [40] serve diverse purposes: future observation synthesis for data augmentation [41], [42], trajectory planning through imagination [13], [43], [44], [45], and auxiliary task formulation [46], [47]. However, the integration of world models with memory for grounded imagination remains underexplored. MBEC [48] pioneered imagination-guided memory retrieval in episodic control, but is limited to episodic history during training rather than spatial-temporal retrieval during inference. Our approach extends this paradigm by unifying world modeling with experience retrieval for navigation reasoning, where the world model both encodes histories for storage and generates imagined states for spatial-temporal retrieval.

# 3 PReLIMInARiES

# 3.1 VLN Formulation

Vision-and-Language Navigation (VLN) requires an agent to follow instructions and navigate towards target. The environment is represented as a connectivity graph $\mathcal { G } = ( \nu , \mathcal { E } )$ , where $\nu$ denotes navigable viewpoints and $\mathcal { E }$ represents traversable edges connecting adjacent viewpoints.

Single-Episode VLN Formulation. In the traditional episodic setting, an agent receives a natural language instruction $\ell$ and is initialized at a starting viewpoint $v _ { 1 } \in \mathcal { V }$ . At each timestep $t ,$ the agent observes a panoramic obseration $o _ { t } = \{ o _ { t } ^ { ( i ) } \} _ { i = 1 } ^ { 3 6 }$ cris  ioal horizontal viewing angles, each captured at three elevation levels (upward, horizontal, downward). The agent's action space at viewpoint $v _ { t }$ includes navigation to any neighboring viewpoint $v _ { j } \in \mathcal { N } ( v _ { t } )$ and a terminal stop action, where $\mathcal { N } ( v _ { t } ) = \{ v _ { j } \in \mathcal { V } : ( v _ { t } , v _ { j } ) \in \mathcal { E } \}$ denotes the set of adjacent viewpoints. The episode terminates when the agent executes a stop action or reaches a maximum step limit $T _ { \mathrm { m a x } }$ .

Memory-Persistent VLN Formulation. While traditional VLN effectively evaluates basic instruction-following capabilities, it fails to capture the requirements of progressive improvement during persistent operation. Memorypersistent adrese $\mathcal { M } = \{ ( \ell ^ { ( k ) } , \mathcal { G } ^ { ( k ) } , \mathcal { O } ^ { ( k ) } , \mathcal { A } ^ { ( k ) } ) \} _ { k = 1 } ^ { \vee }$ that accumulates experiential knowledge across multiple episodes, where for $k$ th episode, $\ell ^ { ( k ) }$ is the instruction, $\begin{array} { r c l } { \dot { \mathcal { G } } ^ { ( k ) } } & { = } & { ( \mathcal { V } ^ { ( k ) } , \mathcal { E } ^ { ( k ) } ) } \end{array}$ is the observed subgraph after $k$ episodes, O(k) = {o(k)rte and $\mathcal { A } ^ { ( k ) } = \{ a _ { t } ^ { ( k ) } \} _ { t = 1 } ^ { \bullet }$ records observations and actions respectively. The bank $\mathcal { M }$ is incrementally updated in each episode and serves as a persistent repository for decisions, enabling progressive performance improvement through accumulated environmental familiarity and learned behavioral patterns.

# 3.2 Dual-Scale Graph Transformer (DUET)

DUET [9] enables topological navigation through topological mapping and global action planning.

Topological Mapping. The agent maintains an incrementally constructed topological representation $\mathcal { G } _ { t } ~ = ~ ( \nu _ { t } , \mathcal { E } _ { t } )$ of the explored environment, where $\mathcal { G } _ { t } ~ \subseteq ~ \mathcal { G }$ represents the observed subset after $t$ navigation steps. The viewpoint set $\nu _ { t }$ is partitioned into three categories: visited viewpoints, frontier viewpoints (observable but unvisited neighbors), and the current viewpoint. At each timestep $t ,$ the topological graph is updated by incorporating the current viewpoint $v _ { t }$ and its navigable neighbors $\check { \mathcal { N } } ( v _ { t } )$ into representations , with corresponding edge updates to $r _ { t } ~ = ~ \{ r _ { t } ^ { ( i ) } \} _ { i = 1 } ^ { 3 6 }$ ar computed through an Visual observation encoder applied to . The visual representation of the current viewpoint $x _ { t }$ is obtained via average pooling of $\boldsymbol { r } _ { t } ,$ while each unvisited neighboring viewpoint $\bar { v } _ { j } \in \bar { N } ( v _ { t } )$ is represented by its corresponding directional embedding r $r _ { t } ^ { ( i _ { j } ) }$ where $i _ { j }$ denotes the view index oriented toward $v _ { j }$ . For viewpoints observed from multiple locations, embeddings are averaged to maintain consistency.

![](images/2.jpg)  
F respectively to determine the final action.

Global Action Planning. DUET combines coarse-scale planning over the topological graph with fine-scale planning over immediate neighbors. The instruction $\ell$ is processed through a transformer to obtain textual representations $\hat { \ell } .$ For coarse-scale planning, node representations $x _ { j }$ for viewpoints $\boldsymbol { v } _ { j } ~ \in ~ \mathcal { V } _ { t }$ are augmented with a special stop token $x _ { 0 }$ . The coarse-scale encoder processes the instruction embedding $\hat { \ell }$ and viewpoint representations $\begin{array} { r l } { X } & { { } = } \end{array}$ $[ x _ { 0 } , x _ { 1 } , \ldots , x _ { | \mathcal { V } _ { t } | } ]$ through cross-modal attention and GraphAware Self-Attention (GASA): states. Given an observation sequence $( o _ { 1 } , o _ { 2 } , \ldots , o _ { T } ) _ { \scriptscriptstyle { \cdot \cdot \cdot } }$ , the world model operates on latent states $z _ { t }$ that capture environmental dynamics. The joint distribution factorizes as:

$$
p ( o , z ) = \prod _ { t = 1 } ^ { T } p ( z _ { t } \mid z _ { t - 1 } ) p ( o _ { t } \mid z _ { t } ) .
$$

To maximize the observation likelihood $p { \big ( } o _ { 1 : T } )$ , the model introduce a variational posterior $q { \big ( } z _ { 1 : T } | o _ { 1 : T } { \big ) }$ and derive the evidence lower bound (ELBO) [49]:

$$
\begin{array} { r l } & { \ln p ( o ) \geq \displaystyle \sum _ { t = 1 } ^ { T } \Big ( \mathbb { E } _ { q ( z _ { t } \mid o _ { \leq t } ) } \big [ \underbrace { \ln p ( o _ { t } \mid z _ { t } ) } _ { \mathcal { I } _ { \mathrm { R E C O V E R } } } \big ] } \\ & { \qquad - \mathbb { E } _ { q ( z _ { t - 1 } \mid o _ { \leq t } ) } \big [ \underline { { \mathrm { K L } \big [ q ( z _ { t } \mid o _ { \leq t } ) \big \| \ p ( z _ { t } \mid z _ { t - 1 } ) \big ] } } \big ] \Big ) . } \end{array}
$$

To empower the model with discriminative power while avoiding pixel-level reconstruction, the term $\mathcal { I } _ { \mathrm { R E C O V E R } }$ is replaced with a contrastive objective [35], [39]. Following the information-theoretic derivation, we can lower-bound TRECovER using noise-contrastive estimation (NCE) [50]:

$$
\mathrm { G A S A } ( X ) = \mathrm { S o f t m a x } \left( \frac { X W _ { q } ( X W _ { k } ) ^ { T } } { \sqrt { d } } + M \right) X W _ { v } ,
$$

where the distance encoding matrix $M = E W _ { e } + b _ { e }$ incorporates the pairwise distance matrix $E$ For fine-scale planning, the fine-scale encoder processes the instruction $\hat { \ell }$ and panoramic features $r _ { t }$ to generate action scores for immediate neighbors $\mathcal { N } ( v _ { t } )$ . The final navigation decision combines both scales through learned dynamic weighting, producing action scores for each candidate viewpoint.

# 3.3 Contrastive Variational World Model

World models provide latent representations of environment dynamics, enabling efficient inference about future where $\mathcal { D }$ represents a mini-batch of negative samples. This contrastive objective trains the model to distinguish between correct state-observation pairs $\left( \boldsymbol { z } _ { t } , \boldsymbol { o } _ { t } \right)$ and incorrect pairs $( z _ { t } , o ^ { \prime } )$ , effectively learning representations that capture environmental detail without explicit reconstruction.

$$
\begin{array} { r l } & { \mathcal { T } _ { \mathrm { R E C O V E R } } \geq \mathbb { E } _ { q ( \boldsymbol { z } _ { t } | \cdot ) } \Bigg [ \ln p ( \boldsymbol { z } _ { t } \mid \boldsymbol { o } _ { t } ) - \ln \sum _ { \boldsymbol { o } ^ { \prime } \in \mathcal { D } } p ( \boldsymbol { z } _ { t } \mid \boldsymbol { o } ^ { \prime } ) \Bigg ] } \\ & { \qquad = \mathcal { I } _ { \mathrm { N C E } } , } \end{array}
$$

# 4 MEMOIR

This section presents Memoir, a memory-persistent VLN agent that employs world model imagination for adaptive TABLE 1 Key notation summary.   

Algorithm 1: Environmental Observation Retrieval 2 for $i \gets 1$ to $| \tau _ { t } |$ do   
3 Initialize $\mathcal { R } _ { \mathrm { t m p } }  \emptyset$   
4 Get $i$ -th order neighbors $\mathcal { N } _ { i } ( v _ { t } )$ from $\mathcal { G } ^ { ( k ) }$   
5 for each vierpoint $v _ { n } \in \mathcal { N } _ { i } ( v _ { t } )$ do   
6 Extract imagined state $\hat { z } _ { t + i }$ from $\tau _ { t }$   
7 Compute compatibility $c _ { i , n }$ via Equation (10)   
8 Add $( v _ { n } , c _ { i , n } )$ to ${ \mathcal { R } } _ { \mathrm { t m p } }$   
9 Sort ${ \mathcal { R } } _ { \mathrm { t m p } }$ by score $c _ { i , n }$ in descending order   
10 Retain top $( 1 - \rho _ { o } \cdot \gamma _ { o } ^ { i - 1 } )$ fraction of ${ \mathcal { R } } _ { \mathrm { t m p } }$   
11 Keep top $W$ nodes in ${ \mathcal { R } } _ { \mathrm { t m p } }$   
12 for each $( v _ { n } , c _ { i , n } ) \in \mathcal { R } _ { t m p }$ do   
13 Find shortest path $P _ { t , n }$ from $v _ { t }$ to $v _ { n }$ in $\mathcal { G } ^ { ( k ) }$   
14 Add path viewpoints: $\mathcal { R }  \mathcal { R } \cup P _ { t , n }$

<table><tr><td>Notation</td><td>Description</td></tr><tr><td>Gt = (Vt, εt)</td><td>Episodic graph at step t (viewpoints, edges)</td></tr><tr><td>G(k) = (V(k), ε(k))</td><td>Persistent graph accumulated over k episodes</td></tr><tr><td>l,l</td><td>Natural language instruction &amp; embedding</td></tr><tr><td>}i=1, rt = {r(i) ,(i)}36 }i=1</td><td>Panoramic observation &amp; features (36 views)</td></tr><tr><td>xt = AvgPool(rt)</td><td>Viewpoint feature via average pooling</td></tr><tr><td>γt, </td><td>Reward signal (distance to goal), stop threshold</td></tr><tr><td>zt, 2t</td><td>Inferred state &amp; imagined state</td></tr><tr><td>ψs, ψo</td><td>State &amp; observation embedding functions</td></tr><tr><td>τt = {zt+i}i=1 lHt</td><td>Imagined trajectory with horizon Ht</td></tr><tr><td>D</td><td>Overshooting dist. and max imagination horizon</td></tr><tr><td>Mo = (Vo, χ)</td><td>Observation bank (viewpoints, features)</td></tr><tr><td>Mh = (Vh, Zh, Th)</td><td>History bank (viewpoints, states, trajectories)</td></tr><tr><td>ci,j , </td><td>Compatibility scores for retrieval</td></tr><tr><td>W , P</td><td>Max width for obs. &amp; max patterns for history</td></tr><tr><td>ρo, γo</td><td>Filter rate &amp; decay factor for obs. retrieval</td></tr><tr><td>θh, Yh</td><td>Base threshold &amp; decay factor for history retrieval</td></tr><tr><td>σc, σf , σh</td><td>Fusion weights for navigation model encoders</td></tr><tr><td>e(c) (f) , sj (h) sj</td><td>Action scores (coarse, fine, history)</td></tr></table>

To adapt the basic contrastive world model that focus solely on environmental dynamics [35] for VLN task, our approach explicitly incorporates instruction conditioning to leverage the strong prior knowledge inherent in VLN tasks. We extend the standard ELBO formulation by incorporating instruction $\ell$ and reward signal $\gamma _ { t }$ (indicating distance to goal):

$$
\begin{array} { l } { { \displaystyle \ln p ( o , \gamma \mid \ell ) \geq \sum _ { t = 1 } ^ { T } \Big ( \mathbb { E } _ { q ( z _ { t } \mid o _ { \le t } , \ell ) } \Big [ \underline { { \ln p ( \gamma _ { t } \mid z _ { t } ) } } \Big ] } \ ~ } \\ { { \displaystyle ~ + \mathbb { E } _ { q ( z _ { t } \mid o _ { \le t } , \ell ) } \Big [ \underline { { \ln p ( z _ { t } \mid o _ { t } ) - \ln \sum _ { o ^ { \prime } \in \mathcal { D } } p ( z _ { t } \mid o ^ { \prime } ) } } \Big ] } \ ~ } \\ { { \displaystyle ~ - \mathbb { E } _ { q ( z _ { t - 1 } \mid o _ { \le t } , \ell ) } \Big [ \underline { { \mathrm { K L } \big [ q ( z _ { t } \mid o _ { \le t } , \ell ) \big ] \mid p \big ( z _ { t } \mid z _ { t - 1 } \big ) } } \Big ] \Big ] } \Big ) , }  \end{array}
$$

where TrewARD encourages accurate goal proximity prediction for imagination termination. The negative sample set $\mathcal { D }$ comprises observations from different timesteps and episodes within each training batch. The contrastive term $\mathcal { T } _ { \mathrm { N C E } }$ is implemented through a learnable compatibility function that measures semantic similarity between latent states and visual observations: 15 for each viewpoint $v _ { n } \in \mathcal { R }$ do

16 Retrieve feature $x _ { n }$ from $\mathcal { M } _ { o }$ for viewpoint $v _ { n }$   
17 Retrieve edges $E _ { n }$ from $\mathcal { G } ^ { ( k ) }$ for $v _ { n }$   
18 Update episodic graph: $\mathcal { G } _ { t }$ .update $( v _ { n } , E _ { n } )$   
19 Store feature $x _ { n }$ for viewpoint $v _ { n }$ experience retrieval. As illustrated in Figure 2, our approach comprises three components: a language-conditioned contrastive world model that encodes histories and imagines future states as retrieval queries, a Hybrid ViewpointLevel Memory (HVM) that stores both environmental observations and navigation histories for retrieval, and an experience-augmented navigation model integrating retrieved knowledge for navigation planning. To facilitate reading, we list the crucial notations in Memoir in Table 1.

# 4.1 Language-Conditioned World Model

1 Initialize retrieval set $\mathcal { R }  \emptyset$ 20 return updated episodic graph $\mathcal { G } _ { t }$ where $x _ { t }$ represents the visual feature extracted through DUET's observation encoder via averge pooling, $\psi _ { s }$ and $\psi _ { o }$ are learned embedding functions that map states and observations to a shared embedding space, $\begin{array} { r } { \dot { \sin ( a , b ) } = \frac { a ^ { \top } b } { \| a \| \| b \| } } \end{array}$ denotes cosine similarity, and $\zeta$ denotes temperature parameter. This formulation enables principled assessment of compatibility between imagined states and observations stored in long-term memory, providing a foundation for similarity-based memory retrieval.

$$
\begin{array} { l } { { f ( z _ { t } , o _ { t } ) = \displaystyle \frac { 1 } { \zeta } \sin ( \psi _ { s } ( z _ { t } ) , \psi _ { o } ( x _ { t } ) ) } } \\ { { p ( z _ { t } \mid o _ { t } ) \propto \exp ( f ( z _ { t } , o _ { t } ) ) , } } \end{array}
$$

To improve the model's long-horizon predictive capability and enhance memory retrieval quality, we extend the ELBO formulation with multi-step overshooting. The $d .$ step overshooting objective encourages accurate prediction over extended horizons:

$$
\begin{array} { r } { \mathcal { I } ^ { ( d ) } = \displaystyle \sum _ { t = 1 } ^ { T } \Big ( \mathbb { E } _ { p ( z _ { t } \mid z _ { t - d + 1 } ) q ( z _ { t - d + 1 } \mid \cdot ) } \big [ \underbrace { \ln p \big ( \gamma _ { t } \mid z _ { t } \big ) } _ { \mathcal { I } \mathrm { S T o p } } \big ] + } \\ { \mathbb { E } _ { p ( z _ { t } \mid z _ { t - d + 1 } ) q ( z _ { t - d + 1 } \mid \cdot ) } \big [ \underbrace { \ln p \big ( z _ { t } \mid o _ { t } \big ) - \ln \sum _ { o ^ { \prime } } p \big ( z _ { t } \mid o ^ { \prime } \big ) } _ { \mathcal { I } _ { \mathrm { N C E } } } - } \\ { \mathbb { E } _ { p ( z _ { t - 1 } \mid z _ { t - d } ) q ( z _ { t - d } \mid \cdot ) } \big [ \mathrm { K L } \big [ q ( z _ { t } \mid o _ { \le t } , \ell ) \mid \mid p \big ( z _ { t } \mid z _ { t - 1 } \big ) \big ] \big ] \Big ) . } \end{array}
$$

With maximum overshooting distance $D$ , the final optimization objective becomes:

$$
\mathcal { T } = \mathcal { T } ^ { ( 1 ) } + \frac { 1 } { D - 1 } \sum _ { d = 2 } ^ { D } \mathcal { T } ^ { ( d ) } .
$$

To efficiently optimize the objective in Equation (8), we adopt the Recurrent State-Space Model (RSSM) architecture [34], comprising four components: Inference Model: $\begin{array} { r l } & { z _ { t } \sim q ( z _ { t } \mid z _ { t - 1 } , o _ { t } , \ell ) } \\ & { \hat { z } _ { t } \sim p ( z _ { t } \mid z _ { t - 1 } ) } \\ & { p ( z _ { t } \mid o _ { t } ) \propto \exp ( f ( z _ { t } , o _ { t } ) ) } \\ & { \hat { \gamma } _ { t } \sim p ( \gamma _ { t } \mid z _ { t } ) , } \end{array}$   
Transition Model:   
Compatibility Model:   
Reward Model: where in practice the inference model takes $x _ { t }$ as input for observation, and $\hat { \ell }$ as input for instruction. The inference model encodes navigation histories into representations for storage, and the transition model generates imagined future states that facilitate similarity-based memory retrieval.

# 4.2 Hybrid Viewpoint-Level Memory (HVM)

Having established how our world model imagine and infer states, we now describe how the imagined states query long-term memory. We introduce a dual-bank memory architecture that maintains both environmental observations and navigation behavioral histories at viewpoint granularity. HVM comprises two complementary banks organized around a persistent graph $\mathcal { G } ^ { ( k ) } = ( \mathcal { V } ^ { ( k ) } , \mathcal { \overline { { E } } } ^ { ( k ) } )$ accumulated over $k$ episodes:

Observation Bank: $\mathcal { M } _ { o } = ( \mathcal { V } _ { o } , \mathcal { X } _ { o } )$ , where $\mathcal { V } _ { o } = \{ v _ { j } \}$ ts he stof fdeoins and $\mathcal { X } _ { o } =$ $\{ x _ { j } \} _ { j = 1 } ^ { | \nu _ { o } | }$ History Bank: $M _ { h } \ = \ ( \mathcal { V } _ { h } , \mathcal { Z } _ { h } , \mathcal { T } _ { h } ) ,$ where $\begin{array} { r l } { ) _ { h } } & { { } = } \end{array}$ $\{ v _ { j } \}$ denotes viewpoints with recorded navigation hhistories, $\begin{array} { r c l } { \mathcal { Z } _ { h } } & { = } & { \{ \{ z _ { j } ^ { ( k ) } \} _ { k = 1 } ^ { N _ { j } } \} _ { j = 1 } ^ { | \mathcal { V } _ { h } | } } \end{array}$ stores inferred agent states from past episodes, and $\begin{array} { l l } { \mathcal { T } _ { h } } & { = } \end{array}$ $\{ \{ \tau _ { j } ^ { ( k ) } \} _ { k = 1 } ^ { N _ { j } } \} _ { j = 1 } ^ { | \mathcal { V } _ { h } | }$ contains corresponding imagined trajectory sequences, where $N _ { j }$ denotes the number of historical visits to viewpoint $v _ { j }$ . At each timestep $t ,$ both memory banks are updated based on $\boldsymbol { v } _ { t } \colon \boldsymbol { \mathcal { M } } _ { o }$ receives the viewpoint feature $x _ { t }$ extracted from observation $o _ { t } ,$ while $\mathcal { M } _ { h }$ stores the inferred state $z _ { t }$ $\dot { \tau _ { t } } = \{ \hat { z } _ { t + i } \} _ { i = 1 } ^ { H _ { t } }$ from the inference model and the imagined trajectory tion (9). $H _ { t }$ represents the imagination horizon, terminating when the predicted distance $\hat { \gamma } _ { t + i }$ falls below threshold $\epsilon$ or reaches maximum horizon $D$ . Environmental Observation Retrieval. Given an imagined ttrajectory $\tau _ { t } ~ = ~ \{ \hat { z } _ { t + i } \} _ { i = 1 } ^ { H _ { t } }$ at vipoint $v _ { t } ,$ we reertrieve observations through topology-guided searching via state-observation compatibility. For imagined state $\hat { z } _ { t + i }$ and stored feature $x _ { j }$ at viewpoint $v _ { j }$ from $\mathcal { M } _ { o } ,$ we compute a compatibility score:

$$
c _ { i , j } = \frac { 1 } { 2 } ( \sin ( \psi _ { s } ( \hat { z } _ { t + i } ) , \psi _ { o } ( x _ { j } ) ) + 1 ) .
$$

This scoring mechanism directly leverages the contrastive objective from Equation (6), ensuring consistency between training and retrieval. For each imagination step $i$ and corresponding neighborhood order, the algorithm identifies all viewpoints in the $i$ -th order neighborhood $\mathcal { N } _ { i } ( v _ { t } ) = \{ v \in \mathcal { V } ^ { ( k ) } : d ( v _ { t } , v ) = i \}$ and computes compatibility scores using Equation (10). Percentile-based filtering retains the top $( 1 - \rho _ { o } \cdot \gamma _ { o } ^ { i - 1 } )$ fraction of viewpoints ranked by score, followed by selecting the top- $W$ viewpoints from 20 return updated episode graph $\mathcal { G } _ { t }$ the retained set. Finally, shortest paths from $v _ { t }$ to all selected viewpoints are added to the episodic graph $\mathcal { G } _ { t }$ . The complete procedure is detailed in Algorithm 1.

<table><tr><td colspan="3">Algorithm 2: Navigation History Retrieval</td></tr><tr><td colspan="3">Input: P Max Patterns Current Viewpoint θh Threshold Tt Imagined States γh Decay Factor Mh History Bank G(k) Persistent Graph Gt Episodic Graph</td></tr><tr><td></td><td>2 for each (z′, τ′)  Q do</td><td>1 Retrieve all patterns Q from Mh for viewpoint vt</td></tr><tr><td>3 4</td><td>Initialize L ← min(|τt|, |Tτ′|), scores C ← for i ← 1 to L do</td><td></td></tr><tr><td>5</td><td>Get imagined state zt+i, z from τt, τ′</td><td></td></tr><tr><td>6 7</td><td>if ci &lt; θh · γ i− then</td><td>Compute compatibility ci via Equation (11)</td></tr><tr><td>8 9</td><td>break Append score: C ← C U {ci}</td><td></td></tr><tr><td>10</td><td>Store pattern with score: (z&#x27;, τ′, C)  Q</td><td></td></tr><tr><td>11</td><td></td><td></td></tr><tr><td></td><td>Sort Q in descending order (by length and score)</td><td></td></tr><tr><td>12</td><td>Retain top P patterns as Q</td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td>13 for each (z&#x27;, T′, C)  Q do</td><td></td></tr><tr><td>14</td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td>15</td><td></td><td></td></tr><tr><td></td><td>for i ← 1 to |C| do</td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td>16</td><td></td><td>Retrieve feature xi from Mo for viewpoint vi</td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td>17</td><td></td><td>Retrieve edges Ei from G(k) for vi</td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td>18</td><td></td><td>Update episodic graph: Gt.update(vi, Ei)</td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td>19</td><td></td><td>Store state z, score ci and xi for viewpoint vi</td></tr></table>

Navigation History Retrieval. History retrieval identifies stored historical navigation patterns that exhibit similar imagined trajectories to the current agent's imagination. This process leverages the insight that agents with similar future expectations likely share comparable strategies and should benefit from each other's experiences. For a stored t trajectory $\tau ^ { \prime } = \{ \hat { z } _ { i } ^ { \prime } \} _ { i = 1 } ^ { H ^ { \prime } }$ at viewpoint $v _ { t }$ from the history bank $\mathcal { M } _ { h } ,$ we perform sequential similarity matching based on imagined trajectory $\boldsymbol { \tau } _ { t } ^ { \star } = \{ \hat { z } _ { t + i } \} _ { i = 1 } ^ { H _ { t } }$ The comptibility between imagined states at step $i$ is computed as:

$$
c _ { i } = \frac { 1 } { 2 } ( \sin ( \psi _ { s } ( \hat { z } _ { t + i } ) , \psi _ { s } ( \hat { z } _ { i } ^ { \prime } ) ) + 1 ) .
$$

For each imagination step $i$ in trajectory, we continue matching until either reaching the minimum of the two trajectory lengths, or encountering a compatibility score below thepeen $ { \bar { \theta } } _ { h } \cdot \gamma _ { h } ^ { i - 1 }$ The compatibility scores up to the matching termination are stored as $\mathcal { C }$ . We rank stored trajectories using a two-stage criterion: matching length (longer matches preferred), and the minimum compatibility score among matched steps. The top- ${ \bf \nabla } \cdot { \cal P }$ trajectory patterns are selected. For each selected pattern, we refrieved the sus $\{ z _ { i } ^ { \prime } , v _ { i } \} _ { i = 1 } ^ { | { \mathcal { C } } | } ,$ $| { \mathcal { C } } |$ viewpoints thiiated subgraph structure into $\mathcal { G } _ { t }$ along with state representations and compatibility scores. The complete procedure is outlined in Algorithm 2.

# 4.3 Navigation Model

At each timestep $t ,$ the agent imagines future states $\tau _ { t }$ and retrieve environmental observation and navigation history according to $v _ { t }$ . The retrieved information is then integrated into the episodic topological graph $\mathcal { G } _ { t }$ maintained by topological mapping. Now, we extend DUET [9] with specialized processing encoders that integrate retrieved experiential knowledge into navigation decisions. Our model processes these retrieved information through dedicated encoders: global observations, local observations and navigation behavioral patterns. The navigation model comprises three branches: Coarse-Scale Encoder. The coarse-scale encoder incorporates retrieved observations by expanding viewpoint representations $X$ with an additional type—retrieved viewpoints. The full viewpoint representations $X = [ x _ { 0 } , x _ { 1 } , \ldots , x _ { | \mathcal { V } _ { t } | } ]$ containing retrieved observations are processed through the coarse-scale encoder for $\hat { X }$ .Global action scores are computed as s s) $s _ { j } ^ { ( c ) } = \mathrm { F F N } ( \hat { x } _ { j } )$ for viewpoint $v _ { j } ,$ providing high-level navigation preferences. Fine-Scale Encoder. The fine-scale encoder processes immediate panoramic feature $r _ { t }$ for $\hat { r } _ { t }$ Local action scores $s _ { j } ^ { ( f ) } =$ FFN((i) are computed for each neighbor $v _ { j } \in \mathcal { N } ( v _ { t } )$ and converted to the global action space:

$$
s _ { j } ^ { ( f ^ { \prime } ) } = \left\{ \begin{array} { l l } { s _ { \mathrm { b a c k } } , } & { \mathrm { i f } v _ { j } \in \mathcal { V } _ { t } \setminus \mathcal { N } ( v _ { t } ) } \\ { s _ { j } ^ { ( f ) } , } & { \mathrm { o t h e r w i s e } , } \end{array} \right.
$$

where $i _ { j }$ denotes the view index oriented toward $v _ { j }$ and $s _ { \mathrm { b a c k } }$ aggregates scores for all visited neighbors of viewpoint $v _ { t }$ to encourage backtracking when necessary.

Navigation-History Encoder. The navigation-history encoder processes retrieved behavioral patterns by fusing historical states with current viewpoint representations. The node set $\nu _ { h }$ includes all viewpoints processed by this branch, comprising both currently visited locations and nodes retrieved from the history bank. For each viewpoint $v _ { j } \in \mathcal V _ { h }$ with retrieved states $\bar { Z } _ { j } = [ z _ { j } ^ { \prime ( 1 ) } , z _ { j } ^ { \prime ( 2 ) } , \dots , z _ { j } ^ { \prime ( N _ { j } ) } ]$ and compatibility scores $C _ { j } = [ c _ { j } ^ { ( 1 ) } , c _ { j } ^ { ( \bar { 2 } ) } , \ldots \bar { } , c _ { j } ^ { ( N _ { j } ) } ] ,$ where $N _ { j }$ denotes the number of retrieved states at $v _ { j } ,$ we compute:

$$
u _ { j } = \left( \operatorname { s o f t m a x } \left( \frac { C _ { j } } { \zeta } \right) \right) ^ { \top } Z _ { j } + x _ { j } .
$$

For visited nodes without retrieved historical states, we simply use the observation $u _ { j } = x _ { j }$ . The fused state representations $U = [ u _ { 1 } , u _ { 2 } , \dots , u _ { | \mathcal { V } _ { h } | } ]$ are processed through a tranformer o produce history-informe action scores $\bar { s _ { i } ^ { ( h ) } }$ which are then mapped to the global action space:

$$
s _ { i } ^ { ( h ^ { \prime } ) } = \left\{ \begin{array} { l l } { s _ { 0 } , } & { \mathrm { i f } v _ { i } \in \mathscr { V } _ { t } \setminus \mathscr { V } _ { h } } \\ { s _ { i } ^ { ( h ) } , } & { \mathrm { o t h e r w i s e } . } \end{array} \right.
$$

Dynamic Fusion. We implement a learned dynamic fusion mechanism that automatically balances contributions from the three branches based on current situational factors. The fusion weights are computed through:

$$
[ \sigma _ { f } , \sigma _ { c } , \sigma _ { h } ] = { \mathrm { S o f t m a x } } ( { \mathrm { F F N } } ( [ \hat { r } _ { 0 } ; \hat { x } _ { 0 } ; \hat { u } _ { 0 } ] ) ) ,
$$

Algorithm 3: Memoir Navigation Loop where ${ \hat { r } } _ { 0 } , { \hat { x } } _ { 0 } ,$ and $\hat { u } _ { 0 }$ represent the encoded stop token representations from fine-scale, coarse-scale, and navigationhistory encoders respectively, and $[ ; ]$ denotes concatenation. The final navigation scores integrate all three branches:

<table><tr><td colspan="2">Input: Tmax Max Step limit M Observation Bank</td></tr><tr><td colspan="2">D Imagination Horizon Mh History Bank Stop threshold €</td></tr><tr><td colspan="2">G(k) Persistent Graph</td></tr><tr><td colspan="2">1 Initialize episodic graph G0 ← Ø 2 Receive initial observation 01, viewpoint v1</td></tr><tr><td colspan="2"></td></tr><tr><td></td><td>3 for step t = 1 to Tmax do</td></tr><tr><td>4</td><td>Update topological graphs Gt and G(k)</td></tr><tr><td>5 6</td><td>Infer current state zt ∼ q(zt | zt−1, 0t, )</td></tr><tr><td>7</td><td>Initialize imagined trajectory Tt ← Ø for i = 1 to D do</td></tr><tr><td>8</td><td>Imagine next state zt+i ~ p(zt+i |zt+i−1)</td></tr><tr><td>9</td><td>Predict reward γt+i ∼ p(γt+i | zt+i)</td></tr><tr><td>10</td><td>Update trajectory Tt ← τt U {zt+i}</td></tr><tr><td>11</td><td>if γt+i &lt;  then</td></tr><tr><td>12</td><td>break</td></tr><tr><td>13</td><td>Gt ← ObsRetrieval(Mo, vt, τt, Gt, G(k))</td></tr><tr><td>14</td><td>// Algorithm 1 Gt ← HistoryRetrieval(Mh, vt, Tt, Gt, G(k))</td></tr><tr><td>15</td><td>// Algorithm 2 Extract viewpoint feature xt from ot</td></tr><tr><td>16</td><td>Mo.add(vt, xt)</td></tr><tr><td>17</td><td>Mh.add(vt, zt, Tt)</td></tr><tr><td>18</td><td>Compute score sj for each candidate node vj</td></tr><tr><td>19</td><td>Select action at ← argmaxj Sj</td></tr><tr><td>20</td><td>if at = stop then</td></tr><tr><td></td><td>break</td></tr><tr><td>21</td><td></td></tr><tr><td>22</td><td>Receive ot+1, vt+1 ← env.step(at)</td></tr></table>

$$
s _ { j } = \sigma _ { f } s _ { j } ^ { ( f ^ { \prime } ) } + \sigma _ { c } s _ { j } ^ { ( c ) } + \sigma _ { h } s _ { j } ^ { ( h ^ { \prime } ) } .
$$

As presented in Algorithm 3, Memoir realizes imagination-guided memory retrieval by generating imagined trajectories as queries to adaptively access relevant observations and behavioral histories from persistent memory. The navigation model integrates retrieved experiences, enabling informed decisions grounded by historical evidence while continuously updating memory banks for progressive improvement across episodes.

# 5 EXPERIMENTS

# 5.1 Experimental Setup

Datasets. We evaluate Memoir on two established memorypersistent VLN benchmarks that provide complementary evaluation perspectives. Iterative Room-to-Room (IR2R) [5] extends the foundational Room-to-Room (R2R) dataset [1] to multi-episode scenarios through structured tours, containing 183 training tours with an average length of 76.6 episodes. The validation splits comprise seen environments (159 tours, average 6.4 episodes) and unseen environments (33 tours, average 71.2 episodes). General Scene Adaptation (GSA-R2R) [6] incorporates 150 Habitat-Matterport3D (HM3D) scenes [51] with 600 paths per scene, providing 90,000 total episodes across 10 evaluation scenarios covering residential and non-residential environments with various instructions including basic navigational commands, scenespecific descriptions, and user-personalized instructions.

Implementation Details. We implement Memoir on three foundational models: DUET [9] and ScaleVLN [16] representing traditional VLN models, and GR-DUET [6] representing the memory-persistent approaches. All models utilize pretrained weights from their respective pretraining phases without task-specific fine-tuning. For rigorous comparison, we retrain all baseline models with identical hyperparameters and experimental conditions, including synchronized episode ordering in GSA-R2R. Our world model implementation employs two architectural variants: GRU and Transformer. Both variants utilize textual embeddings and share the observation encoder with the navigation model. World model pretraining occurs on R2R and augmented trajectories [15] for 5,000 iterations with batch size 32 and learning rate 5e-5, followed by imitation learning at learning rate 1e-5. Results are reported over 3 separate runs. Evaluation Metrics. We employ standard VLN metrics [52] for navigation performance evaluation. To quantify the effectiveness of long-term memory retrieval, we introduce four complementary metrics that evaluate both observation retrieval and history retrieval quality. The metrics include: Trajectory Length (TL): predicted path length in meters. Navigation Error (NE): distance between agent's final position to target in meters. Success Rate (SR): the percentage of final positions less than 3 meters away from the target location. Success Rate penalized by Path Length (SPL): SR normalized by the ratio between the length of the shortest path and the predicted path. Normalized Dynamic Time Warping (nDTW): dynamic time warping normalized between predicted and expert paths. Tour-normalized Dynamic Time Warping (T-nDTW): the overall navigation consistency across complete tours. Observation Accuracy (OA): the precision of retrieved observations from the observation bank $\mathcal { M } _ { o }$ across the episode:

$$
\mathrm { O A } = \frac { | \bigcup _ { t = 1 } ^ { T } ( \mathcal { R } _ { t } \cap \mathcal { V } _ { \mathrm { g t } , t } ^ { o } ) | } { | \bigcup _ { t = 1 } ^ { T } \mathcal { R } _ { t } | } ,
$$

where $\mathcal { R } _ { t }$ denotes viewpoints retrieved from $\mathcal { M } _ { o }$ at timestep $t$ $\gamma _ { \mathrm { g t } , t } ^ { o }$ $D$ steps that exist in the observation bank, where $\gamma _ { o }$ denotes all viewpoints stored in $\mathcal { M } _ { o }$ and $T$ is episode length. Observation Recall (OR): the coverage of relevant environmental observations across the episode:

$$
\mathrm { O R } = \frac { \vert \bigcup _ { t = 1 } ^ { T } ( \mathcal { R } _ { t } \cap \mathcal { V } _ { \mathrm { g t } , t } ^ { o } ) \vert } { \vert \bigcup _ { t = 1 } ^ { T } \mathcal { V } _ { \mathrm { g t } , t } ^ { o } \vert } .
$$

History Accuracy (HA): the precision of retrieved navigation patterns from the history bank $\mathcal { M } _ { h }$ across the episode:

$$
\mathrm { H A } = \frac { \sum _ { t = 1 } ^ { T } \sum _ { j = 1 } ^ { | Q _ { t } | } | \mathcal { V } _ { \mathrm { t r a j } , t , j } ^ { h } \cap \mathcal { V } _ { \mathrm { g t } , t , j } ^ { h } | } { \sum _ { t = 1 } ^ { T } \sum _ { j = 1 } ^ { | Q _ { t } | } | \mathcal { V } _ { \mathrm { t r a j } , t , j } ^ { h } | } ,
$$

where $| Q _ { t } |$ denotes the number of retrieved navigation history patterns at timestep $t$ (as detailed in Algorithm 2), ${ \mathcal V } _ { \mathrm { t r a j } , t , j } ^ { h } \ = \ \{ v _ { 1 } ^ { ( j ) } , v _ { 2 } ^ { ( j ) } , \ldots , v _ { | { \mathcal C } _ { j } | } ^ { \dot { ( j ) } } \}$ represents the sequence of viewpoints in the $j$ -th retrieved navigation history trajectory, and Vh represents the viewpoints on the teacher trajectory that exist in the original history trajectory. History Recall (HR): the coverage of relevant navigation patterns across the episode:

$$
\mathrm { H R } = \frac { \sum _ { t = 1 } ^ { T } \sum _ { j = 1 } ^ { \lvert Q _ { t } \rvert } \lvert \mathcal { V } _ { \mathrm { t r a j } , t , j } ^ { h } \cap \mathcal { V } _ { \mathrm { g t } , t , j } ^ { h } \rvert } { \sum _ { t = 1 } ^ { T } \sum _ { j = 1 } ^ { \lvert Q _ { t } \rvert } \lvert \mathcal { V } _ { \mathrm { g t } , t , j } ^ { h } \rvert } .
$$

# 5.2 Quantitative Analysis

# 5.2.1 Iterative Room-to-Room (IR2R)

Table 2 presents a comparison of Memoir against both traditional and memory-persistent methods on the IR2R benchmark. When applied to traditional VLN models, Memoir demonstrates substantial performance improvements: $1 1 . 1 \%$ SPL enhancement for DUET-based implementations and $5 . 6 \%$ for ScaleVLN-based implementations on unseen scenarios. These results prove that incorporating retrieved information from long-term memory serves as an effective prior for robust navigation decisions, even for models not originally designed for memory persistence. When compared against memory-persistent approaches, Memoir significantly outperforms GR-DUET, achieving $5 . 4 \%$ improvement in SPL on unseen scenarios $( 7 3 . 3 \%$ versus $6 7 . 9 \%$ and $1 1 . 6 \%$ improvement on seen scenarios. This superior performance validates our hypothesis that incorporating complete memory information introduces excessive noise that degrades navigation decisions and reduces flexibility in scenarios with limited experience availability. Our adaptive retrieval approach effectively addresses these limitations. Discussion. While achieving exceptional performance on unseen scenarios, memory-persistent variants often exhibit reduced performance on seen scenarios compared to their traditional counterparts. For instance, DUET achieves $7 4 . 5 \%$ SPL compared to GR-DUET's $5 5 . 1 \%$ on seen environments, with our method experiencing approximately $2 \%$ SPL degradation. This phenomenon stems from: (1) difference in tour lengths between validation splits, seen tours average only 6.4 episodes compared to 71.2 episodes in unseen tours, limiting accumulated experience; (2) regularization effects where long-term memory integration prevents overfitting to training environments by encouraging broader contextual reasoning rather than environmental detail memorization. Memoir substantially reduces this performance gap compared to GR-DUET, demonstrating more balanced memory utilization.

# 5.2.2 General Scene Adaptation (GSA-R2R)

Tables 3, 4, and 5 present Memoir's performance across diverse scene adaptation scenarios. Table 3 compares against adaptation-based methods and memory-persistent approaches across five distinct user instruction styles. Tables 4 and 5 evaluate performance across different environmental characteristics and instruction expressions. Memoir consistently outperforms both adaptation-based and memorybased methods, achieving an average $2 . 3 8 \%$ SR increase and $1 . 5 9 \%$ SPL improvement compared to GR-DUET across eight distinct testing scenarios with aligned experimental configurations. The improvements demonstrate that hybrid memory provides critical context absent in traditional approaches: by accessing past episodes where agents successfully processed similar expressions and executed corresponding actions, Memoir learns from historical patterns that GR-DUET's observation memory cannot capture. TABLE 2 Comparison of navigation performance between Memoir and various VLN methods on the IR2R benchmark.   

<table><tr><td colspan="5"></td><td colspan="5">Val Seen</td><td colspan="5">Val Unseen</td></tr><tr><td>Methods</td><td>PH TH PHI IW</td><td></td><td></td><td>TL↓ NE↓</td><td>nDTW↑</td><td></td><td>SR↑</td><td>SPL ↑ t-nDTW↑</td><td>TL↓</td><td>NE↓</td><td>nDTW↑</td><td>SR↑</td><td>SPL↑</td><td>t-nDTW↑</td></tr><tr><td>HAMT [20]</td><td></td><td></td><td></td><td>10.1 ±0.1 4.2 ±0.1</td><td>71 ±1</td><td>63 ±1</td><td>61 ±1</td><td>58 ±1</td><td>9.4 ±0.1 4.7 ±0.0</td><td></td><td>66 ±0</td><td>56 ±0</td><td>54 ±0</td><td>50 ±0</td></tr><tr><td>TourHAMT [5]</td><td>✓</td><td>✓ ✓</td><td>✓</td><td>9.4 ±0.4 5.8 ±0.1</td><td>59 ±0</td><td>45 ±1</td><td></td><td>43 ±1 45 ±0</td><td></td><td>10.0 ±0.2 6.2 ±0.1</td><td>52 ±0</td><td>39 ±1</td><td>36 ±0</td><td>32 ±1</td></tr><tr><td></td><td>✓</td><td>✓</td><td>✓</td><td>10.5 ±0.3 6.0 ±0.2</td><td></td><td>58 ±1</td><td>45 ±2</td><td>43 ±2</td><td>42 ±1</td><td>10.9 ±0.2 6.8 ±0.2</td><td>51 ±1</td><td>38 ±1</td><td>34 ±1</td><td>31 ±1</td></tr><tr><td></td><td>✓</td><td>✓</td><td></td><td>10.6 ±0.3 6.0 ±0.1</td><td></td><td>58 ±1</td><td>45 ±1</td><td>42 ±1</td><td>42 ±1</td><td>10.3 ±0.3 6.7 ±0.2</td><td>50 ±1</td><td>38 ±1</td><td>34 ±1</td><td>29 ±1</td></tr><tr><td></td><td>✓</td><td></td><td></td><td>10.9 ±0.3 6.1 ±0.1</td><td>58 ±1</td><td>45 ±1</td><td></td><td>42 ±1 41 ±0</td><td></td><td>11.0 ±0.6 6.7 ±0.1</td><td>51 ±0</td><td>38 ±0</td><td>34 ±0</td><td>28 ±1</td></tr><tr><td>OVER-NAV [7]</td><td></td><td></td><td></td><td>9.9 ±0.1 3.7 ± 0.1</td><td>73 ±1</td><td>65 ±1</td><td>63 ±1</td><td>62 ±0</td><td></td><td>9.4 ±0.1 4.1 ±0.1</td><td>69 ±0</td><td>60 ±1</td><td>57 ±0</td><td>55 ±1</td></tr><tr><td colspan="14">Comparison with Traditional VLN Models:</td></tr><tr><td>VLN models pretrained with default protocol:</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>DUET [9]</td><td></td><td></td><td></td><td>12.5 ±0.4 2.2 ±0.1 79.8 ±1.1 79.8 ±0.7 74.5 ±0.9 69.1 ±1.7</td><td></td><td></td><td></td><td></td><td></td><td>14.4 ±0.1 3.5 ±0.0 65.0 ±0.1 69.2 ±0.3 58.0 ±0.1 47.0 ±0.8</td><td></td><td></td><td></td><td></td></tr><tr><td>+Memoir (Ours)</td><td></td><td></td><td></td><td>11.5 ±0.1 2.6 ±0.2 78.9 ±0.9 77.1 ±0.5 72.8 ±0.5 68.0 ±0.8 11.0 ±0.0 2.8 ±0.1 75.2 ±0.0 75.4 ±0.2 69.1 ±0.3 58.8 ±0.4</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>VLN models pretrained with environmental augmentation:</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>ScaleVN 16]</td><td></td><td></td><td></td><td>12.8 ±0.0 2.2 ±0.0 79.6 ±0.4 79.5 ±0.5 74.1 ±0.6 67.0 ±0.2</td><td></td><td></td><td></td><td></td><td></td><td> 13.5 ±0.0 2.7 ±0.0 71.6 ±0.1 76.2 ±0.1 66.5 ±0.2 53.4 ±0.2</td><td></td><td></td><td></td><td></td></tr><tr><td>+Memoir (Ours)</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td colspan="9">11.6 ±0.2.5 ±0.178.7 ±0.176.1 ±0.57.3 ±0.17.1 ±0.0 10.9 ±0.2 .6 ±0.077.2 ±0.677.4 ±0.2 72.1 ±0.4 6. ±. Comparison with Memory-Persistent VLN Models: VLN models pretrained with full navigation graph:</td></tr></table>

TABLE 3 Comparison of navigation performance on the GSA-R2R benchmark with user instructions.

<table><tr><td></td><td colspan="2">Child</td><td colspan="2">Keith</td><td colspan="2">Moira</td><td colspan="2">Rachel</td><td colspan="2">Sheldon</td></tr><tr><td>Methods</td><td>SR ↑</td><td>SPL↑</td><td>SR ↑</td><td>SPL↑</td><td>SR ↑</td><td>SPL↑</td><td>SR ↑</td><td>SPL↑</td><td>SR ↑</td><td>SPL↑</td></tr><tr><td>TourHAMT [5]</td><td>14.6 ±0.2</td><td>12.0 ±0.2</td><td>15.1 ±0.2</td><td>12.3 ±0.1</td><td>13.9 ±0.1</td><td>11.3 ±0.1</td><td>15.3 ±0.1</td><td>12.5 ±0.1</td><td>14.4 ±0.1</td><td>11.8 ±0.1</td></tr><tr><td>OVER-NAV [7]</td><td>20.9 ±0.1</td><td>16.1 ±0.2</td><td>20.5 ±0.1</td><td>16.4 ±0.1</td><td>19.5 ±0.2</td><td>15.4 ±0.2</td><td>20.6 ±0.3</td><td>16.2 ±0.2</td><td>20.5 ±0.1</td><td>16.2 ±0.1</td></tr><tr><td>DUET [9]</td><td>54.3</td><td>44.1</td><td>56.0</td><td>46.3</td><td>52.3</td><td>43.3</td><td>56.3</td><td>46.4</td><td>54.0</td><td>44.4</td></tr><tr><td>+MLM [15]</td><td>54.5 ±0.2</td><td>44.7 ±0.2</td><td>56.4 ±0.3</td><td>46.8 ±0.3</td><td>53.8 ±0.3</td><td>43.6 ±0.4</td><td>56.8 ±0.5</td><td>46.6 ±0.6</td><td>54.5 ±0.4</td><td>44.2 ±0.3</td></tr><tr><td>+MRC [15]</td><td>54.4 ±0.2</td><td>44.2 ±0.1</td><td>56.0 ±0.1</td><td>46.3 ±0.1</td><td>52.3 ±0.2</td><td>43.3 ±0.1</td><td>56.0 ±0.1</td><td>46.2 ±0.2</td><td>53.7 ±0.2</td><td>44.2 ±0.4</td></tr><tr><td>+BT [53]</td><td>57.5 ±0.7</td><td>54.0 ±0.9</td><td>61.2 ±0.3</td><td>57.9 ±0.1</td><td>57.3 ±0.5</td><td>54.0 ±0.6</td><td>61.6 ±0.8</td><td>58.1 ±0.7</td><td>57.6 ±0.5</td><td>54.3 ±0.5</td></tr><tr><td>+TENT [54]</td><td>54.3 ±0.2</td><td>41.7 ±0.1</td><td>55.4 ±0.2</td><td>43.8 ±0.2</td><td>51.7 ±0.2</td><td>41.0 ±0.1</td><td>55.0 ±0.2</td><td>43.2 ±0.2</td><td>53.0 ±0.2</td><td>41.9 ±0.1</td></tr><tr><td>+SAR [55]</td><td>54.5 ±0.5</td><td>41.5 ±0.4</td><td>54.9 ±0.3</td><td>43.1 ±0.2</td><td>51.0 ±0.4</td><td>40.3 ±0.6</td><td>55.3 ±0.5</td><td>43.0 ±0.6</td><td>52.9 ±0.2</td><td>41.4 ±0.4</td></tr><tr><td colspan="9">VLN models pretrained with full navigation graph:</td><td></td><td></td></tr><tr><td>GR-DUET [6]</td><td>65.2 ±0.1</td><td>59.7 ±0.1</td><td>66.7 ±0.1</td><td>62.0 ±0.1</td><td>60.9 ±0.2</td><td>56.2 ±0.2</td><td>67.1 ±0.1</td><td>62.2 ±0.1</td><td>63.9 ±0.1</td><td>58.9 ±0.1</td></tr><tr><td>GR-DUET* [6]</td><td>64.9 ±0.5</td><td>60.5 ±0.4</td><td>65.1 ±0.3</td><td>61.4 ±0.4</td><td>60.5 ±0.3</td><td>56.6 ±0.2</td><td>65.7 ±0.5</td><td>61.7 ±0.4</td><td>63.0 ±0.4</td><td>59.0 ±0.4</td></tr><tr><td>+Memoir (Ours)</td><td>66.5 ±0.5</td><td>61.3 ±0.5</td><td>68.0 ±0.1</td><td>63.6 ±0.2</td><td>62.5 ±0.3</td><td>57.5 ±0.4</td><td>68.2 ±0.1</td><td>63.6 ±0.3</td><td>65.3 ±0.1</td><td>60.4 ±0.3</td></tr></table>

TABLE 4 Comparison of navigation performance on the GSA-R2R benchmark with scene instructions.

<table><tr><td rowspan="3">Methods</td><td colspan="5">Test-N-Scene</td></tr><tr><td>TL↓</td><td>NE ↓</td><td>SR ↑</td><td>SPL↑</td><td>nDTW↑</td></tr><tr><td>TourHAMT [5]</td><td>7.3 ±0.1 8.1 ±0.1</td><td>9.7 ±0.1</td><td>8.0 ±0.1</td><td>32.3 ±0.1</td></tr><tr><td>OVER-NAV [7] DUET [9]</td><td>11.8 ±0.1 14.9</td><td>7.6 ±0.2 6.4</td><td>16.7 ±0.4 39.6</td><td>12.6 ±0.2 30.1</td><td>34.6 ±0.3 40.9</td></tr><tr><td>+MLM [15]</td><td>14.3 ±0.1</td><td>6.5 ±0.1</td><td>39.8 ±0.1</td><td>30.5 ±0.1</td><td>41.1 ±0.1</td></tr><tr><td>+MRC [15]</td><td>14.9 ±0.1</td><td>6.4 ±0.1</td><td>39.7 ±0.1</td><td>30.2 ±0.1</td><td>40.9 ±0.1</td></tr><tr><td>+BT [53]</td><td>8.4 ±0.0</td><td>6.3 ±0.2</td><td>41.2 ±1.5</td><td>38.2 ±1.2</td><td>51.3 ±1.2</td></tr><tr><td>+TENT [54]</td><td>16.4 ±0.1</td><td>6.3 ±0.1</td><td>40.6 ±0.2</td><td></td><td></td></tr><tr><td>+SAR [55]</td><td></td><td></td><td></td><td>28.9 ±0.2</td><td>38.9 ±0.2</td></tr><tr><td></td><td>16.3 ±0.5</td><td>6.0 ±0.2</td><td>41.4 ±0.6</td><td>29.1 ±0.3</td><td>39.0 ±0.3</td></tr><tr><td>VLN models pretrained with full navigation graph:</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>GR-DUET [6]</td><td>10.1 ±0.0</td><td>5.5 ±0.0</td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td>48.1 ±0.1</td><td>42.8 ±0.1</td><td>53.7 ±0.1</td></tr><tr><td>GR-DUET* [6]</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td>9.9 ±0.3</td><td>5.5 ±0.0</td><td>47.1 ±0.5</td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td>42.2 ±0.8</td><td>54.1 ±0.6</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>+Memoir (Ours)</td><td>10.3 ±0.4</td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td>5.1 ±0.0</td><td>50.2 ±0.3</td><td>44.8 ±0.4</td><td>56.2 ±0.6</td></tr></table>

\*Results reproduced under aligned experimental conditions. Discussion. Memoir consistently outperforms GR-DUET, though with smaller gains than against IR2R. This reduced improvement stems from memory density differences, with GSA-R2R accumulating 600 episodes on average, increasing the topological completeness for GR-DUET.

# 5.3 Qualitative Analysis.

Figure 3 demonstrates memory retrieval effectiveness in challenging scenarios where DUET and GR-DUET fail. Given the task of locating a "massage table" with two potential candidates barely visible from the hallway, the DUET agent incorrectly approaches the wrong target without observing the actual target, while the GR-DUET agent becomes confused among numerous candidate locations and produces incorrect decisions. Our model succeeds through the combination of observation retrieval, which identifies promising paths toward relevant locations while controlling redundancy, and history retrieval, which matches similar past episodes targeting "massage room" objectives, prompting the agent to the destination. TABLE 5 Comparison of navigation performance on the GSA-R2R benchmark with basic instructions.   

<table><tr><td rowspan="2">Methods</td><td colspan="5">Test-R-Basic</td><td colspan="5">Test-N-Basic</td></tr><tr><td>TL↓</td><td>NE↓</td><td>SR↑</td><td>SPL ↑</td><td>nDTW↑</td><td>TL↓</td><td>NE↓</td><td>SR↑</td><td>SPL ↑</td><td>nDTW ↑</td></tr><tr><td>TourHAMT [5]</td><td>11.6 ±0.1</td><td>7.4 ±0.1</td><td>14.9 ±0.1</td><td>12.2 ±0.1</td><td>34.7 ±0.1</td><td>9.4 ±0.1</td><td>7.7 ±0.1</td><td>11.0 ±0.2</td><td>8.6 ±0.2</td><td>32.2 ±0.1</td></tr><tr><td>OVER-NAV [7]</td><td>14.1 ±0.1</td><td>6.7 ±0.0</td><td>22.3 ±0.3</td><td>16.8 ±0.2</td><td>37.1 ±0.1</td><td>11.4 ±0.1</td><td>7.1 ±0.1</td><td>16.6 ±0.2</td><td>13.0 ±0.1</td><td>35.0 ±0.2</td></tr><tr><td>DUET [9]</td><td>13.1</td><td>4.2</td><td>57.7</td><td>47.0</td><td>55.6</td><td>14.8</td><td>5.3</td><td>48.1</td><td>37.3</td><td>45.9</td></tr><tr><td>+MLM [15]</td><td>13.1 ±0.1</td><td>4.1 ±0.1</td><td>57.9 ±0.2</td><td>47.3 ±0.1</td><td>55.9 ±0.2</td><td>13.1 ±0.2</td><td>5.3 ±0.1</td><td>48.3 ±0.5</td><td>38.8 ±0.5</td><td>48.4 ±0.3</td></tr><tr><td>+MRC [15]</td><td>13.1 ±0.1</td><td>4.2 ±0.1</td><td>57.7 ±0.1</td><td>47.0 ±0.1</td><td>55.6 ±0.1</td><td>14.7 ±0.1</td><td>5.3 ±0.1</td><td>48.1 ±0.1</td><td>37.3 ±0.1</td><td>45.9 ±0.1</td></tr><tr><td>+BT [53]</td><td>8.0 ±0.1</td><td>3.8 ±0.1</td><td>61.3 ±0.6</td><td>57.7 ±0.3</td><td>70.1 ±0.5</td><td>7.9 ±0.0</td><td>5.2 ±0.1</td><td>49.5 ±0.8</td><td>46.0 ±0.8</td><td>59.4 ±0.9</td></tr><tr><td>+TENT [54]</td><td>14.6 ±0.0</td><td>4.2 ±0.0</td><td>57.2 ±0.4</td><td>44.2 ±0.4</td><td>52.9 ±0.1</td><td>16.2 ±0.1</td><td>5.4 ±0.1</td><td>46.5 ±0.4</td><td>33.7 ±0.2</td><td>42.6 ±0.3</td></tr><tr><td>+SAR [55]</td><td>13.8 ±0.8</td><td>4.0 ±0.1</td><td>57.6 ±0.2</td><td>44.6 ±0.2</td><td>53.0 ±0.2</td><td>16.5 ±0.0</td><td>5.4 ±0.0</td><td>44.6 ±1.5</td><td>31.5 ±1.6</td><td>40.6 ±1.3</td></tr><tr><td colspan="9">VLN models pretrained with full navigation graph:</td><td></td><td></td></tr><tr><td>GR-DUET [6]</td><td>9.4 ±0.0</td><td>3.1 ±0.0</td><td>69.3 ±0.2</td><td>64.3 ±0.1</td><td>71.4 ±0.1</td><td>8.9 ±0.0</td><td>4.4 ±0.0</td><td>56.6 ±0.1</td><td>51.5 ±0.1</td><td>61.0 ±0.1</td></tr><tr><td>GR-DUET* [6]</td><td>8.6 ±0.2</td><td>3.2 ±0.1</td><td>67.6 ±0.5</td><td>63.6 ±0.6</td><td>71.9 ±0.5</td><td>8.7 ±0.4</td><td>4.4 ±0.0</td><td>55.3 ±0.2</td><td>50.4 ±0.3</td><td>60.8 ±0.4</td></tr><tr><td>+Memoir (Ours)</td><td>9.3 ±0.0</td><td>3.0 ±0.0</td><td>69.8 ±0.2</td><td>64.9 ±0.4</td><td>73.3 ±0.2</td><td>9.3 ±0.2</td><td>4.2 ±0.0</td><td>57.7 ±0.1</td><td>52.0 ±0.1</td><td>61.9 ±0.4</td></tr></table>

![](images/3.jpg)  
Ixieea  waiexeal.

TABLE 6 Computational efficiency comparison (batch size $= 4$ .

<table><tr><td rowspan="2">Methods</td><td colspan="2">Training</td><td colspan="2">Inference</td></tr><tr><td>Memory↓</td><td>Latency↓</td><td>Memory↓</td><td>Latency↓</td></tr><tr><td>DUET [9]</td><td>7.2 GB</td><td>0.15s</td><td>2.2 GB</td><td>0.13s</td></tr><tr><td>GR-DUET [6]</td><td>29.4 GB</td><td>4.39s</td><td>9.9 GB</td><td>0.25s</td></tr><tr><td>Memoir (Ours)</td><td>13.1 GB (-55%)</td><td>0.53s (-88%)</td><td>2.6 GB (-74%)</td><td>0.31s (+28%)</td></tr></table>

# 5.4 Ablation Studies & Analyses

Computational Efficiency. Table 6 demonstrates Memoir's computational advantages over the memory-persistent baseline. While DUET operates with minimal memory overhead, GR-DUET's complete memory retention strategy dramatically increases resource requirements (29.4GB training, 9.9GB inference) due to processing all accumulated observations simultaneously. Our retrieval mechanism achieves substantial efficiency gains: $5 5 \%$ reduction in training memory and $8 8 \%$ reduction in training latency, representing an $8 . 3 \times$ speedup. During inference, memory usage decreases by $7 4 \%$ approaching DUET's efficiency while maintaining memory-persistent capabilities. The slight inference latency increase (0.31s vs 0.25s) reflects the overhead of imaginationguided retrieval, suggesting opportunities for future optimization through caching or parallel processing. These results establish Memoir as the first memory-persistent VLN approach that achieves both practical efficiency and state-ofthe-art performance, making continuous learning viable for resource-constrained deployment while advancing navigation capabilities beyond existing methods.

![](images/4.jpg)  
F   
Fig. 4. Performance scaling across tour progression on IR2R.

TABLE 7 Ablation of components for memory retrieval.

<table><tr><td colspan="3">Observation</td><td colspan="3">History</td><td colspan="9">IR2R Val Unseen</td></tr><tr><td></td><td>Retr Rand Full PERF</td><td></td><td></td><td></td><td>Retr Rand Full Perf</td><td></td><td></td><td>TL↓NE↓ SR↑SPL↑NDTW↑OR↑OA↑HR↑HA↑</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td colspan="10">The upper-bound of long-term memory retrieval</td><td colspan="7"></td></tr><tr><td></td><td></td><td>✓</td><td></td><td></td><td></td><td>9.77</td><td>0.51</td><td>95.44</td><td>93.40</td><td>93.68</td><td></td><td>100</td><td>100</td><td>100</td><td>100</td></tr><tr><td></td><td colspan="7"></td><td>12.24 2.81</td><td>72.33</td><td>63.97</td><td>70.35</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td></td><td colspan="7"></td><td>10.44</td><td>2.86 74.67</td><td>69.98</td><td>76.29</td><td>100</td><td>9.81</td><td>100</td><td>19.33</td></tr><tr><td></td><td>✓</td><td>✓</td><td></td><td></td><td>✓</td><td></td><td>10.97</td><td>2.76</td><td>75.82</td><td>70.34</td><td>76.03</td><td>59.05 21.31</td><td></td><td>36.65</td><td>22.11</td></tr><tr><td>✓</td><td></td><td></td><td></td><td>✓</td><td></td><td></td><td>10.77</td><td>2.58</td><td>76.63</td><td>71.70</td><td>78.08</td><td>97.05</td><td>23.33</td><td>36.24</td><td>21.94</td></tr><tr><td></td><td>✓</td><td></td><td></td><td></td><td></td><td></td><td>10.80</td><td>2.61</td><td>76.63</td><td>71.03</td><td>76.98</td><td>58.83</td><td>21.72</td><td>98.36</td><td>22.40</td></tr><tr><td>✓</td><td></td><td></td><td></td><td>✓ ✓</td><td></td><td></td><td>10.32</td><td>2.53</td><td>78.03</td><td>73.46</td><td>79.46</td><td>96.49</td><td>24.58</td><td>96.52</td><td>24.21</td></tr></table>

RETRretrieval vmagination; RAND:rando sampling; FULL coplee incorporation; PERFrace rtrival. TABLE 8 Ablation of world model variants.   

<table><tr><td></td><td></td><td colspan="5">IR2R Val Unseen</td></tr><tr><td>MODEL</td><td>Dist</td><td>SR ↑</td><td>SPL ↑</td><td>OR ↑</td><td>OA ↑</td><td>HR↑ HA↑</td></tr><tr><td>GRU</td><td>5</td><td>| 76.12</td><td>71.48</td><td>97.54</td><td>16.90</td><td>30.21 30.78</td></tr><tr><td>+overshoot</td><td>5</td><td>| 76.71</td><td>72.30</td><td>96.21</td><td>18.60</td><td>98.24 21.65</td></tr><tr><td>Transformer</td><td>1 3</td><td>73.44 75.18</td><td>67.24 70.21</td><td>31.25 76.71</td><td>40.33 26.05</td><td>26.26 30.74 29.11 32.29</td></tr><tr><td></td><td>5</td><td>77.14</td><td>72.11</td><td>96.44</td><td>17.86</td><td>28.58 32.32</td></tr><tr><td>+overshoot</td><td>1</td><td>73.09</td><td>67.26</td><td>31.15</td><td>43.22</td><td>59.43 29.24</td></tr><tr><td></td><td>3</td><td>76.63 78.03</td><td>71.99 73.46</td><td>77.04 96.49</td><td>28.48 24.58 96.52</td><td>93.40 25.06 24.21</td></tr></table>

Performance Scaling. Figure 4 demonstrates the scaling performance as agents complete episodes within IR2R tours. Memoir exhibits the most pronounced improvement with sustained high performance till the tour's end. In contrast, DUET maintains relatively stable but lower performance $6 5 \mathrm { - } 7 2 \%$ SR, $5 5 – 6 3 \%$ SPL) without learning from accumulated experience. GR-DUET shows inconsistent scaling as its performance notably deteriorates at $9 0 \%$ progression, falling below DUET's baseline. This comparison validates that imagination-guided memory retrieval enables more consistent and substantial performance gains from accumulated experience compared to simple memory incorporation.

Memory Components. Table 7 presents results validating the effectiveness of memory components. The "perfect" variant directly incorporates memories leading to target locations, simulating ideal world model behavior with perfect retrieval capabilities, achieving $9 3 . 4 0 \%$ SPL on unseen environments, highlighting the necessity of retrieval and serving as an performance upper bound. The variant with both observation and history components disabled yields the lowest performance, followed by complete longterm memory incorporation. Random memory selection enhances navigation performance by $0 . 3 6 \%$ SPL, while substituting random selection with our retrieval approach improves navigation performance for both memory types, demonstrating that both observations and histories contain valuable signals-provided they are accessed selectively. Our retrieval variant achieves optimal navigation performance and retrieval accuracy $( 2 4 . 5 8 \%$ OA, $2 4 . 2 1 \%$ HA). Conversely, complete memory incorporation decreases accuracy, and random selection significantly reduces recall (OR and HR), demonstrating that predictive imagination enable precise memory filtering unavailable to uninformed and exhaustive approaches. The substantial gap relative to the ideal world model reveals current challenges in the world model's ability to capture environmental dynamics accurately. The world model would benefit from data scaling and advanced architectures to further enhance future performance. TABLE 9 Ablation of navigation history integration.   

<table><tr><td colspan="6">IR2R Val Unseen</td></tr><tr><td>Hist Encoder</td><td>Embed Type</td><td>SR ↑</td><td>SPL↑</td><td>nDTW↑</td><td>t-nDTW↑</td></tr><tr><td></td><td>VP + State</td><td>76.59</td><td>72.35</td><td>78.60</td><td>65.57</td></tr><tr><td>✓</td><td>VP</td><td>76.54</td><td>71.48</td><td>78.24</td><td>65.66</td></tr><tr><td>√</td><td>State</td><td>77.35</td><td>72.83</td><td>78.70</td><td>66.37</td></tr><tr><td>v</td><td>VP + State</td><td>78.03</td><td>73.46</td><td>79.46</td><td>66.44</td></tr></table>

TABLE 10 Ablation of expert policies.

<table><tr><td></td><td colspan="3">IR2R Val Unseen</td><td colspan="3">GSA Test-R-Basic</td></tr><tr><td>Expert PoLicy</td><td>SR↑</td><td>SPL ↑</td><td>nDTW↑</td><td>SR ↑</td><td>SPL↑</td><td>nDTW↑</td></tr><tr><td>SPL</td><td>76.20</td><td>71.71</td><td>78.03</td><td>67.34</td><td>62.22</td><td>71.30</td></tr><tr><td>+random sample</td><td>78.03</td><td>73.46</td><td>79.46</td><td>69.64</td><td>64.91</td><td>73.29</td></tr></table>

World Model. Table 8 evaluates world model variants comparing GRU and Transformer architectures with and without overshooting. Transformer variant outperform GRU variant by $1 . 1 6 \%$ SPL, $7 . 6 8 \%$ observation retrieval accuracy (OA), and $2 . 5 6 \%$ history retrieval accuracy (HA) under 5-step overshooting distance. With increased imagination steps, retrieval effectiveness generally increases as the exploration horizon broadens, as recall improves for observations (OR) and histories (HR), enabling more informed navigation decisions. However, retrieval accuracy decreases due to exponentially increasing candidates in topological graphs with greater distances. The overshooting objective significantly enhances OA and HR, contributing to robust navigation performance $( + 1 . 3 6 \%$ SPL). These results validate that effective retrieval requires both powerful predictive models (Transformer over GRU) and grounded training (overshooting) to balance exploration breadth with query precision. TABLE 11 Ablation of world model pretraining.   

<table><tr><td></td><td colspan="3">IR2R Val Unseen</td><td colspan="3">GSA Test-R-Basic</td></tr><tr><td>PRETRAIN</td><td>SR↑</td><td>SPL↑</td><td>nDTW ↑</td><td>SR↑</td><td>SPL↑</td><td>nDTW ↑</td></tr><tr><td></td><td>74.93</td><td>71.55</td><td>78.12</td><td>64.62</td><td>61.52</td><td>71.30</td></tr><tr><td>✓</td><td>78.03</td><td>73.46</td><td>79.46</td><td>69.64</td><td>64.91</td><td>73.29</td></tr></table>

TABLE 12 Ablation of observation completion.

<table><tr><td rowspan="2"></td><td colspan="3">IR2R Val Unseen</td><td colspan="3">GSA Test-R-Basic</td></tr><tr><td>Neighbor obs SR ↑</td><td>SPL ↑</td><td>nDTW↑</td><td>SR↑</td><td>SPL↑</td><td>nDTW↑</td></tr><tr><td>Partial</td><td>77.05</td><td>72.15</td><td>78.22</td><td>68.35</td><td>63.90</td><td>72.89</td></tr><tr><td>Completion</td><td>78.03</td><td>73.46</td><td>79.46</td><td>69.64</td><td>64.91</td><td>73.29</td></tr></table>

History Integration. Table 9 compares strategies for incorporating retrieved histories. Concatenating viewpoint features with state features in the dedicated encoder yields optimal performance, achieving $1 . 9 8 \%$ higher SPL than viewpoint features alone and $0 . 6 3 \%$ higher than state features alone. This indicates that state features carry crucial pattern information, while still benefiting from observation representation enhancement. Without incorporating history encoder, where historical representation is concatenated with coarse-scale encoder inputs, performance decreases by $1 . 1 1 \%$ SPL, demonstrating the necessity of separating duties across three distinct encoders. Expert Policy Strategy. Table 10 compares training strategies when memory retrieval dynamically expands the available action space beyond immediate neighbors. Random sampling among multiple optimal paths during training (achieving $7 3 . 4 6 \%$ SPL) outperforms deterministic SPLbased expert selection $( 7 1 . 7 1 \%$ SPL) by providing better policy regularization and robustness to navigation choices. World Model Pretraining. Table 11 demonstrates the importance of proper world model initialization. Pretraining the world model components on navigation trajectories before joint training improves performance by $1 . 9 1 \%$ SPL on IR2R and $3 . 3 9 \%$ SPL on GSA-R2R, indicating that randomly initialized world models provide poor retrieval signals. Observation Completion. Table 12 demonstrates that completing partial observations at non-retrieved viewpoints using stored features from $\mathcal { M } _ { o }$ significantly enhances environmental understanding. When a viewpoint in the episodic graph lacks complete visual information, retrieving its stored panoramic feature enables more informed decisionmaking. Neighbor Incorporation. Table 13 studies the retrieval strategy that incorporates adjacent viewpoints of retrieved nodes during observation retrieval. Including immediate neighbors provides richer spatial context about connectivity and surrounding environment, enabling the coarse-scale encoder to make better-informed planning decisions. This approach improves SPL by $2 . 5 5 \%$ and $1 . 8 9 \%$ on respective benchmark. Parameter Study. Figure 5 analyzes the impact of key retrieval hyperparameters. For observation retrieval, SR improves with reduced filter rates and increased search width, peaking at $\rho _ { o } = 0 . 2$ and $W = 1 2$ before degrading as excessive context introduces noise. This indicates incorporating a broader range of viewpoint observations facilitates more informed navigation decisions. For navigation history retrieval, the model prioritizes precision over recall, achieving optimal performance at $\theta _ { h } = 0 . 2$ with $P = 1 0$ A secondary optimum occurs at threshold $\theta _ { h } ~ = ~ 1 . 0$ and max patterns $\bar { P } = 2 0$ (decay factor $\gamma _ { h } = 0 . 8 )$ , where highly restrictive similarity thresholds compensate through increased pattern acceptance. TABLE 13 Ablation of neighborhood incorporation.   

<table><tr><td></td><td colspan="3">IR2R Val Unseen</td><td colspan="3">GSA Test-R-Basic</td></tr><tr><td>RETRIEVE</td><td>SR ↑</td><td>SPL↑</td><td>nDTW↑</td><td>SR↑</td><td>SPL↑</td><td>nDTW ↑</td></tr><tr><td>VP only</td><td>76.46</td><td>70.91</td><td>77.45</td><td>67.44</td><td>63.02</td><td>71.83</td></tr><tr><td>VP + neighbors</td><td>78.03</td><td>73.46</td><td>79.46</td><td>69.64</td><td>64.91</td><td>73.29</td></tr></table>

![](images/5.jpg)  
Fig. 5. Study of hyper-parameters of retrieval on IR2R val-unseen. Left: Environmental observation retrieval. Right: Navigation history retrieval.

# 5.5 Failure Analysis

Figure 6 presents a scenario where Memoir fails despite functioning as designed. In Episode A, the agent must navigate to a bedroom absent from previous episodes. Observation retrieval identifies two distracting bedroom entrances as candidates, while history retrieval surfaces Episodes B (failed) and C (succeeded), both targeting a different bedroom. Retrieval Limitations. The retrieved observations prefer incorporating abundant promising candidates as discovered in Figure 5(a), highlighting both bedroom entrances as semantically relevant but failing to discriminate the critical spatial feature—"nearest to the desk." Retrieved histories similarly cannot distinguish Episodes B and C despite different goals. In Episode B, premature imagination termination after one step limits retrieval to only the nearest entrances, preventing correct target discovery. These failures reveal world model deficiencies in predictive retrieval for both memory types. Exploration-Exploitation Trade-off. Episode A fails when both retrieval types converge on the same incorrect location. The agent prioritizes high-similarity histories as discovered in Figure 5(b), defaulting to exploitation over exploration even when retrieval fails to cover the true goal. It also fails to distinguish task outcomes, treating Episodes B and C equally rather than learning from success. This highlights a new challenge: determining when accumulated experience should be trusted versus when novel alternatives warrant investigation.

![](images/6.jpg)

Future Work. These failure modes suggest two potential research directions for advancing imagination-guided memory retrieval. First, enhanced world modeling capability through larger-scale pretraining and explicit spatial relationship modeling could address both retrieval inaccuracies in distinguishing spatially distinct targets and premature imagination horizons that affect retrieval scope. Second, confidence-arare retrieval to determine when retrieved experience should be trusted, requiring retrieval confidence estimation to dynamically balance exploitation against exploration. The substantial performance gap between our method $( 7 3 . 4 6 \%$ SPL) and the oracle retrieval upper bound $( 9 3 . 4 0 \%$ SPL in Table 7) demonstrates significant room for improvement in these directions.

# 6 CONCLUSION

This work introduces Memoir, a memory-persistent VLN agent employing predictive world modeling for adaptive experience retrieval. Unlike traditional imagine-planning that generates trajectories in isolation, we ground imagination with explicit memory through a language-conditioned world model, Hybrid Viewpoint-Level Memory (HVM) storing observations and behavioral patterns, and an experience-augmented navigation model. Extensive experiments demonstrate $5 . 4 \%$ SPL improvement on IR2R with $8 . 3 \times$ training speedup and $7 4 \%$ inference memory reduction, validating that predictive retrieval of both environmental and behavioral memories enables more effective navigation. The oracle retrieval performance $9 3 . 4 \%$ SPL) demonstrates the potential of imagination-guided retrieval. Future work should explore enhanced world modeling and confidenceaware exploration mechanisms to narrow this gap, establishing a principled framework connecting predictive simulation with explicit memory for embodied AI.

# REFERENCES

[1] P. Anderson et al., "Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments,' in CVPR, 2018, pp. 36743683.   
[2] Y. Qi et al., "Reverie: Remote embodied visual referring expression in real indoor environments," in CVPR, 2020, pp. 99829991.   
[3] A. Ku, P. Anderson, R. Patel, E. Ie, and J. Baldridge, "Room-acrossroom: Multilingual vision-and-language navigation with dense spatiotemporal grounding," in EMNLP, 2020, pp. 43924412.   
[4] S. Wani, S. Patel, U. Jain, A. Chang, and M. Savva, "Multion: Benchmarking semantic map memory using multi-object navigation," in NeurIPS, vol. 33, 2020, pp. 97009712.   
[5] J. Krantz et al., "Iterative vision-and-language navigation," in CVPR, 2023, pp. 14 92114 930.   
[6] H. Hong, Y. Qiao, S. Wang, J. Liu, and Q. Wu, "General scene adaptation for vision-and-language navigation," in ICLR, 2025.   
[7] G. Zhao, G. Li, W. Chen, and Y. Yu, "Over-nav: Elevating iterative vision-and-language navigation with open-vocabulary detection and structured representation," in CVPR, 2024, pp. 16 29616 306.   
[8] Q. Zheng, D. Liu, C. Wang, J. Zhang, D. Wang, and D. Tao, Esceme: Vision-and-language navigation with episodic scene memory," IJCV, pp. 121, 2024.   
[9] S. Chen, P.-L. Guhur, M. Tapaswi, C. Schmid, and I. Laptev, "Think global, act local: Dual-scale graph transformer for vision-andlanguage navigation," in CVPR, 2022, pp. 16 53716 547.   
[10] M. Seeber et al., "Human neural dynamics of real-world and imagined navigation," Nat. Hum. Behav., vol. 9, no. 4, pp. 781793, 2025.   
[11] M. Karl, F. Kock, B. W. Ritchie, and J. Gauss, "Affective forecasting and travel decision-making: An investigation in times of a pandemic," Ann. Tour. Res., vol. 87, p. 103139, 2021.   
[12] Y. W. Li and L. C. Wan, "Inspiring tourists' imagination: How and when human presence in photographs enhances travel mental simulation and destination attractiveness," Tour. Manag., vol. 106, p. 104969, 2025.   
[13] H. Wang, W. Liang, L. Van Gool, and W. Wang, "Dreamwalker: ICCV, 2023, pp. 1087310883. navigation," in NeurIPS, vol. 31, 2018.   
[15] W. Hao, C. Li, X. Li, L. Carin, and J. Gao, "Towards learning a generic agent for vision-and-language navigation via pretraining," in CVPR, 2020, pp. 13 13713 146. navigation," in ICC, 2023, pp. 1200120.   
[] J. Zhang et l., "Navid: Video-based vlm plans the next step for vision-and-language navigation," in RSS, 2024.   
[18] Y. Xu, Y. Pan, Z. Liu, and H. Wang, "Flame: Learning to navigate with multimodal llm in urban environments," in AAAI, vol. 39, 2025, pp. 90059013.   
[19] Y. Hong, Q. Wu, Y. Qi, C. Rodriguez-Opazo, and S. Gould, "Vln bert: A recurrent vision-and-language bert for navigation," in CVPR, 2021, pp. 16431653.   
[20] S. Chen, P.-L. Guhur, C. Schmid, and I. Laptev, "History aware multma onndan t," i NeurIPS, vol. 34, 2021, pp. 58345847.   
[21] H. Wang, W. Wang, W. Liang, C. Xiong, and J. Shen, "Structured .   
[22] E. Parisotto and R. Salakhutdinov, "Neural map: Structured memory for deep reinforcement learning," in ICLR, 2018.   
. eo framework based on multimodal large language models," arXiv preprint arXiv:2507.13152, 2025.   
[24] C. Wang, S. Wei, and J. Qi, "Matchnav: Llm-based enhanced description and instruction matching in vision-and-language navigation," Inf. Fusion, vol. 125, p. 103444, 2026.   
[25] J. F. Henriques and A. Vedaldi, "Mapnet: An allocentric spatial memory for mapping environments," in CVPR, 2018, pp. 8476 8484.   
[26] S. K. Ramakrishnan, Z. Al-Halah, and K. Grauman, "Occupancy anticipation for efficient exploration and navigation," in ECCV, 2020, pp. 400418.   
[27] V. Cartillier, Z. Ren, N. Jain, S. Lee, I. Essa, and D. Batra, "Semantic mapnet: Building allocentric semantic maps and representations fro egocentric views," in AAAI, vol. 35, 2021, pp. 964972. maps for robot navigation," in ICRA, 2023, pp. 10 60810 615.   
[9] Z. Te  6: bird's-eye view," in WACV, 2024, pp. 373382.   
[30] R. Liu, X. Wang, W. Wang, and Y. Yang, "Bird's-eye-view scene graph for vision-language navigation," in ICCV, 2023, pp. 10968 10 980.   
[31] D. S. Chaplot, R. Salakhutdinov, A. Gupta, and S. Gupta, "Neural icl samal ation, R, 00 . 12884.   
[32] N. Kim, O. Kwon, H. Yoo, Y. Choi, J. Park, and S. Oh, "Topological semantic graph memory for image-goal navigation," in CoRL, 2023, pp. 393402.   
grounded robot navigation," in RSS, 2024.   
Hae   Leag latn na  pan om pixels," in ICML, 2019, pp. 25552565.   
[35] D. Hafner, T. Lillicrap, J. Ba, and M. Norouzi, "Dream to control: Learning behaviors by latent imagination," in ICLR, 2020.   
[36] J. Lin et al., "Learning to model the world with language," in ICML, vol. 235, 2024, pp. 2999230017.   
[ J. W, H. M C. De, and . Lo, "re-ra cetalzd in NeurIPS, vol. 36, 2024.   
[38] X. Ma, S. Chen, D. Hsu, and W. S. Lee, "Contrastive variational reinforcement learning for complex observations," in CoRL, 2021, pp. 959972.   
[39] M. Okada and T. Taniguchi, "Dreaming: Model-based reinforceICRA, 2021, pp. 42094215.   
[40] A. Bar, G. Zhou, D. Tran, T. Darrell, and Y. LeCun, "Navigation world models," in CVPR, 2025, pp. 15 79115 801.   
[41] J. Li and M. Bansal, "Panogen: Text-conditioned panoramic environment generation for vision-and-language navigation," in NeurIPS, vol. 36, 2023, pp. 21 87821 894.   
[42] X. Yao, J. Gao, and C. Xu, "Navmorph: A self-evolving world model for vision-and-language navigation in continuous environments," arXiv preprint arXiv:2506.23468, 2025.   
[43] J. Y. Koh, H. Lee, Y. Yang, J. Baldridge, and P. Anderson, "Pathdreamer: A world model for indoor navigation," in ICCV, 2021, pp. 1473814748.   
[44] G. Georgakis, K. Schmeckpeper, K. Wanchoo, S. Dan, E. Miltsakaki, D. Roth, and K. Daniilidis, "Cross-modal map learning for vision and language navigation," in CVPR, 2022, pp. 15 460 15 470.   
[45] Y. Pan, Y. Xu, Z. Liu, and H. Wang, "Planning from imagnation: Episodic simulation and episodic memory for vision-andlanguage navigation," in AAAI, vol. 39, 2025, pp. 63456353.   
[46] J. Li and M. Bansal, "Improving vision-and-language navigation by geneating uture-vie ma smanti," i , 202, pp. 10 80310 812.   
[47] S. Wang et al., "Monodream: Monocular vision-language navigation with panoramic dreaming," arXiv preprint arXiv:2508.02549, 2025.   
[48] H. Le, T. Karimpanal George, M. Abdolshah, T. Tran, and S. Venkatesh, "Model-based episodic memory induces dynamic hybrid controls," in NeurIPS, vol. 34, 2021, pp. 3031330 325.   
[49] D. P. Kingma and M. Welling, "Auto-encoding variational bayes," in ICLR, 2013.   
[50] A. v. d. Oord, Y. Li, and O. Vinyals, "Representation learning with contrastive predictive coding," arXiv preprint arXiv:1807.03748, 2018.   
[51] S. K. Ramakrishnan et al., "Habitat-matterport 3d dataset (hm3d): 1000 large-scale 3d environments for embodied ai," in NeurIPS D&B, 2021.   
[52] P. Anderson et l., "On evaluation of embodied navigation agents," arXiv preprint arXiv:1807.06757, 2018.   
[53] H. Wang, W. Wang, T. Shu, W. Liang, and J. Shen, "Active visual information gathering for vision-language navigation," in ECCV, 2020, pp. 307322.   
[54] D. Wang, E. Shelhamer, S. Liu, B. Olshausen, and T. Darrell, "Tent: Fully test-time adaptation by entropy minimization," in ICLR, 2021.   
[5] . Niu et al., "Towards stable test-time adaptation in dynamic wild world," in ICLR, 2023.