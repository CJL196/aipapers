# Adversarial Distribution Matching for Diffusion Distillation Towards Efficient Image and Video Synthesis

Yanzuo $\mathbf { L u } ^ { 1 , 2 }$ , Yuxi Ren2, Xin Xia², Shanchuan $\mathbf { L i n } ^ { 2 }$ , Xing Wang2, Xuefeng Xiao2\*, Andy J. $\mathbf { M } \mathbf { a } ^ { 1 , 3 , 4 \dagger }$ , Xiaohua Xie1,3,4,5, Jian-Huang Lai1,3,4,5

1Sun Yat-Sen University 2ByteDance Seed Vision 3Guangdong Provincial Key Laboratory of Information Security Technology, China 4Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, China 5Pazhou Lab (HuangPu), Guangzhou, China

oliveryanzuolu@gmail.com,xiaoxuefeng.ailab@bytedance.com,majh8@mail.sysu.edu.cn

# Abstract

Distribution Matching Distillation (DMD) is a promising score distillation technique that compresses pre-trained teacher diffusion models into efficient one-step or multi-step student generators. Nevertheless, its reliance on the reverse Kullback-Leibler (KL) divergence minimization potentially induces mode collapse (or mode-seeking) in certain applications. To circumvent this inherent drawback, we propose Adversarial Distribution Matching (ADM), a novel framework that leverages diffusion-based discriminators to align the latent predictions between real and fake score estimators for score distillation in an adversarial manner. In the context of extremely challenging one-step distillation, we further improve the pre-trained generator by adversarial distillation with hybrid discriminators in both latent and pixel spaces. Different from the mean squared error used in DMD2 pre-training, our method incorporates the distributional loss on ODE pairs collected from the teacher model, and thus providing a better initialization for score distillation fine-tuning in the next stage. By combining the adversarial distillation pre-training with ADM finetuning into a unified pipeline termed DMDX, our proposed method achieves superior one-step performance on SDXL compared to DMD2 while consuming less GPU time. Additional experiments that apply multi-step ADM distillation on SD3-Medium, SD3.5-Large, and CogVideoX set a new benchmark towards efficient image and video synthesis.

# 1. Introduction

Recent methods for accelerating diffusion models [12, 17, 63, 65] have concentrated on reducing the sampling steps through distillation. The distillation process generally trains a more efficient generator (a.k.a student model) that approximates the output distribution of the pretrained teacher. This field has observed multiple pathways developing in parallel with progressive distillation [23, 58], consistency distillation [18, 34, 35, 43, 55, 66, 69, 70, 92], score distillation [3, 16, 21, 38, 55, 59, 61, 8487, 93], rectified flow [27, 28, 30, 71, 81] and adversarial distillation [23, 24, 43, 55, 60, 61, 85, 91]. They are currently appearing as distinct yet non-exclusive research directions.

![](images/1.jpg)  
Figure 1. In these images, some are generated by the baseline SDXL via 50NFE, the others with our DMDX in 1NFE. Can you tell which is the accelerated one? Answers in the footnote#.

Distribution Matching Distillation (DMD) [85, 86] is a promising score distillation [51] approach, which distills the powerful text-to-image diffusion model SDXL-Base [56] into a one-step generator with great fidelity. While DMD's primary contribution lies in introducing additional regularizers to constrain the distribution matching loss, its predecessor Diff-Instruct [37] pioneered the use of a fake score estimator to approximate the student model's output distribution. This contrasts with the distillation loss in Adversarial Diffusion Distillation (ADD) [61], which directly employs the student model itself as score estimator. Theoretically, an intrinsic correspondence for DMD and the distillation loss in ADD can be equivalently found between Variational Score Distillation (VSD) [74] and Score Distillation Sampling (SDS) [51] in text-to-3D generation, where SDS represents a specialized instance of VSD through the use of a single-point Dirac distribution as the variational distribution [74]. And thus the VSD loss in DMD performs better than the SDS loss in ADD. Intuitively, since the capacity of a diffusion model decreases significantly when distilled for few-step generations [23], the student model no longer serves as a decent score estimator as the teacher. Therefore, the score-based distillation loss in ADD hardly contributes to its final performance.

However, the optimization of DMD loss relies on the reverse Kullback-Leibler (KL) divergence minimization which is zero-forcing that drives low-probability regions to zero, causing the model to focus on only a few dominant modes and potentially leading to mode collapse [45]. To enhance sample diversity, DMD [86] employs an ODE-based (Ordinary Differential Equation) regularizer with synthetic data, while DMD2 [85] introduces a GAN-based (Generative Adversarial Network [8]) regularizer with real data to counterbalance this side effect. Subsequent efforts such as Moment Matching Distillation (MMD) [59], Score identity Distillation (SiD) [93] and Score Implicit Matching (SIM) [38] all used a variant of Fisher divergence to align the fake score estimator with pre-trained real score estimator [16]. Despite great success, their ability for distribution matching is limited by their dependence on a predefined form of explicit divergence metric. In this work, we would like to raise and investigate a question: Can we bypass the limitations of a predefined divergence by developing a framework that learns an implicit, data-driven discrepancy measure, thereby enabling more flexible and fine-grained matching of complex, high-dimensional distributions?

Since the multifaceted alignment requirements in complex multimodal text-conditioned image or even video generation may not be fully captured, this motivates our exploration of more adaptive discrepancy learning paradigms. As our first contribution, we show how to align the latent predictions between real and fake score estimators to realize score distillation in an adversarial manner with prior knowledge of teacher model and dynamically learnable parameters, termed Adversarial Distribution Matching (ADM). This is different from the ODE-based or GAN-based regularizer in DMD and DMD2 that is used to counterbalance the mode collapse effect of reverse KL divergence. In contrast, we are performing distribution matching by means of GAN training to replace and circumvent the use of DMD loss, which we will provide more discussion in Sec. 4.1.3.

Our second contribution concerns about the extremely challenging one-step distillation, where we observe that the score distillation has a higher risk of gradient exploding and vanishing. We attribute the issue more to the lower overlap of support sets between the student and teacher distributions, not just the approximation errors in the fake score estimator as attributed in DMD2 [85]. In other words, while score distillation yields superior generation quality, it imposes higher requirements on initialization especially when distilled in extremely few step, which we will provide more in-depth analysis in Sec. 4.3.2. Although we notice that an ODE-based pre-training on synthetic data is used for SDXL one-step distillation in DMD2 implementation, the mean squared error loss was probably still not enough to provide more overlapping regions of support sets between the student and teacher distributions. Our experiments demonstrate that when providing a better initialization for distribution matching, the effect of Two Time-scale Update Rule (TTUR) [2] to the final performance is very limited.

Naturally, our third contribution focuses on providing a better initialization for further score-based fine-tuning. We employ adversarial distillation to trade off sample quality and mode converge inspired by SDXL-Lightning [23] and LADD [60]. With this distribution-level loss optimization, we can pre-train the student model to capture more potential modes of the teacher model distribution, especially through a hybrid discriminator in both latent and pixel spaces which we will introduce later in Sec. 4.2. To facilitate the diversity, we also propose employing a cubic timestep schedule for the generator to bias towards higher noise levels.

By combining adversarial distillation pre-training with ADM fine-tuning into a unified pipeline termed DMDX, our one-step SDXL provides competitive fidelity compared to the baseline with $5 0 \times$ acceleration in the Number of Function Evaluation (NFE) as shown in Fig. 1. More experiments on multi-step ADM distillation across the best existing diffusion models including SD3-Medium, SD3.5- Large [6], and CogVideoX [83] consistently achieve new benchmarks for efficient image and video synthesis.

# 2. Related Work

Progressive Distillation was proposed in [58] to distill multi-step prediction into one-step prediction of the same distance along the trajectory. SDXL-Lightning [23] extended this idea by utilizing GAN training and achieved one-step high-resolution $( 1 0 2 4 \mathrm { p x } )$ generation for the first time. This approach involving multi-stage process can be quite cumbersome, as it requires iteratively distilling from its predecessor, halving the sampling steps each time.

Consistency Distillation proposed [7, 34, 35, 64, 66] to enforce the consistency property into diffusion model, i.e. predictions towards original sample are consistent for arbitrary pairs of noisy timesteps belong to the same trajectory. Subsequent efforts extended it into the trajectory consistency to relax the training objectives, i.e. predictions towards noisy samples of arbitrary subsequent timesteps are consistent, including CTM [18], TCD [92], TSCD [55] and PCM [69].

Rectified Flow [27, 28] aims to obtain faster straight trajectories through multiple reflow processes that iteratively learn the velocity of many ODE pairs from its predecessor. PeRFlow [81] tried splitting the trajectory into pieces and applying piece-wise rectification with real data samples. In this paper, we empirically found the straightness can also be satisfied via an adversarial distillation paradigm without the need for repeatedly collecting tremendous synthetic data.

Score Distillation intuitively tries to keep that a sample appearing in the student model distribution at a specific noise level is with the same probability as it does in the teacher model distribution,. In light of the motivation that relying on a single divergence might be problematic, a concurrent work Score-of-Mixture Distillation (SMD) [16] shares our perspective and explicitly designed a class of $\alpha$ -skew JensenShannon (JS) divergences for optimization. In contrast, we implicitly measure the discrepancy between the fake and real score estimators in an adversarial manner, facilitating a more capable and adaptive discrepancy learning paradigm along the distillation process.

Adversarial Distillation employs a discriminator to align the student model distribution with a specific target distribution [40, 94]. While methods UFOGen [79], DMD2 [85], and APT [24] directly align with the real data distribution, SDXL-Lightning [23] and Hyper-SD [55] utilize intermediate timestep predictions from the teacher model as approximation objectives. Inspired by LADD [60], our adversarial distillation pre-training aligns with ODE-based synthetic data generated from the teacher model.

Human Feedback Learning was firstly considered by ReFL [78] in diffusion models that includes two stages of reward model training and preference fine-tuning. HyperSD [55] treated this as a standalone technique, ultimately applying LoRA insertion to shift the output distribution of the low-step generator toward human preferences. Subsequent studies, notably [36, 39], further attempted to integrate CFG and Score-based divergence.

# 3. Preliminaries

# 3.1. Diffusion Model

Given a training data distribution $p _ { \mathrm { d a t a } }$ with standard deviation $\sigma _ { \mathrm { d a t a } }$ , the diffusion model [12] generates samples by reversing the forward diffusion process that progressively adds noise to a data sample $\pmb { x } _ { 0 } \sim p _ { d a t a }$ as,

$$
\mathbf { \boldsymbol { q } } ( \mathbf { \boldsymbol { x } } _ { t } | \mathbf { \boldsymbol { x } } ) \sim \mathcal { N } ( \alpha _ { t } \mathbf { \boldsymbol { x } } , \sigma _ { t } ^ { 2 } \mathbf { \boldsymbol { I } } ) ,
$$

where $\alpha _ { t } \geq 0 , \sigma _ { t } > 0$ are specified noise schedules such that $\alpha _ { t } / \sigma _ { t }$ satisfies monotonically decreasing w.r.t. $t$ and larger $t$ indicates greater noise. We consider two different formulations for denoising models.

DDPM and DDIM [12, 63] assume discrete-time schedules with $t \in [ 1 , T ]$ (typically $T = 1 0 0 0$ and noise prediction parameterization\* [58]. The training objective is given by,

$$
\begin{array} { r } { \mathbb { E } _ { \pmb { x } _ { 0 } , t , \epsilon \sim \mathcal { N } ( \mathbf { 0 } , I ) } [ w ( t ) \| \epsilon _ { \theta } ( \pmb { x } _ { t } , t ) - \epsilon \| _ { 2 } ^ { 2 } ] , } \end{array}
$$

where $w ( t )$ is a weighting function and $\epsilon _ { \theta }$ is a neural network with parameters $\theta$ . The noise schedule is defined as $\alpha _ { t } = \sqrt { \bar { \alpha } _ { t } } , \sigma _ { t } = \sqrt { 1 - \bar { \alpha } _ { t } }$ such that ${ \pmb x } _ { t } = \sqrt { \bar { \alpha } _ { t } } { \pmb x } _ { 0 } +$ $\sqrt { 1 - \bar { \alpha } _ { t } } \epsilon$ For DDIM sampling, it solves the ProbabilityFlow Ordinary Differential Equations $( P F  – O D E )$ [65] by $\begin{array} { r } { d \bar { \bf x } _ { t } = \epsilon _ { \theta } \big ( \frac { { \bar { \bf x } } _ { t } } { \sqrt { \bar { \sigma } _ { t } ^ { 2 } + 1 } } \big ) d \bar { \boldsymbol { \sigma } } _ { t } } \end{array}$ where $\begin{array} { r } { \bar { \mathbfit { x } } _ { t } = \frac { \mathbfit { x } _ { t } } { \sqrt { \bar { \alpha } _ { t } } } } \end{array}$ and $\begin{array} { r } { \bar { \sigma } _ { t } = \sqrt { \frac { 1 - \bar { \alpha } _ { t } } { \bar { \alpha } _ { t } } } } \end{array}$ starting from $\pmb { x } _ { T } \sim \mathcal { N } ( \mathbf { 0 } , I )$ and stopping at $\scriptstyle { \mathbf { { \mathit { x } } } } _ { 0 }$ .

Flow Matching [26, 27, 30] uses velocity prediction parameterization [58] and continuous-time coefficients (typically $\alpha _ { t } = 1 - t , \sigma _ { t } = t$ with $t \in [ 0 , T = 1 ] \forall$ . The conditional probability path or the velocity we say can be given as $\begin{array} { r } { \pmb { v } _ { t } = \frac { d \alpha _ { t } } { d t } \pmb { x } _ { 0 } + \frac { \bar { d } \sigma _ { t } } { d t } \epsilon } \end{array}$

$$
\begin{array} { r } { \mathbb { E } _ { \pmb { x } _ { 0 } , t , \epsilon \sim \mathcal { N } ( \mathbf { 0 } , I ) } [ w ( t ) \| \pmb { v } _ { \theta } ( \pmb { x } _ { t } , t ) - \pmb { v } _ { t } \| _ { 2 } ^ { 2 } ] , } \end{array}
$$

where $w ( t )$ is a weighting function and ${ \pmb v } _ { \theta }$ is a neural network parameterized by $\theta$ The sampling procedure starts from $t = T$ with $\pmb { x } _ { T } \sim \mathcal { N } ( \mathbf { 0 } , I )$ and stops at $t = 0$ , solving the PF-ODE by $d { \pmb x } _ { t } = { \pmb v } _ { \theta } ( { \pmb x } _ { t } , t ) d t$ .

# 3.2. Distribution Matching Distillation

DMD [85, 86] distills pretrained diffusion models ${ \cal F } _ { \phi } ( { \bf x } _ { t } , t )$ into one-step or multi-step efficient generators $G _ { \theta } ( \pmb { x } _ { t } , t )$ by minimizing the reverse KL divergence between the target distribution $p _ { \mathrm { r e a l } }$ and the efficient generator output distribution $p _ { \mathrm { f a k e } }$ . The gradient of DMD objective w.r.t. $\theta$ is,

$$
\begin{array} { r } { \nabla _ { \theta } \mathcal { L } _ { \mathrm { D M D } } = \underset { z , t ^ { \prime } , t , x _ { t } } { \mathbb { E } } - [ ( s _ { \mathrm { r e a l } } ( { x _ { t } } ) - s _ { \mathrm { f a k e } } ( { x _ { t } } ) ) \frac { d G _ { \theta } ( z , t ^ { \prime } ) } { d \theta } ] , } \end{array}
$$

where $\mathbf z ~ \sim ~ \mathcal N ( \mathbf 0 , I )$ , $t ^ { \prime }$ is randomly selected from predefined generator schedule, $t \sim \mathcal { U } ( 0 , T )$ , and noisy samples $\pmb { x } _ { t } = \pmb { q } ( \pmb { x } _ { t } | \hat { \pmb { x } } _ { 0 } )$ are obtained by randomly diffusing the generator output $\hat { \textbf { \textit { x } } } _ { 0 } ~ = ~ G _ { \theta } ( z , t ^ { \prime } )$ . Score functions $s _ { \mathrm { r e a l } } ( { \pmb x } _ { t } ) = \nabla _ { { \pmb x } _ { t } } \log p _ { \mathrm { r e a l } } ( { \pmb x } _ { t } )$ , $s _ { \mathrm { f a k e } } ( \pmb { x } _ { t } ) = \nabla _ { \pmb { x } _ { t } } \log p _ { \mathrm { f a k e } } ( \pmb { x } _ { t } )$ are vector fields that point towards higher density of data at a given noise level [17, 65] for $p _ { \mathrm { r e a l } }$ and $p _ { \mathrm { f a k e } }$ , respectively.

While the real score estimator is the teacher model $\mathbf { \mathcal { F } } _ { \phi } ( \mathbf { \boldsymbol { x } } _ { t } , t )$ itself, the fake score estimator $\mathbf { \boldsymbol { f } } _ { \psi } ( \mathbf { \boldsymbol { x } } _ { t } , t )$ is initialized the same as $\mathbf { \mathcal { F } } _ { \phi } ( \mathbf { \boldsymbol { x } } _ { t } , t )$ and dynamically learned to describe $p _ { \mathrm { f a k e } }$ with pretrain loss as Eqs. (2) and (3). In practice, the gradient in Eq. (4) is computed as,

$$
g r a d ( \hat { { \pmb x } } _ { 0 } , { \pmb x } _ { t } , t ) = \frac { { \pmb f } _ { \psi } ( { \pmb x } _ { t } , t ) - { \pmb F } _ { \phi } ( { \pmb x } _ { t } , t ) } { \| \hat { { \pmb x } } _ { 0 } - { \pmb F } _ { \phi } ( { \pmb x } _ { t } , t ) \| _ { 1 } }
$$

such that the training loss is implemented like,

$$
\mathcal { L } _ { \mathrm { D M D } } ( \theta ) = \underset { z , t ^ { \prime } , t , { \pmb x } _ { t } } { \mathbb { E } } [ \| \hat { \pmb x } _ { 0 } - s g ( \hat { \pmb x } _ { 0 } - g r a d ( \hat { \pmb x } _ { 0 } , { \pmb x } _ { t } , t ) ) \| _ { 2 } ^ { 2 } ] ,
$$

where $s g ( \cdot )$ denotes the stop gradient operation.

![](images/2.jpg)

# 4. Methodology

# 4.1. Adversarial Distribution Matching

Instead of using a predefined divergence between the fake and real distributions, we use an implicit, data-driven discrepancy measure through an adversarial discriminator. Specifically, our discriminator $D _ { \tau } ( \pmb { x } _ { t } , t )$ consists of a frozen latent diffusion model initialized the same as teacher model $\mathbf { \mathcal { F } } _ { \phi } ( \mathbf { \boldsymbol { x } } _ { t } , t )$ and multiple trainable heads added upon different UNet [57] or DiT [49] blocks. Given the noisy sample ${ \pmb x } _ { t } = { \pmb q } ( { \pmb x } _ { t } | \hat { \pmb x } _ { 0 } )$ diffused from the output of few-step generator $\hat { \pmb { x } } _ { 0 } = \pmb { G } _ { \theta } ( \pmb { z } , t ^ { \prime } )$ , the score estimators no longer $\pmb { x } _ { 0 } ^ { \mathrm { f a k e } } = \pmb { f } _ { \psi } ( \pmb { x } _ { t } , t )$ and ${ \pmb x } _ { 0 } ^ { \mathrm { r e a l } } = { \pmb F } _ { \phi } ( { \pmb x } _ { t } , t )$ as used in Eq. (5).

Instead, we set a fixed timestep interval $\Delta t$ (defaults to $T / 6 4 )$ and solve the PF-ODE w.r.t. $( t - \Delta t )$ , such that the fake sample $\pmb { x } _ { t - \Delta t } ^ { \mathrm { f a k e } }$ and real sample ${ \pmb x } _ { t - \Delta t } ^ { \mathrm { r e a l } }$ can be obtained to serve as score predictions and sent into the discriminator. The discriminator hierarchically aggregates features from frozen backbone layers and dynamically weights them through multiple learnable heads, establishing an adaptive discrepancy metric that leverages both diffusion priors and data-driven trainable dynamics. We use Hinge loss [22] to train the generator $G _ { \theta } ( \pmb { x } _ { t } , t )$ and discriminator $D _ { \tau } ( \pmb { x } _ { t } , t )$ $\pmb { x } _ { t - \Delta t } ^ { \mathrm { f a k e } }$ ${ \pmb x } _ { t - \Delta t } ^ { \mathrm { r e a l } }$ iction

$$
\mathcal { L } _ { \mathrm { G A N } } ( \theta ) = \underset { { \substack { \mathbf { x } _ { t - \Delta t } ^ { \mathrm { f a k e } } } } } { \mathbb { E } } [ - D _ { \tau } ( { \boldsymbol x } _ { t - \Delta t } ^ { \mathrm { f a k e } } , t - \Delta t ) ]
$$

$$
\begin{array} { r } { \mathcal { L } _ { \mathrm { G A N } } ( \tau ) = \underset { x _ { t - \Delta t } ^ { \mathrm { f a k e } } , x _ { t - \Delta t } ^ { \mathrm { r e a l } } } { \mathbb { E } } \left[ \operatorname* { m a x } ( 0 , 1 + D _ { \tau } ( x _ { t - \Delta t } ^ { \mathrm { f a k e } } , t - \Delta t ) ) \right. } \\ { \left. + \operatorname* { m a x } ( 0 , 1 - D _ { \tau } ( x _ { t - \Delta t } ^ { \mathrm { r e a l } } , t - \Delta t ) ) \right] } \end{array}
$$

In conjunction with the dynamically learned fake model, we clarify our training procedure in Appendix A. The overall pipeline is demonstrated in Fig. 2.

# 4.1.1. Motivation of Discriminator Timestep $( t - \Delta t )$

Given that the final objective of score distillation is to match the probability flows varying with noise levels of student and teacher models to exactly the same, timestep information must be considered when measuring the discrepancy between distributions. This coincides with our discriminator design that uses a pre-trained diffusion model, and we take a small step alongside the PF-ODE, succeeding in preserving the input timestep information of score estimators.

# 4.1.2. Data-driven Effect

The flexibility of using discriminators for distributional discrepancy measure is not only in the noise level of score function, but also within the distillation process. As distillation iterates, the model is exposed to increasingly diverse data, causing the discrepancies in modes between the two distributions to change. In the early phase of training where the discrepancy is significant, a more global evaluation is necessary, whereas in the later phases, when the discrepancy becomes minor, a more localized, fine-grained optimization might become essential. In other words, driven by the volume of data, the divergence measure employed across different training phases can vary.

![](images/3.jpg)  
Figure 3. Changes of DMD loss over multi-step ADM distillation for CogVideoX. Note that we did not optimize this objective directly during ADM distillation but recorded it over iterations.

# 4.1.3. Relationship with DMD and DMD2

To mitigate the mode collapse issue in DMD loss, an ODEbased regularizer and a GAN-based regularizer are additionally used for distillation in DMD [85] and DMD2 [85], respectively. However, these two regularizers do not fundamentally address the mode-seeking behavior introduced by reverse KL divergence as shown in Fig. 4(a), but rather counterbalance it by trade-offs between losses. In ADM, our adversarial loss is actually playing the role of DMD loss to realize score distillation with an implicit, data-driven discrepancy measure instead of a predefined divergence. Therefore, our motivation for using GAN training in ADM is different from that of DMD2 [85] and we don't require additional regularizers.

Intuitively, the learnable discriminator can approximate any nonlinear function to implicitly measure distribution divergence, which probably inherently encompasses the reverse KL divergence in DMD loss. As shown in Fig. 3, we visualize the changes of DMD loss in Eq. (6) during the multi-step ADM distillation on CogVideoX [83]. Though not directly optimize on Eq. (6), the results indicate a very steady downward trend that supports our assumption. More theoretical discussion is provided in Sec. 4.3.3.

# 4.2. Adversarial Distillation Pre-training

To stabilize the extremely difficult one-step distillation, we opt to provide a better initialization for ADM fine-tuning with adversarial distillation pre-training on synthetic data. Our pre-training configuration refer to Rectified Flow [27] in several aspects, where we 1) collect the ODE pairs from teacher model in an offline manner, 2) construct noisy samples by linearly interpolation between pure noise and clean data sample of the ODE pair, 3) alter the prediction target of the generator to the velocity of ODE pair.

As for adversarial training, we formulate a latent-space discriminator $D _ { \tau _ { 1 } } ( \pmb { x } _ { t } , t )$ initialized from teacher model and a pixel-space discriminator $D _ { \tau _ { 2 } } ( \pmb { x } )$ initialized from the vision encoder of SAM [19] model, respectively, as shown in Fig. 2. We also append multiple trainable heads to both the backbone networks similar to the practice in ADM. All these contribute to increasing the discriminative capability, facilitating the student model in discovering more potential modes in the teacher model distribution. Specifically, let $\tilde { \pmb { x } } _ { 0 } = \pmb { G } _ { \theta } ( \pmb { x } _ { t } , t )$ denote the predicted PF-ODE endpoint of generator, where $\mathbf { \Delta } _ { \mathbf { \mathcal { X } } _ { t } }$ represents the noisy sample interpolated between an ODE pair $( { \pmb x } _ { T } , { \pmb x } _ { 0 } )$ with random timestep $t \in [ 0 , T ]$ . For latent-space discriminator, we diffuse the generator output with another random noise and timestep $t ^ { \prime } ~ \in ~ ( 0 , T ]$ ,getting $\tilde { \mathbf { x } } _ { t ^ { \prime } } = \mathbf { q } ( \tilde { \mathbf { x } } _ { t ^ { \prime } } | \tilde { \mathbf { x } } _ { 0 } )$ as its input. For pixel-space discriminator, the generator output will be first decoded via VAE decoder and then sent into the vision encoder. The training objective, also based on Hinge loss [22], encourages the generator output $\tilde { \mathbf { x } } _ { 0 }$ to be closer to the synthetic data sample $\scriptstyle { \mathbf { { \mathit { x } } } } _ { 0 }$ :

$$
\mathcal { L } _ { \mathrm { G A N } } ( \theta ) = \underset { \tilde { \mathbf { x } } _ { 0 } , t ^ { \prime } } { \mathbb { E } } - [ \lambda _ { 1 } D _ { \tau _ { 1 } } ( \tilde { { \mathbf { x } } } _ { t ^ { \prime } } , t ^ { \prime } ) + \lambda _ { 2 } D _ { \tau _ { 2 } } ( \tilde { { \mathbf { x } } } _ { 0 } ) ]
$$

$$
\begin{array} { r } { \mathcal { L } _ { \mathrm { G A N } } ( \tau _ { 1 } , \tau _ { 2 } ) = \underset { x _ { 0 } , \tilde { x } _ { 0 } , t ^ { \prime } } { \mathbb { E } } [ \lambda _ { 1 } \cdot \operatorname* { m a x } ( 0 , 1 + D _ { \tau _ { 1 } } ( \tilde { x } _ { t ^ { \prime } } , t ^ { \prime } ) ) } \\ { + \lambda _ { 2 } \cdot \operatorname* { m a x } ( 0 , 1 + D _ { \tau _ { 2 } } ( \tilde { x } _ { 0 } ) ) } \\ { + \lambda _ { 1 } \cdot \operatorname* { m a x } ( 0 , 1 - D _ { \tau _ { 1 } } ( x _ { t ^ { \prime } } , t ^ { \prime } ) ) } \\ { + \lambda _ { 2 } \cdot \operatorname* { m a x } ( 0 , 1 - D _ { \tau _ { 2 } } ( x _ { 0 } ) ) ] } \end{array}
$$

We empirically found that setting balancing coefficients $\lambda _ { 1 } = 0 . 8 5 , \lambda _ { 2 } = 0 . 1 5$ produces visually coherent results.

# 4.2.1. Cubic Generator Timestep Schedule

Intuitively, higher noise levels encourage exploration of new modes by weakening the restrictive information encoded in latent representations. Therefore, we propose employing a cubic timestep schedule for the generator. The schedule maps uniform $[ 0 , T )$ samples through $[ 1 ~ -$ $( t / T ) ^ { 3 } ] * T$ , non-linearly concentrating values near $T$ with heavy noise similar to LADD [60].

# 4.2.2. Uniform Discriminator Timestep Schedule

Unlike in ADM where the discriminator is firstly used for score distillation, the utilization of latent-space discriminator is common for adversarial distillation works. Inspired by SDXL-Lightning [23], where they found that the diffusion encoder is trained to focus on high-frequency details at lower timesteps and low-frequency structures at higher timesteps, we set a uniform $( 0 , T ]$ for discriminator timestep $t ^ { \prime }$ to capture both advantages during pre-training.

# 4.2.3. Relationship with LADD

Our motivation to perform adversarial distillation on synthetic data is inspired by LADD [60], but differs in a lot that we 1) construct noisy samples via ODE pairs in the style of Rectified Flow [27] rather than random noise, 2) develop a cubic generator timestep schedule that facilitate deterministic Euler sampling rather than consistency ones, 3) introduce an additional pixel-space encoder to increase the capability of discriminator and find more modes.

# 4.3. Discussion

# 4.3.1. Difference between ADM and ADP

One question may appear like what is the difference between these two adversarial approaches with latent-space discriminators? The effectiveness in score distillation correlates with the fact that score function $\nabla _ { \pmb { x } } \log p ( \pmb { x } ; \pmb { \sigma } ( t ) )$ is defined at different noise levels $\sigma ( t )$ . In contrast, adversarial distillation only aligns the distribution of clean data samples when $t = 0$ . While the latent-space discriminator in pre-training is capturing information at different scales and details by randomly diffusing the generator output, the crux for ADM is more than that by solving the PF-ODE of both score estimators. In other words, ADM additionally supervises the complete denoising process through noisy samples that are with higher density at different noise levels of respective distribution.

This leads to a situation in ADM when two distributions initialized with less overlap in support sets, the noisy samples remain in regions unfamiliar to each other, and the discriminator can easily distinguish between them thus leading to extreme gradient signals. However, since Gaussian noise is isotropic, we artificially create overlapping regions for the randomly diffused samples in ADP to make the discrimination more difficult, resulting in relatively smooth gradients. Therefore, our ADM still falls under score distillation given that it encourages the entire probability flows to be closer, while the pre-training belongs to adversarial distillation because it only cares about the clean data distribution at $t = 0$ .

# 4.3.2. Importance of Pre-training

Another question we haven't discussed so far is why do we need pre-training for one-step score distillation? Taking the reverse KL divergence used by DMD loss as an example:

$$
\mathbb { D } _ { \mathrm { K L } } ( p _ { \mathrm { f a k e } } \Vert p _ { \mathrm { r e a l } } ) = \int p _ { \mathrm { f a k e } } ( x ) \log \frac { p _ { \mathrm { f a k e } } ( x ) } { p _ { \mathrm { r e a l } } ( x ) } d x .
$$

When employing one-step distillation, the generator output is worse in visual fidelity and structural integrity compared to multi-step sampling, leading $p _ { \mathrm { f a k e } } ( { \pmb x } )  0$ where lT  ) approaching zero $0 \cdot ( - \infty )$ causes the optimization to avoid regions where $p _ { \mathrm { r e a l } } ( { \pmb x } ) > 0$ but $p _ { \mathrm { f a k e } } ( \pmb { x } )$ has negligible density, a phenomenon called zero-forcing. Instead of fully covering $p _ { \mathrm { r e a l } }$ s support, $p _ { \mathrm { f a k e } }$ collapses to a subset of modes of $p _ { \mathrm { r e a l } }$ , inducing mode-seeking behavior as illustrated in Fig. 4(a). During training this manifests itself as gradient vanishing sometimes. And conversely, the fuzzy samples that the diffusion model typically produces through one step are also not within the teacher model distribution, yielding $p _ { \mathrm { r e a l } } ( { \pmb x } )  0$ where $p _ { \mathrm { f a k e } } ( { \pmb x } ) > 0$ in regions and the integrand $\begin{array} { r } { p _ { \mathrm { f a k e } } ( \pmb { x } ) \log \frac { p _ { \mathrm { f a k e } } ( \pmb { x } ) } { 0 } } \end{array}$ diverging to $+ \infty$ , and thus resulting in numerical instability and gradient exploding.

Similarly, when the support sets of the student and teacher distributions overlap hardly at all, forward KL divergence approaches $+ \infty$ where $p _ { \mathrm { f a k e } } ( { \pmb x } ) > 0$ , while JS divergence saturates to a constant $\log 2$ and Fisher divergence may degenerate without definition. Therefore, when this assumption is undermined, many of the single divergences no longer apply and a better initialization with more overlapping regions becomes essential as exemplified in Fig. 4(b).

![](images/4.jpg)  
Figure 4. Illustration for theoretical discussion.

# 4.3.3. Theoretical Objective

The last question is why ADM is better than DMD loss theoretically? In fact, the Hinge GAN [22] we use has been proven to minimize Total-Variation Distance (TVD) [68]:

$$
T V ( p _ { \mathrm { f a k e } } , p _ { \mathrm { r e a l } } ) = \int | p _ { \mathrm { f a k e } } ( { \pmb x } ) - p _ { \mathrm { r e a l } } ( { \pmb x } ) | d { \pmb x }
$$

That is to say, when the discriminator is sufficiently rich and well trained, the theoretical optimum of Hinge loss upon convergence minimizes TVD. When the support sets of fake and real distributions have minimal overlap, TVD provides two key advantages over reverse KL divergence: 1) Symmetry: TVD yields the same discrepancy measure regardless of the initial distribution, whereas the asymmetric reverse KL may exhibit mode-seeking behavior and neglect other portions of the overall distribution. For example, as illustrated in Fig. 4(c), TVD maintains substantial loss values and provides optimization directions covering the mode when $p _ { \mathrm { f a k e } } ( { \pmb x } )  0$ while $p _ { \mathrm { r e a l } } ( { \pmb x } ) ~ > ~ 0$ , whereas the reverse KL divergence suffers from gradient vanishing issues in such scenarios, as discussed in Sec. 4.3.2. 2) Boundedness: TVD is bounded within [0,1], and thus mitigating disruptions from outliers during training, especially in our high-dimensional multimodal text-conditioned image and video distributions, avoiding the numerical instability inherent to reverse KL divergence caused by gradient exploding.

Table 1. Quantitative results on fully fine-tuning SDXL-Base.   

<table><tr><td>Method</td><td>Step NFE</td><td></td><td>CLIP Score</td><td>Pick Score</td><td>HPSv2</td><td>MPS</td></tr><tr><td>ADD [61] (512px)</td><td>1</td><td>1</td><td>35.0088</td><td>22.1524</td><td>27.0971</td><td>10.4340</td></tr><tr><td>LCM [34]</td><td>1</td><td>2</td><td>28.4669</td><td>20.1267</td><td>23.8246</td><td>4.8134</td></tr><tr><td>Lightning [23]</td><td>1</td><td>1</td><td>33.4985</td><td>21.9194</td><td>27.1557</td><td>10.2285</td></tr><tr><td>DMD2 [85]</td><td>1</td><td>1</td><td>35.2153</td><td>22.0978</td><td>27.4523</td><td>10.6947</td></tr><tr><td>DMDX (Ours)</td><td>1</td><td>1</td><td>35.2557</td><td>22.2736</td><td>27.7046</td><td>11.1978</td></tr><tr><td>SDXL-Base [56]</td><td>25</td><td>50</td><td>35.0309</td><td>22.2494</td><td>27.3743</td><td>10.7042</td></tr></table>

Table 2. Quantitative results on LoRA fine-tuning SD3-Medium and fully fine-tuning SD3.5-Large.   

<table><tr><td>Method</td><td>Step NFE</td><td></td><td>CLIP Score</td><td>Pick Score</td><td>HPSv2</td><td>MPS</td></tr><tr><td>TSCD [61]</td><td>4</td><td>8</td><td>34.0185</td><td>21.9665</td><td>27.2728</td><td>10.8600</td></tr><tr><td>PCM [69] (Shift=1)</td><td>4</td><td>4</td><td>33.5042</td><td>21.9703</td><td>27.3680</td><td>10.5707</td></tr><tr><td>PCM [69] (Shift=3)</td><td>4</td><td>4</td><td>33.3818</td><td>21.9396</td><td>27.1146</td><td>10.5635</td></tr><tr><td>PCM [69] (Stoch.)</td><td>4</td><td>4</td><td>33.4185</td><td>21.8822</td><td>27.3177</td><td>10.5200</td></tr><tr><td>Flash [3]</td><td>4</td><td>4</td><td>34.3978</td><td>22.0904</td><td>27.2586</td><td>10.6634</td></tr><tr><td>ADM (Ours)</td><td>4</td><td>4</td><td>34.9076</td><td>22.5471</td><td>28.4492</td><td>11.9543</td></tr><tr><td>SD3-Medium[ [6]</td><td>25</td><td>50</td><td>34.7633</td><td>22.2961</td><td>27.9733</td><td>11.3652</td></tr><tr><td>LADD [60]</td><td>4</td><td>4</td><td>34.7395</td><td>22.3958</td><td>27.4923</td><td>11.4372</td></tr><tr><td>ADM (Ours)</td><td>4</td><td>4</td><td>34.9730</td><td>22.8842</td><td>27.7331</td><td>12.2350</td></tr><tr><td>SD3.5-Large [6]</td><td>25</td><td>50</td><td>34.9668</td><td>22.5087</td><td>27.9688</td><td>11.5826</td></tr></table>

<table><tr><td>Method</td><td>Step</td><td>NFE</td><td>Final Score</td><td>Quality Score</td><td>Semantic Score</td></tr><tr><td>ADM</td><td>8</td><td>8</td><td>78.584</td><td>80.825</td><td>69.621</td></tr><tr><td>+ Longer Training ×2</td><td>8</td><td>8</td><td>80.764</td><td>83.031</td><td>71.693</td></tr><tr><td>ADM w/ CFG</td><td>8</td><td>16</td><td>79.865</td><td>80.938</td><td>75.569</td></tr><tr><td>+ Longer Training ×2</td><td>8</td><td>16</td><td>81.796</td><td>83.008</td><td>76.947</td></tr><tr><td>CogVideoX-2b [83]</td><td>100</td><td>200</td><td>80.036</td><td>80.801</td><td>76.974</td></tr><tr><td>ADM</td><td>8</td><td>8</td><td>82.067</td><td>83.227</td><td>77.423</td></tr><tr><td>ADM w/ CFG</td><td>8</td><td>16</td><td>80.982</td><td>82.165</td><td>76.251</td></tr><tr><td>CogVideoX-5b [83]</td><td>100</td><td>200</td><td>81.226</td><td>81.785</td><td>78.987</td></tr></table>

Table 3. Quantitative results on fully fine-tuning CogVideoX.

# 5. Experiments

Models. For one-step distillation, we employ both Adversarial Distillation Pre-training (ADP) and ADM fine-tuning on SDXL-Base [56], termed DMDX. For multi-step distillation, we only employ ADM training on both text-toimage models SD3-Medium, SD3.5-Large [6], and text-tovideo models CogVideoX-2b, CogVideoX-5b [83]. Following most concurrent works, we didn't use ClassifierFree Guidance (CFG) [11] in our text-to-image model. We tried conducting CFG-integrated experiments on the textto-video model, with the approach detailed in Sec. 5.2.

Datasets. No visual data is required for both ADP and ADM proposed in this work. For image generators, we utilize the text prompts from JourneyDB [67] that exhibits high-level of detail and specificity for training. For video generators, we collect training prompts from OpenVid1M [47], Vript [82] and Open-Sora-Plan-v1.1.0 [50].

Evaluation. The image generators are evaluated on 10K prompts from C0CO 2014 [25] following DMD2 [85]. We report the CLIP score [52] and human preference benchmarks PickScore [20], HPSv2 [75] and MPS [90] as many concurrent works including Hyper-SD [55] and Emu3 [73]. But we don't include Hyper-SD in the one-step quantitative comparison because one-step Hyper-SDXL has been optimized directly on human feedback with ReFL [78]. Instead, we compare with the TSCD algorithm proposed therein on SD3-Medium [13], since 4-step Hyper-SD3 LoRA is without ReFL optimization. The video generators are evaluated on VBench [14], which consists of multiple quality and semantic dimensions comprehensively.

![](images/5.jpg)  
Figure 5. Qualitative results on fully fine-tuning SDXL-Base

Hyperparameters. Despite having multiple models to train in our proposed ADP and ADM, we achieve satisfactory visual fidelity and structural integrity without extensive hyperparameter tuning. For the remainder of this work, we only adjust the learning rate of generator for different experiments. The optimizer settings for discriminators and fake models are uniform across all the experiments. Unless longer training is specifically noted, we train the same for only 8K iterations with a batch size of 128 and 8 for text-toimage and text-to-video models, respectively. More specific implementation details are provided in Appendix B.

# 5.1. Efficient Image Synthesis

Tab. 1 quantitatively compares our two-stage approach combing ADP and single-step ADM distillation with existing one-step distillation methods on fully fine-tuning SDXL-Base. The results show that our method achieves excellent performance on both image-text alignment and human preference, which is consistent with the qualitative comparisons in Fig. 5, including better portrait aesthetic, animal hair details, subject-background separation and physical structure.

<table><tr><td>Ablation</td><td>CLIP Score</td><td>Pick Score</td><td>HPSv2</td><td>MPS</td></tr><tr><td colspan="5">Ablation on adversarial distillation.</td></tr><tr><td>A1: Rectified Flow [27]</td><td>27.4376</td><td>20.0211</td><td>23.6093</td><td>4.4518</td></tr><tr><td>A2: DINOv2 as pixel-space</td><td>34.1836</td><td>21.8750</td><td>27.1039</td><td>10.2407</td></tr><tr><td>A3: λ1 = 0.7, λ2 = 0.3</td><td>33.6943</td><td></td><td>21.6344 26.8902</td><td>9.9633</td></tr><tr><td>A4: λ1 = 1.0, λ2 = 0.0</td><td>33.8929</td><td></td><td>21.7395 26.7869</td><td>10.0757</td></tr><tr><td>A5: w/o ADM (ADP only)</td><td>35.7723</td><td>22.0095</td><td> 27.3499</td><td>10.6646</td></tr><tr><td colspan="5">Ablation on score distillation.</td></tr><tr><td>B1: ADM w/o ADP</td><td>32.5020</td><td>21.7631</td><td>26.8732</td><td>10.8986</td></tr><tr><td>B2: DMD LosS w/o ADP</td><td>32.7482</td><td>21.0341</td><td>25.9680</td><td>8.8977</td></tr><tr><td>B3: DMD LosS w/ ADP</td><td>34.5119</td><td>21.9366</td><td>27.3985</td><td>10.6046</td></tr><tr><td>B4: DMDX (Ours)</td><td>35.2557</td><td>22.2736</td><td>27.7046</td><td>11.1978</td></tr></table>

Table 4. Quantitative results on ablation studies.

For multi-step ADM distillation, it can serve as a standalone score distillation method. We tried both fully finetuning and LoRA fine-tuning [13] configurations and the quantitative results in Tab. 2 demonstrate our superior performance. Qualitative results are provided in Appendix C.

# 5.2. Efficient Video Synthesis

As quantitatively shown in Tab. 3, except for the regular 8-step ADM distillation for both sizes of CogVideoX, we also try integrating Classifier-Free Guidance (CFG) [11] for text-to-video task. Specifically, we assign the CFG scale for real model by randomly sampling within the range [5.0, 7.0], while assigning the few-step generator's scale through explicit subtraction of 2.0 from the real model's value. Empirically, we found that this subtractive offset requires progressive enlargement as the targeted sampling steps decreases. There is no CFG for the fake model. The VBench [14] results demonstrate that our few-step generators achieve comparable performance to the base model with $9 2 . 9 6 \%$ acceleration. We conduct additional evaluations on the longer training of 2B model, motivated by the observation that DMD loss did't converge well at 8K iterations as Fig. 3 indicates. Experimental results demonstrate that the DMD loss is also approximately optimized by the learnable discriminator during ADM distillation. More quantitative and qualitative results refer to Appendix C.

# 5.3. Ablation Studies

In Tab. 4, we conduct extensive ablation studies on fully fine-tuning SDXL-Base to validate our effectiveness. Qualitative comparisons are provided in Appendix C.

Effect of ADP. The following conclusions can be drawn: 1) Single reflow process provides very limited effectiveness (A1), while multiple processes achieving effective rectification requires considerable computational expense. 2) Using SAM [19] offers more imaging fidelity compared to the widely adopted DINOv2 [48] (A2 / A5), which is likely due to SAM's higher resolution of $1 0 2 4 \mathrm { p x }$ (compared to DINOv2's 518px). 3) The weighting $\lambda _ { 2 }$ for pixel space should neither be too large nor too small. Excessive weighting leads to degraded structural integrity (A3 / A5), while insufficient weighting results in blurriness (A4 / A5). Qualitative results in Appendix $\textrm { C }$ provide more visible distinction.

Table 5. Ablation study on two time-scale update rule.   

<table><tr><td>TTUR</td><td>Training Time</td><td>CLIP Score</td><td>Pick Score</td><td>HPSv2</td><td>MPS</td></tr><tr><td>1</td><td>×1.00</td><td>35.2557</td><td>22.2736</td><td>27.7046</td><td>11.1978</td></tr><tr><td>4</td><td>× 1.85</td><td>35.2583</td><td>22.2773</td><td>27.7255</td><td>11.2720</td></tr><tr><td>8</td><td>×2.53</td><td>35.3299</td><td>22.2883</td><td>27.7586</td><td>11.2838</td></tr></table>

Table 6. Quantitative diversity evaluation on PartiPrompts [88].   

<table><tr><td></td><td>ADD</td><td>LCM</td><td>Lightning</td><td>DMD2</td><td>Ours</td><td>Teacher</td></tr><tr><td>LPIPS↑</td><td>0.6071</td><td>0.6257</td><td>0.6707</td><td>0.6715</td><td>0.7156</td><td>0.6936</td></tr></table>

Effect of ADM. We summarize the key findings: 1) The absence of ADP exerts substantial performance degradation $\left( \mathtt { B } 1 / \mathtt { B } 4 \right)$ , which is aligned with our analysis in Sec. 4.3.2. 2) Without a regularizer, the DMD loss underperforms compared to standalone ADM (B1/B2), indicating its poor robustness. 3) Although DMD loss optimization also benefits from ADP (B2 / B3), its distribution matching capability remains inferior to ADM (B3 /B4).

Effect of TTUR. Tab. 5 presents the impact of different TTUR settings on final performance and training duration. The results demonstrate that increasing TTUR yields only marginal performance gains while nearly doubling training time, rendering this trade-off clearly unwarranted. This highlights the critical role of our proposed ADP in onestep distillation and suggests that the training instability in DMD2 likely stems from insufficient support set overlap.

Diversity evaluation. Following DMD2 [85], we generate 4 samples per prompt on Partiprompts [88] with different seeds, and report the averaged pairwise LPIPS similarities [89] in Tab. 6. The results indicate that our method significantly outperforms others in diversity. More randomly curated multi-seed samples can be found in Appendix C.

# 6. Limitations

We realize that one weakness is that teacher model might require CFG to produce accurate score prediction. Our experiments suggest this is a common characteristic of score distillation methods generally, not a limitation unique to our approach. This limits the application to guidance-distilled models such as FLUX.1-dev [1], which could be a promising research topic for future work.

# Acknowledgments

This work was supported in part by the National Natural Science Foundation of China (U22A2095, 12326618, 62276281), Guangdong Basic and Applied Basic Research Foundation, China (2024A1515011882) and the Project of Guangdong Provincial Key Laboratory of Information Security Technology (2023B1212060026).

# References

[1] Black Forest Labs. Flux.1-dev. https : / / huggingface.co/black-forest-labs/FLux.1- dev, 2024. 8   
[2] Naresh Babu Bynagari. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In NeurIPS, 2017. 2   
[3] Clement Chadebec, Onur Tasar, Eyal Benaroche, and Benjamin Aubin. Flash diffusion: Accelerating any conditional diffusion model for few steps image generation. arXiv preprint arXiv:2406.02347, 2024. 1, 7, 2   
[4] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174, 2016. 2   
[5] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. 1   
[6] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. 2, 7, 1   
[7] Zhengyang Geng, Ashwini Pokle, William Luo, Justin Lin, and J Zico Kolter. Consistency models made easy. In ICLR, 2025.2   
[8] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NeurIPS, 2014. 2   
[9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, pages 770778, 2016. 1   
10] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016. 1   
11] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS Workshops, 2021. 7, 8   
12] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. 1, 3   
13] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In ICLR, 2022. 7, 8   
[14] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. Vbench: Comprehensive benchmark suite for video generative models. In CVPR, 2024. 7, 8, 2, 3   
[15] Sam Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Shuaiwen Leon Song, Samyam Rajbhandari, and Yuxiong He. Deepspeed ulysses: System optimizations for enabling training of extreme long sequence transformer models. arXiv preprint arXiv:2309.14509, 2023. 2   
[16] Tejas Jayashankar, J. Jon Ryu, and Gregory Wornell. Scoreof-mixture training: Training one-step generative models made simple via score estimation of mixture distributions. arXiv preprint arXiv:2502.09609, 2025. 1, 2, 3   
[17] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. In NeurIPS, 2022. 1, 3   
[18] Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka, Yutong He, Yuki Mitsufuji, and Stefano Ermon. Consistency trajectory models: Learning probability flow ode trajectory of diffusion. In ICLR, 2024. 1, 3   
[19] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, and Ross Girshick. Segment anything. In ICCV, pages 3992 4003, 2023. 5, 8, 1   
[20] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. In NeuriPS, 2023. 7   
[21] Jonas Kohler, Albert Pumarola, Edgar Schönfeld, Artsiom Sanakoyeu, Roshan Sumbaly, Peter Vajda, and Ali Thabet. Imagine flash: Accelerating emu diffusion models with backward distillation. arXiv preprint arXiv:2405.05224, 2024. 1   
[22] Jae Hyun Lim and Jong Chul Ye. Geometric gan. arXiv preprint arXiv:1705.02894, 2017. 4, 5, 6   
[23] Shanchuan Lin, Anran Wang, and Xiao Yang. Sdxllightning: Progressive adversarial diffusion distillation. arXiv preprint arXiv:2402.13929, 2024. 1, 2, 3, 5, 7   
[24] Shanchuan Lin, Xin Xia, Yuxi Ren, Ceyuan Yang, Xuefeng Xiao, Lu Jiang, and ByteDance Seed. Diffusion adversarial post-training for one-step video generation. arXiv preprint arXiv:2501.08316, 2025. 1, 3   
[25] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollár. Microsoft coco: Common objects in context. In ECCV, 2014. 7   
[26] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2023. 3   
[27] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In ICLR, 2022. 1, 3, 5, 8   
[28] Xingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, and Oiang Liu. Instaflow: One step is enough for high-qualitv diffusion-based text-to-image generation. In ICLR, 2024. 1, 3   
[29] Iya Loshchilov and Fran Hutter. Decopld weight decy regularization. In ICLR, 2019. 2   
[30] Cheng Lu and Yang Song. Simplifying, stabilizing and scaling continuous-time consistency models. arXiv preprint arXiv:2410.11081, 2024. 1, 3   
[31] Yanzuo Lu, Manlin Zhang, Yiqi Lin, Andy J. Ma, Xiaohua Xie, and Jianhuang Lai. Improving pre-trained masked autoencoder via locality enhancement for person reidentification. In PRCV, pages 509521, 2022. 5   
[32] Yanzuo Lu, Meng Shen, Andy J Ma, Xiaohua Xie, and JianHuang Lai. Mlnet: Mutual learning network with neighborhood invariance for universal domain adaptation. In AAAI, pages 39003908, 2024. 5   
[33] Yanzuo Lu, Manlin Zhang, Andy J Ma, Xiaohua Xie, and Jianhuang Lai. Coarse-to-fine latent diffusion for poseguided person image synthesis. In CVPR, pages 64206429, 2024.5   
[34] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing highresolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023. 1, 2, 7   
[35] Simian Luo, Yiqin Tan, Suraj Patil, Daniel Gu, Patrick von Platen, Apolinário Passos, Longbo Huang, Jian Li, and Hang Zhao. Lcm-lora: A universal stable-diffusion acceleration module. arXiv preprint arXiv:2311.05556, 2023. 1, 2   
[36] Weijian Luo. Diff-instruct++: Training one-step text-toimage generator model to align with human preferences. In TMLR, 2024. 3   
[37] Weijian Luo, Tianyang Hu, Shifeng Zhang, Jiacheng Sun, Zhenguo Li, and Zhihua Zhang. Diff-instruct: A universal approach for transferring knowledge from pre-trained diffusion models. In NeurIPS, pages 7652576546, 2023. 1   
[38] Weijian Luo, Zemin Huang, Zhengyang Geng, J. Zico Kolter, and Guo-jun Qi. One-step diffusion distillation through score implicit matching. In NeurIPS, 2024. 1, 2   
[39] Weijian Luo, Colin Zhang, Debing Zhang, and Zhengyang Geng. David and goliath: Small one-step model beats large diffusion with score post-training. In ICML, 2025. 3   
[40] Yihong Luo, Xiaolong Chen, Xinghua Qu, Tianyang Hu, and Jing Tang. You only sample once: Taming one-step text-toimage synthesis by self-cooperative diffusion gans. In ICLR, 2025. 3   
[41] Hongxu Ma, Guanshuo Wang, Fufu Yu, Qiong Jia, and Shouhong Ding. Ms-detr: Towards effective video moment retrieval and highlight detection by joint motion-semantic learning. In ACMMM, 2025. 5   
[42] Hongxu Ma, Chenbo Zhang, Lu Zhang, Jiaogen Zhou, Jihong Guan, and Shuigeng Zhou. Fine-grained zero-shot object detection. In ACMMM, 2025. 5   
[43] Xiaofeng Mao, Zhengkai Jiang, Fu-Yun Wang, Wenbing Zhu, Jiangning Zhang, Hao Chen, Mingmin Chi, and Yabiao Wang. Osv: One step is enough for high-quality image to video generation. arXiv preprint arXiv:2409.11367, 2024. 1   
[44] Yuxi Mi, Zhizhou Zhong, Yuge Huang, Qiuyang Yuan, Xuan Zhao, Jianqing Xu, Shouhong Ding, Shaoming Wang, Rizen Guo, and Shuigeng Zhou. Data synthesis with diverse styles for face recognition via 3dmm-guided diffusion. In CVPR, pages 2120321214, 2025. 5   
[45] Thomas Minka. Divergence measures and message passing. Microsoft Research, Technical Report, 2005. 2   
[46] Movie Gen Team. Movie gen: A cast of media foundation models. https://ai.meta.com/static resource/movie-gen-research-paper,2024.2   
[47] Kepan Nan, Rui Xie, Penghao Zhou, Tiehan Fan, Zhenheng Yang, Zhijie Chen, Xiang Li, Jian Yang, and Ying Tai. Openvid-1m: A large-scale high-quality dataset for text-tovideo generation. arXiv preprint arXiv:2407.02371, 2024. 7   
[48] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Hervé Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision. TMLR, 2024. 8, 1, 5   
[49] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. 4, 1   
[50] PKU-Yuan Lab and Tuzhan AI. Open-sora-planvl.1.0. https://huggingface.co/datasets/ LanguageBind/Open-Sora-Plan-v1.1.0,2024.7   
[51] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. In ICLR, 2023. 1, 2   
[52] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, 2021. 7,6   
[53] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models. In SC, 2020. 2   
[54] Prajit Ramachandran, Barret Zoph, and Quoc V. Le. Searching for activation functions. arXiv preprint arXiv:1710.05941, 2017. 1   
[55] Yuxi Ren, Xin Xia, Yanzuo Lu, Jiacheng Zhang, Jie Wu, Pan Xie, Xing Wang, and Xuefeng Xiao. Hyper-sd: Trajectory segmented consistency model for efficient image synthesis. In NeurIPS, 2024. 1, 3, 7   
[56] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 1, 7, 5   
[57] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In MICCAI, 2015. 4, 1   
[58] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In ICLR, 2022. 1, 2, 3   
[59] Tim Salimans, Thomas Mensink, Jonathan Heek, and Emiel Hoogeboom. Multistep distillation of diffusion models via noment matohina, In NaurIDS, 2024   
[UV] AUI DauI, 1uCIIE DucUI, 1II DUCNIII, Aicas Blattmann, Patrick Esser, and Robin Rombach. Fast highresolution image synthesis with latent adversarial diffusion distillation. arXiv preprint arXiv:2403.12015, 2024. 1, 2, 3, 5, 7   
[61] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation. In ECCV, 2024.   
[62] Meng Shen, Yanzuo Lu, Yanxu Hu, and Andy J. Ma. Collaborative learning of diverse experts for source-free universal domain adaptation. In ACM MM, pages 20542065, 2023. 5   
[63] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR, 2021. 1, 3   
[64] Yang Song and Prafulla Dhariwal. Improved techniques for training consistency models. In ICLR, 2024. 2   
[65] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In ICLR, 2021. 1, 3   
[66] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. In ICML, 2023. 1, 2   
[67] Keqiang Sun, Junting Pan, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu, Renrui Zhang, Aojun Zhou, Zipeng Qin, Yi Wang, Jifeng Dai, Yu Qiao, Limin Wang, and Hongsheng Li. Journeydb: A benchmark for generative image understanding. arXiv preprint arXiv:2307.00716, 2023. 7   
[68] Zhiqiang Tan, Yunfu Song, and Zhijian Ou. Calibrated adversarial algorithms for generative modelling. Stat, 8:e224, 2019. 6   
[69] Fu-Yun Wang, Zhaoyang Huang, Alexander William Bergman, Dazhong Shen, Peng Gao, Michael Lingelbach, Keqiang Sun, Weikang Bian, Guanglu Song, Yu Liu, Hongsheng Li, and Xiaogang Wang. Phased consistency model. arXiv preprint arXiv:2405.18407, 2024. 1, 3, 7, 2   
[70] Fu-Yun Wang, Zhaoyang Huang, Weikang Bian, Xiaoyu Shi, Keqiang Sun, Guanglu Song, Yu Liu, and Hongsheng Li. Animatelcm: Computation-efficient personalized style video generation without personalized video data. In SIGGRAPH ASIA Technical Communications, 2024. 1   
[71] Fu-Yun Wang, Ling Yang, Zhaoyang Huang, Mengdi Wang, and Hongsheng Li. Rectified diffusion: Straightness is not your need in rectified flow. arXiv preprint arXiv:2410.07303, 2024. 1   
[72] Shibo Wang and Pankaj Kanwar. Bfloat16: The secret to high performance on cloud tpus. https: / cloud.google.com / blog / products / ai machine-learning/bfloat16-the-secret to-high-performance-on-cloud-tpus,2019.2   
[73] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, Yingli Zhao, Yulong Ao, Xuebin Min, Tao Li, Boya Wu, Bo Zhao, Bowen Zhang, Liangdong Wang, Guang Liu, Zheqi He, Xi Yang, Jingjing Liu, Yonghua Lin, Tiejun Huang, and Zhongyuan Wang. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. 7   
[74] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and aiverse text-to-sa generaton witn variauonal score aistiuation. In NeurIPS, 2023. 2   
[75] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: A solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023. 7   
[76] Yuxin Wu and Kaiming He. Group normalization. In ECCV, 2018. 1   
[77] Xuefeng Xiao, Lianwen Jin, Yafeng Yang, Weixin Yang, Jun Sun, and Tianhai Chang. Building fast and compact convolutional neural networks for offline handwritten chinese character recognition. Pattern Recognition, 72:7281, 2017. 5   
[78] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for textto-image generation. In NeurIPS, 2023. 3, 7   
[79] Yanwu Xu, Yang Zhao, Zhisheng Xiao, and Tingbo Hou. Ufogen: You forward once large scale text-to-image generation via diffusion gans. In CVPR, 2024. 3   
[80] Zunnan Xu, Zhentao Yu, Zixiang Zhou, Jun Zhou, Xiaoyu Jin, Fa-Ting Hong, Xiaozhong Ji, Junwei Zhu, Chengfei Cai, Shiyu Tang, et al. Hunyuanportrait: Implicit condition control for enhanced portrait animation. In CVPR, pages 15909 15919, 2025. 5   
[81] Hanshu Yan, Xingchao Liu, Jiachun Pan, Jun Hao Liew, Qiang Liu, and Jiashi Feng. Perflow: Piecewise rectified flow as universal plug-and-play accelerator. In NeurIPS, 2024. 1, 3   
[82] Dongjie Yang, Suyuan Huang, Chengqiang Lu, Xiaodong Han, Haoxin Zhang, Yan Gao, Yao Hu, and Hai Zhao. Vript: A video is worth thousands of words. arXiv preprint arXiv:2406.06040, 2024. 7   
[83] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, Da Yin, Xiaotao Gu, Yuxuan Zhang, Weihan Wang, Yean Cheng, Ting Liu, Bin Xu, Yuxiao Dong, and Jie Tang. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 2, 3, 5, 7   
[84] Hongwei Yi, Shitong Shao, Tian Ye, Jiantong Zhao, Qingyu Yin, Michael Lingelbach, Li Yuan, Yonghong Tian, Enze Xie, and Daquan Zhou. Magic 1-for-1: Generating one minute video clips within one minute. arXiv preprint arXiv:2502.07701, 2025. 1   
[85] Tianwei Yin, Michaël Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and William T. Freeman. Improved distribution matching distillation for fast image synthesis. In NeurIPS, 2024. 1, 2, 3, 5, 7, 8   
[86] Tianwei Yin, Michaël Gharbi, Richard Zhang, Eli Shechtman, Frédo Durand, William T. Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In CVPR, pages 66136623, 2024. 1, 2, 3   
[87] Tianwei Yin, Qiang Zhang, Richard Zhang, William T Freeman, Fredo Durand, Eli Shechtman, and Xun Huang. From slow bidirectional to fast causal video generators. arXiv nrenrint arYiv:2412 07772, 20241   
[88] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive models for content-rich text-to-image generation. TMLR, 2022. 8   
[89] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In CVPR, pages 586 595, 2018. 8   
[90] Sixian Zhang, Bohan Wang, Junqiang Wu, Yan Li, Tingting Gao, Di Zhang, and Zhongyuan Wang. Learning multidimensional human preference for text-to-image generation. In CVPR, pages 80188027, 2024. 7   
[91] Zhixing Zhang, Yanyu Li, Yushu Wu, Yanwu Xu, Anil Kag, Ivan Skorokhodov, Willi Menapace, Aliaksandr Siarohin, Junli Cao, Dimitris Metaxas, Sergey Tulyakov, and Jian Ren. Sf-v: Single forward video generation model. In NeurIPS, 2024. 1   
[92] Jianbin Zheng, Minghui Hu, Zhongyi Fan, Chaoyue Wang, Changxing Ding, Dacheng Tao, and Tat-Jen Cham. Trajectory consistency distillation: Improved latent consistency distillation by semi-linear consistency function with trajectory mapping. arXiv preprint arXiv:2402.19159, 2024. 1, 3   
[93] Mingyuan Zhou, Huangjie Zheng, Zhendong Wang, Mingzhang Yin, and Hai Huang. Score identity distillation: Exponentially fast distillation of pretrained diffusion models for one-step generation. In ICML, 2024. 1, 2   
[94] Mingyuan Zhou, Huangjie Zheng, Yi Gu, Zhendong Wang, and Hai Huang. Adversarial score identity distillation: Rapidly surpassing the teacher in one step. In ICLR, 2025. 3

# Adversarial Distribution Matching for Diffusion Distillation Towards Efficient Image and Video Synthesis

Supplementary Material

# A. Adversarial Distribution Matching

During the ADM distillation process, the fake score estimator, generator, and discriminator are updated alternately. The Algorithm 1 below clarifies the training procedure. Our ablation experiments in Sec. 5.3 demonstrate that TTUR has minimal impact on the final performance. Therefore, in our experiments, we set TTUR to 1, meaning that the fake model and generator are updated at the same frequency.

# Algorithm 1 ADM Training Procedure

1: Input: pretrained teacher model as real score estimator $F _ { \phi }$   
2:Output: few-step generator $G _ { \theta }$ with schedule $\{ t _ { 0 } , t _ { 1 } , . . . , t _ { N } \}$   
3:Initialize: fake score estimator $f _ { \psi } \gets F _ { \phi }$ , generator $G _ { \theta } \gets F _ { \phi }$   
latent-space discriminator $D _ { \tau }  F _ { \phi }$ with multiple trainable heads,   
generator iteration genIter $ 0$ , global iteration globalIter $ 0$   
4: while genIter $<$ maxIter do   
5: globalIter $+ = 1$   
6:   
7: / / update fake score estimator $f _ { \psi }$   
8: sample pure noise $\mathbf { \boldsymbol { z } } \sim \mathcal { N } ( \mathbf { 0 } , I )$   
9: solve the PF-ODE w.r.t. $N$ steps in schedule $\pmb { x } _ { 0 } \gets \pmb { G } _ { \theta } ( \pmb { z } , \cdot )$   
10: sample new pure noise $z _ { f }$ and random timestep $t _ { f }$   
11: update $\psi$ with $( \pmb { x } _ { 0 } , t _ { f } , \pmb { z } _ { f } )$ and pretrain loss in Eq. (2) or Eq. (3)   
12: if not (globalIter $\% \mathrm { T } \mathrm { \bar { T } U R } ) = = 0$ then continue   
13:   
14: / / update generator $G _ { \theta }$   
15: sample pure noise $\hat { z }$ and random index $n \in [ 1 , N ]$   
16: solve the PF-ODE w/o grad following $t _ { N }  t _ { N - 1 }  . . .  t _ { n }$   
i.e. $\hat { z } = \hat { \pmb { x } } _ { t _ { N } }  \hat { \pmb { x } } _ { t _ { N - 1 } }  . . .  \hat { \pmb { x } } _ { t _ { n } }$ .   
17: solve the PF-ODE w/ grad w.r.t. $t _ { 0 }$ , i.e. $\hat { \pmb { x } } _ { 0 } = \pmb { G } _ { \theta } ( \hat { \pmb { x } } _ { t _ { n } } , t _ { n } )$   
18: sample new pure noise $z _ { g }$ and random timestep $t \sim \mathcal { U } ( 0 , T )$   
29: difse ssa mple $\scriptstyle { \hat { \mathbf { x } } } _ { 0 }$ vi $z _ { g }$ an eo $\pmb { x } _ { t } = \pmb { q } ( \pmb { x } _ { t } | \hat { \pmb { x } } _ { 0 } )$   
$f _ { \psi }$ $( t - \Delta t )$ $\pmb { x } _ { t - \Delta t } ^ { \mathrm { f a k e } }$   
21: solve the PF-ODE of $\mathbf { \Delta } _ { F _ { \phi } }$ w.t. $( t - \Delta t )$ to obtain $\pmb { x } _ { t - \Delta t } ^ { \mathrm { r e a l } }$   
22: update $\theta$ with $( { \pmb x } _ { t - \Delta t } ^ { \mathrm { f a k e } } , t - \Delta t )$ and Eq. (7)   
23: genIter $+ = 1$   
24:   
25: / / update discriminator $\scriptstyle { D _ { \tau } }$   
26: uppdate $\tau$ with $( { \pmb x } _ { t - \Delta t } ^ { \mathrm { f a k e } } , { \pmb x } _ { t - \Delta t } ^ { \mathrm { r e a l } } , t - \Delta t )$ and Eq. (8)   
27: end while

# B. Implementation Details

# B.1. 2D Discriminator Design

In Fig. 6, we thoroughly illustrate the design of our discriminators and the difference between two training stages. For all the trainable heads appended to discriminator backbone for text-to-image experiments, we have a fixed 2D design following SDXL-Lightning [23], which consists of simple blocks of $4 \times 4$ 2D convolution with a stride of 2, group normalization [76] with 32 groups, and SiLU activation [10, 54] layer. The difference is that we will append multiple heads at different layers of the network. Whether it is the output of UNet [57], DiT [49] or ViT [5], we uniformly reshape it into [Batch, Channel, Height, W idth] and then use it as the input to the discriminator head. For SDXL [56], we take the output of the last ResNet [9] of each block (including down-sampling, mid and upsampling blocks), yielding a total of 7 discriminator heads. For SD3 series [6] models, we take the output of each DiT block, yielding 24 and 38 discriminator heads for SD3- Medium and SD3.5-Large, respectively. For SAM [19] and DINOv2 [48], we take the output of layers 3, 6, 9 and 12, yielding 4 discriminator heads.

<table><tr><td></td><td>Training Iteration</td><td>GPU Number</td><td>Elapsed Time</td><td>GPU Hours</td><td>Micro BatchSize Memory</td><td>Max</td></tr><tr><td>DMD2</td><td>20K</td><td>64</td><td>60 hours</td><td>3840</td><td>2</td><td>-</td></tr><tr><td>DMDX</td><td>8K+8K</td><td>32</td><td>70 hours</td><td>2240</td><td>4</td><td>39.6 GiB</td></tr><tr><td>- ADP</td><td>8K</td><td>32</td><td>55 hours</td><td>1760</td><td>4</td><td>39.6 GiB</td></tr><tr><td>- ADM</td><td>8K</td><td>32</td><td>15 hours</td><td>480</td><td>4</td><td>24.1 GiB</td></tr></table>

Table 7. Comparisons on A100 GPU efficiency with DMD2. The elapsed time for ADP already includes collection of ODE pairs.

# B.2. 3D Discriminator Design

Our 3D discriminator head for text-to-video latent diffusion models consists of simple blocks of $3 \times 3 \times 3$ 3D convolution with a stride of 1, $3 \times 3$ 2D convolution with a stride of 2, group normalization with 32 groups and SiLU activation layer. This is similar to the design in 2D discriminator head except that we additionally insert several 3D convolution layers to extract time-dependent feature. The output of specific blocks within video DiT backbone are reshaped into [Batch, Channel, Time, Height, Width] and input to corresponding discriminator head. In practice, we extract features every 3 DiT blocks due to the computational effort of 3D convolution, yielding a total of 10 and 14 discrimiantor heads for 2B and 5B models, respectively.

# B.3. GPU efficiency.

In Tab. 7, we present the training configurations and GPU consumption of our proposed method compared to DMD2. The table demonstrates that we actually achieve better performance over DMD2 with less GPU time and don't impose excessive demands on GPU memory. Although maintaining more networks during training process, our implementation attains manageable memory footprint with several optimizations detailed later.

![](images/6.jpg)  
Figure 6. Illustration of our discriminator design and the difference between ADM and ADP.

# B.4. Memory efficiency.

To reduce GPU memory footprint and improve efficiency, we utilize several acceleration techniques in our implementation including Fully Sharded Data Parallel (FSDP) [53], gradient checkpoint [4] and BF16 mixed precision [72]. For text-to-video models, we additionally integrate, Context Parallel (CP) [83] and Sequence Parallel (SP) [15] following common practice in MovieGen[46] to speed up training and inference. More importantly, a CPU offloading technique that has been built into Pytorch FSDP is essential for training multiple networks to save memory.

With CPU offloading enabled, each parameter along with the corresponding gradient and optimizer state can be offoaded from the GPU to CPU memory. In conjunction with gradient checkpointing, the GPU memory footprint in the forward and backward process is nearly the same as when there is only one single network, because the peak memory is now determined by the maximum activation of each block. This comes at the cost of increased CPU memory and longer time per iteration. While the CPU memory is usually sufficient and cheap, our more effective approaches require fewer iterations to achieve convergence and satisfactory results, and as Tab. 7 show that our DMDX takes less time than DMD2 on one-step SDXL distillation.

# B.5. Hyperparameters.

For all models of the optimizer (including generator, fake model and discriminator in both text-to-image and text-tovideo experiments), we use AdamW [29] optimizer without weight decay, with beta parameters (0.0, 0.99) to capture the changes in distribution more up-to-date. The learning rates of discriminator and fake model across all of our experiments are fixed at 5e-6 and 1e-6, respectively.

For SDXL, the learning rates for generator during ADP and ADM training are 1e-6 and 1e-7, respectively. As for multi-step ADM distillation, the learning rates for generator of SD3-Medium LoRA training and SD3.5-Large fully finetuning are given to 1e-6 and 1e-8, respectively. In case of text-to-video diffusion distillation, we set the same learning rate 1e-7 for different 8-step CogVideoX generators.

Among all the ADM experiments, the Classifier-Free Guidance (CFG) is required for real model as DMD does [85]. For SDXL, SD3-Medium, SD3.5-Large, and CogVideoX, the uniformly random sampling ranges for the CFG values are set to [6.0, 8.0], [6.0, 8.0], [3.0, 4.0], and [5.0, 7.0], respectively. The chosen ranges are based on the recommended CFG values from the original baseline's inference with some allowable variations. We observed that this setting is adequate for achieving satisfactory distilled performance without requiring extensive tuning.

The fake model training does not incorporate CFG and uses the same loss function as the standard pre-training of diffusion models, except that we didn't set any dropout. For noise-parameterized models, the prediction target is noise, while for velocity-parameterized models, it is velocity.

# C. Main Results

# C.1. Efficient Image Synthesis

Fig. 7 qualitatively compares our method with other stateof-the-art distillation techniques on SD3 [6] series models. The results demonstrate that our method is competitive to the original model in terms of color, detail, structure and image-text alignment, while outperforming other methods including TSCD, PCM [69], Flash [3] and LADD [60].

# C.2. Efficient Video Synthesis

Tabs. 8 and 9 present the details of VBench [14] results on the base model and few-step generators of CogVideoX [83]. In Figs. 11 to 16, we present several cases for qualitative comparisons between our CogVideoX [83] generators and baseline model. The results show that our 8-step generators are generally semantically comparable to the original model, even with semantic enhancements on some cases, e.g., the change of light in Fig. 11 and the movement of the sheep in Fig. 14. While in terms of imaging quality, generators with CFG are generally more detailed and have more delicate textures than those without CFG. The deficiencies in detail are reflected in, for example, the slightly rough hand and the incorrect number of fingers in Fig. 15, whereas the one with CFG is much more natural. As well as the generator without CFG is also much higher in color contrast, which visually looks sometimes too vibrant to be sufficiently realistic. These demonstrate the importance of CFG for text-to-video models, which might not be fully reflected by quantitative metrics.

Table 8. VBench [14] detailed results on overall scores and separate score for each quality dimension.   

<table><tr><td rowspan="2">Method</td><td rowspan="2">Step NFE</td><td rowspan="2"></td><td rowspan="2">Final Score</td><td colspan="2">Quality Semantic Subject</td><td rowspan="2">Consistency</td><td rowspan="2">Background Consistency</td><td rowspan="2">Temporal Flickering</td><td rowspan="2">Motion</td><td colspan="3">Dynamic Aesthetic Imaging</td></tr><tr><td>Score</td><td>Score</td><td>Smoothness</td><td>Degree Quality</td><td>Quality</td></tr><tr><td>ADM</td><td>8</td><td>8</td><td>78.58</td><td>80.82</td><td>69.62</td><td>96.72</td><td>96.55</td><td>97.01</td><td>98.14</td><td>48.61</td><td>57.80</td><td>65.28</td></tr><tr><td>+Longer Training ×2</td><td>8</td><td>8</td><td>80.76</td><td>83.03</td><td>71.69</td><td>96.58</td><td>96.71</td><td>98.12</td><td>97.68</td><td>73.33</td><td>57.90</td><td>65.72</td></tr><tr><td>ADM w/ CFG</td><td>8</td><td>16</td><td>79.86</td><td>80.93</td><td>75.56</td><td>96.16</td><td>96.96</td><td>96.86</td><td>97.69</td><td>54.44</td><td>59.78</td><td>63.18</td></tr><tr><td>+Longer Training ×2</td><td>8</td><td>16</td><td>81.79</td><td>83.00</td><td>76.94</td><td>96.83</td><td>96.90</td><td>98.51</td><td>98.07</td><td>63.05</td><td>61.03</td><td>64.62</td></tr><tr><td>CogVideoX-2b</td><td>100</td><td>200</td><td>80.03</td><td>80.80</td><td>76.97</td><td>92.53</td><td>95.22</td><td>97.79</td><td>97.00</td><td>69.44</td><td>60.38</td><td>60.69</td></tr><tr><td>ADM</td><td>8</td><td>8</td><td>82.06</td><td>83.22</td><td>77.42</td><td>96.42</td><td>96.87</td><td>96.96</td><td>97.69</td><td>68.88</td><td>61.17</td><td>69.01</td></tr><tr><td>ADM w/ CFG</td><td>8</td><td>16</td><td>80.98</td><td>82.16</td><td>76.25</td><td>96.15</td><td>96.59</td><td>95.99</td><td>98.57</td><td>56.66</td><td>61.01</td><td>68.68</td></tr><tr><td>CogVideoX-5b</td><td>100</td><td>200</td><td>81.22</td><td>81.78</td><td>78.98</td><td>92.52</td><td>96.68</td><td>98.34</td><td>96.97</td><td>70.55</td><td>61.67</td><td>61.88</td></tr></table>

Table 9. VBench [14] detailed results on separate score for each semantic dimension.   

<table><tr><td>Method</td><td></td><td>Step NFE</td><td>Object Class</td><td>Multiple Objects</td><td>Human Action</td><td>Color</td><td>Spatial Relationship</td><td>Scene</td><td>Appearance Style</td><td>Temporal Style</td><td>Overall Consistency</td></tr><tr><td>ADM</td><td>8</td><td>8</td><td>83.97</td><td>47.19</td><td>87.40</td><td>77.79</td><td>62.93</td><td>42.64</td><td>24.16</td><td>22.35</td><td>25.27</td></tr><tr><td>+Longer Training ×2</td><td>8</td><td>8</td><td>87.84</td><td>56.53</td><td>85.00</td><td>80.28</td><td>69.52</td><td>44.33</td><td>23.15</td><td>22.60</td><td>25.11</td></tr><tr><td>ADM w/CFG</td><td>8</td><td>16</td><td>89.55</td><td>64.78</td><td>92.60</td><td>82.31</td><td>62.61</td><td>52.73</td><td>24.31</td><td>24.46</td><td>26.12</td></tr><tr><td>+Longer Training ×2</td><td>8</td><td>16</td><td>91.67</td><td>71.58</td><td>92.20</td><td>82.01</td><td>71.79</td><td>50.26</td><td>23.54</td><td>24.54</td><td>26.30</td></tr><tr><td>CogVideoX-2b</td><td>100</td><td>200</td><td>80.01</td><td>67.23</td><td>98.60</td><td>89.98</td><td>49.05</td><td>68.60</td><td>24.04</td><td>25.37</td><td>25.68</td></tr><tr><td>ADM</td><td>8</td><td>8</td><td>92.94</td><td>65.89</td><td>95.80</td><td>84.97</td><td>72.92</td><td>56.06</td><td>22.63</td><td>23.64</td><td>26.17</td></tr><tr><td>ADM w/ CFG</td><td>8</td><td>16</td><td>89.41</td><td>69.89</td><td>97.00</td><td>71.35</td><td>81.26</td><td>53.90</td><td>21.48</td><td>23.79</td><td>25.92</td></tr><tr><td>CogVideoX-5b</td><td>100</td><td>200</td><td>87.64</td><td>67.34</td><td>99.60</td><td>83.93</td><td>68.24</td><td>56.35</td><td>25.16</td><td>25.82</td><td>27.79</td></tr></table>

![](images/7.jpg)  
Fgre7. Qualitaive reults n LoRA fe-tuni SD3-Mediu nd fule-tuning SD3.5-Large

# C.3. Ablation Studies

As for ablation on adversarial distillation shown in Fig. 8, the two main problems with other baseline settings are structure and blurriness. When using MSE loss for a single reflow process as in Rectified Flow [27], it is obvious that it is struggling to generate a structurally visible image. And switching the SAM [19] model to DINOv2 [48], we can clearly see the structural collapse of both the robot and the face in the figure, which is unexpected and may be caused by the fact that its input resolution is only $5 1 8 \mathrm { p x }$ ,and the images we generate are all $1 0 2 4 \mathrm { p x }$ need to be resized before they can be input. Another possible explanation is that the prior knowledge used by SAM for instance segmentation is richer than that provided by DINOv2 for discriminative self-supervised learning, which facilitates the generation of local fine-grained details. The structural problems encountered when increasing the weight of pixel-space $\lambda _ { 2 }$ are similar, while decreasing its weight causes a very noticeable blurring that is clearly visible in the figure, so we suggest setting $\lambda _ { 1 } = 0 . 8 5 , \lambda _ { 2 } = 0 . 1 5$ is a reasonable configuration.

![](images/8.jpg)  
Figure 8. Qualitative comparisons for ablation studies on adversarial distillation.

![](images/9.jpg)  
Figure 9. Qualitative comparisons for ablation studies on score distillation.

![](images/10.jpg)  
Figure 10. Qualitative diversity comparisons with DMD2.

In Fig. 9, we provide qualitative comparisons for ablation studies on score distillation. Compared to the baseline without ADM (ADP only), we can see that the ADM distillation indeed serves as a fine-tuning process to refine the generator in terms of both color, detail and the most notable structure. Although standalone ADM can also produce efficient generator, the noise artifact within 1-step generations as similarly observed by [23, 85] still exists, and with our ADP this issue can be addressed well. Notably, the visualization results demonstrate that employing the DMD loss without ADP integration induces substantially severe noise artifacts. Compared to using ADM alone, its qualitative disadvantage is much more pronounced than the gap observed in the quantitative results. With ADP, the DMD loss generates relatively good results, yet it remains inferior to ADM in terms of visual fidelity and structural integrity. This indicates that its distribution matching capability is weaker than that of ADM, which is consistent with our analysis in the quantitative results of Sec. 5.3.

Additionally, we showcase additional randomly curated multi-seed samples in Fig. 10 compared with DMD2, clearly demonstrating that our images exhibit richer variations in texture, color, brightness, contrast and structural composition.

# D. Broader Impact

Considering that many current methods leverage generated data from foundation models as assistance [44], our acceleration approach for diffusion models can substantially expedite this process, thereby benefiting numerous downstream tasks such as recognition [77], detection [42], retrieval [31, 41], domain adaptation [32, 62], etc. Alternatively, we can train LoRA to acquire an acceleration plugin, enhancing the efficiency of customized vertical models for image [33] or video [80] generation.

# E. Prompt List

Below we list the text prompts used for the generated content shown in this paper (from top to bottom, from left to right). Note that since models like SDXL-Base [56] only use CLIP [52] as a text encoder, which only supports a maximum of 77 tokens, the response and text-image alignment may be insufficient for some long prompts and its limited capacity in understanding.

# We use the following prompts for Fig. 5:

•A beautiful woman facing to the camera, smiling confidently, colorful long hair, diamond necklace, deep red lip, medium shot, highly detailed, realistic, masterpiece.   
• An owl perches quietly on a twisted branch deep within an ancient forest. Its sharp yellow eyes are keen and watchful.   
A young badger delicately sniffing a yellow rose, with a lion lurking in the background.   
•A pickup truck going up a mountain switchback. We use the following prompts for Fig. 7:   
• A photograph of a giant diamond skull in the ocean, featuring vibrant colors and detailed textures.   
A still of Doraemon from "Shaun the Sheep" by Aardman Animation.   
•A pizza is displayed inside a pizza box.   
• movie still of a man and a robot in a moment of horror, movie still, cinematic composition, cinematic light, by edgar wright and david lynch   
harry potter as a skyrim character   
film still of Tom Cruise as Ironman in the Avengers   
A beautiful award winning picture of a cute cat in front of a dark background. The cat is a cat-peacock hybrid and has a peacock tail and short peacock feathers on the body. fuffy, extremely detailed, stunning, high quality, atmospheric lighting   
a cute animal that's a penguin cat hybrid We use the following prompts for Fig. 8:   
• A colorful tin toy robot runs a steam engine on a path near a beautiful flower meadow in the Swiss Alps with a montin panorama in the backround,captured inl shot with motion blur and depth of field.   
A portrait painting of Leighann Vail.   
A photo of a mechanical angel woman with crystal wings, in the sci-fi style of Stefan Kostic, created by Stanley Lau and Artgerm.   
•A painting depicting a foothpath at Indian summer with an epic evening sky at sunset and low thunder clouds. We use the following prompts for Fig. 9:   
A bear walks through a group of bushes with a plant in its mouth.   
A falcon in flight, depicted in a highly detailed painting by Ilya Repin, Phil Hale, and Kent Williams.   
•A steampunk pocketwatch owl is trapped inside a glass jar b d  ass mist.   
Some giraffes are walking around the zoo exhibit.

![](images/11.jpg)  
E .

![](images/12.jpg)  
a path leading up to a quaint house, all bathed in the soft glowof autumn sunlight.

![](images/13.jpg)  
.

![](images/14.jpg)  
up, framed by the picturesque landscape, embodying tranquility and the simple beauty of nature.

![](images/15.jpg)  
Fi- io T scene ends with a close-up of her thoughtful smile, suggesting a moment of discovery or reflection.

![](images/16.jpg)  
Fli- o: capturing the essence of a peaceful day in Paris.