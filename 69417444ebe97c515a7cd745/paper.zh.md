# 自回归对抗后训练用于实时互动视频生成

林善川\* 杨策源 何浩† 姜剑文‡ 任宇曦 夏鑫 赵雪峰 小陆 江 字节跳动 Seed https://seaweed-apt.com/2

# 摘要

现有大规模视频生成模型计算复杂度高，限制了其在实时和交互应用中的采用。在本研究中，我们提出了自回归对抗后训练（AAPT），旨在将预训练的潜在视频扩散模型转化为实时的交互式视频生成器。我们的模型以自回归的方式每次生成一个潜在帧，使用单次神经函数评估（1NFE）。该模型可以实时将结果流式传输给用户，并接收交互响应作为控制，生成下一个潜在帧。与现有方法不同，我们的方法将对抗训练作为自回归生成的有效范式进行探索。这不仅使我们能够设计一种在生成单步帧时更加高效的架构，并充分利用KV缓存，还能够以学生强制（student-forcing）的方式训练模型，从而有效减少长视频生成中的误差积累。我们的实验表明，在单个H100上，我们的8B模型以$7 3 6 \times 4 1 6$的分辨率实现了实时24fps流式视频生成，或者在$8 \times \mathrm { H 1 0 0 }$上以$1 2 8 0 \times 7 2 0$的分辨率生成时长可达一分钟（1440帧）。近年来，视觉内容创作领域因视频生成基座模型的兴起而发生了转变。这些模型推动了多种强大应用的发展，包括文本到视频生成、图像到视频合成以及基于各种多模态信号的可控视频创作。在此基础上，研究人员开始探索更具野心的应用方向。其中一个令人兴奋的方向是将视频生成模型用作互动游戏引擎和世界模拟器。与离线视频合成不同，交互式视频生成需要模型实时响应用户输入，并在世界演变的同时持续生成一致的视频。尽管扩散模型能够生成高质量视频，但在实时交互视频生成时代价很高。早期的方法逐帧应用扩散模型。然而，这些方法在每次帧生成步骤中需要重新处理上下文帧，因此造成了高冗余。为了解决这个问题，扩散强制引入了渐进噪声，以平行化跨帧去噪。最近的工作进一步通过结合因果注意力、KV缓存和步骤蒸馏降低推理成本，目前最佳模型实现了四个去噪步骤。同时，基于令牌的自回归生成——由大型语言模型（LLMs）所推广——提供了一种替代方案。像VideoPoet这样的视频生成模型将.video生成视为下一令牌预测任务，并可以直接利用KV缓存来提高生成效率。然而，每个令牌的解码仍然是顺序的，限制了并行性，并使得满足实时需求变得困难。在本研究中，我们旨在解决交互式视频生成的三个核心挑战：（1）实现实时视频生成吞吐量，（2）维持交互信号的低延迟，以及（3）实现持续时长的因果视频生成。为此，我们探索对抗训练作为一种新范式，并提出自回归对抗后训练（AAPT）作为将预训练的视频扩散变换器转变为高效自回归生成器的有效策略。我们的方法提供了几个优势。首先，它很快。我们的模型以单次前向传递（1NFE）自回归预测每个潜在帧，同时充分利用KV缓存。我们的架构设计进一步使其效率比蒸馏到单步骤的等效扩散模型高出$2 \times$。其次，它在较长时间内保持更好的质量。我们的对抗方法支持全面的学生强制训练，降低长视频生成中的误差积累。此外，我们的学生强制方法不需要配对的真实目标，使我们能够训练长视频生成器，绕过短时训练数据的限制。这一点很重要，因为在大多数数据集中，单个连续的几十秒镜头非常稀少。我们通过实证展示了这些优势。在速度方面，我们的8B参数模型在单个H100 GPU上以$7 3 6 \times 4 1 6$分辨率实现了实时24fps视频生成，而在$8 \times \mathrm { H 1 0 0 }$ GPUs上以$1 2 8 0 \times 7 2 0$分辨率生成，延迟仅为0.16秒，显著优于CausVid，一个在$6 4 0 \times 3 5 2$分辨率下以9.4fps运作的5B模型，延迟为1.30秒。在时长方面，我们的模型能够生成持续60秒（1440帧）的视频流，同时充分利用KV缓存。这显著超过了之前最佳的单步生成器APT，仅支持49帧。我们的实验专注于图像到视频（I2V）生成场景，其中第一帧由用户提供，因为大多数交互应用采用这种设置。我们在两个交互应用上展示了我们的方法——姿态条件下虚拟人生成和相机控制的世界探索，用户可以通过交互输入实时引导视频生成。评估结果表明，我们的模型表现与先进水平相当。

# 1 相关工作

一阶段视频生成 早期的视频生成模型 [3, 81] 使用生成对抗网络 (GANs) [18] 能够通过单次网络评估实现快速生成。然而，根据现代标准，其质量、时长和分辨率均较差。扩散模型 [28, 84] 是当前最先进的技术，然而其迭代生成过程缓慢且成本昂贵。生成几秒钟的高分辨率视频可能需要几分钟。现有研究试图通过提出更高效的公式 [51, 55, 35]、采样器 [59, 60, 90]、架构 [109, 121, 120, 119, 98, 17, 66]、缓存 [63, 53, 125] 和蒸馏等方式降低推理成本。特别是，步骤蒸馏 [74, 83, 82, 58, 49, 48, 72, 116, 115, 77, 76, 97, 50, 55, 110, 8, 61, 56, 112, 42, 37] 在图像领域被广泛研究，并被应用于视频模型。Seaweed [78] 和 FastHunyuan [15] 报告称，生成 5 秒 $1 2 8 0 \times 7 2 0$ 24fps 的视频可以蒸馏到 8 或 6 步，质量几乎没有明显下降。为了进一步减少步骤，SF-V [123] 和 OSV [64] 探索了仅使用单一步骤生成 2 秒的 $1 0 2 4 \times 7 6 8$ 7fps 图像到视频生成。最近，APT [49] 在 $8 \times \mathrm { H 1 0 0 }$ GPU 上实现了实时的 2 秒 $1 2 8 0 \times 7 2 0$ 24fps 的文本到视频生成，使用了单一步骤。这激励了更多下游应用探索一阶段视频生成 [99, 11]。我们的方法将对抗后训练 (APT) 扩展到自回归视频生成场景。

流式长视频生成 早期的流式和长视频生成研究应用训练-free或管道方法于小规模的图像和视频生成模型，但在质量上有所限制。现代大规模视频扩散模型，如MovieGen、Hunyuan、Wan2.1和Seaweed，采用变换器架构，并在更高的分辨率和帧率上进行训练。然而，由于注意力计算的平方增长，这些模型通常仅训练生成最多5秒的视频。为了支持长视频生成，这些模型也在视频延伸任务上进行训练，该任务将前几帧作为条件提供给模型。在推理时，这允许模型扩展生成，并将结果以5秒为单位流式传输给用户。该扩展只能进行几次，随后误差积累会达到临界点。最近的研究还探索了具有线性复杂度的架构以直接生成长视频，但它们并未针对流式应用进行设计。

最近，扩散强制方法被提出用于视频生成。它将逐步噪声水平分配给帧块，以便解码以因果流方式进行。早期的工作使用双向注意力机制。近期研究已经转向使用带有键值缓存的因果注意力机制。值得注意的是，SkyReel-2 和 MAGI-1 是从头开始训练的扩散强制视频生成模型。CausVid 探索将现有的双向视频扩散模型转换为因果扩散强制生成器。这些方法中的一些还应用了步骤蒸馏以提高速度。MAGI-1 将模型蒸馏到 8 步，并以一块输出 24 帧。它在 24fps 下报告了在 $2 4 \times \mathrm { H 1 0 0 }$ GPU 上生成 $1 2 8 0 \times 7 2 0$ 的实时效果。然而，这样的计算量限制了广泛采用。CausVid 将模型蒸馏到 4 步，并以一块输出 16 帧。它能够在单个 H100 GPU 上以 9.4fps 生成 $6 4 0 \times 3 2 0$ 的视频。相比之下，我们的方法显著更快。我们的模型仅使用单步，并在单个 H100 GPU 上以 $7 3 6 \times 4 1 6$ 分辨率实现 24fps 流式输出，或者在 $8 \times \mathrm { H 1 0 0 }$ GPU 上以 $1 2 8 0 \times 7 2 0$ 分辨率输出。此外，我们的方法一次生成一个潜在帧（4 个视频帧），以最小化延迟。

需要注意的是，这些扩散强制模型仍然仅在固定时长窗口内进行训练，例如5秒。早期不使用KV缓存的方法可以运行滑动窗口，但这在使用KV缓存时成为一个问题，因为感受野无限增长。应用滑动窗口并丢弃KV令牌无济于事，因为缓存中剩余的令牌是在过去计算的，仍然携带着感受野。在推理阶段的简单外推导致了分布外行为。因此，像CausVid、SkyReel-V2和MAGI1之类的方法仍然需要在推理过程中应用扩展技术，通过重新启动并重新计算一些重叠的上下文帧来生成长视频。除了扩散强制目标自然支持具有不同噪声水平的输入令牌外，因此可以在开始时将上下文帧作为干净的潜在帧提供，而不需要额外的训练。然而，这并非理想，因为这会导致实际流媒体应用中的等待时间。相反，我们的方法支持使用KV缓存流式生成长达一分钟的视频，而无需停止和重新处理。

大语言模型（LLMs）广泛采用因果变换器架构进行自回归生成。最值得注意的是，注意力被遮罩以防止关注未来的词元，输入为过去的预测，输出目标则向后移动一个以预测下一个词元。最近的研究表明，图像和视频也可以以这种自回归方式生成。尽管使用KV缓存的因果生成在计算上是高效的，但逐词生成阻碍了并行化，并且在高分辨率生成时速度较慢。一些研究探索了在推理过程中一次解码多个词元，但这会影响质量，并且一次性解码整个帧具有挑战性。我们的架构受到LLMs的启发，但我们的模型一次生成一帧词元，采用对抗目标进行训练，优化了快速生成的能力。交互式视频生成 我们的论文展示了我们模型在两个应用中的实时交互生成能力：姿势控制的虚拟人类视频生成和相机控制的世界探索。我们简要介绍了每个子领域的相关工作。

近期研究探讨了使用视频生成模型创建互动游戏环境和世界模拟的可能性。通常情况下，首先给出第一帧，然后模型在用户控制下持续预测下一帧（图像到世界）。控制可以是动作空间中的离散状态或通用摄像机位置嵌入。然而，现有视频生成方法的高计算成本极大地限制了解析度和帧率。例如，GameNGen和MineWorld仅以数亿参数的小型模型生成约 $3 2 0 \times 2 4 0$ 分辨率、 $6 { \sim } 2 0 \mathrm { f p s }$ 的视频。近期的工作，如Genie-2、Oasis、Matrix等，已朝着大规模架构和更高分辨率发展。尽管许多工作报告称其方法可以实时运行，但具体的硬件要求并未详细说明。交互式视频生成在虚拟人类领域也具有重要潜力。通常，首先给出第一帧以建立身份，然后给出姿态或其他多模态条件来驱动对象。现有工作使用扩散模型结合扩展技术生成长视频。推断速度依然是限制其适用性的重要瓶颈，尤其是在离线人类视频生成任务中。

![](images/1.jpg)  

Figure 1: Generator (left) is a block causal transformer. The initial frame 0 is provided by the user at the first autoregressive step, along with text, condition, and noise as inputs to the model to generate the next frame in a single forward pass. Then, the generated frame is recycled as input, along with new conditions and noise, to recursively generate further frames. KV cache is used to avoid recomputation of past tokens. A sliding window is used to ensure constant speed and memory for the generation of arbitrary lengths. Discriminator (right) uses the same block causal architecture. Condition inputs are shifted to align with the frame inputs. Since it is initialized from the diffusion weights, we replace the noise channels with frame inputs following APT.



我们的目标是将一个预训练的视频扩散模型转变为一个快速的、每个潜在帧的因果生成器，以便于实时交互应用。我们通过一种新的方法称为自回归对抗后训练（AAPT）实现这一目标。本节讨论AAPT的架构变换和训练过程。

# 2.1 因果架构

![](images/2.jpg)  

Figure 2: Ours is more efficient than one-step diffusion forcing (DF).

我们的方法建立在一个预训练的视频扩散模型之上，该模型采用扩散变换器（DiT）架构，并通过三维变分自编码器（VAE）在空间和时间上压缩的潜在空间中运行。由于我们的模型在潜在空间中操作，除非另有说明，我们将潜在帧简单称为帧。我们的扩散变换器具有80亿（8B）参数。它以文本嵌入词元、嘈杂的视觉词元和扩散时间步作为输入，并对所有文本和视频词元进行双向全注意力计算。首先，我们通过用块因果注意力替换全注意力，将双向DiT转换为因果自回归架构。具体而言，文本词元仅关注自身，视觉词元关注文本词元以及前一帧和当前帧的视觉词元。之后，我们更改模型输入。如图1所示，除了原始扩散模型使用的常规噪声和条件输入外，我们还更改模型，使其通过通道连接接受来自前一个自回归步骤生成的过去帧，首次自回归步骤则使用用户提供的输入帧。在推理过程中，我们的模型以自回归方式运行。在每一步自回归中，它重用注意力KV缓存，并在单次前向传播中生成下一帧。生成的帧和新的控制条件一起被循环利用，作为下一步自回归的输入。为了防止注意力计算和KV缓存大小的无限增长，视觉词元最多参考$N$帧过去的帧，但始终关注文本词元和第一帧。值得注意的是，尽管每个注意力层使用了大小为$N$的窗口，堆叠多个层会导致更大的有效感受野。我们的架构类似于大型语言模型（LLMs），但有一个重要区别：与传统的下一个词元预测采用Softmax层输出词元概率不同，我们的模型在单次前向传播中为下一帧生成所有词元，并通过噪声进行采样。此外，我们的输入回收方法在效率上也优于一步扩散逼迫，如图2所示。扩散逼迫并未针对一步生成场景进行优化。当使用KV缓存时，扩散逼迫在每一步自回归中需要对两帧进行计算，而我们的模型仅需要计算一帧。

# 2.2 训练过程

为了创建一个一步、逐帧的自回归生成器，我们的训练过程涉及三个顺序阶段：（1）扩散适应，（2）一致性蒸馏，以及（3）对抗训练。 扩散适应 我们加载预训练权重，并使用扩散目标微调模型以适应架构。我们应用教师强制训练，从数据集中提供真实帧作为过去帧输入。输出目标向前移动一帧，以便模型执行下一帧预测。除了纯噪声外，仍然使用带噪声的潜空间和扩散时间步 $t \sim \mathcal{U}(0, T)$，以便进行常规扩散训练。所有帧应用相同的噪声水平。这类似于大型语言模型的训练，其中所有自回归步骤是并行训练的。 一致性蒸馏 我们在对抗训练之前应用一致性蒸馏[83]，作为初始化步骤，以加速收敛，遵循APT[49]。我们修改后的公式和架构与原始一致性蒸馏过程完全兼容，无需修改。我们省略了无分类器指导（CFG），因为我们发现它在我们的自回归生成设置中引入了伪影。 对抗训练 我们扩展了APT[49]至自回归设置，改进了判别器设计、训练策略和损失目标。对于判别器模型，我们使用与我们的判别器主干相同的因果生成器架构，从扩散权重后适应中初始化，并插入logit输出投影层。我们用帧替换噪声输入，并随机采样时间步 $t \sim \mathcal{U}(0, T)$ 以实现快速适应。与APT判别器设计的一个显著不同之处在于，我们的设计为每一帧计算输出logit，而不是针对整个片段。这种设计自然地支持并行多时长判别，受到多分辨率判别[39, 38]的启发。我们发现，使用教师强制训练的模型在推理时会遭遇显著的错误累积。为了解决这个问题，我们在对抗训练框架中引入了一种学生强制的方法。具体来说，生成器仅使用真实的第一帧，并将实际生成的结果回收作为下一自回归步骤的输入。在每个训练步骤中，生成器以自回归方式使用 KV 缓存生成视频，准确匹配推理行为，同时判别器评估传递帧输入来自梯度图的稳定性。我们允许梯度通过 KV 缓存流动，以更新所有参数。对于损失，我们使用 R3GAN[31] 目标，因为我们的初步实验发现，它比非饱和损失[18] 更稳定。具体而言，我们采用相对损失[36]，并应用APT[49]中提出的近似 R1 和 R2 正则化[73, 65]。 长视频训练 为了使模型能学习连续生成长视频，必须在长时长的单次视频（例如，3060秒）上进行训练。然而，这种长的单次视频在大多数训练数据集中相对稀少，其中平均镜头时长仅为8秒。缺乏长时长训练导致推理时的时间外推能力差。为了解决数据限制，我们让生成器生成一个长视频，例如60秒，并将其拆分为短片段，例如10秒，供判别器评估。我们保留1秒的重叠时长进行判别器评估，以鼓励片段的连贯性。判别器在生成的片段和来自数据集的真实视频上进行训练。这个目标确保每个生成的长视频片段符合数据分布。为了适应GPU内存，我们还让生成器一次只生成一个片段供判别器评估。为了生成下一个片段，生成器重用上一个片段的分离KV缓存。每评估一个片段后，梯度被反向传播以进行损失累积。这种技术可以用于训练非常长的生成器，代价是训练时间的增加。我们发现，这种技术显著提高了长时长视频生成的质量。这是通过对抗训练中的判别器实现的。与需要真实目标的监督目标不同，判别器对于每个输入帧并不需要显式监督。相反，它学习区分真实视频和生成视频。因此，模型可以从每个视频样本中学习，而不是依赖于有限数量的长时长视频。

# 2.3 交互式生成应用

我们首先训练一个通用图像到视频生成任务的模型，未考虑交互条件。这使我们能够在标准基准上评估生成质量。然后，我们分别针对姿态条件的人类生成任务和相机条件的世界探索任务训练两个独立的模型。这使我们能够使用两种不同的条件信号评估可控性。对于姿态条件的人类视频生成任务，我们从训练视频中提取并编码人类姿态，并将其作为逐帧条件提供给模型，遵循文献[46]。类似地，对于相机条件的世界探索生成任务，我们遵循文献[25]，提取并编码相机的原点和方向作为Plücker嵌入，并进行了少量修改以更好地支持因果生成。我们使用与这些先前工作[46, 25]相似的训练数据集。有关我们的架构、实现和训练参数的更多细节，请参阅我们的补充材料。

# 3 评估

实验设置 我们使用因果 3D 卷积变分自编码器 (VAE) [118] 以时间上压缩视频 4 倍，空间上压缩 8 倍。因此，我们的模型自回归地生成 4 幅视频帧。第一帧输入作为潜在帧由 VAE 独立压缩。由于我们的 VAE 是因果的，它自然支持流式解码。我们使用注意力窗口大小 $N = 30$ 来关注 30 幅潜在帧（5 秒）。有关训练设置的更多细节，请参见补充材料。

基准和度量 根据之前的工作[117]，我们在标准的VBenchI2V基准[32]上评估我们的方法，包括120帧短视频生成和1440帧长视频生成。为了进行比较，我们选择CausVid [117]、Wan2.1 [95]、Hunyuan [44]、MAGI-1 [75]、SkyReel-V2以及我们自己的扩散模型作为基准。这些模型之所以被选择，是因为CausVid是快速流式生成的最先进技术，其他模型是开放源码的视频生成基础模型，支持I2V。需要注意的是，CausVid是一个闭源模型，仅报告120帧12fps生成的VBench-I2V。Wan2.1和Hunyuan是双向扩散模型，仅支持最高到120帧生成。MAGI-1和SkyReel-V2是支持任意长度流式解码的扩散强制模型，因此我们将它们纳入1440帧的比较。我们的方法在$7 3 6 \times 4 1 6$分辨率下进行评估和比较。附加的推理设置和$1 2 8 0 \times 7 2 0$的结果在补充材料中提供。 主要结果 图3定性比较了我们的一个分钟（1440帧）视频生成的方法与SkyReel-V2、MAGI-1和我们的扩散基线。所有三者在20到30秒后都显示出强烈的误差累积。对于我们的扩散基线，我们实验了使用较低的CFG比例或使用重缩放[47]，但这并未减轻曝光问题，反而可能导致更多的结构变形，因此我们保持在CFG 10。我们还展示了我们的AAPT模型仅在10秒的持续时间上训练，无法推广到长视频，如图3d所示。长视频训练至关重要，如图3e所示。图4展示了我们模型在不同主题和场景下的更多结果。表1显示，相较于最先进的方法，我们的方法在量化指标上取得了竞争性能。在120帧I2V生成中，AAPT相较于扩散基线提高了帧质量分数和图像条件分数，是所有比较方法中表现最好的。帧质量的提升与APT [49]中的发现一致，即对抗性训练可以提高视觉质量。AAPT在时间质量分数上略微下降，较扩散基线有所减少，但仍高于Wan，且与Hunyuan非常接近。我们注意到，CausVid在时间质量分数上异常高，可能是因为它在12fps数据上训练，这通常导致比其他24fps模型更高的动态程度，而动态程度分数是整体时间质量的主要区分因素。对于1440帧

![](images/3.jpg)  

Figure 3: Qualitative comparison on one-minute, 1440-frame, VBench-I2V generation.

![](images/4.jpg)  

Figure 4: More results of our AAPT model for one-minute, 1440-frame, VBench-I2V generation.

在I2V生成中，AAPT在各项比较中获得了最佳质量得分，并且在条件评分上优于扩散基线。我们注意到，SkyReel-V2和MAGI-1的图像条件评分高于我们的AAPT和扩散基线，原因是MAGI-1的大多数视频是静态的。这在其动态程度评分上明显较低，并在图3的定性可视化中有所体现。

Table 1: Quantitative comparisons on VBench-I2V [32]. \* denotes metrics that need special interpretation as discussed in the main text. The 6 quality metrics are aggregated as temporal quality and frame quality according to VBench-Competition. The best metrics are highlighted in bold.   

<table><tr><td colspan="2"></td><td colspan="8">Quality</td><td colspan="2">Condition</td></tr><tr><td colspan="2">Frames Method</td><td>Temporal Frame Quality</td><td>Quality</td><td>Subject</td><td>Background</td><td>Motion Consistency Consistency Smoothness Degree</td><td>Dynamic Aesthetic Imaging</td><td>Quality</td><td>Quality</td><td>I2V Subject</td><td>I2V Background</td></tr><tr><td rowspan="5">120</td><td>CausVid [117]</td><td>|*92.00 65.00 |</td><td></td><td></td><td></td><td></td><td>Not Reported</td><td></td><td></td><td></td><td></td></tr><tr><td>Wan 2.1 [95]</td><td></td><td>87.95 66.58</td><td>93.85</td><td>96.59</td><td>97.82</td><td>39.11</td><td></td><td>63.56 69.59</td><td>96.82</td><td>98.57</td></tr><tr><td>Hunyuan [44]</td><td></td><td>89.80 64.18</td><td>93.06</td><td>95.29</td><td>98.53</td><td>54.80</td><td>60.58</td><td>67.78</td><td>97.71</td><td>97.97</td></tr><tr><td>Ours (Diffusion)</td><td>90.40 66.08</td><td></td><td>94.58</td><td>96.76</td><td>98.80</td><td>52.52</td><td>62.44</td><td>69.71</td><td>97.89</td><td>99.14</td></tr><tr><td>Ours (AAPT)</td><td>89.51</td><td>66.58</td><td>96.22</td><td>96.66</td><td>99.19</td><td>42.44</td><td>62.09</td><td>71.06</td><td>98.60</td><td>99.36</td></tr><tr><td rowspan="4">1440</td><td>SkyReel-V2 [9]</td><td>82.19</td><td>53.67</td><td>78.43</td><td>86.38</td><td>99.28</td><td>47.15</td><td>53.68</td><td>53.65</td><td>96.50</td><td>98.07</td></tr><tr><td>MAGI-1 [75]</td><td>80.79 60.01</td><td></td><td>82.23</td><td>89.27</td><td>98.54</td><td>25.45</td><td>52.26</td><td>67.75</td><td>*96.90</td><td>*98.13</td></tr><tr><td>Ours (Diffusion)</td><td>86.65 60.49</td><td></td><td>82.38</td><td>89.48</td><td>98.29</td><td>66.26</td><td>56.46</td><td>64.51</td><td>95.01</td><td>97.72</td></tr><tr><td>Ours (AAPT)</td><td>89.79 62.16</td><td></td><td>87.15</td><td>89.74</td><td>99.11</td><td>76.50</td><td>56.77</td><td>67.55</td><td>96.11</td><td>97.52</td></tr></table>

Table 2: Quantitative comparison on poseconditioned human video generation task. Metrics better than ours are highlighted in bold.   

<table><tr><td>Method</td><td>|AKD↓ IQA↑ ASE↑ FID↓ FVD↓</td></tr><tr><td>DisCo AnimateAnyone MimicMotion</td><td>9.313 3.707 2.396 57.12 64.52 5.747 3.843 2.718 26.87 37.67</td></tr><tr><td>CyberHost</td><td>8.536 3.977 2.842 23.43 3 22.97 3.123 4.087 2.967 20.04 7.72</td></tr><tr><td>OmniHuman-1</td><td>2.136 4.111 2.986 19.50 7.32</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>Ours (AAPT) | 2.740 4.077 2.973 22.43 11.78</td></tr></table>

Table 3: Quantitative comparison on cameraconditioned world exploration task. Metrics better than ours are highlighted in bold.   

<table><tr><td>Method</td><td>| FVD↓ Mov↑ Trans↓ Rot↓ Geo↑ Apr↑</td></tr><tr><td>MotionCtrl</td><td>|221.23 102.21 0.3221 2.78 57.87 0.7431</td></tr><tr><td>CameraCtrl</td><td>199.53 133.37 0.2812 2.81 52.12 0.7784</td></tr><tr><td>CameraCtrl2</td><td>73.11 698.51 0.1527 1.58 88.70 0.8893</td></tr><tr><td></td><td>Ours (AAPT)| 61.33 521.23 0.1185 1.63 81.25 0.9012</td></tr></table>

![](images/5.jpg)  

Figure 5: Pose-conditioned virtual human

![](images/6.jpg)  

Figure 6: Camera-controlled world exploration

姿态条件的人类视频生成 我们在条件姿态的人类视频生成任务上评估我们的方法，使用以前工作的协议和测试集 [45]。姿态准确性通过平均关键点距离（AKD）进行评估，关键点是使用 DWPose 提取的 [113]。为了评估视觉质量，我们使用 Q-Align [107]，一种视觉语言模型，来评估图像质量（IQA）和美学（ASE）。此外，Fréchet Inception Distance（FID） [27] 和 Fréchet Video Distance（FVD） [91] 测量生成样本与真实样本之间的分布一致性。为进行比较，我们包括四种最近的基于 UNet 的扩散模型，即 Disco [101]、AnimateAnyone [30]、MimicMotion [122]、CyberHost [45]，以及最先进的基于 DiT 的 OmniHuman-1 [46]。表 2 展示了定量指标。在六种比较方法中，我们的方法在姿态准确性方面表现强劲，仅次于最先进的基线 OmniHuman-1。在视觉质量方面，我们的方法始终位列第二或第三，紧随 CyberHost 之后。图 5 显示了我们方法的可视化。 相机条件的世界探索 我们在相机条件的世界探索任务上验证我们的方法，也遵循以前工作的协议 [25]。我们计算 FVD、移动强度（Mov）、平移误差（Trans）、旋转误差（Rot）、几何一致性（Geo）和外观一致性（Apr）。详细信息见补充材料。我们与以前的最先进方法进行比较，即 MotionCtrl [104]、CameraCtrl 1 和 2 [24, 25]。表 3 显示我们的方法在六个指标中的三个上达到了新的最先进水平，其余指标则紧随 CameraCtrl2。推理速度 我们在表 4 中比较我们的方法与其他流媒体视频生成方法的吞吐量和延迟。我们的方法显著更快，同时性能与最先进水平相当。

# 4 消融研究

Table 4: Latency and throughput comparison.   

<table><tr><td>Method</td><td colspan="4">Params H100 Resolution NFE[Latency FPS</td></tr><tr><td>CausVid Ours</td><td>5B 8B</td><td>1× 640×352 1× 736×416</td><td>4 1</td><td>1.30s 9.4 0.16s 24.8</td></tr><tr><td>MAGI-1 SkyReelV2 Ours</td><td>24B 14B 8B</td><td>8× 736 ×416 8× 960×544 8× 1280×720</td><td>8 60 1</td><td>7.00s 3.43 4.50s 0.89 0.17s 24.2</td></tr></table>

长视频训练 表 5 报告了在不同训练时长下生成一分钟视频的 VBench-I2V 指标。具体来说，训练时间为 60 秒的模型显著优于仅训练 10 秒的模型，显示出长视频训练的有效性。可视化结果见图 3d。

Table 5: One-minute generation performance using different training durations.   

<table><tr><td>Training Duration</td><td>| Temporal Quality</td><td>Frame Quality</td></tr><tr><td>10s</td><td>85.86</td><td>57.92</td></tr><tr><td>20s</td><td>85.60</td><td>65.69</td></tr><tr><td>60s</td><td>89.79</td><td>62.16</td></tr></table>

教师强迫与学生强迫 尽管扩散适应和一致性蒸馏仅支持教师强迫，但对抗训练可以采用教师强迫或学生强迫的方式进行。我们在补充材料中描述了设置。我们发现，使用教师强迫对抗目标训练的模型在推理时无法生成合适的视频，如图 7 所示。内容在生成过程开始仅几个帧后就开始显著漂移。学生强迫训练对于减轻误差积累至关重要。尽管早期研究发现，在训练时对输入添加高斯噪声可以减少推理时的漂移，但这并未从根本上解决与学生强迫训练相同的分布差距。我们将进一步探索留待未来的工作。 限制 为了保持一致性，我们的模型在维持主体和场景方面可能会遇到困难。这既与生成器有关，也与鉴别器有关。我们的生成器出于简化采用了基本的滑动窗口。我们将更多架构和优化的探索留待未来的工作。对于鉴别器，目前基于段的鉴别无法强制长范围一致性。通过向鉴别器添加身份嵌入可能会有所缓解。对于训练速度，长视频训练过程可能较慢。对于质量，我们发现一步生成仍然会产生缺陷，一旦缺陷出现，可能会在场景中持续很长时间，因为鉴别器也会强制执行时间一致性。需要更多研究来提高一步生成的质量。对于时长，我们在零-shot 五分钟生成上测试了我们的模型。我们的模型仍然可以生成内容，但存在伪影。我们在补充材料中提供了示例。

![](images/7.jpg)  

Figure 7: Models trained with teacher-forcing adversarial objective fail to generate proper content at inference.

# 5 结论

我们引入了自回归对抗后训练（AAPT），这是一种将对抗训练作为范例，旨在将视频扩散模型转变为适合实时交互应用的快速自回归生成器的方法。我们的模型在性能上可与最佳方法相媲美，同时效率显著更高。我们还分析了其局限性，并旨在在未来的工作中解决这些问题。

# 资金的确认与披露

我们感谢叶伟浩在评估方面的协助。我们感谢宋祖泉和郑军如在计算基础设施方面的支持。我们感谢王建意和林志杰在工作期间的讨论。本研究得到了字节跳动种子基金的全力资助。

参考文献 [1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat 等. Gpt-4技术报告. arXiv预印本 arXiv:2303.08774, 2023. [2] Eloi Alonso, Adam Jelley, Vincent Micheli, Anssi Kanervisto, Amos J Storkey, Tim Pearce 和 François Fleuret. 用于世界建模的扩散：视觉细节在Atari中至关重要. 神经信息处理系统进展, 37:5875758791, 2024. [3] Tim Brooks, Janne Hellsten, Miika Aittala, Ting-Chun Wang, Timo Aila, Jaako Lehtinen, Ming-Yu Liu, Alexei Efros 和 Tero Karras. 生成动态场景的长视频. 神经信息处理系统进展, 35:3176931781, 2022. [4] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnur, Joe Taylor, Troy Luhman, Eric Luhman 等. 视频生成模型作为世界模拟器. OpenAI 博客, 1:8, 2024. [5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell 等. 语言模型是少量样本学习者. 神经信息处理系统进展, 33:18771901, 2020. [6] Jake Bruce, Michael D Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps 等. Genie：生成交互环境. 在第41届国际机器学习会议上, 2024. [7] Boyuan Chen, Diego Martí Monsó, Yilun Du, Max Simchowitz, Russ Tedrake 和 Vincent Sitzmann. 扩散强迫：下一个标记预测与全序列扩散相遇. 神经信息处理系统进展, 37:2408124125, 2024. [8] Dar-Yen Chen, Hmrishav Bandyopadhyay, Kai Zou 和 Yi-Zhe Song. NitroFusion：通过动态对抗训练实现高保真单步扩散. arXiv预印本 arXiv:2412.02030, 2024. [9] Guibin Chen, Dixuan Lin, Jiangping Yang, Chunze Lin, Juncheng Zhu, Mingyuan Fan, Hao Zhang, Sheng Chen, Zheng Chen, Chengchen Ma 等. Skyreels-v2：无限长度的影视生成模型. arXiv预印本 arXiv:2504.13074, 2025. [10] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu 和 Chong Ruan. Janus-pro：通过数据和模型扩展实现统一的多模态理解与生成. arXiv预印本 arXiv:2501.17811, 2025. [11] Zheng Chen, Zichen Zou, Kewei Zhang, Xiongfei Su, Xin Yuan, Yong Guo 和 Yulun Zhang. Dove：高效的一步扩散模型用于现实世界视频超分辨率. arXiv预印本 arXiv:2505.16239, 2025. [12] Suhwan Cho, Minhyeok Lee, Seunghoon Lee, Chaewon Park, Donghyeong Kim 和 Sangyoun Lee. 将运动视为选项以减少无监督视频目标分割中的运动依赖. 在IEEE/CVF计算机视觉应用冬季会议论文集中, 页码 5140-5149, 2023. [13] Etched Decart, Quinn McIntyre, Spruce Campbell, Xinlei Chen 和 Robert Wachen. Oasis：变换器中的宇宙. URL: https://oasis-model.github.io, 2024. [14] Patrick Esser, Sumith Kulal, Andreas Blattman, Rahim Entezari, Jonas Müller, Harry Saini, Ya Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel 等. 为高分辨率图像合成扩展整流流变换器. 在第41届国际机器学习会议上, 2024. [15] FastHunyuan. https://huggingface.co/FastVideo/FastHunyuan. [16] Ruili Feng, Han Zhang, Zhantao Yang, Jie Xiao, Zhilei Shu, Zhiheng Liu, Andy Zheng, Yukun Huang, Yu Liu 和 Hongyang Zhang. 矩阵：通过实时移动控制进行无限期世界生成. arXiv预印本 arXiv:2412.03568, 2024. [17] Yu Gao, Jiancheng Huang, Xiaopeng Sun, Zequn Jie, Yujie Zhong 和 Lin Ma. Matten：带有Mamba注意力的视频生成. arXiv预印本 arXiv:2405.03025, 2024. [18] Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courvile 和 Yoshu Bengi. 生成对抗网络. 神经信息处理系统进展, 27, 2014. [19] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan 等. Llama 3系列模型. arXiv预印本 arXiv:2407.21783, 2024. [20] Albert Gu 和 Tri Dao. Mamba：具选择性状态空间的线性时间序列建模. arXiv预印本 arXiv:2312.00752, 2023. [21] Albert Gu, Karan Goel 和 Christopher Ré. 使用结构化状态空间高效地建模长序列. arXiv预印本 arXiv:2111.00396, 2021. [22] Junliang Guo, Yang Ye, Tianyu He, Haoyu Wu, Yushu Jiang, Tim Pearce 和 Jiang Bian. Mineworld：Minecraft上的实时开源交互世界模型. arXiv预印本 arXiv:2504.08388, 2025. [23] Yuwei Guo, Ceyuan Yang, Ziyan Yang, Zhibei Ma, Zhijie Lin, Zhenheng Yang, Dahua Lin 和 Lu Jiang. 用于视频生成的长上下文微调. arXiv预印本 arXiv:2503.10589, 2025. [24] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li 和 Ceyuan Yang. Cameractrl：为文本到视频生成实现相机控制. arXiv预印本 arXiv:2404.02101, 2024. [25] Hao He, Ceyuan Yang, Shanchuan Lin, Yinghao Xu, Meng Wei, Liangke Gui, Qi Zhao, Gordon Wetzstein, Lu Jiang 和 Hongsheng Li. Cameractrl i：通过相机控制的视频扩散模型进行动态场景探索. arXiv预印本 arXiv:2503.10592, 2025. [26] Roberto Henschel, Levon Khachatryan, Hayk Poghosyan, Daniil Hayrapetyan, Vahram Tadevosyan, Zhangyang Wang, Shant Navasardyan 和 Humphrey Shi. Streaming T2V：从文本生成一致、动态和可扩展的长视频. arXiv预印本 arXiv:2403.14773, 2024. [27] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler 和 Sepp Hochreiter. 使用双时间尺度更新规则训练的GAN收敛于局部纳什均衡. 神经信息处理系统进展, 30, 2017. [28] Jonathan Ho, Ajay Jain 和 Pieter Abbeel. 去噪扩散概率模型. 神经信息处理系统进展, 33:68406851, 2020. [29] Jonathan Ho 和 Tim Salimans. 无分类器扩散引导. arXiv预印本 arXiv:2207.12598, 2022. [30] Li Hu. 动画任意人：用于角色动画的一致且可控的图像到视频合成. 在IEEE/CVF计算机视觉与模式识别会议论文集中, 页码 8153-8163, 2024. [31] Nick Huang, Aaron Gokaslan, Volodymyr Kuleshov 和 James Tompkin. GAN已死，长存GAN！现代GAN基线. 神经信息处理系统进展, 37:4417744215, 2024. [32] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit 等. Vbench：视频生成模型的综合基准套件. 在IEEE/CVF计算机视觉与模式识别会议论文集中, 页码 2180-21818, 2024. [33] Sam Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Shuaiwen Leon Song, Samyam Rajbhandari 和 Yuxiong He. DeepSpeed Ulysses：使极长序列Transformer模型训练可行的系统优化. arXiv预印本 arXiv:2309.14509, 2023. [34] Jianwen Jiang, Chao Liang, Jiaqi Yang, Gaojie Lin, Tianyun Zhong 和 Yanbo Zheng. Loopy：驯服音频驱动的肖像化身，带有长期运动依赖. 在第十三届国际表征学习会议上, 2025. [35] Yang Jin, Zhicheng Sun, Ningyuan Li, Kun Xu, Hao Jiang, Nan Zhuang, Quzhe Huang, Yang Song, Yadong Mu 和 Zhouchen Lin. 金字塔流匹配用于高效视频生成建模. arXiv预印本 arXiv:2410.05954, 2024. [36] Alexia Jolicoeur-Martineau. 相对判别器：标准GAN缺失的关键元素. arXiv预印本 arXiv:1807.00734, 2018. [37] Minguk Kang, Richard Zhang, Connelly Barnes, Sylvain Paris, Suha Kwak, Jaesik Park, Eli Shechtman, Jun-Yan Zhu 和 Taesung Park. 将扩散模型蒸馏为条件GAN. 在欧洲计算机视觉会议论文集中, 页码 428-447. Springer, 2024.

[38] Animesh Karnewar 和 Oliver Wang. Msg-gan：生成对抗网络的多尺度梯度. 载于 IEEE/CVF 计算机视觉与模式识别会议论文集, 页码 7799-7808, 2020. [39] Tero Karras, Timo Aila, Samuli Laine 和 Jaakko Lehtinen. GANs 的渐进增长以提高质量、稳定性和变异性. arXiv 预印本 arXiv:1710.10196, 2017. [40] Jihwan Kim, Junoh Kang, Jinyoung Choi 和 Bohyung Han. Fifo-diffusion: 从文本生成无限视频而无需训练. arXiv 预印本 arXiv:2405.11473, 2024. [41] Akio Kodaira, Chenfeng Xu, Toshiki Hazama, Takanori Yoshimoto, Kohei Ohno, Shogo Mitsuhori, Soichi Sugano, Hanying Cho, Zhijian Liu 和 Kurt Keutzer. Streamdiffusion: 实时交互生成的管道级解决方案. arXiv 预印本 arXiv:2312.12491, 2023. [42] Jonas Kohler, Albert Pumarola, Edgar Schönfeld, Artsiom Sanakoyeu, Roshan Sumbaly, Peter Vajda 和 Ali Thabet. Imagine flash: 通过逆向蒸馏加速 EMU 扩散模型. arXiv 预印本 arXiv:2405.05224, 2024. Vighnesh Birodkar, Jimmy Yan, Ming-Chang Chiu 等. Videopoet: 用于零样本视频生成的大语言模型. arXiv 预印本 arXiv:2312.14125, 2023. [44] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jiani Zhang 等. Huyuanvideo: 大型视频生成模型的系统框架. arXiv 预印本 arXiv:2412.03603, 2024. [45] Gaojie Lin, Jianwen Jiang, Chao Liang, Tianyun Zhong, Jiaqi Yang, Zerong Zheng 和 Yanbo Zheng. Cyberhost: 用于基于音频驱动的对话体生成的一阶段扩散框架. 参与第十三届国际学习表征会议, 2025. [46] Gaojie Lin, Jianwen Jiang, Jiaqi Yang, Zerong Zheng 和 Chao Liang. Omnihuman-1: 重思一阶段条件人体动画模型的扩展. arXiv 预印本 arXiv:2502.01061, 2025. [47] Shanchuan Lin, Bingchen Liu, Jiashi Li 和 Xiao Yang. 通用扩散噪声调度和样本步骤存在缺陷. 载于 IEEE/CVF 计算机视觉应用冬季会议论文集, 页码 5404-5411, 2024. [48] Shanchuan Lin, Anran Wang 和 Xiao Yang. Sdxl-lightning: 渐进式对抗扩散蒸馏. arXiv 预印本 arXiv:2402.13929, 2024. [49] Shanchuan Lin, Xin Xia, Yuxi Ren, Ceyuan Yang, Xuefeng Xiao 和 Lu Jiang. 用于一步视频生成的扩散对抗后训练. arXiv 预印本 arXiv:2501.08316, 2025. [50] Shanchuan Lin 和 Xiao Yang. Animatediff-lightning: 跨模型扩散蒸馏. arXiv 预印本 arXiv:2403.12706, 2024. [51] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel 和 Matt Le. 流匹配生成建模. arXiv 预印本 arXiv:2210.02747, 2022. [52] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan 等. Deepseek-v3 技术报告. arXiv 预印本 arXiv:2412.19437, 2024. [53] Feng Liu, Shiwei Zhang, Xiaofeng Wang, Yujie Wei, Haonan Qiu, Yuzhong Zhao, Yingya Zhang, Qixiang Yu 等. arXiv:2411.19108, 2024. [54] Lijie Liu, Tianxiang Ma, Bingchuan Li, Zhuowei Chen, Jiawei Liu, Qian He 和 Xinglong Wu. Phantom: 通过跨模态对齐实现主体一致的视频生成. arXiv 预印本 arXiv:2502.11079, 2025. [5] Xingao Lu, Chenyue Gong 和 Qiang L. Flowraight 和 ast: 学习生成和训练数据的修正流. arXiv 预印本 arXiv:2209.03003, 2022. [56] Xingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng 等. Instaflow: 一步足以实现高质量的基于扩散的文本到图像生成. 参与第十二届国际学习表征会议, 2023. [57] Ilya Loshchilov 和 Frank Hutter. 解耦权重衰减正则化. arXiv 预印本 arXiv:1711.05101, 2017. [58] Cheng Lu 和 Yang Song. 简化、稳定和扩展连续时间一致性模型. arXiv 预印本 arXiv:2410.11081, 2024. [59] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li 和 Jun Zhu. Dpm-solver: 用于扩散概率模型采样的快速 ODE 求解器，约 10 步. 神经信息处理系统进展, 35:5775-7787, 2022. [60] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li 和 Jun Zhu. Dpm-solver++: 扩散概率模型引导采样的快速求解器. arXiv 预印本 arXiv:2211.01095, 2022. [61] Weijian Luo, Tianyang Hu, Shifeng Zhang, Jiacheng Sun, Zhenguo Li 和 Zhihua Zhang. Diff-instruct: 从预训练扩散模型中转移知识的通用方法. 神经信息处理系统进展, 36:7652-576546, 2023. [62] Yuxuan Luo, Zhengkun Rong, Lizhen Wang, Longhao Zhang, Tianshu Hu 和 Yongming Zhu. Dreamactor-m1: 全面、表现力强且鲁棒的人像动画生成，采用混合指导. arXiv 预印本 arXiv:2504.01724, 2025. [63] Xinyin Ma, Gongfan Fang, Michael Bi Mi 和 Xinchao Wang. Learning-to-cache: 通过层缓存加速扩散变换器. 神经信息处理系统进展, 37:133282-133304, 2024. [64] Xiaofeng Mao, Zhengkai Jiang, Fu-Yun Wang, Jiangning Zhang, Hao Chen, Mingmin Chi, Yabiao Wang 和 Wenhan Luo. osv: 一步足以实现高质量的图像到视频生成. arXiv 预印本 arXiv:2409.11367, 2024. [65] Lars Mescheder, Andreas Geiger 和 Sebastian Nowozin. 哪些 GAN 的训练方法实际上会收敛？ 参与国际机器学习会议, 页码 3481-3490. PMLR, 2018. [66] Shentong Mo 和 Yapeng Tian. 使用双向 ssms 扩展扩散 mamba 以实现高效的图像和视频生成. arXiv 预印本 arXiv:2405.15881, 2024. [67] Jack Parker-Holder, Philip Ball, Jake Bruce, Vibhavari Dasagi, Kristian Holsheimer, Christos Kaplanis, Alexandre Moufarek, Guy Scully, Jeremy Shar, Jimmy Shi, Stephen Spencer, Jessica Yung, Michael Dennis, Sultan Kenjeyev, Shangbang Long, Vlad Mnih, Harris Chan, Maxime Gazeau, Bonnie Li, Fabio Pardo, Luyu Wang, Lei Zhang, Frederic Besse, Tim Harley, Anna Mitenkova, Jane Wang, Jeff Clune, Demis Hassabis, Raia Hadsell, Adrian Bolton, Satinder Singh 和 Tim Rocktäschel. Genie 2: 大规模基础世界模型. 2024. [68] William Peebles 和 Saining Xie. 具有可扩展的扩散模型和变换器. 参与 IEEE/CVF 国际计算机视觉会议论文集, 页码 4195-4205, 2023. [69] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-Yao Ma, Ching-Yao Chuang 等. Movie gen: 多媒体基础模型的阵容. arXiv 预印本 arXiv:2410.13720, 2024. [70] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark 等. 从自然语言监督中学习可转移的视觉模型. 参与国际机器学习会议, 页码 8748-8763. PmLR, 2021. [71] Shuhuai Ren, Shuming Ma, Xu Sun 和 Furu Wei. 下一块预测: 通过半自回归建模生成视频. arXiv 预印本 arXiv:2502.07737, 2025. [72] Yuxi Ren, Xin Xia, Yanzuo Lu, Jiacheng Zhang, Jie Wu, Pan Xie, Xing Wang 和 Xuefeng Xiao. Hyper-sd: 轨迹分段一致性模型以实现高效图像合成. arXiv 预印本 arXiv:2404.13686, 2024. [73] Kevin Roth, Aurelien Lucchi, Sebastian Nowozin 和 Thomas Hofmann. 通过正则化稳定生成对抗网络的训练. 神经信息处理系统进展, 30, 2017. [74] Tim Salimans 和 Jonathan Ho. 渐进蒸馏加速扩散模型的快速采样. arXiv 预印本 arXiv:2202.00512, 2022. [75] Sand-AI. Magi-1: 大规模自回归视频生成, 2025. [76] Axel Sauer, Frederic Boesel, Tim Dockhorn, Andreas Blattman, Patrick Esser 和 Robin Rombach. 通过潜在对抗扩散蒸馏实现快速高分辨率图像合成. 载于 SIGGRAPH Asia 2024 会议论文, 页码 1-11, 2024. [77] Axel Sauer, Dominik Lorenz, Andreas Blattmann 和 Robin Rombach. 对抗扩散蒸馏. 参与欧洲计算机视觉会议, 页码 87-103. Springer, 2024. [78] Team Seawead, Ceyuan Yang, Zhijie Lin, Yang Zhao, Shanchuan Lin, Zhibei Ma, Haoyuan Guo, Hao Chen, Lu Qi, Sen Wang 等. Seaweed-7b: 成本效益的视频生成基础模型训练. arXiv 预印本 arXiv:2504.08685, 2025. [79] Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani 和 Tri Dao. Flashattention3: 快速且准确的异步低精度注意力. 神经信息处理系统进展, 37:6865-8685, 2024. [80] Noam Shazeer. 快速变换器解码：一个写头就足够了. arXiv 预印本 arXiv:1911.02150, 2019. [81] Ivan Skorokhodov, Sergey Tulyakov 和 Mohamed Elhoseiny. Stylegan-v: 一种具有 Stylegan2 性价比、图像质量和优点的连续视频生成器. 载于 IEEE/CVF 计算机视觉与模式识别会议论文集, 页码 3626-3636, 2022. [82] Yang Song 和 Prafulla Dhariwal. 改进一致性模型的训练技术. arXiv 预印本 arXiv:2310.14189, 2023. [83] Yang Song, Prafulla Dhariwal, Mark Chen 和 Ilya Sutskever. 一致性模型. 2023. [84] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon 和 Ben Poole. 通过随机微分方程的基于评分的生成建模. arXiv 预印本 arXiv:2011.13456, 2020. [85] Michal Stypulkowski, Konstantinos Vougioukas, Sen He, Maciej Ziba, Stavros Petridis 和 Maja Pantic. 扩散头: 扩散模型在对话面部生成中胜过 GAN. 载于 IEEE/CVF 计算机视觉应用冬季会议论文集, 页码 5091-5100, 2024. [86] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo 和 Yunfeng Liu. Roformer: 具有旋转位置嵌入的增强型变换器. 神经计算, 568:127063, 2024. [87] Chameleon Team. Chameleon: 混合模态早期融合基础模型. arXiv 预印本 arXiv:2405.09818, 2024. [88] Zachary Teed 和 Jia Deng. Raft: 光流的递归全对场转换. 载于计算机视觉 ECCV 2020: 第十六届欧洲会议, 英国格拉斯哥, 2020年8月23日至28日, 论文集, 第 II 部分，第 16 卷，页码 402-419. Springer, 2020. [89] Linrui Tian, Qi Wang, Bang Zhang 和 Liefeng Bo. Emo: 生成具有音频的视频表情肖像. 载于欧洲计算机视觉会议论文集, 页码 244-260. Springer, 2024. [90] Ye Tian, Xin Xia, Yuxi Ren, Shanchuan Lin, Xing Wang, Xuefeng Xiao, Yunhai Tong, Ling Yang 和 Bin Cui. 无需训练的扩散加速与瓶颈采样. arXiv 预印本 arXiv:2503.18940, 2025. [91] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphaël Marinier, Marcin Michalski 和 Sylvain Gelly. Fvd: 视频生成的新指标. [92] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski 和 Sylvain Gelly. 朝着准确的视频生成模型：新的指标与挑战. arXiv 预印本 arXiv:1812.01717, 2018. [93] Dani Valevski, Yaniv Leviathan, Moab Arar 和 Shlomi Fruchter. 扩散模型是实时游戏引擎. arXiv 预印本 arXiv:2408.14837, 2024. [94] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kai, 等. 解耦的宽度衰减正则化. 神经信息处理系统进展, 30, 2017. [95] Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yag, Jianyuan Zeng 等. Wan: 开放的先进大型视频生成模型. arXiv 预印本 arXiv:2503.20314, 2025. [96] Fu-Yun Wang, Wenshuo Chen, Guanglu Song, Han-Jia Ye, Yu Liu 和 Hongsheng Li. Gen-l-video: 多文本到长视频生成，采用时间共去噪. arXiv 预印本 arXiv:2305.18264, 2023. [97] Fu-Yun Wang, Zhaoyang Huang, Weikang Bian, Xiaoyu Shi, Keqiang Sun, Guanglu Song, Yu Liu 和 Hongsheng Li. Animatelcm: 无需个性化视频数据的计算高效个性化风格视频生成. 载于 SIGGRAPH Asia 2024 技术交流, 页码 1-5, 2024. [98] Hongjie Wang, Chih-Yao Ma, Yen-Cheng Liu, Ji Hou, Tao Xu, Jialiang Wang, Felix Juefei-Xu, Yaqiao Luo, Peizhao Zhang, Tingbo Hou 等. Lingen: 朝着高分辨率分钟级文本到视频生成，具有线性计算复杂性. arXiv 预印本 arXiv:2412.09856, 2024. [99] Jianyi Wang, Shanchuan Lin, Zhijie Lin, Yuxi Ren, Meng Wei, Zongsheng Yue, Shangchen Zhou, Hao Chen, Yang Zhao, Ceyuan Yang 等. Seedvr2: 通过扩散对抗后训练实现的一步视频恢复. arXiv 预印本 arXiv:2506.05301, 2025.

[100] Jianyuan Wang, Nikita Karaev, Christian Rupprecht, 和 David Novotny. Vggsfm：视觉几何基础的深度运动重建. 发表在IEEE/CVF计算机视觉与模式识别会议论文集中，第21686-21697页，2024年。 [101] Tan Wang, Linjie Li, Kevin Lin, Yuanhao Zhai, Chung-Ching Lin, Zhengyuan Yang, Hanwang Zhang, Zicheng Liu, 和 Lijuan Wang. Disco：用于真实人类舞蹈生成的解耦控制. 发表在IEEE/CVF计算机视觉与模式识别会议论文集中，第9326-9336页，2024年。 [102] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yuez Wang, Zhen Li, Qiying Yu, 等. Emu3：下一词预测就是你所需要的一切. arXiv预印本arXiv:2409.18869，2024年。 [103] Yuqing Wang, Shuhuai Ren, Zhijie Lin, Yujin Han, Haoyuan Guo, Zhenheng Yang, Difan Zou, Jiashi Feng, 和 Xihui Liu. 并行自回归视觉生成. arXiv预印本arXiv:2412.15119，2024年。 [104] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, 和 Ying Shan. Motionctrl：用于视频生成的统一且灵活的运动控制器. 发表在ACM SIGGRAPH 2024会议论文集中，第111页，2024年。 [105] Jeffrey Willtte, Heejun Lee, Youngwan Lee, Myeongjae Jeon, 和 Sung Ju Hwang. 无需训练的通过级联KV缓存进行指数上下文扩展. arXiv预印本arXiv:2406.17808，2024年。 [106] Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xiu Co Ruan 和 JaDeal ncitalnd生成. arXiv预印本arXiv:2410.13848，2024年。 [107] Haoning Wu, Zicheng Zhang, Weixia Zhang, Chaofeng Chen, Liang Liao, Chunyi Li, Yixuan Gao, Annan Wang, Erli Zhang, Wenxiu Sun, 等. Q-align：通过离散文本定义水平教授LMMS进行视觉评分. arXiv预印本arXiv:2312.17090，2023年。 [108] Desai Xie, Zhan Xu, Yicong Hong, Hao Tan, Difan Liu, Feng Liu, Arie Kaufman, 和 Yang Zhou. 进步的自回归视频扩散模型. arXiv预印本arXiv:2410.08151，2024年。 [109] Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang Li, Ligeng Zhu, Yao Lu, 等. Sana：高效的高分辨率图像合成通过线性扩散变换器. arXiv预印本arXiv:2410.10629，2024年。 [110] Yanwu Xu, Yang Zhao, Zhisheng Xiao, 和 Tingbo Hou. Ufogen：一次前向的大规模文本到图像生成通过扩散GAN。发表在IEEE/CVF计算机视觉与模式识别会议论文集中，第8196-8206页，2024年。 [111] Hanshu Yan, Jun Hao Liew, Long Mai, Shanchuan Lin, 和 Jiashi Feng. Magicprop：通过运动感知外观传播的基于扩散的视频编辑. arXiv预印本arXiv:2309.00908，2023年。 [112] Hanshu Yan, Xingchao Liu, Jiachun Pan, Jun Hao Liew, Qiang Liu, 和 Jiashi Feng. Perflow：作为通用即插即用加速器的分段修正流. arXiv预印本arXiv:2405.07510，2024年。 [113] Zhendong Yang, Ailing Zeng, Chun Yuan, 和 Yu Li. 通过两阶段蒸馏实现有效的全身姿态估计. 发表在IEEE/CVF国际计算机视觉大会论文集，第4210-4220页，2023年。 [114] Yang Ye, Junliang Guo, Haoyu Wu, Tianyu He, Tim Pearce, Tabish Rashid, Katja Hofmann, 和 Jiang Bian. 快速自回归视频生成通过对角解码. arXiv预印本arXiv:2503.14070，2025年。 [115] Tianwei Yin, Michal Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, 和 Bill Freeman. 改进的分布匹配蒸馏用于快速图像合成. 神经信息处理系统进展，37:4745-547487，2024年。 [116] Tianwei Yin, Michaël Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William T Freeman, 和 Taesung Park. 通过分布匹配蒸馏实现一步扩散. 发表在IEEE/CVF计算机视觉与模式识别会议论文集中，第6613-6623页，2024年。 [117] Tianwei Yin, Qiang Zhang, Richard Zhang, William T Freeman, Fredo Durand, Eli Shechtman, 和 Xun Huang. 从慢双向到快速因果视频生成. arXiv预印本arXiv:2412.07772，2024年。 [118] Lijun Yu, José Lezama, Nitesh B Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, 等. 语言模型胜过扩散，tokenizer是视觉生成的关键. arXiv预印本arXiv:2310.05737，2023年。 [119] Jint Zhang, Haofe Huang, Pengle Zhang, Jiawei, Jun Zhu, 和 Jian Che. Sageatteteal报告：用于即插即用推理加速的精确4位注意力. arXiv预印本arXiv:2411.10958，2024年。 [120] Jinto Zhag, Haofe Huang, Pengle Zhang, Jun Zhu, Jiani Chen, 等. Sageattenti：用于即插即用推理加速的精确8位注意力. arXiv预印本arXiv:2410.02367，2024年。 [121] Peiyuan Zhang, Yongqi Chen, Runlong Su, Hangliang Ding, Ion Stoica, Zhenghong Liu, 和 Hao Zhang. 快速视频生成通过滑动瓦片注意力. arXiv预印本arXiv:2502.04507，2025年。 [122] Yuang Zhang, Jiaxi Gu, Li-Wen Wang, Han Wang, Junqi Cheng, Yuefeng Zhu, 和 Fangyuan Zou. Mimicmotion：高质量人类运动视频生成与信心感知的姿态指导. arXiv预印本arXiv:2406.19680，2024年。 [123] Zhixing Zhang, Yanyu Li, Yushu Wu, Anil Kag, Ivan Skorokhodov, Willi Menapace, Aliaksandr Siarohin, Junli Cao, Dimitris Metaxas, Sergey Tulyakov, 等. Sf-v：单向视频生成模型. 神经信息处理系统进展，37:103599-103618，2024年。 [124] Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, 等. Pytorch fsdp：关于扩展完全分片数据并行的经验. arXiv预印本arXiv:2304.11277，2023年。 [125] Chang Zou, Xuyang Liu, Ting Liu, Siteng Huang, 和 Linfeng Zhang. 通过token-wise特征缓存加速扩散变换器. arXiv预印本arXiv:2410.05317，2024年。

# 模型架构

扩散变换器 我们的扩散变换器主要遵循 MMDiT 设计。它具有 80 亿个参数和 36 个变换器块。判别器采用相同的架构。因此，我们的生成器和判别器在对抗训练中总共包含 160 亿个参数。 块级因果注意力 我们在循环中使用 Flash Attention 3 实现块级因果注意力。我们发现这为训练提供了合理的性能。我们将在未来的工作中探索更多性能改进。Flash Attention 可以自然地采用而不会产生性能损失。 位置嵌入 由于生成的持续时间对我们的因果架构变得无关紧要，我们修改了三维旋转位置嵌入（RoPE）。具体而言，位置嵌入沿空间维度动态拉伸，以帮助模型适应不同的分辨率，而位置嵌入在时间维度上则以固定间隔变化，以支持任意长度的训练和生成。 并行性 我们采用 FSDP 进行数据并行。在需要递归向前调用以避免重复参数聚集的学生强迫训练中，我们对生成器使用 ZERO 2，对所有其他模块使用 ZERO 3 以节省内存。我们还采用 Ulysses 作为我们的上下文并行策略。我们将每个视频样本分片到 8 个 GPU 上。每个变换器块还利用梯度检查点以满足内存要求。

# B 训练细节

扩散适应 在将架构更改为块因果注意力并添加回收输入通道后，我们首先通过扩散训练对模型进行适应。

我们遵循原始模型使用流匹配参数化[51]。具体来说，给定样本 $x_{0}$ 和噪声 $\epsilon$，输入通过线性插值得到 $x_{t} = (1 - t) \cdot x_{0} + t \cdot \epsilon$。扩散时间步均匀采样 $\bar{t} \sim \mathcal{U}(0, 1)$，然后通过一个平移函数 ${\mathrm{shift}}(t, s) := (s \times t) / (1 + (s - 1) \bar{t})$ 传递，$s = 24$。请注意，整个视频片段使用相同的时间步，而不是采用对每一帧分配独立时间步的扩散强制方法[7]。我们的模型预测速度 $v = \epsilon - x_{0}$ 并使用均方误差损失进行惩罚。我们应用教师-学生范式，并提供真实帧的逐帧预测。

我们在整个过程中使用学习率为1e-5和权重衰减比例为0.01的AdamW优化器[57]。首先，我们在 $736 \times 416$ （通过面积等价于 $640 \times 480$）的5秒视频上训练20,000次，批量大小为256。然后，我们增加 $1280 \times 720$ 的视频进行另外 $^ {\mathrm{6k}}$ 迭代，批量大小为128。最后，我们将 $736 \times 416$ 分辨率视频的最大时长提高到15秒，进行 $4 \mathrm{k\Omega}$ 迭代，批量大小为0。这允许模型在最终阶段没有任何样本的标记。一致性蒸馏 然后我们应用一致性蒸馏[82]来创建一个一步生成器。在训练阶段，如 APT [49] 所发现的，我们继承了与最后的扩散适应阶段相同的AdamW设置和数据集设置。我们在32个固定步骤上训练模型，这些步骤均匀选择，然后通过具有偏移因子 $s = 24$ 的平移函数进行处理。我们不应用无分类器引导[29]。我们继续使用教师强制范式，提供真实标注帧作为回收输入，并按照扩散适应方法将噪声输入和输出目标平移一帧。我们遵循改进的一致性蒸馏技术[82]，不对一致性目标应用指数移动平均。对于一致性蒸馏不需要额外的修改。模型训练5,000次。对抗训练 最后，我们执行对抗训练。在这一阶段，我们切换到学生模型。生成的结果并行产生logits，以进行多时长鉴别。我们遵循 APT [49] 从一致性蒸馏权重初始化生成器，并从扩散适应权重初始化鉴别器。我们改用相对配对损失[36]：

$$
\mathcal { L } _ { R p G A N } ( x _ { 0 } , \epsilon ) = f ( D ( G ( \epsilon , c ) , c ) - D ( x _ { 0 } , c ) ) ,
$$

其中 $G, D$ 分别表示生成器和判别器，$f_{G}(x) = -\log(1 + e^{-x})$ 或 $f_{D}(x) = -\log(1 + e^{x})$ 分别用作它们的更新步骤，$c$ 表示文本条件和其他交互条件。我们通过 APT [49] 提出的近似技术计算 R1 和 R2 正则化 [73, 65]：

$$
\mathcal { L } _ { a R 1 } = \lambda \| D ( x _ { 0 } , c ) - D ( \mathcal { N } ( x _ { 0 } , \sigma \mathbf { I } ) , c ) \| _ { 2 } ^ { 2 } ,
$$

$$
\mathcal { L } _ { a R 2 } = \lambda \| D ( G ( \epsilon , c ) , c ) - D ( \mathcal { N } ( G ( \epsilon , c ) , \sigma \mathbf { I } ) , c ) \| _ { 2 } ^ { 2 } ,
$$

其中 $\epsilon = 0.1$ 和 $\lambda = 1000$。由于鉴别器初始化自扩散模型，我们遵循 APT 通过随机均匀采样提供时间步 $t \sim \mathcal{U}(0, 1)$。我们不对鉴别器的时间步进行偏移。我们使用 RMSProp 优化器，$\alpha = 0.9$，遵循 APT [49]。我们首先进行没有长视频扩展训练技术的训练。视频时长在 5 至 10 秒之间。我们使用学习率为 3e-6 进行训练，遵循 APT [49]，批次大小为 256，共进行 500 次生成器更新。得到的模型最多只生成 10 秒的视频，对于超过 10 秒的视频会出现漂移。然后我们应用长视频训练技术。训练视频仍然为 5 秒到 10 秒之间，并且我们将其扩展一次，重叠 1 秒，总时间最长为 19 秒（$10 + (10 - 1)$）。这个阶段训练 500 次生成器更新。然后将扩展次数增加到 5 次，总最大持续时间为 55 秒（$10 + 5 \times (10 - 1)$）。我们发现有必要将批次大小减少到 64，并将学习率增加到 1e-5，以便在合理的时间内使模型进行适当的修改。由于在学生强迫模式下，生成器必须在每次自回归步骤中重复进行模型前向计算，我们将 FSDP 切换到 ZERO 2 模式，以便在每台机器上保存所有模型参数。这避免了重复参数收集，改善了训练效率。鉴别器和文本编码器仍然采用 ZERO 3 来分割所有模型参数，以节省内存。计算资源方面，我们使用 256 个 H100 GPU 进行最终训练，并在需要时进行梯度累加，以达到最终批次大小。模型训练大约需要天，其中扩散适配和长视频对抗训练占据了大部分时间。

# C 变分自编码器

我们训练了一个轻量级的变分自编码器解码器，以满足实时预算。具体而言，我们原始的变分自编码器解码器在每个分辨率尺度上具有 3 个残差块，并且每个分辨率尺度的通道为 [128, 256, 512, 512]。我们的轻量级变分自编码器解码器将每个分辨率的残差块数量减少到 2 个，并将通道减少到 [64, 128, 256, 512]。这使得速度提升近 3 倍，而没有明显的质量下降。

# D 教师强制对抗训练，生成器自回归地运行并使用 KV 缓存，将实际生成的帧作为下一个自回归步骤的输入。鉴别器并行评估结果。为了实现教师强制，生成器将真实标注的视频帧作为过去预测的输入，并并行预测下一帧。鉴别器自回归运行，并始终使用真实视频的 KV 缓存来关注真实标注的过去帧。

图8可视化了教师强制对抗训练。具体而言，在教师强制模式下，给定输入 $I_1, I_2, I_3$ 的生成器生成独立输出 $\bar {O_2}, O_3, \bar {O_4}$。即，输出 $O_3$ 仅与 $I_2$ 相关，而与 $O_2$ 无关。因此，鉴别器必须独立评估生成结果及其正确的依赖关系，以产生 logits $L_2, L_3, L_4$。由于鉴别器转换器是因果的，因此可以通过使用 KV 缓存来节省重复计算。

![](images/8.jpg)  

Figure 8: Teacher-forcing adversarial training

我们已经进行了使用教师强迫的实验。现有的长短期记忆网络（LLMs）能够以教师强迫的模式进行训练，因为它们使用离散的词汇编码单词，在这种情况下，轻微的不准确性影响较小。然而，我们的模型预测的是整个数据集的连续潜在值，因此轻微的不准确性将会累积。

# E 结果回收的重要性

我们对训练过程进行了设置，并将回收输入掩码为零张量，除了第一帧，它接收用户的图像。我们发现，未使用回收输入进行训练的模型无法生成大幅运动。一些动作变得不连贯。视频可视化已在我们的网站上提供。

# F I2V 评估

主文中的表格比较了我们模型在 $7 3 6 \times 4 1 6$ 设置下的表现。对于我们比较的其他模型，我们在很大程度上遵循了每个模型的默认采样设置，包括步骤数量和CFG [29]。我们还使用每个模型的默认分辨率，以确保模型已经在预期分辨率上进行了适当训练。具体而言，我们对 Hunyuan [44] 使用 $8 9 6 \times 5 4 4$，对 Wan2.1 [95] 使用 $8 3 2 \times 4 6 4$，对 SkyReel-V2 [9] 使用 $9 6 0 \times 5 4 4$。我们注意到，根据 VBench-I2V [32] 的要求，除了 SkyReel-V2 只对每个提示运行 1 个样本且将采样步骤从默认的 50 减少到较低值外，我们对所有比较每个提示运行了 5 个样本。原因是 SkyReel-V2 在生成新的视频时计算资源消耗过大。此外，我们在表 6 中提供了在 $1 2 8 0 \times 7 2 0$ 分辨率下的评估指标。请注意，$1 2 8 0 \times 7 2 0$ 是在较小的注意力窗口大小 $N = 1 5$ 下进行训练和推理的，以适应内存。

Table 6: Quantitative VBench-I2V [32] metrics on $1 2 8 0 \times 7 2 0$ compared to $7 3 6 \times 4 1 6$   

<table><tr><td rowspan="2"></td><td rowspan="2">Frames Method Resolution</td><td rowspan="2"></td><td colspan="7">Quality Subject</td><td rowspan="2"></td><td colspan="2">Condition</td></tr><tr><td>Temporal Quality</td><td>Frame Quality</td><td>Background Consistency Consistency</td><td>Motion Smoothness</td><td>Degree</td><td>Quality</td><td>Dynamic Aesthetic Imaging Quality</td><td>I2V</td><td>I2V Subject Background</td></tr><tr><td rowspan="2">1440</td><td rowspan="2">Ours</td><td>736×416</td><td>89.79</td><td>62.16</td><td>87.15</td><td>89.74</td><td>99.11</td><td>76.50</td><td>56.77</td><td>67.55</td><td>| 96.11</td><td>97.52</td></tr><tr><td>1280×720</td><td>88.24</td><td>64.30</td><td>87.95</td><td>90.10</td><td>99.16</td><td>63.29</td><td>57.79</td><td>70.80</td><td>96.51</td><td>98.18</td></tr></table>

# G 摄像头条件的世界探索

训练 我们对CameraCtrl II [25]进行了一些修改，以更好地支持因果生成。首先，CameraCtrl I使用Plücker嵌入来表示相机的位置和方向，其中它处理值和行界限。我们更改为每一帧相对前一帧。因此，Plücker嵌入仅表示相邻帧之间的相机变化，以防止值的无界增长。其次，CameraCtrl II使用原始的Plücker坐标来表示每个相机光线，由方向向量和动量向量组成。动量向量编码位移信息，是通过光线上某一点和光线的叉积计算得出的。我们认为Plücker嵌入的精确度对学习至关重要，因此我们直接利用相机光线的方向。这里，模型的缩放因子是一个之前未探讨的超参数。我们将坐标输入缩放到大约1个标准差，以简化模型学习。我们还丢弃那些相机嵌入值非常大的样本。这些异常值是由不准确的相机估计引起的，对训练的稳定性有害。最后，我们对新通道的输入投影使用随机初始化，而不是零初始化。我们发现，随机初始化有助于模型更快速地适应新输入。相机条件模型与I2V模型单独训练。我们从I2V扩散适应权重开始，并继续在相机条件任务上进行训练。一致性蒸馏和额外调整是I2V模型的步骤。对于长视频扩展训练，我们随机抽取新的相机轨迹来扩展部分。评估 我们的评估指标遵循CameraCtrl I [25]。具体来说，我们计算Fréchet视频距离（FVD）[92]与真实视频的关系。我们计算通过RAFT提取的[88]前景物体的运动强度（Mov），这些物体由TMO生成[12]的分割掩码识别。总体转移（Ro）使用VGGSfM [100]与真实值进行比较。几何一致性（Geo）计算为VGGSfM成功估计相机参数的比例。这指示生成的视频的三维几何一致性的质量。外观一致性（Apr）通过比较每帧LIP [70]视觉嵌入与整个视频剪辑平均值的余弦距离来计算。

# H 社会影响

我们的工作提出了一种用于交互式应用程序的实时流视频生成的新方法。与现有方法相比，我们的方法更快且计算效率更高。这可能会促进更多实时交互式应用程序的采用。我们认为我们的工作不会带来显著的负面社会影响。我们的方法生成的视频仍然存在一些缺陷，容易被识别为生成的视频，这使得该技术无法被用于恶意目的。