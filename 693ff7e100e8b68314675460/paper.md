# UnityVideo: Unified Multi-Modal Multi-Task Learning for Enhancing World-Aware Video Generation

Jiehui Huang1, † Yuechen Zhang2 Xu He3 Yuan Gao4 Zhi Cen4 Bin Xia2 Yan Zhou4 Xin Tao4 Pengfei Wan4 Jiaya Jia1   
1HKUST 2CUHK 3Tsinghua University 4Kling Team, Kuaishou Technology Projects: https://jackailab.github.io/Projects/UnityVideo

![](images/1.jpg)

# Abstract

Recent video generation models demonstrate impressive synthesis capabilities but remain limited by single-modality conditioning, constraining their holistic world understanding. This stems from insufficient cross-modal interaction and limited modal diversity for comprehensive world knowledge representation. To address these limitations, we introduce UnityVideo, a unified framework for world-aware video generation that jointly learns across multiple modalities (segmentation masks, human skeletons, DensePose, optical flow, and depth maps) and training paradigms. Our approach features two core components: (1) dynamic noising to unify heterogeneous training paradigms, and (2) a modality switcher with an in-context learner that enables unified processing via modular parameters and contextual learning. We contribute a large-scale unified dataset with 1.3M samples. Through joint optimization, UnityVideo accelerates convergence and significantly enhances zero-shot generalization to unseen data. We demonstrate that UnityVideo achieves superior video quality, consistency, and improved alignment with physical world constraints. Code and data can be found at: https://github.com/ dvlab-research/UnityVideo

# 1. Introduction

Large language models (LLMs) have achieved strong generalization and reasoning ability by unifying diverse text-based modalities, including natural language, code, and mathematical expressions, within a single training paradigm [10, 22, 26, 29, 35, 41, 56]. This integration of complementary text sub-modalities improves task performance and supports emergent reasoning. Similarly, recent video foundation models show promising world modeling as scale and parameters increase [3, 19, 23, 43, 49]. However, visual scaling has largely centered on RGB video alone, analogous to training language models only on plain text. This gap motivates the question of whether unifying visual sub-modalitiessuch as depth, optical flow, segmentation, skeleton, and DensePosecan strengthen a model's understanding of the physical world, as unified text learning has done for LLMs. Recent work indicates that video generation can benefit from single auxiliary input, such as depth maps, optical flow, skeletons, and segmentation masks [32, 48, 55, 59]. Many approaches use a one-way interaction: conditioning RGB generation on auxiliary modalities for controllable synthesis [4, 16, 18, 51], or predicting these modalities from RGB via inverse estimation [13, 17, 45]. A few recent frameworks [1, 5, 6, 40, 53] explore bidirectional interactions and report gains in motion and geometric understanding through shared representations across modalities. Despite this progress, the effect of a unified training paradigm on cross-modal interaction and world awareness remains unclear. Can joint training on multiple modalities and tasks improve reasoning, accelerate convergence, and yield emergent perception? Single-modal learning limits a model's ability to infer physical dynamics, encouraging distribution fitting rather than reasoning. In practice, different modalities provide complementary cues: instance segmentation separates categories [20, 42, 62], DensePose distinguishes body parts [12, 34], and skeletons encode finegrained motion [30, 36]. This is demonstrated in Fig. 2, jointly learning from complementary information across diverse modalities benefits the convergence in video generation, further offers a path toward more comprehensive world modeling and improved zero-shot generalization. UnityVideo is presented motivated by these observations. UnityVideo is a unified framework for multimodal video generation, estimation, and joint modeling. UnityVideo integrates multiple modalities and training paradigms to accelerate convergence, enhance zeroshot generalization, and promote mutual gains across tasks and modalities. The framework introduces a light-weight modality-adaptive learner that maps heterogeneous modality signals into a shared feature space, enabling plug-andplay selection of inputs at inference. To further improve generalization, we design an in-context learner that leverages internal contextual cues to enable text-driven video object segmentation without external detectors [28]. We also devise a dynamic noise scheduling strategy that switches among different training objectives, including joint generation, video estimation, and controllable generation, within a single training cycle to encourage cross-task synergy.

![](images/2.jpg)  
Figure 2. Training on unified modalities benefits video generation. Unified multi-modal and multi-task joint training achieves the lowest final loss on RGB video generation, outperforming single-modality joint training and RGB finetuning baseline.

We curate OpenUni, a large-scale dataset of 1.3M multimodal video samples to enable this unified training paradigm, and construct a high-quality benchmark, UniBench. UniBench contains 30K synthetic videos and a subset of the training data, with ground-truth depth and optical flow rendered in Unreal Engine. These resources provide a solid basis for fair and comprehensive evaluation. As shown in Fig 1, UnityVideo is a general-purpose model that performs both video generation and estimation, and it generalizes in a zero-shot manner to novel objects that not provided in training data. Extensive quantitative and qualitative results demonstrate that our model outperforms existing approaches across multiple downstream tasks. Our main contributions are summarized as follows: •We propose UnityVideo, a novel unified framework for integrating multiple video tasks and modalities, enabling mutual knowledge transfer, better convergence, and improved performance over single-task baselines. We introduce a modality-adaptive switcher, an in-context learner, and a dynamic noise scheduling strategy that together enable efficient joint training across diverse objectives and scalability to larger datasets. • We construct and release OpenUni, a 1.3M-pair multimodal video dataset, and UniBench, a 30K-sample benchmark derived from Unreal Engine for fair evaluation of unified video models.

# 2. Releated Work

# 2.1. Video Generation

Large-scale video generation has advanced world modeling and physical reasoning [7, 27, 39, 44, 49, 52], improving a model's ability to capture physical dynamics [2, 8, 15, 24, 49]. Recent work integrates additional visual signals such as depth, camera pose, and optical flow to jointly model video [1, 5, 40]. Two main directions have emerged: (i) encoding multiple modalities into a shared latent space and using flow-matching to jointly predict video and auxiliary modalities, enabling mutual gains [5, 53]; and (ii) conditioning generation on multi-modal inputs for controllable synthesis, allowing simultaneous compliance with multiple control signals and improved visual quality [16, 18]. Despite strong results, most studies isolate either a single architecture or a single modality, limiting cross-task synergy. In contrast, we unify multi-task learning in a single framework and analyze how such unification enhances world perception and generalization.

# 2.2. Video Reconstruction

Videos contain rich world knowledge, and classical vision methods estimate depth, camera pose, and optical flow directly from RGB [21, 40]. Recent diffusion-based approaches learn bidirectional mappings between conditions and video without external modules, revealing intrinsic bidirectional capacity in flow matching frameworks [13, 17, 37]. Representative systems such as Aether [40], GeoVideo [1], and 4DNex [9] couple video with geometric modalities, and EgoTwin [53] links skeletons and video. Bidirectional interactions also appear between video and audio [44] and between video and text [7, 54], for example UniVerse-1 for audio and video [44] and UniVid or Omni-Video for text and video. However, prior work has not fully unified diverse modalities or systematically studied their synergy, and it rarely activates in-context abilities for strong zero-shot generalization. Our approach addresses these gaps through joint training across modalities and tasks, yielding a unified model with stronger zero-shot performance and clearer insights into cross-modal coupling.

# 3. Method

UnityVideo unifies video generation and multimodal understanding within a single diffusion transformer. As illustrated in Fig. 3, the model processes RGB video $V _ { r }$ , text condition $C$ , and auxiliary modality $V _ { m }$ through a shared DiT backbone $u ( \cdot )$ . During training, we dynamically sample task types and apply corresponding noise schedules (Sec. 3.1). To handle multiple modalities within this unified architecture, we introduce an In-Context Learner and a Modality-Adaptive Switcher (Sec. 3.2). Through progressive curriculum training (Sec. 3.3), the model achieves simultaneous convergence across all tasks and modalities.

# 3.1. Unifying Multiple Tasks

Conventional video generation models are trained for specific tasks in isolation, limiting their ability to leverage cross-task knowledge. We extend the flow matching framework [25] to support three complementary training paradigms within a single architecture. UnityVideo simultaneously handles three objectives: generating RGB videos from auxiliary modalities $( u ( V _ { r } | V _ { m } , C ) )$ , estimating auxiliary modalities from RGB videos $( u ( V _ { m } | V _ { r } ) )$ , and jointly generating both from noise $( u ( V _ { r } , V _ { m } | C ) )$ . The $V _ { r }$ and $V _ { m }$ tokens are concatenated along the width dimension and interact through the self-attention module. Following [18, 38], we incorporate 3D RoPE within the DiT backbone's self-attention to effectively distinguish cross-modal spatiotemporal positions.

Dynamic Task Routing. To enable concurrent optimization across these three paradigms, we introduce probabilistic task selection during training. At each iteration, we sample one task type with probabilities $p _ { \mathrm { c o n d } }$ , $p _ { \mathrm { e s t } }$ ,and $p _ { \mathrm { j o i n t } }$ (where $p _ { \mathrm { c o n d } } + p _ { \mathrm { e s t } } + p _ { \mathrm { j o i n t } } = 1 )$ , which determines the noise schedule applied to RGB and modality tokens at timestep $t$ .For conditional generation, as depicted in the right part of Fig. 3, RGB tokens are gradually denoised from noise $\left( t \sim \left[ 0 , 1 \right] \right)$ while modality tokens remain clean $( t = 0 )$ . For modality estimation, RGB tokens remain clean while modality tokens are noised. For joint generation, both token types are independently corrupted with noise. We assign task probabilities inversely proportional to their learning difficulty: $p _ { \mathrm { c o n d } } < p _ { \mathrm { e s t } } < p _ { \mathrm { j o i n t } }$ . This strategy prevents the catastrophic forgetting common in sequential stage-wise training, allowing the model to learn all three distributions concurrently.

# 3.2. Unifying Multiple Modalities

Joint training across different modalities can significantly enhance the performance of individual tasks, as in Fig. 2. However, processing diverse modalities with shared parameters requires explicit mechanisms to distinguish them. We introduce two complementary designs: a context learner for semantic-level modality awareness and a modality-adaptive switcher for architecture-level modulation.

In-Context Learner. To leverage the model's inherent contextual reasoning capability, we inject modality-specific textual prompts $C _ { m }$ that describe the modality type (e.g., "depth map," "human skeleton") rather than video content. This design fundamentally differs from contentdescribing captions $C _ { r }$ . Given concatenated RGB tokens $V _ { r }$ and modality tokens $V _ { m }$ , we perform dual-branch crossattention separately: $V _ { r } ^ { \prime } = \operatorname { C r o s s A t t n } ( V _ { r } , C _ { r } )$ for RGB features with content captions, and $V _ { m } ^ { \prime } = \mathrm { C r o s s A t t n } ( V _ { m } , C _ { m } )$ for modality features with type descriptions, before recombining them for subsequent processing. This lightweight mechanism introduces negligible computational overhead while enabling compositional generalization. For instance, training with the phrase "two persons" allows the model to generalize to "two objects" during segmentation tasks, as the model learns to interpret modality-level semantics rather than memorizing content-specific patterns. Detailed analysis is provided in the experimental section.

![](images/3.jpg)  
tokens (left), and modality unification via the proposed Modality-Aware AdaLN Table (center).Specifically, $L _ { r }$ and $L _ { m }$ denote the respectively. $C _ { r }$ and $C _ { m }$ represent the prompt condition for RGB video content and in-context modaliy learning prompt, while $V _ { r }$ and $V _ { m }$ correspond to the token sequences from the RGB and auxiliary modalities, respectively.

Modality-Adaptive Switcher. While text-based differentiation provides semantic awareness, it may become insufficient as the number of modalities scales. We therefore introduce a learnable embedding list $\mathbf { L } _ { m } = \left\{ L _ { 1 } , L _ { 2 } , \ldots , L _ { k } \right\}$ for $k$ modalities to enable explicit architecture-level modulation. Within each DiT block, AdaLN-Zero [31] produces modulation parameters (scale $\gamma$ , shift $\beta$ , gate $\alpha$ )for RGB features based on timestep embeddings. We extend this mechanism by learning modality-specific parameters: $\gamma _ { m } , \beta _ { m } , \alpha _ { m } = \mathbf { M L P } ( L _ { m } + t _ { \mathrm { e m b } } )$ , where $L _ { m } \in P _ { m }$ is the modality embedding and $t _ { \mathrm { e m b } }$ is the timestep embedding. This design enables plug-and-play modality selection during inference. To further reduce modality confusion and stabilize outputs, we initialize modality expert input-output layers as a dedicated encoding and prediction head for each modality. Further details are provided in the Appendix A.

# 3.3. Training Strategy

Curriculum Learning for Multiple Modalities. Naively training all modalities jointly from scratch leads to slow convergence and suboptimal performance. We categorize modalities into two groups based on their spatial alignment properties. Pixel-aligned modalities (optical flow, depth, DensePose) allow direct pixel-to-pixel correspondence with RGB frames, while pixel-unaligned modalities (segmentation, skeleton) require additional visual rendering steps. We adopt a two-stage curriculum strategy: Stage 1 trains only pixel-aligned modalities on curated single-person data, establishing a strong foundation for spatial correspondence learning. Stage 2 incorporates all modalities and diverse scene datasets, covering both human-centric and general scenarios. This progressive strategy enables the model to understand all five modalities while supporting robust zeroshot inference on unseen modality combinations.

![](images/4.jpg)  
Figure 4. OpenUni dataset. OpenUni contains 1.3M pairs of unified multimodal data, designed to enrich video modalities with more comprehensive world perception.

OpenUni Dataset. Our training data comprises $1 . 3 \ \mathrm { m i l }$ . lion video clips spanning five modalities: optical flow, depth, dense pose, skeleton, and segmentation. As illustrated in Figure 4, we collect real-world videos from multiple sources and extract modality annotations using pre-trained models. The dataset includes 370,358 singleperson clips, 97,468 two-person clips, 489,445 clips from Koala36M [47], and 343,558 clips from OpenS2V [60], totaling $1 . 3 { \bf M }$ samples for training. To prevent overfitting to specific datasets or modalities, we partition each batch into four balanced groups, ensuring uniform sampling across all modalities and sources. More details on training data are provided in Appendix C.

# 3.4. Training Objective

Following Conditional Flow Matching [25], our framework employs a dynamic training strategy that adaptively switches between three modes by selectively noising different modalities. The mode-specific losses are:

$$
\begin{array} { r l } & { \mathcal { L } _ { \mathrm { c o n d } } ( \theta ; t ) = \mathbb { E } \left[ \Vert u _ { \theta } ( r _ { t } , [ m _ { 0 } , c _ { \mathrm { t x t } } ] , t ) - v _ { r } \Vert ^ { 2 } \right] , } \\ & { \quad \mathcal { L } _ { \mathrm { e s t } } ( \theta ; t ) = \mathbb { E } \left[ \Vert u _ { \theta } ( m _ { t } , r _ { 0 } , t ) - v _ { m } \Vert ^ { 2 } \right] , } \\ & { \mathcal { L } _ { \mathrm { j o i n t } } ( \theta ; t ) = \mathbb { E } \left[ \Vert u _ { \theta } ( [ r _ { t } , m _ { t } ] , c _ { \mathrm { t x t } } , t ) - [ v _ { r } , v _ { m } ] \Vert ^ { 2 } \right] , } \end{array}
$$

where $r _ { t } = ( 1 - t ) r _ { 0 } + t r _ { 1 }$ and $m _ { t } = ( 1 - t ) m _ { 0 } + t m _ { 1 }$ denote the interpolated latents at timestep $t \in [ 0 , 1 ]$ , with $r$ and $m$ representing RGB video and auxiliary modality (e.g., optical flow, depth) respectively. The velocity fields are defined as $v _ { r } = r _ { 1 } - r _ { 0 }$ and $v _ { m } = m _ { 1 } - m _ { 0 }$ , where $r _ { 0 } , m _ { 0 }$ are clean latents encoded from real data and $r _ { 1 } , m _ { 1 } \sim \mathcal { N } ( 0 , I )$ are independent Gaussian noise. The text conditioning $c _ { \mathrm { t x t } }$ is obtained from a pre-trained text encoder. Eq. (1) enables conditional generation of RGB video from auxiliary modality, Eq. (2) performs modality estimation from RGB video, and Eq. (3) jointly generates both modalities from text. During training, each sample in a batch is randomly assigned to one of the three modes, enabling all tasks to contribute gradients within a single optimization step. This unified formulation allows seamless multi-task learning within a single architecture.

# 4. Experiment

In this section, we first provide implementation details in Sec. 4.1, followed by the main results in Sec. 4.2. We conduct extensive benchmarks on both modality estimation and video generation tasks, comparing UnityVideo against state-of-the-art methods. The results demonstrate that UnityVideo exhibits strong unified capabilities across all settings. Subsequently, Sec. 4.3 presents ablation studies that validate the effectiveness of our design choices. Finally, we analyze the convergence behavior and zero-shot generalization ability of UnityVideo, complemented by a user study. Additional analysis of UnityVideo's zero-shot generalization and its reasoning abilities about video modalities are provided in the Appendix B.

# 4.1. Experimental Setup

Training Details. We use an internal DiT backbone with 10B parameters as our core architecture. Training is conducted in two stages. In the first stage, the model is trained on a human-centric dataset containing 500K video clips for 16K steps. In the second stage, we scale up training to a larger dataset of 1.3M video clips for an additional 40K steps. The model is trained with a batch size of 32 and a learning rate of $5 \times 1 0 ^ { - 5 }$ . During inference, we use 50 DDIM sampling steps with a CFG scale of 7.5.

Baselines. Since our framework introduces a novel unified paradigm for video generation and estimation, no directly comparable models exist. We therefore evaluate against leading models in three related categories: (1) Video Generation: We compare with text-to-video models, including the commercial model Keling-1.6, and open-source models OpenSora [33], Hunyuan-13B [19], and Wan-2.1-13B [43]. For controllable generation, we include VACE [16] and Full-DiT [18]. We also consider models capable of jointly generating video and depth, such as Aether [40]. (2) Video Estimation: We evaluate against diffusion-based depth estimation models, including DepthCrafter [13], Geo4D [17], and Aether [40]. Additional results are provided in the Appendix. (3) Video Segmentation: We compare with two recent segmentation models that support prompt-based object segmentation, SAMWISE [11] and SeC [61]. To ensure fair comparisons, all evaluations are conducted on the public VBench [14] dataset and our newly constructed UniBench dataset, specifically designed for unified video tasks. UniBench comprises 200 high-quality samples obtained from Unreal Engine (UE) for accurate video estimation evaluation [57], and 200 manually curated samples covering diverse modalities from real-world videos [18] for controllable generation and segmentation assessments. More details are provided in Appendix C. Evaluation Metrics. To comprehensively assess the performance of our model, we evaluate it across three categories of metrics. (1) Video Quality. We measure visual and temporal quality using multiple perceptual and consistency-based metrics [14], including subject consistency, background consistency, aesthetic quality, imaging quality, temporal flickering, motion smoothness, and dynamic degree. These metrics collectively evaluate spatial fidelity, aesthetic appeal, and temporal coherence of the generated videos. (2) Depth Estimation. For quantitative evaluation of video-based depth prediction [13], we report the absolute relative error (AbsRel) and the percentage of predicted depths within a 1.25 factor of the ground truth $( \delta ~ < ~ 1 . 2 5 )$ . (3) Video Segmentation. To evaluate segmentation accuracy [61, 62], we adopt standard metrics for both instance and semantic segmentation tasks, namely the mean Average Precision (mAP) and mean Intersectionover-Union (mIoU). TabuntivcoarnVicoolabltioext-vitioiomati. achieves superior or competitive performance across all metrics.   

<table><tr><td rowspan="2"></td><td rowspan="2"></td><td colspan="4">Video Generation - VBench &amp; UniBench Dataset</td><td colspan="4">Video Estimation - UniBench Dataset</td></tr><tr><td colspan="4">VBench</td><td colspan="2">Segmentation</td><td colspan="2">Depth</td></tr><tr><td>Tasks</td><td>Models</td><td>Background Consistency</td><td>Aesthetic Quality</td><td>Overall Consistency</td><td>Dynamic Degree</td><td>mIoU ↑</td><td>mAP↑</td><td>Abs Rel ↓</td><td>δ &lt; 1.25 ↑</td></tr><tr><td rowspan="5">Text2Video</td><td>Kling1.6</td><td>95.33</td><td>60.48</td><td>21.76</td><td>47.05</td><td></td><td></td><td></td><td></td></tr><tr><td>OpenSora2</td><td>96.51</td><td>61.51</td><td>19.87</td><td>34.48</td><td>-</td><td></td><td></td><td></td></tr><tr><td>HunyuanVideo-13B</td><td>96.28</td><td>53.45</td><td>22.61</td><td>41.18</td><td></td><td>-</td><td>-</td><td>-</td></tr><tr><td>Wan2.1-14B</td><td>96.78</td><td>63.66</td><td>21.53</td><td>34.31</td><td></td><td>-</td><td></td><td></td></tr><tr><td>Aether</td><td>95.28</td><td>48.25</td><td>20.26</td><td>37.32</td><td></td><td>-</td><td>0.025</td><td>97.95</td></tr><tr><td>Controllable</td><td>full-dit</td><td>95.58</td><td>54.82</td><td>20.12</td><td>49.50</td><td>-</td><td>-</td><td>-</td><td></td></tr><tr><td>Generation</td><td>VACE</td><td>93.61</td><td>51.24</td><td>17.52</td><td>61.32</td><td>-</td><td>-</td><td>-</td><td>- -</td></tr><tr><td>Depth Video</td><td>depth-crafter</td><td></td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>0.065</td><td>96.94</td></tr><tr><td>Reconstruction</td><td>Geo4D</td><td>- -</td><td></td><td></td><td>-</td><td>-</td><td>-</td><td>0.053</td><td>97.94</td></tr><tr><td>Video</td><td>SAMWISE</td><td></td><td>-</td><td></td><td>-</td><td>62.21</td><td>20.12</td><td>-</td><td>-</td></tr><tr><td>Segmentation</td><td>SeC</td><td>- -</td><td>-</td><td>-</td><td>-</td><td>65.52</td><td>22.23</td><td></td><td></td></tr><tr><td>Unified ControGen,</td><td>UnityVideo</td><td>96.04</td><td>54.63</td><td>21.86</td><td>64.42</td><td></td><td></td><td></td><td></td></tr><tr><td>T2V, and Estimation</td><td>UnityVideo</td><td>97.44</td><td>64.12</td><td>23.57</td><td>47.76</td><td>68.82</td><td>23.25</td><td>0.022</td><td>98.98</td></tr></table>

# 4.2. Main Results

This section validates the superior performance of UnityVideo compared to single-task approaches. We comprehensively evaluate UnityVideo on text-to-video generation, controllable generation, and modality estimation tasks, demonstrating both improved generation quality and enhanced world perception capabilities. Quantitative Comparison. As shown in Tab. 1, UnityVideo achieves competitive results across all tasks, demonstrating strong overall performance. For text-tovideo generation, we report the result of the depth-RGB joint generation. Our model obtains the best results on all metrics. We attribute this to joint training across multiple modalities, which enables collaborative refinement and enhances the model's world perception capabilities, leading to superior video quality. Compared to previous controllable generation methods, UnityVideo excels in background consistency, overall consistency, and dynamic degree, while maintaining competitive aesthetic quality. This indicates that our model better understands and leverages control conditions, benefiting from multi-task joint training that enables the model to go beyond simply following control signals. Furthermore, through joint training with multimodal data, UnityVideo outperforms single-modality models such as Geo4D, Aether, and SeC on both video segmentation and depth estimation tasks. These results confirm that the unified training framework enhances the model's perception and reasoning capabilities for complex visual scenes. Qualitative Comparison. As shown in Fig. 2 (A), compared to advanced text-to-video models, UnityVideo demonstrates superior world perception. Our model exhibits stronger adherence to physical principles, more accurately reflecting the physical phenomenon of refraction. Furthermore, as shown in Fig. 2 (B), compared to advanced controllable generation methods, UnityVideo not only follows depth guidance more faithfully but also maintains overall video quality. In contrast, other methods often exhibit noticeable background flickering, with subject regions sometimes distorted by surrounding context. For modality estimation tasks, as shown in Fig. 2 (C) and (D), UnityVideo produces finer edge details,a wider field of view, and accurate 3D point clouds, benefiting from the complementary nature of multiple modalities. Similarly, in other modality estimation tasks (Fig. 2 (E)), our model demonstrates strong reasoning capabilities, achieving accurate estimation on unseen data and overcoming the overfitting issues observed in other specialized models [12, 58]. Overall, these qualitative results confirm that joint training across multiple tasks and modalities yields significant improvements over single-task or single-modality approaches. This unified framework proves effective in enhancing the model's perception and reasoning capabilities for the physical world. Additional visual results can be found in Appendix D.

![](images/5.jpg)  
adherence to control conditions, and a more detailed understanding of auxiliary modalities.

Table 2. Ablation study comparing single-modality and multimodal training. Only: single modality; Ours: multiple modalities.   

<table><tr><td></td><td>Subject Consistency</td><td>Background Consistency</td><td>Imaging Quality</td><td>Overall Consistency</td></tr><tr><td>Baseline</td><td>96.51</td><td>96.06</td><td>64.99</td><td>23.17</td></tr><tr><td>Only Flow</td><td>97.82</td><td>97.14</td><td>67.34</td><td>23.70</td></tr><tr><td>Only Depth</td><td>98.13</td><td>97.29</td><td>69.09</td><td>23.48</td></tr><tr><td>Ours-Flow</td><td>97.97 (+1.46)</td><td>97.19 (+1.13)</td><td>69.36 (+4.37)</td><td>23.74 (+0.57)</td></tr><tr><td>Ours-Depth</td><td>98.01 (+1.50)</td><td>97.24 (+1.18)</td><td>69.18 (+4.19)</td><td>23.75 (+0.58)</td></tr></table>

# 4.3. Abalation Study

Our ablation study addresses two core questions: (a) Does unified training across multiple modalities and tasks enable mutual benefits between modalities, and in what aspects are these benefits manifested? (b) Are our proposed architectural designs effective? What roles do the In-Context Learner and Modality Switcher play in the model? The following experiments address these questions. Impact of Different Modalities. To quantitatively evaluate the impact of unified multimodal training on video generation, we compare two commonly used modalities, depth and optical flow, as shown in Tab. 2. The results show that joint training consistently improves performance across all metrics compared to the baseline. Furthermore, unified training with multiple modalities yields additional gains, particularly in image quality and overall consistency. This indicates that unifying diverse modalities not only provides complementary supervision during training but also enables mutual enhancement between modalities. Effect of Multi-Task Training. To further quantify the mutual benefits between different training tasks within our unified framework, we train models separately on Joint Generation and Controllable Generation tasks, both guided by depth modality. Results are summarized in Tab. 3. We find that training only on the ControlGen task leads to performance degradation compared to the baseline. However, unified multi-task training recovers and even surpasses this performance, achieving improvements across all metrics. Similarly, compared to training only on Joint Generation, unified training shows only slight decreases in subject consistency and background consistency, while overall performance still outperforms the baseline, demonstrating the effectiveness of multi-task interaction.

Table 3. Ablation study on single-task versus unified multi-task training. Only: single-task; Ours: unified multi-task.   

<table><tr><td></td><td>Subject Consistency</td><td>Background Consistency</td><td>Temporal Flickering</td><td>Motion Smoothness</td></tr><tr><td>Baseline</td><td>96.51</td><td>96.06</td><td>98.73</td><td>99.30</td></tr><tr><td>Only ControlGen</td><td>96.53</td><td>95.58</td><td>98.45</td><td>99.28</td></tr><tr><td>Only JointGen</td><td>98.01</td><td>97.24</td><td>99.10</td><td>99.44</td></tr><tr><td>Ours-ControlGen</td><td>96.53 (+0.02)</td><td>96.08 (+0.02)</td><td>98.79 (+0.06)</td><td>99.38 (+0.08)</td></tr><tr><td>Ours-JointGen</td><td>97.94 (+1.43)</td><td>97.18 (+0.63)</td><td>99.13 (+0.40)</td><td>99.48 (+0.18)</td></tr></table>

Impact of Architectural Design. We investigate the impact of two architectural strategies, In-Context Learner and Modality Switcher, on model performance. To ensure consistent evaluation, we perform text-to-video generation conditioned on depth guidance during inference. Results shown in Tab. 4 and Fig. 6 demonstrate that each strategy effectively improves performance through multimodal fusion. Furthermore, combining both strategies yields additional significant gains, confirming their complementary roles in facilitating unified multimodal learning.

Table 4. Ablation study on architectural designs.   

<table><tr><td></td><td>Subject Consistency</td><td>Background Consistency</td><td>Temporal Flickering</td><td>Motion Smoothness</td></tr><tr><td>Baseline</td><td>96.51</td><td>96.06</td><td>98.73</td><td>99.30</td></tr><tr><td>w/ In-Context Learner</td><td>97.92</td><td>97.08</td><td>99.04</td><td>99.42</td></tr><tr><td>w/ Modality Switcher</td><td>97.94</td><td>97.18</td><td>99.13</td><td>99.48</td></tr><tr><td>Ours</td><td>98.31</td><td>97.54</td><td>99.35</td><td>99.54</td></tr></table>

Table 5. World perception evaluation comparing UnityVideo with state-of-the-art models.   

<table><tr><td rowspan="2"></td><td colspan="3">User Study Score (%)</td><td colspan="2">Automatic Score</td></tr><tr><td>Physical Quality</td><td>Semantic Quality</td><td>Overall Preference</td><td>Subject Consistency</td><td>Motion Smoothness</td></tr><tr><td>Kling1.6</td><td>10.15</td><td>21.25</td><td>20.20</td><td>83.47</td><td>98.08</td></tr><tr><td>HunyuanVideo</td><td>24.15</td><td>26.10</td><td>20.35</td><td>97.53</td><td>98.35</td></tr><tr><td>Wan2.1</td><td>27.20</td><td>22.40</td><td>27.65</td><td>97.73</td><td>98.30</td></tr><tr><td>Ours</td><td>38.50</td><td>30.25</td><td>31.80</td><td>98.01</td><td>99.33</td></tr></table>

![](images/6.jpg)  
A woman is performing a series of push-ups on a red mat in aliving room…   
Figure 6. Unlike single-modality training, unified multimodal learning provides complementary supervision that strengthens both motion understanding and geometric perception.

# 4.4. Model Analyze

As shown in Fig. 7, the proposed In-Context Learner effectively generalizes a fixed two-person segmentation task to unseen two-object scenarios. In contrast, using only the Modality Switcher fails to achieve such generalization. Moreover, during unified training, as the model gradually learns additional modalities (e.g., depth), we observe improved motion understanding and more accurate text responses in RGB videos, demonstrating the complementary roles of different modalities throughout training. User Study. We conduct a user study using a standard win-rate protocol to evaluate our model's understanding of the physical world [50]. The questionnaire contains 12 randomly selected videos generated with WISA-80K prompts [46], presented in random order. Each sample is rated by at least three annotators on (i) physical quality, (ii) semantic quality (PF), and (iii) overall quality. For automatic evaluation, we adopt two VBench [14] metrics: dynamism and aesthetics. In total, we collect 70 completed responses, and the results are summarized in Tab. 5. The study shows that our method achieves the best performance across both human evaluations and automatic metrics.

![](images/7.jpg)  
Figure 7. The In-Context Learner generalizes segmentation to unseen objects, while unified training enhances depth and semantic understanding in RGB video.

# 5. Limitation and Future Work

While UnityVideo significantly advances unified visual modeling, several directions remain for future work. The current VAE occasionally introduces reconstruction artifacts, which could be addressed through fine-tuning or improved autoencoder architectures. Additionally, scaling to larger backbones and incorporating more visual modalities may further enhance emergent world understanding capabilities. Despite these limitations, UnityVideo establishes a strong foundation for unified multi-modal video understanding and represents an important step toward comprehensive world models across diverse visual representations.

# 6. Conclusion

We present UnityVideo, a unified framework that models multiple visual modalities and tasks within a single diffusion transformer. By leveraging modal-adaptive learning, UnityVideo enables bidirectional learning between RGB video and auxiliary modalities (depth, optical flow, segmentation, skeleton, and DensePose), achieving mutual enhancement across both tasks. Our experiments demonstrate state-of-the-art performance across diverse benchmarks with strong zero-shot generalization to unseen modality combinations. To support this research, we contribute OpenUni, a large multimodal dataset with 1.3M synchronized samples, and UniBench, a high-quality evaluation benchmark with ground-truth annotations. UnityVideo paves the way toward unified multimodal modeling as a promising step toward next-generation world models.

# References

[1] Yunpeng Bai, Shaoheng Fang, Chaohui Yu, Fan Wang, and Qixing Huang. Geovideo: Introducing geometric regularization into video generation model. In NeurIPS, 2025. 2, 3   
[2] Hritik Bansal, Clark Peng, Yonatan Bitton, Roman Goldenberg, Aditya Grover, and Kai-Wei Chang. Videophy-2: A challenging action-centric physical commonsense evaluation in video generation. arXiv preprint arXiv:2503.06800, 2025. 3   
[3] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, et al. Video generation models as world simulators. OpenAI Blog, 1(8):1, 2024. 2   
[4] Yuanhao Cai, He Zhang, Xi Chen, Jinbo Xing, Yiwei Hu, Yuqian Zhou, Kai Zhang, Zhifei Zhang, Soo Ye Kim, Tianyu Wang, et al. Omnivcus: Feedforward subject-driven video customization with multimodal control conditions. arXiv preprint arXiv:2506.23361, 2025. 2   
[5] Hila Chefer, Uriel Singer, Amit Zohar, Yuval Kirstain, Adam Polyak, Yaniv Taigman, Lior Wolf, and Shelly Sheynin. Videom: Joint appearance-motion representations  ehanced motion generation in video models. arXiv preprint arXiv:2502.02492, 2025. 2, 3   
[6] Junyi Chen, Haoyi Zhu, Xianglong He, Yifan Wang, Jianjun Zhou, Wenzheng Chang, Yang Zhou, Zizun Li, Zhoujie Fu, Jiangmiao Pang, et al. Deepverse: 4d autoregressive video generation as a world model. arXiv preprint arXiv:2506.01103, 2025. 2   
[7] Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Zhenyu Tang, Li Yuan, et al. Sharegpt4video: Improving video understanding and generation with better captions. NeurIPS, 37:19472 19495, 2024. 3   
[8] Yunuo Chen, Junli Cao, Anil Kag, Vidit Goel, Sergei Korolev, Chenfanfu Jiang, Sergey Tulyakov, and Jian Ren. Towards physical understanding in video generation: A 3d point regularization approach. arXiv preprint arXiv:2502.03639, 2025. 3   
[9] Zhaoxi Chen, Tianqi Liu, Long Zhuo, Jiawei Ren, Zeng Tao, He Zhu, Fangzhou Hong, Liang Pan, and Ziwei Liu. 4dnex: Feed-forward 4d generative modeling made easy. arXiv preprint arXiv:2508.13154, 2025. 3   
10] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. 2   
11] Claudia Cuttano, Gabriele Trivigno, Gabriele Rosi, Carlo Masone, and Giuseppe Averta. Samwise: Infusing wisdom in sam2 for text-driven video segmentation. In CVPR, pages 33953405, 2025. 5   
12] Riza Alp Güler, Natalia Neverova, and Iasonas Kokkinos. Densepose: Dense human pose estimation in the wild. In CVPR, pages 72977306, 2018. 2, 6   
13] Wenbo Hu, Xiangjun Gao, Xiaoyu Li, Sijie Zhao, Xiaodong Cun, Yong Zhang, Long Quan, and Ying Shan. Depthcrafter: Generating consistent long depth sequences for open-world videos. In CVPR, pages 20052015, 2025. 2, 3, 5, 6   
[14] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In CVPR, pages 2180721818, 2024. 5, 8   
[15] Sihui Ji, Xi Chen, Xin Tao, Pengfei Wan, and Hengshuang Zhao. Physmaster: Mastering physical representation for video generation via reinforcement learning. arXiv preprint arXiv:2510.13809, 2025. 3   
[16] Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin Pan, and Yu Liu. Vace: All-in-one video creation and editing. ICCV, 2025. 2, 3, 5   
[17] Zeren Jiang, Chuanxia Zheng, Iro Laina, Diane Larlus, and Andrea Vedaldi. Geo4d: Leveraging video generators for geometric 4d scene reconstruction. arXiv preprint arXiv:2504.07961, 2025. 2, 3, 5   
[18] Xuan Ju, Weicai Ye, Quande Liu, Qiulin Wang, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, and Qiang Xu. Fulldit: Multi-task video generative foundation model with full attention. ICCV, 2025. 2, 3, 5   
[19] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: A systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 2,5   
[20] Irena Koprinska and Sergio Carrato. Temporal video segmentation: A survey. Signal processing: Image communication, 16(5):477500, 2001. 2   
[21] Akshay Krishnan, Xinchen Yan, Vincent Casser, and Abhijit Kundu. Orchid: Image latent diffusion for joint appearance and geometry generation. ICCV, 2025. 3   
[22] Dawei Li, Bohan Jiang, Liangjie Huang, Alimohammad Beigi, Chengshuai Zhao, Zhen Tan, Amrita Bhattacharjee, Yuxuan Jiang, Canyu Chen, Tianhao Wu, et al. From generation to judgment: Opportunities and challenges of llm-as-ajudge. In EMNLP, pages 27572791, 2025. 2   
[23] Xuanyi Li, Daquan Zhou, Chenxu Zhang, Shaodong Wei, Qibin Hou, and Ming-Ming Cheng. Sora generates videos with stunning geometrical consistency. arXiv preprint arXiv:2402.17403, 2024. 2   
[24] Minghui Lin, Xiang Wang, Yishan Wang, Shu Wang, Fengqi Dai, Pengxiang Ding, Cunxiang Wang, Zhengrong Zuo, Nong Sang, Siteng Huang, et al. Exploring the evolution of physics cognition in video generation: A survey. arXiv preprint arXiv:2503.21765, 2025. 3   
[25] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. ICLR, 2023. 3, 5   
[26] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. 2   
[27] Kai Liu, Wei Li, Lai Chen, Shengqiong Wu, Yanhao Zheng, Jiayi Ji, Fan Zhou, Rongxin Jiang, Jiebo Luo, Hao Fei, et al. Javisait: Joint auaio-viaeo airrusion transiormer witn nierarchical spatio-temporal prior synchronization. arXiv preprint arXiv:2503.23377, 2025. 3   
[28] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In ECCV, pages 3855. Springer, 2024. 2   
[29] Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. The ai scientist: Towards fully automated open-ended scientific discovery. arXiv preprint arXiv:2408.06292, 2024. 2   
[30] Romero Morais, Vuong Le, Truyen Tran, Budhaditya Saha, Moussa Mansour, and Svetha Venkatesh. Learning regularity in skeleton trajectories for anomaly detection in videos. In CVPR, pages 1199612004, 2019. 2   
[31] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, pages 41954205, 2023. 4   
[32] Bohao Peng, Jian Wang, Yuechen Zhang, Wenbo Li, MingChang Yang, and Jiaya Jia. Controlnext: Powerful and efficient control for image and video generation. arXiv preprint arXiv:2408.06070, 2024. 2   
[33] Xiangyu Peng, Zangwei Zheng, Chenhui Shen, Tom Young, Xinying Guo, Binluo Wang, Hang Xu, Hongxin Liu, Mingyan Jiang, Wenjun Li, et al. Open-sora 2.0: Training a commercial-level video generation model in $2 0 0 \mathrm { k }$ arXiv preprint arXiv:2503.09642, 2025. 5   
[34] Artsiom Sanakoyeu, Vasil Khalidov, Maureen S McCarthy, Andrea Vedaldi, and Natalia Neverova. Transferring dense pose to proximal animal classes. In CVPR, pages 5233 5242, 2020. 2   
[35] Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Michael Moor, Zicheng Liu, and Emad Barsoum. Agent laboratory: Using llm agents as research assistants. arXiv preprint arXiv:2501.04227, 2025.2   
[36] Yukun Su, Guosheng Lin, Jinhui Zhu, and Qingyao Wu. Human interaction learning on 3d skeleton point clouds for video violence recognition. In ECCV, pages 7490. Springer, 2020.2   
[37] Yang-Tian Sun, Xin Yu, Zehuan Huang, Yi-Hua Huang, Yuan-Chen Guo, Ziyi Yang, Yan-Pei Cao, and Xiaojuan Qi. Unigeo:Taming video diffusion for unified consistent geometry estimation. arXiv preprint arXiv:2505.24521, 2025. 3   
[38] Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. Ominicontrol: Minimal and universal control for diffusion transformer. In ICCV, pages 14940 14950, 2025. 3   
[39] Zhiyu Tan, Hao Yang, Luozheng Qin, Jia Gong, Mengping Yang, and Hao Li. Omni-video: Democratizing unified video understanding and generation. arXiv preprint arXiv:2507.06119, 2025. 3   
[40] Aether Team, Haoyi Zhu, Yifan Wang, Jianjun Zhou, Wenzheng Chang, Yang Zhou, Zizun Li, Junyi Chen, Chunhua Shen, Jiangmiao Pang, et al. Aether: Geometric-aware unified world modeling. arXiv preprint arXiv:2503.18945, 2025 2.2   
[41] Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, et al. Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534, 2025. 2   
[42] Yi-Hsuan Tsai, Ming-Hsuan Yang, and Michael J Black. Video segmentation via object flow. In CVPR, pages 3899 3908, 2016. 2   
[43] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 2, 5   
[44] Duomin Wang, Wei Zuo, Aojie Li, Ling-Hao Chen, Xinyao L Deyu Zhou,Zixin in, Xil ai Daxi Jiang, andGa YUn of experts. arXiv preprint arXiv:2509.06155, 2025. 3   
[45] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. In CVPR, pages 5294 5306, 2025.2   
[46] Jing Wang, Ao Ma, Ke Cao, Jun Zheng, Zhanjie Zhang, Jiasong Feng, Shanyuan Liu, Yuhang Ma, Bo Cheng, Dawei Leng, et al. Wisa: World simulator assistant for physics-aware text-to-video generation. arXiv preprint arXiv:2503.08153, 2025. 8   
[47] Qiuheng Wang, Yukai Shi, Jiarong Ou, Rui Chen, Ke Lin, Jiahao Wang, Boyuan Jiang, Haotian Yang, Mingwu Zheng, Xin Tao, et al. Koala-36m: A large-scale video dataset improving consistency between fine-grained conditions and video content. In CVPR, pages 84288437, 2025. 5   
[48] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: A unified and flexible motion controller for video generation. In SIGGRAPH, pages 111, 2024. 2   
[49] Thaddäus Wiedemer, Yuxuan Li, Paul Vicol, Shixiang Shane Gu, Nick Matarese, Kevin Swersky, Been Kim, Priyank Jaini, and Robert Geirhos. Video models are zero-shot learners and reasoners. arXiv preprint arXiv:2509.20328, 2025. 2, 3   
[50] Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, et al. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025. 8   
[51] Shengqiong Wu, Weicai Ye, Jiahao Wang, Quande Liu, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, Shuicheng Yan, Hao Fei, et al. Any2caption: Interpreting any condition to caption for controllable video generation. arXiv preprint arXiv:2503.24379, 2025. 2   
[52] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. ICLR, 2025. 3   
[53] Jingqiao Xiu, Fangzhou Hong, Yicong Li, Mengze Li, Wentao Wang, Sirui Han, Liang Pan, and Ziwei Liu. Egotwin: Dreaming body and view in first person. arXiv preprint arXiv:2508.13013. 2025. 2. 3 [54] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using vq-vae and transformers. arXiv preprint arXiv:2104.10157, 2021. 3 [55] Yichao Yan, Jingwei Xu, Bingbing Ni, Wendong Zhang, and Xiaokang Yang. Skeleton-aided articulated motion generation. In ACMMM, pages 199207, 2017. 2 [56] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. 2 [57] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1037110381, 2024. 5 [58] Zhendong Yang, Ailing Zeng, Chun Yuan, and Yu Li. Effective whole-body pose estimation with two-stages distillation. In ICCV, pages 42104220, 2023. 6 [59] Guy Yariv, Yuval Kirstain, Amit Zohar, Shelly Sheynin, Yaniv Taigman, Yossi Adi, Sagie Benaim, and Adam Polyak. Through-the-mask: Mask-based motion trajectories for image-to-video generation. In CVPR, pages 18198   
18208, 2025. 2 [60] Shenghai Yuan, Xianyi He, Yufan Deng, Yang Ye, Jinfa Huang, Bin Lin, Jiebo Luo, and Li Yuan. Opens2v-nexus: A detailed benchmark and million-scale dataset for subjectto-video generation. arXiv preprint arXiv:2505.20292, 2025.   
5 [61] Zhixiong Zhang, Shuangrui Ding, Xiaoyi Dong, Songxin He, Jianfan Lin, Junsong Tang, Yuhang Zang, Yuhang Cao, Dahua Lin, and Jiaqi Wang. Sec: Advancing complex video object segmentation via progressive concept construction. arXiv preprint arXiv:2507.15852, 2025. 5, 6 [62] Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li, Jianfeng Wang, Lijuan Wang, Jianfeng Gao, and Yong Jae Lee. Segment everything everywhere all at once. NeurIPS,   
36:1976919782, 2023. 2, 6

# UnityVideo: Unified Multi-Modal Multi-Task Learning for Enhancing World-Aware Video Generation

# Appendix

The appendix contains the following sections: More Analysis of Model Design •More Experiments and Analysis Details of OpenUni and UniBench More Visuals and Applications

# A. More Analysis of Model Design

# A.1. Modal Interaction Analysis

To further investigate the cross-modal interactions within our unified framework, we visualize the evolution of selfattention maps throughout the training process. We partition the attention map into four distinct regions based on modality interactions: self-modality regions comprising (RGB, RGB) and (Flow, Flow), and cross-modality regions consisting of (RGB, Flow) and (Flow, RGB), where Flow represents various auxiliary modality features. As illustrated in Figure 1, our analysis reveals three key findings. First, as joint training progresses, the interaction between RGB and auxiliary modalities becomes progressively more pronounced (A), indicating deepening cross-modal feature exchange. Second, the visualization results demonstrate that the model learns increasingly rich geometric representations with improved text-following capabilities (B), validating the effectiveness of our unified training paradigm in enhancing both visual understanding and conditional generation quality. This empirical evidence confirms that our unified framework not only enables technical integration of multiple modalities but also facilitates meaningful featurelevel interactions that contribute to improved world modeling capabilities.

# A.2. Modality-Specific Output Layers

While our modality switcher and in-context learner effectively differentiate between modalities, we observed occasional modality confusion as the number of modalities scales. For instance, when instructed to generate segmentation masks, the model infrequently produces skeleton outputs instead. This confusion stems from all modalities sharing a common output layer, which can conflate distinct modality-specific features at the final projection stage. To address this limitation, we introduce modalityspecific output layers (adaptive layer) while maintaining a unified input layer (share layer) for cross-modal information sharing. Each modality receives its own dedicated output projection layer, initialized independently, while the input processing remains shared to preserve inter-modal knowledge transfer. This architectural refinement ensures clear modality boundaries during generation without sacrificing the benefits of unified representation learning.

Table 1. Comparison of different layer strategies.   

<table><tr><td></td><td>Subject Consistency</td><td>Background Consistency</td><td>Temporal Flickering</td><td>Motion Smoothness</td><td>Averaged</td></tr><tr><td>Baseline</td><td>96.51</td><td>96.06</td><td>98.73</td><td>99.30</td><td>97.650</td></tr><tr><td>Share Layer</td><td>98.31</td><td>97.54</td><td>99.35</td><td>99.54</td><td>98.685</td></tr><tr><td>Adaptive Layer</td><td>98.26</td><td>97.49</td><td>999.44</td><td>99.61</td><td>98.700</td></tr></table>

Table 2. Comparison with standalone T2V. Joint generation achieves better performance, with unified modality showing further improvements.   

<table><tr><td></td><td>Subject Consistency</td><td>Background Consistency</td><td>Imaging Quality</td><td>Overall Consistency</td><td>Averaged</td></tr><tr><td>Baseline T2V</td><td>96.51</td><td>96.06</td><td>64.99</td><td>23.17</td><td>70.1825</td></tr><tr><td></td><td>96.51</td><td>97.23</td><td>66.52</td><td>23.44</td><td>70.9250</td></tr><tr><td colspan="6">Depth Modality</td></tr><tr><td>JointGen (Depth)</td><td>98.13 (+1.62)</td><td>97.29 (+0.06)</td><td>69.09 (+2.57)</td><td>23.48 (+0.04)</td><td>71.998 (+1.073)</td></tr><tr><td>JointGen (Unified)</td><td>98.01 (+1.50)</td><td>97.24 (+0.01)</td><td>69.18 (+2.66)</td><td>23.75 (+0.31)</td><td>72.045 (+1.120)</td></tr><tr><td colspan="6">Optical Flow Modality</td></tr><tr><td>JointGen (Optical Flow)</td><td>97.82 (+1.31)</td><td>97.14 (-0.09)</td><td>67.34 (+0.82)</td><td>23.70 (+0.26)</td><td>71.500 (+0.575)</td></tr><tr><td>JointGen (Unified)</td><td>97.97 (+1.46)</td><td>97.19 (-0.04)</td><td>69.36 (+2.84)</td><td>23.74 (+0.30)</td><td>72.065 (+1.140)</td></tr><tr><td colspan="6">Densepose Modality</td></tr><tr><td>JointGen (Densepose)</td><td>98.08 (+1.57)</td><td>97.38 (+0.15)</td><td>67.05 (+0.53)</td><td>23.49 (+0.05)</td><td>71.500 (+0.575)</td></tr><tr><td>JointGen (Unified)</td><td>98.03 (+1.52)</td><td>97.30 (+0.07)</td><td>70.20 (+3.68)</td><td>23.53 (+0.09)</td><td>72.265 (+1.340)</td></tr></table>

As shown in Table 1, this lightweight design effectively eliminates modality confusion during scaled training while maintaining comparable performance across metrics. The modality-specific output layers provide improved flexibility and achieve balanced performance across diverse evaluation criteria, validating this architectural choice for scalable multi-modal generation.

# B. More Experiments and Analysis

# B.1. Compare with T2V

While results in main paper demonstrates promising gains from joint generation over the baseline, we further investigate whether joint generation provides advantages over standard supervised fine-tuning (SFT) for text-to-video generation. We conduct extensive ablation studies across different modalities, training models with identical data and steps to ensure fair comparison of their text-to-video capabilities.

![](images/8.jpg)  
improve, reflecting more coherent cross-modal integration.

As shown in Table 2, all modality configurations with joint generation achieve significant improvements over both the baseline and T2V-only training. Each auxiliary modality contributes distinct supervisory signals that enhance the model's visual understanding, confirming the complementary nature of different modalities. Moreover, unified multimodal training outperforms single-modality joint training by achieving better balance across evaluation dimensions, with substantial gains in overall performance (Averaged column). These results validate that diverse modality supervision collectively strengthens video generation through mutual reinforcement rather than simply additive improvements.

# B.2. Scalability with Increasing Modalities

To demonstrate UnityVideo's ability to continuously improve with expanded modality training, we evaluate performance scaling on both joint generation and controllable generation tasks. As shown in Table 3, UnityVideo achieves consistent performance gains across all metrics as the number of modalities increases. Specifically, we compare models trained with three modalities (depth, optical flow, and DensePose) against those trained with five modalities (additionally incorporating skeleton and segmentation). The results reveal monotonic improvements across all evaluation criteria, confirming that our framework effectively leverages additional modality supervision without suffering from negative interference. This strong scalability suggests that UnityVideo's architecture can accommodate further expansion in both model parameters and modality diversity, potentially enabling emergent world perception capabilities as the framework scales. The consistent gains validate our unified training paradigm as a promising foundation for developing increasingly comprehensive video world models through continued modality integration.

Table 3. Analysis of the benefits brought by extended modal training for joint generation and control generation.   

<table><tr><td></td><td>Subject Consistency</td><td>Background Consistency</td><td>Temporal Flickering</td><td>Motion Smoothness</td></tr><tr><td>Baseline</td><td>96.51</td><td>96.06</td><td>98.73</td><td>99.30</td></tr><tr><td colspan="5">Joint Generation</td></tr><tr><td>Depth</td><td>96.53</td><td>95.58</td><td>98.45</td><td>99.28</td></tr><tr><td>Three Modalities</td><td>98.01</td><td>97.24</td><td>99.10</td><td>99.44</td></tr><tr><td>Five Modalities</td><td>98.31</td><td>97.54</td><td>99.35</td><td>99.54</td></tr><tr><td colspan="5">Control Generation</td></tr><tr><td>Depth</td><td>97.78</td><td>96.79</td><td>98.80</td><td>99.30</td></tr><tr><td>Three Modalities</td><td>97.83</td><td>96.86</td><td>98.87</td><td>99.33</td></tr><tr><td>Five Modalities</td><td>97.87</td><td>97.32</td><td>99.57</td><td>99.39</td></tr></table>

# B.3. The influence of different modalities

As shown in main paper, incorporating additional modalities yields further improvements for the JointGeneration task compared with training on a single modality. To examine whether this benefit also extends to ControlGeneration, we conduct the ablation study summarized in Table 4. Here, Only denotes models trained on ControlGeneration using a single modality, while Ours refers to models trained jointly with three modalities. All training data and iteration budgets are kept strictly identical to ensure a fair comparison. The results show that unified multimodal training consistently outperforms single-modality training on the ControlGeneration task. These findings demonstrate that UnityVideo effectively strengthens positive cross-modal interactions across tasks, enabling each modality to benefit from the shared training paradigm."

![](images/9.jpg)  
l compared with current state-of-the-art video generation models.

Table 4. The gain of joint modal training compared with single modal on ControlGeneration tasks.   

<table><tr><td></td><td>Subject Consistency</td><td>Background Consistency</td><td>Temporal Flickering</td><td>Motion Smoothness</td><td>Averaged</td></tr><tr><td>Baseline</td><td>96.51</td><td>96.06</td><td>98.73</td><td>99.30</td><td>97.65</td></tr><tr><td colspan="6">Depth Modality</td></tr><tr><td>ControlGen</td><td>97.78</td><td>96.79</td><td>98.80</td><td>99.30</td><td>98.1675</td></tr><tr><td>(Depth)</td><td>(+1.27)</td><td>(+0.73)</td><td>(+0.07)</td><td>(+0.00)</td><td>(+0.5175)</td></tr><tr><td>Unified</td><td>97.83</td><td>96.86</td><td>98.87</td><td>99.33</td><td>98.2225</td></tr><tr><td>(Depth)</td><td>(+1.32)</td><td>(+0.80)</td><td>(+0.14)</td><td>(+0.03)</td><td>(+0.5725)</td></tr><tr><td colspan="6">Optical Flow Modality</td></tr><tr><td>ControlGen</td><td>97.40</td><td>96.59</td><td>98.67</td><td>99.23</td><td>97.9725</td></tr><tr><td>(Optical Flow)</td><td>(+0.89)</td><td>(+0.53)</td><td>(-0.06)</td><td>(-0.07)</td><td>(+0.3225)</td></tr><tr><td>ControlGen (Unified)</td><td>97.47</td><td>96.72</td><td>98.83 (+0.10)</td><td>99.32</td><td>98.0850</td></tr><tr><td></td><td>(+0.96)</td><td>(+0.66)</td><td></td><td>(+0.02)</td><td>(+0.4350)</td></tr><tr><td colspan="6">Densepose Modality</td></tr><tr><td>ControlGen</td><td>97.01</td><td>96.47</td><td>98.58</td><td>99.10</td><td>97.790</td></tr><tr><td>(Densepose)</td><td>(+0.50)</td><td>(+0.41)</td><td>(-0.15)</td><td>(+0.20)</td><td>(+0.5050)</td></tr><tr><td>ControlGen</td><td>97.58</td><td>96.79</td><td>98.90</td><td>99.35</td><td>98.1550</td></tr><tr><td>(Unified)</td><td>(+1.07)</td><td>(+0.73)</td><td>(+0.17)</td><td>(+0.05)</td><td>(+0.5050)</td></tr></table>

# B.4. World perception comparison

To further assess our model's world understanding capabilities, we conduct comprehensive evaluations using physicsfocused prompts that test fundamental physical principles. As shown in Figure 2, we evaluate models on scenarios involving refraction, collision dynamics, and other physical phenomena that require accurate world modeling. Our results demonstrate that UnityVideo exhibits superior understanding of physical laws compared to baseline methods. The model accurately captures light refraction through transparent media, realistic collision responses between objects, and physically plausible motion trajectories. These improvements stem from the complementary supervision provided by auxiliary modalities—depth enhances spatial reasoning, optical flow captures motion dynamics, and segmentation clarifies object boundaries—collectively enabling more accurate physical world modeling. This enhanced physical reasoning capability further validates the effectiveness of our unified multimodal training paradigm in developing world-aware video generation models.

# C. Details of OpenUni and UniBench

# C.1. OpenUni

The OpenUni dataset leverages diverse data sources and comprehensive modality extraction to create a large-scale multimodal training corpus. We employ multiple pretrained models to extract modality-specific features and implement rigorous filtering pipelines to ensure data quality and usability.

![](images/10.jpg)  

UE Data : Used for evaluating video estimation tasks   
and (ii) diverse real-world videos with rich multimodal annotations for assessing video generation quality. Our data curation process follows strict quality criteria. We first filter source videos based on temporal, aesthetic, and resolution constraints: minimum duration of 5 seconds, aesthetic score exceeding 80/100, and spatial resolution above 512 pixels. Videos containing embedded text or subtitles are removed using OCR-based detection to prevent contamination of visual modalities. For each retained video, we extract corresponding modality annotations using specialized models—depth from Depth Anything V2, optical flow from RAFT, segmentation from SAM, skeleton from DWPose, and DensePose from Meta's implementation. Automated quality metrics further filter low-quality modality extractions, ensuring reliable ground-truth annotations across all modalities. Through this systematic pipeline, we obtain approximately 1.3M high-quality multimodal video pairs, each containing synchronized annotations across five modalities. This comprehensive dataset enables effective unified training while maintaining consistency and quality across diverse visual representations.

# C.2. UniBench

To address the absence of standardized evaluation benchmarks for unified multimodal video tasks, we construct UniBench with two distinct evaluation categories tailored to different task requirements. For video estimation tasks requiring ground-truth annotations, we generate synthetic data using Unreal Engine to obtain pixel-accurate depth maps and optical flow. As shown in Figure 3, for controllable generation and text-to-video tasks requiring diverse modality conditions, we curate high-quality samples from our test split. Specifically, we create 200 synthetic video sequences with precise ground-truth depth and optical flow using Unreal Engine's rendering pipeline. These sequences feature significant camera and object motion to comprehensively evaluate depth estimation capabilities under challenging conditions. For generation tasks, we select 200 high-quality samples from the test subset, each containing complete annotations across all five modalities. This dual-track evaluation strategy enables rigorous assessment of both reconstruction accuracy and generation quality within our unified framework.

# D. More Visuals and Applications

Figure 4 and 5 showcases UnityVideo's extensive generalization capabilities across three core tasks: controllable generation, video estimation, and joint generation. The model accepts arbitrary modality inputs for precise controllable generation while supporting flexible modality estimation for diverse subjects and scenarios. Our framework demonstrates remarkable zero-shot generalization beyond its training distribution. While trained primarily on single-person data, UnityVideo successfully generalizes to multi-person scenarios for all modality estimations. Similarly, skeleton estimation capabilities trained on human subjects transfer effectively to animal motion capture without additional fine-tuning. The model also exhibits robust cross-domain transfer, accurately estimating depth and segmentation for out-of-distribution objects and scenes. These diverse examples collectively demonstrate that UnityVideo's unified training paradigm not only achieves technical integration across modalities but also develops genuine world understanding that enables flexible generalization to novel contexts and subjects.

![](images/11.jpg)  
Figure 4. Representative outputs of UnityVideo on Video Estimation. The model consistently produces coherent RGB videos and aligned modalities—including densepose, optical flow, skeleton, and depth—demonstrating reliable cross-modal generation and estimation across diverse scenarios from human activities to animal motion.

![](images/12.jpg)  
Figure 5. Representative outputs of UnityVideo on Text2Video and Control Generation. The model consistently produces coherent RGB videos and aligned modalities—including segmentation, densepose, optical flow, skeleton, and depth—demonstrating reliable cross-modal generation and estimation across various indoor and outdoor scenes with multiple subjects.