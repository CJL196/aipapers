# Memorize-and-Generate: Towards Long-Term Consistency in Real-Time Video Generation

Tianrui Zhu1\*, Shiyi Zhang $^ { 1 }$ \*, Zhirui Sun1, Jingqi Tian1, and Yansong Tang $^ { . 1 }$ † 1Tsinghua Shenzhen International Graduate School, Tsinghua University {zhutr25, sy-zhang23, sunzr25, tjq25}@mails.tsinghua.edu.cn tang.yansong@sz.tsinghua.edu.cn \*Equal contribution †Corresponding author Abstract. Frame-level autoregressive (frame-AR) models have achieved significant progress, enabling real-time video generation comparable to bidirectional diffusion models and serving as a foundation for interactive world models and game engines. However, current approaches in long video generation typically rely on window attention, which naively discards historical context outside the window, leading to catastrophic forgetting and scene inconsistency; conversely, retaining full history incurs prohibitive memory costs. To address this trade-off, we propose Memorize-and-Generate (MAG), a framework that decouples memory compression and frame generation into distinct tasks. Specifically, we train a memory model to compress historical information into a compact KV cache, and a separate generator model to synthesize subsequent frames utilizing this compressed representation. Furthermore, we introduce MAG-Bench to strictly evaluate historical memory retention. Extensive experiments demonstrate that MAG achieves superior historical scene consistency while maintaining competitive performance on standard video generation benchmarks.

![](images/1.jpg)  

Keywords: Video Generation $\cdot$ Historical scene Consistency $\cdot$ KV cache Compression   
Fig. 1: The inference pipeline. MAG performs real-time streaming video generation at 16 FPS on a single GPU. Compared to baselines, MAG achieves $3 \times$ memory compression. Simultaneously, MAG is capable of generating scenes beyond the current field of view based on memory, ensuring global historical consistency.

# 1 Introduction

Research in the field of video generation is currently undergoing a paradigm shift from diffusion models based on bidirectional attention to autoregressive diffusion generation models operating in the temporal dimension. The former, exemplified by Wan2.1 [43], is capable of generating high-fidelity short video clips from text prompts within a limited time interval. Furthermore, subsequent works have introduced additional control conditions [2, 22, 56], allowing these models to generate diverse videos that better align with user requirements. However, the computational demands of bidirectional attention mean that users often face minutes of generation time to create video clips lasting only a few seconds. Moreover, constrained by fixed temporal windows, these diffusion models remain unable to generate minute-level videos. Consequently, frame-autoregressive models [1, 12, 23, 55] have emerged as the most promising approach for efficient long video generation.

Frame-level autoregressive (frame-AR) video generation models are trained on spatial diffusion and can leverage temporal KV cache for acceleration. Compared to bidirectional attention, this causal attention mechanism effectively halves the computational load. Nevertheless, achieving high-fidelity frames typically requires setting the denoising steps between 20 and 50, which results in persistent high latency and low throughput during long video generation. For instance, MAGI-1 [1] requires several minutes to generate a single frame. To address this, CausVid [52] and Self Forcing [20] explored the use of DMD [50, 51] methods to convert lightweight bidirectional attention models into causal frameAR models, successfully reducing the denoising steps to 4. Self-forcing [20] is capable of generating high-quality short video content in real-time on consumer GPUs, also achieving performance comparable to the original model. Recent work [9,30, 48] has successfully transferred the Self Forcing [20] training methodology directly to long video tasks.

Despite these impressive advancements, existing long video generation methods [9, 30, 35, 48, 55] continue to struggle with the trade-off between memory retention and computational resource consumption. Specifically, ensuring historical scene consistency—such as maintaining scene integrity when a camera returns to an original position after panning away—remains a critical challenge. Most recent works [9, 30, 48] have adopted window attention or rolling window operations, compromising the spatial complexity pressure inherent in long videos. For example, a one-second video clip can occupy nearly 1.5GB, meaning one minute of content is sufficient to fill the current state-of-the-art GPUs. However, reliance on short window attention leads to catastrophic forgetting; previous world models utilizing short windows [13, 14,54,57] often generate completely different scenes when looking left and then returning to the right. Later in this paper, we identify similar issues in recent video generation works, which we attribute to overly coarse memory strategy designs. Context as Memory [53] attempts to combine 3D perspectives with 2D representations to precisely select necessary frames, reducing redundant computation, but still necessitates retaining every historical frame since any frame could potentially be re-selected, leading to rapidly inflating memory consumption. Alternatively, TTT-video [10] attempts to internalize memory into model parameters to avoid KV cache growth. However, optimizing these parameters during the inference sacrifices real-time performance, and the algorithm's complexity makes it difficult to scale up and deploy.

In this work, we analyze the training pipeline of recent approaches and identify a degenerate solution issue within the optimization objective. Consequently, we reformulate the training pipeline of Self Forcing [20] with historical context for long video generation, incorporating a loss function with text-free condition to reinforce the preservation of historical consistency. Furthermore, we introduce a learnable historical memory compression strategy into the frame-AR framework to simultaneously address the challenges of historical consistency and GPU memory consumption. We advocate decoupling memory compression and history-conditioned generation into two distinct tasks, necessitating the training of separate models. The memory model learns to reconstruct original pixel frames from the compressed KV cache, whereas the generator model learns to synthesize the next frame utilizing this compressed cache. Based on these insights, we propose Memorize-and-Generate (MAG), a concise paradigm that not only precisely memorizes and reconstructs historical information but also achieves high compression ratios, significantly reducing memory usage without incurring additional computational overhead. Additionally, to facilitate fair comparison and evaluation of historical consistency across different methods, we curated MAG-Bench, a lightweight benchmark consisting entirely of videos with camera trajectories that leave and subsequently return to the scene, designed to accurately quantify a model's capacity for historical scene retention. Experimental results demonstrate that MAG exhibits superior consistency in video generation tasks while maintaining competitive performance in frame quality and text alignment metrics. On MAG-Bench, it demonstrates significantly better historical scene retention compared to existing methods.

# 2 Related Work

Bidirectional Attention Video Generation. Both early research [6, 15, 16, 46] and recent high-quality short video generation [25, 43, 49] typically employ a bidirectional attention mechanism. This paradigm treats the video as a holistic entity, allowing the model to simultaneously perceive both past and future context during the generation process. Currently, mainstream architectures have evolved from early Spatio-temporal U-Nets [36] to Diffusion Transformer (DiT) [32] designs, enabling efficient scaling of model parameters and leading to the emergence of excellent open-source models [25, 33, 43, 49, 59]. Notably, Wan2.1 [43] represents a state-of-the-art example; its lightweight 1.3B variant remains highly competitive even compared to significantly larger models. However, it is important to note that while bidirectional attention ensures high-quality context, it is inherently incompatible with the growing demand for real-time performance.

Autoregressive Video Generation. To transcend duration limits and enable streaming generation, research focus has gradually shifted toward temporal autoregressive paradigms. Initial autoregressive models [17, 34, 47] directly discretized visual tokens via VQVAE [42] for full-sequence generation but yielded limited video quality. Subsequent works [23, 24, 27] combined temporal autoregression with spatial diffusion, advancing frame-by-frame or chunk-by-chunk. However, these early explorations faced severe exposure bias, as errors accumulated along the temporal dimension, causing frame degradation. Diffusion Forcing [4] proposed adding noise to each frame independently to enhance the model's error-correction capability. Similarly, Rolling Diffusion [37] designed a window with progressively increasing noise for joint denoising and streaming output. Nevertheless, these methods failed to solve the exposure bias problem. Self Forcing [20] addressed this by enforcing strict training-inference consistency and utilizing the DMD distillation framework [50, 51] to produce high-quality content in real-time. Recent adaptations [9, 30, 48] have successfully transferred this method to long video tasks, generating minute-level videos without degradation. This demonstrates that by ensuring the training and inference processes are identical, the model can effectively manage temporal error accumulation autonomously. Memory Representations for Long-Term Consistency. Despite exciting progress in real-time performance and content quality, maintaining historical consistency remains a core challenge in long video generation. Furthermore, there is often an inherent trade-off between historical consistency and limited GPU memory. This issue is particularly critical for world models [13, 14, 54, 57]. Existing works [19,26,39,53] have primarily investigated three memory representation paradigms. Explicit 3D Memory: Representative methods like Memory Forcing [19] and WorldExplorer [39] advocate converting video into 3D point cloud structures. These approaches progressively add new frames to a global 3D map and leverage reprojection as a geometric constraint to guide generation. Such explicit memory possesses a natural advantage in addressing spatial drift and historical consistency. However, their performance is heavily contingent on backend reconstruction algorithms [7, 44]. Failures in texture-poor or highly dynamic scenes can result in reconstruction errors that lead to persistent generation artifacts. Implicit 2D Latent Memory: Another line of work explores establishing memory within a 2D latent space. Genie 3 [11, 28] demonstrates that excellent historical consistency can be achieved using only 2D representations, though the technical details remain undisclosed. Context as Memory [53] designs a memory extraction strategy where the generation of the next frame relies solely on historical frames with significant viewpoint overlap. While this approach effectively combines 3D perspectives with 2D representations, the requirement to retain every historical frame continues to impose substantial hardware challenges. Weight Memory: TTT [41] proposes updating model weights during inference, effectively compressing information into the parameters. Theoretically, this method achieves a fixed $O ( 1 )$ state size and infinite context memory, enabling precise reproduction of long-range details. TTT-video [10] implements this approach, successfully generating animated shorts with preserved character consistency. However, performing optimization during the inference process introduces large computational burdens, thereby sacrificing real-time capabilities.

# 3 Method

# 3.1 Preliminary

Recent successful works [9,30, 48] transfer the Self Forcing [20] to long video generation. The core method involves DMD distillation [50,51], which minimizes the KL divergence between the generator's output distribution $p _ { \theta } ^ { G } ( x )$ and the teacher model's output distribution $p ^ { \prime } \left( x \right)$ , utilizing short clips uniformly sampled from long videos for gradient backpropagation. For the sake of brevity and clarity, we omit the randomly sampled timestep $t$ in the following sections, removing a default expectation term to reduce formula length and complexity. The gradient of the optimization objective can be approximated as the difference between two score functions:

$$
\begin{array} { r l } & { \nabla _ { \boldsymbol { \theta } } \mathcal { L } _ { \mathrm { D M D } } = \mathbb { E } _ { \boldsymbol { x } } \left[ \nabla _ { \boldsymbol { \theta } } \mathrm { K L } \left( p _ { \boldsymbol { \theta } } ^ { \mathcal { S } } ( \boldsymbol { x } ) \ \lVert p ^ { \mathcal { T } } ( \boldsymbol { x } ) \right) \right] } \\ & { \qquad \approx \mathbb { E } _ { i \sim U \{ 1 , k \} } \mathbb { E } _ { z \sim \mathcal { N } ( 0 , I ) } \left[ s ^ { \mathcal { T } } ( \boldsymbol { x } _ { i } ) - s _ { \boldsymbol { \theta } } ^ { \mathcal { S } } ( \boldsymbol { x } _ { i } ) \frac { d G _ { \boldsymbol { \theta } } ( \boldsymbol { z } _ { i } ) } { d \boldsymbol { \theta } } \right] , } \end{array}
$$

where $i \sim U \{ 1 , k \}$ , and $x _ { i }$ and $z _ { i }$ represent a clip uniformly sampled from $k$ segments used to calculate the DMD loss. Here, $z$ denotes Gaussian noise, $G _ { \theta } ( z )$ represents the output of the generator parameterized by $\theta$ , $\tau$ and $\boldsymbol { S }$ denote the teacher and student models respectively, and $s ( x _ { i } )$ is score function from teacher or student. During training, the student model utilizes Flow Matching [29, 31] to learn the generator's output; consequently, the student model distribution $p ^ { s }$ can be viewed as a proxy distribution for the generator.

# 3.2 Rethinking DMD Optimization in Long Video Generation

When $i > 1$ or the video extends to a longer duration, the generator's output distribution can be denoted as $p _ { \theta } ^ { G } ( x | h , T )$ , where $h$ represents the history frames and $T$ represents the text condition. The model is required to generate new clips that are consistent with both the historical context and the text condition. However, the modeling approach described in Sec. 3.1, which only relies on randomly sampled clips, neglects the critical role of historical information. In practice, recent works utilize the original T2V teacher model as a substitute for a more powerful teacher model capable of supporting history condition inputs. Consequently, the existing optimization objective is formulated as:

$$
p _ { \theta } ^ { G } ( x | h , T )  p ^ { T } ( x | T ) \approx p ^ { T } ( x | h , T )
$$

As the original teacher model lacks the ability to provide supervision signals for historical consistency, this implies the existence of a shortcut or degenerate solution: $p _ { \theta } ^ { G } ( x | h , T )  p _ { \theta } ^ { G } ( x | T )$ . First, text and historical context exhibit a high correlation, relying on text is sufficient to generate video of enough quality. Second, as the base model is a bidirectional T2V model, it inherently tends to converge towards relying on the text first when adapted to AR model, thereby neglecting the utilization of historical information. To address this degenerate solution, we introduce a simple modification: when $i > 1$ , the generator predicts $x$ using an empty text condition. The form of the loss function remains unchanged, but the sampling source of $x$ is altered. The new loss function is defined as:

$$
\begin{array} { r l } & { \nabla _ { \boldsymbol { \theta } } \mathcal { L } _ { \mathrm { h i s t o r y } } = \mathbb { E } _ { \boldsymbol { x } \sim p _ { \boldsymbol { \theta } } ^ { G } ( \boldsymbol { x } | h , \boldsymbol { \vartheta } ) } [ \nabla _ { \boldsymbol { \theta } } D _ { K L } ( p _ { \boldsymbol { \theta } } ^ { S } ( \boldsymbol { x } ) \| p ^ { \mathcal { T } } ( \boldsymbol { x } ) ) ] } \\ & { \qquad \nabla _ { \boldsymbol { \theta } } \mathcal { L } = ( 1 - \lambda ) \nabla _ { \boldsymbol { \theta } } \mathcal { L } _ { \mathrm { D M D } } + \lambda \nabla _ { \boldsymbol { \theta } } \mathcal { L } _ { \mathrm { h i s t o r y } } } \end{array}
$$

Here, $\lambda$ is a hyperparameter balancing the two losses, specifically implemented via random sampling. During training, the model is forced to learn to generate next frame based solely on historical clips. This is a more challenging task, but it facilitates the learning and modeling of physical consistency and world knowledge. In contrast, the method in Sec. 3.1 tends to over-align with text information, ignoring the intrinsic contextual correlations within the autoregressive process.

![](images/2.jpg)  
Fig. 2: The training pipeline. The training process of MAG comprises two stages. In the first stage, we train the memory model for the triple compressed KV cache, retaining only one frame within a full attention block. The loss function requires the model to reconstruct the pixels of all frames in the block from the compressed cache. The process utilizes a customized attention mask to achieve efficient parallel training. In the second stage, we train the generator model within the long video DMD training framework to adapt to the compressed cache provided by the frozen memory model.

# 3.3 Memorize-and-Generate Framework

Causal autoregressive models [9, 20, 48] first convert noise into video frames through a few denoising steps. Subsequently, these video frames are fed back into the model, where the KV cache of all tokens is retained block-by-block; this cache serves as the model's memory and historical condition. However, video sequences contain a massive number of tokens. Due to hardware constraints, it is impossible to retain all historical information. As mentioned in Sec. 2, most works apply simple window attention [9, 48], where the model only retains frames from the most recent 2-3 seconds, making it impossible to achieve historical consistency. To achieve this goal, we argue that all historical frame information should be preserved, as any fine-grained detail could potentially be reused. Therefore, to reduce the memory overhead associated with this goal, we propose decoupling the memory process of generating the final step's KV cache from the pipeline and implementing compression at the cache level. Next, we follow the methods of Self Forcing [20] and Sec. 3.2 to train the generator model. The frozen memory model executes the generation of the final step's KV cache, while the generator model maintains its original training objective. This scheme allows us to perform both near-lossless compression on long video content and real-time performance.

# 3.4 Memory Model Design and Training

We believe the design of the memory model should adhere to two principles: evaluable fidelity and no increase in inference latency. Ideally, it should exist directly in the form of a KV cache. First, memory fidelity is the foundation of the final generation results; however, high-quality generation does not necessarily imply perfect memory preservation. For instance, methods based on sliding windows [48] or RNN hidden states [8] are generally considered to suffer from information bottlenecks. However, these methods cannot quantify the extent of memory loss or identify the specific steps where the bottleneck occurs, thereby hindering our research and analysis of the relationship between memory and generation results. Second, dynamic token compression during generation [55] often requires regenerating the KV cache or extra computation, sacrificing real-time performance and design simplicity. We maintain that during streaming generation or input, once memory compression is completed, the historical cache or token sequence should not be frequently altered. Drawing upon the aforementioned principles and inspired by Autoencoders (AE) [38], we conceptualize the KV cache as compressed latent features and construct an encode-decode framework upon this foundation. In the streaming video generation process, the minimal output unit typically consists of several frames forming a block, within which full attention is applied. We treat the intra-block full attention computation as the encoder, retaining only a subset of the KV cache, such as the final frame. Subsequently, we task the model with denoising random noise to reconstruct all frames within the block based on this retained KV cache, functioning as the decoder. Since the dimensions of the KV cache represent a significant expansion relative to the input tokens, there is substantial compression at the cache level. This feasibility is illustrated on the left side of Fig. 2.

It is important to note that the encoder and decoder are implemented as a single model, the resulting memory model. Parameter sharing is enabled because the model can simultaneously produce the KV cache and the reconstruction results; these dual output branches naturally constitute an encode-decode workflow. We train with from few-step Flow Matching [29, 31], which significantly enhances training efficiency. Driven by the gradients from the loss, the model learns during the encoding phase to compress the block's information into a single frame via full at tention, while the decoding phase reconstructs the original pixels using this compressed information. Consequently, we can assess memory fidelity by simply observing the reconstructed video. Furthermore, to strictly ensure traininginference consistency, we randomize the start index of the Rotary Positional Embeddings [40]. This encourages the model to learn that the compression task is independent of the video's temporal duration, allowing a memory model trained on short clips to be directly applied to long videos. Subsequent experiments demonstrate that this method achieves near-lossless compression. Moreover, the trained memory model can seamlessly replace the KV cache generation step in the final pipeline without introducing any additional computational overhead.

![](images/3.jpg)  
Fig. 3: The attention mask of memory model training. We achieve efficient parallel training of the encode-decode process by concatenating noise and clean frame sequences. By masking out the KV cache of other frames within the block, the model is forced to compress information into the target cache.

# 3.5 Streaming Long Video Generation Training

The training of the generator model in MAG primarily follows the protocols established in LongLive [48] and Self-forcing++ [9]. However, distinct from these works, we employ the modeling approach detailed in Sec. 3.2. In each training step, the generator produces a 5-second short video clip, which may be conditioned on an empty text prompt. Subsequently, we calculate the DMD loss based on this clip to obtain the supervision signal. When the generator operates with an empty text condition, it is compelled to generate correct content based solely on historical information to align with the signal produced by the teacher model, thereby reinforcing the constraint of the historical condition. We then proceed with several identical generation steps and the resulting short video clips are utilized to update the student model, ensuring it represents the current output distribution of the generator. In practice, by calibrating the ratio of generator training steps to student model training steps, as well as the number of short clips within the long video, we ensure that during the rolling process, the generator's training provides uniform supervision across every time step. This guarantees the model's robustness over long-duration generation.

![](images/4.jpg)  
Fig. 4: Examples from MAG-Bench. MAG-Bench is a lightweight benchmark comprising 176 videos featuring indoor, outdoor, object, and video game scenes. The benchmark also provides appropriate switch times to guide the model toward correct continuation using a few frames.

# 3.6 MAG-Bench: Historical Consistency Evaluation

Historical consistency is particularly critical for future video generation tasks involving complex camera movements or even scene cuts. However, existing benchmarks primarily focus on image quality and text alignment, lacking dedicated data with back-and-forth camera movements to evaluate the model's ability to retain objects that have exited the frame. To bridge this gap, we collected a lightweight dataset. Furthermore, we discovered that simply inputting historical frames with active camera movement into the KV cache effectively guides the model to continue generating scenes that have moved out of frame, thereby facilitating evaluation. Since this process adheres to a "memorize-and then-generate" workflow, we term it MAG-Bench. To ensure that the camera movement videos in MAG-Bench are strictly symmetrical to calculate the reconstruction loss, we first collected high-quality videos with singular camera movements. We then synthesized high-quality "scene backtracking" videos through reverse playback. A schematic of the videos within the benchmark, along with the partition between memory and generation segments, is illustrated in Fig. 4.

# 4 Experiments

# 4.1 Implementation Details

Training. Following concurrent work, we utilize Wan2.1-T2V-1.3B [43] as our base model to ensure fair comparison; this model generates 5-second clips at 16

FPS with a resolution of 832 $\times$ 480. First, adhering to the Self Forcing [20] training pipeline, we train the ODE-initialized model for 300 steps. This reproduces the capabilities of Self Forcing, [20] enabling the generation of 5-second short videos in few-step conditions. Subsequently, we proceed to train the memory model and generator model according to the scheme outlined in Sec. 3.2. The memory model is initialized from the aforementioned 300-step Self Forcing model and trained for 2,000 steps on VPData [3], which contains 390K high-quality realworld videos, using empty text as the condition. The generator model is then initialized from the trained memory model. This strategy ensures that the feature space of the cache is shared during the early stages of training, thereby stabilizing the subsequent process. Following concurrent work, the generator model is trained for approximately 1,400 steps using text prompts sampled from VidProM [45], which have been extended by LLM. For all training phases, whether the supervised training of the memory model or the DMD training of the generator model, we set the batch size to 64, the generator learning rate to $2 . 0 \times 1 0 ^ { - 6 }$ , and the student model learning rate to $4 . 0 \times 1 0 ^ { - 7 }$ , consistent with or similar to concurrent studies [20, 48]. Furthermore, during generator training, we set the probability of using empty text conditioning to 0.6. Each text prompt generates a long video consisting of 7 clips. To ensure the generator receives uniform supervision across the temporal dimension during the rolling process, we adopt a strategy where the student model is trained on 5 clips for every 1 clip used to train the generator model.

![](images/5.jpg)  
Fig. 5: Visualization of Memory Model reconstruction results. We display two examples featuring texture detail variations and significant camera movement. Visually, the trained Memory Model achieves near-lossless reconstruction of the original pixels under a $3 \times$ compression setting.

Evaluation. For the memory model, we utilize the standard training and test splits of VPData [3] and report PSNR, SSIM, LPIPS, and MSE on the test set to evaluate the effectiveness of compression and reconstruction. For the generator model, we employ VBench [21] and VBench-Long [58], to assess performance in text-to-video tasks, covering both quality and text alignment. It is important to note that, to ensure fair comparison with recent work, we follow Self Forcing [20] by using extended prompts for both the 5-second tests and the 30-second tests. For MAG-Bench, we refer [53] and report the PSNR, SSIM, and LPIPS of the predicted video compared to the ground truth to quantify memory capability on pixel level. During experiments, we observed that slight discrepancies in camera movement speed between the predicted video and ground truth could lead to significant pixel-level errors, even if the scene consistency was perceptually accurate. Therefore, we report metrics based on Best Match LPIPS: we first match each predicted frame to the most similar ground truth frame based on the perceptual loss of the pretrained model, and then calculate the remaining metrics. All generation results from distilled models are sampled using identical initial noise and random seeds, utilizing the parameter settings provided in the corresponding papers and codebases. FPS tests on a H100 GPU.

![](images/6.jpg)  
Fig. 6: Qualitative comparison on T2V tasks. We present 5-second and 30-second video clips sampled from VBench [21] and VBench-Long [58], respectively. All methods utilize identical prompts and random initialization noise.

# 4.2 Text-to-Video Generation Comparison

We selected representative recent models as baselines. Wan2.1 [43] serves as our base model and stands as an excellent example of open-source bidirectional attention models for short video generation. SkyReels-V2 [5], a diffusion forcing model of the same size, represents non-distilled autoregressive generation. Self Forcing [20] and LongLive [48] represent significant breakthroughs in recent distillation work; Self-forcing uses full history as a condition to generate 5-second clips, while LongLive [48] employs a 6-frame sliding window attention to generate minute-level videos. Note that Wan2.1 [43] and SkyReels-V2 [5] require dozens of denoising steps to achieve high-quality video, which naturally results in higher image quality scores. We primarily compare our approach against distilled works with consistent experimental settings to ensure fair evaluation. As shown in Tab. 1 and Fig. 6, our model achieves a highly competitive score of 83.52 on the short video generation task. Furthermore, our method outperforms existing approaches in both background and object consistency, attributed to our maintenance of cache fidelity and a modeling approach that explicitly prioritizes historical consistency. Similar results are observed in long video generation tasks. Moreover, our model achieves a real-time inference speed of 21.7 FPS, which is the fastest among the compared methods. This speed advantage stems from our denser historical information compression, which reduces the sequence length required for attention. Although LongLive [48] utilizes a smaller window, its use of LoRA [18] adapters and the sliding window shift operations introduces computational overhead; consequently, our method maintains a slightly faster average speed than other methods.

Table 1: Quantitative comparison with relevant baselines on the 5-second VBench [21]. We compare against recent and representative open-source methods with similar parameter sizes and distillation processes. Evaluations are conducted using extended VBench prompts. "_" denotes that the data is cited from the reference but this metric was not disclosed. FPS is measured on a single H100 GPU.   

<table><tr><td rowspan="2">Model</td><td rowspan="2">Throughput FPS↑</td><td colspan="5">Vbench scores on 5s ↑</td></tr><tr><td>Total</td><td>Quality</td><td>Semantic</td><td>Background</td><td>Subject</td></tr><tr><td>Multi-step model</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>SkyReels-V2 [5]</td><td>0.49</td><td>82.67</td><td>84.70</td><td>74.53</td><td>-</td><td>-</td></tr><tr><td>Wan2.1 [43]</td><td>0.78</td><td>84.26</td><td>85.30</td><td>80.09</td><td>97.29</td><td>96.34</td></tr><tr><td colspan="7">Few-step distillation model</td></tr><tr><td>CausVid [52]</td><td>17.0</td><td>82.46</td><td>83.61</td><td>77.84</td><td>-</td><td>-</td></tr><tr><td>Self Forcing [20]</td><td>17.0</td><td>83.98</td><td>84.75</td><td>80.86</td><td>96.21</td><td>96.80</td></tr><tr><td>Self Forcing++ [9]</td><td>17.0</td><td>83.11</td><td>83.79</td><td>80.37</td><td>-</td><td>-</td></tr><tr><td>Longlive [48]</td><td>20.7</td><td>83.32</td><td>83.99</td><td>80.68</td><td>96.41</td><td>96.54</td></tr><tr><td>MAG</td><td>21.7</td><td>83.52</td><td>84.11</td><td>81.14</td><td>97.44</td><td>97.02</td></tr></table>

Table 2: Quantitative comparison with relevant baselines on the 30-second VBench-long [58]. We compare against recent long video generation methods based on Self Forcing. Evaluations are conducted using extended VBench prompts.   

<table><tr><td rowspan="2">Model</td><td colspan="5">Vbench scores on 30s ↑</td></tr><tr><td>Total</td><td>Quality</td><td>Semantic</td><td>Background</td><td>Subject</td></tr><tr><td rowspan="2">Self Forcing [20] Longlive [48]</td><td>82.57</td><td>83.30</td><td>79.68</td><td>97.03</td><td>97.80</td></tr><tr><td>82.69</td><td>83.28</td><td>80.32</td><td>97.21</td><td>98.36</td></tr><tr><td>MAG</td><td>82.85</td><td>83.30</td><td>81.04</td><td>97.99</td><td>99.18</td></tr></table>

# 4.3 Historical Consistency Comparison

We selected recent distillation methods for a comparative analysis of historical consistency. As illustrated in Tab. 3, our method significantly outperforms existing approaches across quantitative metrics. This is primarily because our method retains all historical information, whereas sliding window operations typically preserve only the most recent 2-3 seconds of history. Furthermore, while Self Forcing [20] and CausaVid [52] also retain the full historical frame cache, they do not enforce the model to learn the utilization of historical information during training. Fig. 7 demonstrates a scenario where the camera leaves a scene and subsequently returns via camera movement guidance. Only our method maintains the best scene consistency, while other methods exhibit forgetting and hallucinations in various regions.

![](images/7.jpg)  
Fig. 7: Qualitative comparison on MAG-Bench. We primarily display the visual results of comparable distilled models. Prior to these frames, the models receive and memorize historical frames. Red boxes highlight instances of scene forgetting and hallucinations exhibited by other methods.

Table 3: Quantitative experiments on our MAG-Bench. "Ground Truth" and "History Context" denote the model continuing generation based on ground truth frames and its own predicted frames, respectively, with the latter representing a significantly more challenging task.

<table><tr><td rowspan="2">Method</td><td colspan="3">History Context Comparison</td><td colspan="3">Ground Truth Comparison</td></tr><tr><td>PSNR↑</td><td>SSIM↑</td><td>LPIPS↓</td><td>PSNR↑</td><td>SSIM↑</td><td>LPIPS↓</td></tr><tr><td rowspan="3">Self Forcing [20] CausVid [52] Longlive [48]</td><td>14.46</td><td>0.48</td><td>0.49</td><td>15.65</td><td>0.51</td><td>0.42</td></tr><tr><td>15.13</td><td>0.50</td><td>0.41</td><td>17.21</td><td>0.56</td><td>0.31</td></tr><tr><td>16.42</td><td>0.53</td><td>0.32</td><td>18.92</td><td>0.62</td><td>0.22</td></tr><tr><td rowspan="2">w/o stage 1 MAG</td><td>17.19</td><td>0.54</td><td>0.31</td><td>19.04</td><td>0.60</td><td>0.22</td></tr><tr><td>18.99</td><td>0.60</td><td>0.23</td><td>20.77</td><td>0.66</td><td>0.17</td></tr></table>

# 4.4 Ablation Studies

Memory Model Compression Rate. We adopt a strategy of retaining only the cache of the final frame for subsequent use after performing full attention computation on a block of frames. In our setting, the compression rate is equivalent to the number of frames within a block, which acts as a hyperparameter affecting throughput in other works. Tab. 4 indicates that reconstruction quality decreases slightly as the compression rate increases. Fig. 5 shows that when the compression rate is set to 3, the model reconstructs original pixels with visually negligible impact. Since a block size of three frames is a widely accepted parameter balancing throughput and latency [9, 20, 48], we selected a compression rate of 3. However, the results in Tab. 4 suggest that higher compression rates remain a viable avenue for future exploration.

Table 4: Ablation study on compression rates. We vary the compression rates by adjusting the number of frames contained within a block. "Block=1" indicates no compression.   

<table><tr><td>Rates</td><td>PSNR↑</td><td>SSIM↑</td><td>LPIPS↓</td><td>MSE×102 ↓</td></tr><tr><td>block=1</td><td>34.81</td><td>0.93</td><td>0.025</td><td>0.08</td></tr><tr><td>block=3</td><td>31.73</td><td>0.90</td><td>0.045</td><td>0.56</td></tr><tr><td>block=4</td><td>29.89</td><td>0.88</td><td>0.059</td><td>1.28</td></tr><tr><td>block=5</td><td>28.64</td><td>0.86</td><td>0.071</td><td>1.96</td></tr></table>

Memory Model Ablation Study. To demonstrate the necessity of memory compression, we trained a baseline method that employs direct 3 $\times$ downsampling. As shown in Tab. 3, omitting the first stage of memory compression training results in poorer consistency. Simple downsampling discards a substantial amount of detail and information, leading to potential forgetting during scene reconstruction. Therefore, ensuring cache fidelity through the first stage of training proves to be an effective strategy.

# 5 Discussion

Conclusion. In this work, we propose MAG, a framework for long video generation comprising two models dedicated to memory compression and next-frame generation, respectively. To evaluate the historical scene consistency of existing methods, we construct MAG-Bench, a lightweight benchmark. Experimental results demonstrate that the memory model can reconstruct original pixels under 3 $\times$ compression. Furthermore, the generator model synthesizes high-quality content in real-time with superior background and subject consistency. Besides, it significantly outperforms existing methods in maintaining historical consistency. Limitations and Future Work. During our experiments, we identified two primary limitations. (1) While this work ensures the fidelity of the compressed KV cache, the lack of data explicitly targeting context consistency makes it difficult for the generator model to learn how to optimally select and utilize extensive historical frames. (2) Although the DMD distillation framework is data-free, this characteristic hinders its direct extension to action-based world models. Substantial resources are still required to train a capable teacher model. In future work, we aim to address these challenges to enhance the feasibility of realizing world models.

# References 1. ai, S., Teng, H., Jia, H., Sun, L., Li, L., Li, M., Tang, M., Han, S., Zhang, T., Zh, W.Q., Luo, W., Kag, X., Sun, Y., Cao, Y., Hu, Y., Lin, Y., Fa, Y., T Z. Zha, Z. W, Z. Liu Z. Shi D. u, G. Sun, H. Pan, H. W J., S J., Cui M. Hu, M. Yan . Yin, S. Z, S., Liu T. Yin, X., , X., Song, X., Hu, X., Zhang, Y., Li, Y.: Magi-1: Autoregressive video generation at scale (2025)2   
Bai, J., Xia, . Fu, X., Wag, X., Mu, L. Cao, J., Liu, Z., Hu, H., Bai, X. Wan, P., et al.: Recammaster: Camera-controlled generative rendering from a single video. arXiv preprint arXiv:2503.11647 (2025) 2   

3. Bian, Y., Zhang, Z., Ju, X., Cao, M., Xie, L., Shan, Y., Xu, Q.: Videopainter: Any-length video inpainting and editing with plug-and-play context control. In: SIGGRAPH. pp. 112 (2025) 10 4. Chen, B., Martí Monsó, D., Du, Y., Simchowitz, M., Tedrake, R., Sitzmann, V.: Diffusion forcing: Next-token prediction meets full-sequence diffusion. In: NeurIPS. pp. 2408124125 (2024) 4 5. Chen, G., Lin, D., Yang, J., Lin, C., Zhu, J., Fan, M., Zhang, H., Chen, S., Chen, Z., Ma, C., Xiong, W., Wang, W., Pang, N., Kang, K., Xu, Z., Jin, Y., Liang, Y., Song, Y., Zhao, P., Xu, B., Qiu, D., Li, D., Fei, Z., Li, Y., Zhou, Y.: Skyreels-v2: Infinite-length film generative model (2025) 11, 12 6. Chen, H., Xia, M., He, Y., Zhang, Y., Cun, X., Yang, S., Xing, J., Liu, Y., Chen, Q., Wang, X., Weng, C., Shan, Y.: Videocrafter1: Open diffusion models for highquality video generation (2023) 3   
7.Chen, S., Guo, H., Zhu, S., Zhang, F., Huang, Z., Feng, J., Kang, B.: Video depth anything: Consistent depth estimation for super-long videos. arXiv:2501.12375 (2025) 4 8. Chen, T., Ding, Z., Li, A., Zhang, C., Xiao, Z., Wang, Y., Jin, C.: Recurrent autoregressive diffusion: Global memory meets local attention. arXiv preprint arXiv:2511.12940 (2025) 7 9. Cui, J., Wu, J., Li, M., Yang, T., Li, X., Wang, R., Bai, A., Ban, Y., Hsieh, C.J.: Self-forcing++: Towards minute-scale high-quality video generation. arXiv preprint arXiv:2510.02283 (2025) 2, 4, 5, 7, 8, 12, 14 10. Dalal, K., Koceja, D., Xu, J., Zhao, Y., Han, S., Cheung, K.C., Kautz, J., Choi, Y., Sun, Y., Wang, X.: One-minute video generation with test-time training. In: CVPR. pp. 1770217711 (2025) 3, 5   
11.Deepmind, G.: Genie 3. https://deepmind.google/blog/genie-3-a-new-frontier-forworld-models/ (2025) 4 12. Gu, Y., Mao, w., Shou, M.Z.: Long-context autoregressive video modeling with next-frame prediction. arXiv preprint arXiv:2503.19325 (2025) 2 13. Guo, J., Ye, Y., He, T., Wu, H., Jiang, Y., Pearce, T., Bian, J.: Mineworld: a real-time and open-source interactive world model on minecraft. arXiv preprint arXiv:2504.08388 (2025) 2, 4 14. He, X., Peng, C., Liu, Z., Wang, B., Zhang, Y., Cui, Q., Kang, F., Jiang, B., An, M., Ren, Y., Xu, B., Guo, H.X., Gong, K., Wu, C., Li, W., Song, X., Liu, Y., Li, E., Zhou, Y.: Matrix-game 2.0: An open-source, real-time, and streaming interactive world model. arXiv preprint arXiv:2508.13009 (2025) 2, 4 15. He, Y., Yang, T., Zhang, Y., Shan, Y., Chen, Q.: Latent video diffusion models for high-fidelity long video generation (2022) 3 ing for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868 (2022) 4 18. Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W., et al.: Lora: Low-rank adaptation of large language models. In: ICLR. p. 3 (2022) 12 19. Huang, J., Hu, X., Han, B., Shi, S., Tian, Z., He, T., Jiang, L.: Memory forcing: Spatio-temporal memory for consistent scene generation on minecraft. arXiv preprint arXiv:2510.03198 (2025) 4 20. Huang, X., Li, Z., He, G., Zhou, M., Shechtman, E.: Self forcing: Bridging the train-test gap in autoregressive video diffusion. arXiv preprint arXiv:2506.08009 (2025) 2, 3, 4, 5, 7, 10, 11, 12, 13, 14   
Hu, Z., He, Y., Yu, J. Zhag, F. Si, C. Jiang, Y. Zhag, Y., Wu, T. Ji, Q, Chanpaisit, N., Wang, Y., Chen, X., Wang, L., Lin, D., Qiao, Y., Liu, Z.: VBench: Comprehensive benchmark suite for video generative models. In: CVPR (2024) 10, 11, 12 22. Jiang, Z., Han, Z., Mao, C., Zhang, J., Pan, Y., Liu, Y.: Vace: All-in-one video creation and editing. In: ICCV. pp. 1719117202 (2025) 2   
3.Jin, Y., Sun, Z., Li, N. Xu, K. Xu, K., Jiang, H., Zhuang, N., Huang, Q., Song, Y., Mu, Y., Lin, Z.: Pyramidal flow matching for efficient video generative modeling (2024) 2, 4 24. Kim, J., Kang, J., Choi, J., Han, B.: Fifo-diffusion: Generating infinite videos from text without training. In: NeurIPS. pp. 8983489868 (2024) 4   
2Kong, W., Tian, Q., Zhang, Z., Min, R. Dai, Z., Zhou, J., Xiong, J., Li, X., Wu, B., Zhang, J., et al.: Hunyuanvideo: A systematic framework for large video generative models. arXiv preprint arXiv:2412.03603 (2024) 3 26. Li, R., Torr, P., Vedaldi, A., Jakab, T.: Vmem: Consistent interactive video scene generation with surfel-indexed view memory. arXiv preprint arXiv:2506.18903 (2025) 4 27. Li, Z., Hu, S., Liu, S., Zhou, L., Choi, J., Meng, L., Guo, X., Li, J., Ling, H., Wei, F.: Arlon: Boosting diffusion transformers with autoregressive models for long video generation. arXiv preprint arXiv:2410.20502 (2024) 4 28. Lin, Y., Lee, M., Zhang, Z., AlQuraishi, M.: Out of many, one: Designing and scaffolding proteins at the scale of the structural universe with genie 2. arXiv preprint arXiv:2405.15489 (2024) 4 29. Lipman, Y., Chen, R.T., Ben-Hamu, H., Nickel, M., Le, M.: Flow matching for generative modeling. arXiv preprint arXiv:2210.02747 (2022) 5, 8 30. Liu, K., Hu, W., Xu, J., Shan, Y., Lu, S.: Rolling forcing: Autoregressive long video diffusion in real time. arXiv preprint arXiv:2509.25161 (2025) 2, 4, 5 31. Liu, X., Gong, C., Liu, Q.: Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003 (2022) 5, 8 32. Peebles, W., Xie, S.: Scalable diffusion models with transformers. In: ICCV. pp. 41954205 (2023)3 33. Peng, X., Zheng, Z., Shen, C., Young, T., Guo, X., Wang, B., Xu, H., Liu, H., Jiang, M., Li, W., Wang, Y., Ye, A., Ren, G., Ma, Q., Liang, W., Lian, X., Wu, X., Zhong, Y., Li, Z., Gong, C., Lei, G., Cheng, L., Zhang, L., Li, M., Zhang, R., Hu, S., Huang, S., Wang, X., Zhao, Y., Wang, Y., Wei, Z., You, Y.: Open-sora 2.0: Training a commercial-level video generation model in \$200k. arXiv preprint arXiv:2503.09642 (2025) 3 35. Ren, X., Xu, L., Xia, L., Wang, S., Yin, D., Huang, C.: Videorag: Retrieval-augmented generation with extreme long-context videos. arXiv preprint arXiv:2502.01549 (2025) 2 36. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedical image segmentation. In: MICCAI. pp. 234241. Springer (2015) 3 37. Ruhe, D., Heek, J., Salimans, T., Hoogeboom, E.: Rolling diffusion models. In: ICML (2024) 4 38. Rumelhart, D.E., Hinton, G.E., Williams, R.J.: Learning internal representations by error propagation. Tech. rep. (1985) 7 39. Schneider, M.A., Höllein, L., Niener, M.: Worldexplorer: Towards generating fully navigable 3d scenes. In: SIGGRAPH Asia. pp. 111 (2025) 4 40. Su, J., Lu, Y., Pan, S., Wen, B., Liu, Y.: Roformer: Enhanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864 (2021) 8 41. Sun, Y., Wang, X., Liu, Z. Miller, J., Efros, A., Hardt, M.: Test-time training with self-supervision for generalization under distribution shifts. In: ICML. pp. 92299248 (2020) 4 42. Van Den Oord, A., Vinyals, O., et al.: Neural discrete representation learning. In: NeurIPS (2017) 4   

43. Wan, T., Wang, A., Ai, B., Wen, B., Mao, C., Xie, C.W., Chen, D., Yu, F., Zhao, H., Yang, J., Zeng, J., Wang, J., Zhang, J., Zhou, J., Wang, J., Chen, J., Zhu, K., Zo, K., Yan, K. Hg, L. Feg M. Zag, N., Li, P. Wu, P. Chu, R.Fg R., Zhang, S., Sun, S., Fang, T., Wang, T., Gui, T., Weng, T., Shen, T., Lin, W., Wang, W., Wang, W., Zhou, W., Wang, W., Shen, W., Yu, W., Shi, X., Huang, X., Xu, X., Kou, Y., Lv, Y., Li, Y., Liu, Y., Wang, Y., Zhang, Y., Huang, Y., Li, Y., Wu, Y., Liu, Y., Pan, Y., Zheng, Y., Hong, Y., Shi, Y., Feng, Y., Jiang, Z., Han, Z., Wu, Z.F., Liu, Z.: Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314 (2025) 2, 3, 9, 11, 12 44. Wang, J., Chen, M., Karaev, N., Vedaldi, A., Rupprecht, C., Novotn, D.: VGGT: visual geometry grounded transformer. In: CVPR (2025) 4 45. Wang, W., Yang, Y.: Vidprom: A million-scale real prompt-gallery dataset for text-to-video diffusion models. In: NeurIPS. pp. 6561865642 (2024) 10 46. Xing, J., Xia, M., Zhang, Y., Chen, H., Wang, X., Wong, T.T., Shan, Y.: Dynamicrafter: Animating open-domain images with video diffusion priors (2023) 3   
7Yan, W., Zhang, Y., Abbeel, P., Srinivas, A.: Videot: Video enerationusing vq-vae and transformers. arXiv preprint arXiv:2104.10157 (2021) 4 48. Yang, S., Huang, W., Chu, R., Xiao, Y., Zhao, Y., Wang, X., Li, M., Xie, E., Chen, Y., Lu, Y., Chen, S.H.Y.: Longlive: Real-time interactive long video generation (2025) 2, 4, 5, 7, 8, 10, 11, 12, 13, 14 49. Yang, Z., Teng, J., Zheng, W., Ding, M., Huang, S., Xu, J., Yang, Y., Hong, W., Zhang, X., Feng, G., et al.: Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072 (2024) 3 50. Yin, T., Gharbi, M., Park, T., Zhang, R., Shechtman, E., Durand, F., Freeman, W.T.: Improved distribution matching distillation for fast image synthesis. In: NeurIPS (2024) 2, 4, 5 51. Yin, T., Gharbi, M., Zhang, R., Shechtman, E., Durand, F., Freeman, W.T., Park, T.: One-step diffusion with distribution matching distillation. In: CVPR (2024) 2, 4,5 52. Yin, T., Zhang, Q., Zhang, R., Freeman, W.T., Durand, F., Shechtman, E., Huang, X.: From slow bidirectional to fast autoregressive video diffusion models. In: CVPR (2025) 2, 12, 13 53. Yu, J., Bai, J., Qin, Y., Liu, Q., Wang, X., Wan, P., Zhang, D., Liu, X.: Context as memory: Scene-consistent interactive long video generation with memory retrieval. arXiv preprint arXiv:2506.03141 (2025) 2, 4, 11 54. Yu, J., Qin, Y., Wang, X., Wan, P., Zhang, D., Liu, X.: Gamefactory: Creating new games with generative interactive videos. arXiv preprint arXiv:2501.08325 (2025) 2, 4 55. Zhang, L., Cai, S., Li, M., Wetzstein, G., Agrawala, M.: Frame context packing and drift prevention in next-frame-prediction video diffusion models. In: NeurIPS (2025) 2, 7 56. Zhang, S., Zhuang, J., Zhang, Z., Shan, Y., Tang, Y.: Flexiact: Towards flexible action control in heterogeneous scenarios. In: SIGGRAPH. pp. 111 (2025) 2 57. Zhang, Y., Peng, C., Wang, B., Wang, P., Zhu, Q., Kang, F., Jiang, B., Gao, Z., Li, E., Liu, Y., Zhou, Y.: Matrix-game: Interactive world foundation model. arXiv preprint arXiv:2506.18701 (2025) 2, 4 58. Zheng, D., Huang, Z., Liu, H., Zou, K., He, Y., Zhang, F., Zhang, Y., He, J., Zheng, W.S., Qiao, Y., Liu, Z.: VBench-2.0: Advancing video generation benchmark suite for intrinsic faithfulness. arXiv preprint arXiv:2503.21755 (2025) 10, 11, 12 59. Zheng, Z., Peng, X., Yang, T., Shen, C., Li, S., Liu, H., Zhou, Y., Li, T., You, Y.: Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404 (2024) 3