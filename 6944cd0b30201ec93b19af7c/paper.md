# A Comprehensive Survey on World Models for Embodied AI

Xinqing Li, Xin He, Le Zhang, Member, IEEE, Min Wu, Senior Member, IEEE, Xiaoli Li, Fellow, IEEE, and Yun Liu

Abstract—Embodied AI requires agents that perceive, act, and anticipate how actions reshape future world states. World models serve as internal simulators that capture environment dynamics, enabling forward and counterfactual rollouts to support perception, prediction, and decision making. This survey presents a unified framework for world models in embodied AI. Specifically, we formalize the problem setting and learning objectives, and propose a three-axis taxonomy encompassing: (1) Functionality, Decision-Coupled vs. General-Purpose; (2) Temporal Modeling, Sequential Simulation and Inference vs. Global Difference Prediction; (3) Spatial Representation, Global Latent Vector, Token Feature Sequence, Spatial Latent Grid, and Decomposed Rendering Representation. We systematize data resources and metrics across robotics, autonomous driving, and general video settings, covering pixel prediction quality, state-level understanding, and task performance. Furthermore, we offer a quantitative comparison of state-of-the-art models and distill key open challenges, including the scarcity of unified datasets and the need for evaluation metrics that assess physical consistency over pixel fidelity, the trade-off between model performance and the computational efficiency required for real-time control, and the core modeling difficulty of achieving long-horizon temporal consistency while mitigating error accumulation. Finally, we maintain a curated bibliography athttps://github.com/Li-Zn-H/AwesomeWorldModels. Index Terms—World Models, Embodied AI, Temporal Modeling, Spatial Representation. # E MBODIED, how their actions will alter future world states [1], [2]. Central to this capability is the world model, an internal simulator that captures environment dynamics to support both forward and counterfactual rollouts for perception, prediction, and decision making [3], [4]. This survey focuses on world models that yield actionable predictions for embodied agents, distinguishing them from static scene descriptors or purely generative visual models that do not capture controllable dynamics.

Cognitive science suggests humans construct internal models of the world by integrating sensory inputs. These models do not merely predict and simulate future events but also shape perception and guide action [5][7]. Motivated by this view, early AI research on world models was rooted in modelbased reinforcement learning (RL), where latent state-transition models were used to improve sample efficiency and planning performance [8]. The seminal work of Ha and Schmidhuber [9] crystallized the term world model and inspired the Dreamer series [10][12], highlighting how learned dynamics can drive imagination-based policy optimization. More recently, advances in large-scale generative modeling and multimodal learning have expanded world models beyond their initial focus on policy learning into general-purpose environment simulators capable of high-fidelity future prediction, exemplified by models like Sora [13] and V-JEPA 2 [14]. This expansion has diversified functional roles, temporal modeling strategies. and spatial representations, while introducing inconsistencies in terminology and taxonomy across sub-communities. Faithfully capturing environment dynamics requires addressing both the temporal evolution of states and the spatial encoding of scenes [3]. Long-horizon rollouts are susceptible to error accumulation, which establishes coherence as a central challenge in video prediction and policy imagination [15], [16]. Similarly, coarse or 2D-centric layouts provide insufficient geometric detail for handling challenges such as occlusion, object permanence, and geometry-aware planning. In contrast, volumetric or 3D occupancy representations such as neural fields [17] and structured voxel grids [18] provide geometric structure that better supports forecasting and control. Taken together, these points establish temporal modeling and spatial representation as core design dimensions that fundamentally influence the predictive horizon, physical fidelity, and downstream performance of embodied agents. Several recent surveys have organized the rapidly growing literature on world models. Overall, these surveys follow two main approaches. The first is a function-oriented perspective. For example, Ding et al. [4] categorized relevant works based on the two core functions of understanding and prediction, while Zhu et al. [19] presented a framework based on the core capabilities of world models. The second approach is application-driven, focusing on specific domains such as autonomous driving. Notably, Guan et al. [20] and Feng et al. [21] provided overviews of world model techniques for autonomous driving. To address the lack of a unified taxonomy in the context of embodied AI, this work introduces a framework centered on the three core axes of functionality, temporal modeling, and spatial representation. At the functional level, this framework distinguishes between decision-coupled and general-purpose models. At the temporal level, it differentiates between Sequential Simulation and Inference versus Global Difference Prediction. Finally, at the spatial level, it encompasses a range of representations from latent features to explicit geometry and neural fields. This framework offers a unified structure for organizing existing approaches and integrates standardized datasets and evaluation metrics. This structure facilitates quantitative comparisons and provides a panoramic, actionable knowledge map for future research. Fig. 1 presents an overview of the structure and taxonomy of this paper. We begin in $\ S \amalg$ by outlining the core concepts and theoretical foundations of world models. $\ S \mathrm { I I I }$ introduces our three-axis taxonomy and maps representative methods onto this framework. $\ S \mathrm { I V }$ surveys the datasets and evaluation metrics used for training and assessment. $\ S \mathrm { V }$ offers a quantitative comparison of state-of-the-art models. $\ S \mathrm { V I }$ discusses open challenges and promising research directions, and $\ S \ V \mathrm { I I }$ concludes the survey.

# II. BACKGROUND

# A. Core Concepts

As discussed in $\ S \mathrm { I }$ , a world model functions as an internal simulator of environmental dynamics. Its functionality rests on three aspects: • Simulation & Planning, which uses learned dynamics to generate plausible future scenarios, allowing agents to assess potential actions through imagination without real-world interaction. • Temporal Evolution, which learns how the encoded state evolves, enabling temporally consistent rollouts. • Spatial Representation, which encodes scene geometry at an appropriate fidelity, using formats such as latent tokens or neural fields to provide context for control. These three pillars provide the conceptual foundation for the taxonomy introduced in $\ S \mathrm { I I I }$ and are formalized in the mathematical framework that follows.

# B. Mathematical Formulation of World Models

We formalize the environment interaction as a POMDP [24]. For notational consistency, we define a null initial action $a _ { 0 }$ at $t = 0$ , which allows the dynamics to be written uniformly. At each step $t \geq 1$ , the agent receives an observation $o _ { t }$ and takes an action $a _ { t }$ , while the true state $s _ { t }$ remains unobserved. To handle this partial observability, world models infer a learned latent state $z _ { t }$ using a one-step filtering posterior, where the previous latent state $z _ { t - 1 }$ is assumed to summarize the relevant history. Finally, $z _ { t }$ is used to reconstruct $o _ { t }$ :

$$
\begin{array} { l l } { { \mathrm { D y n a m i c s ~ P r i o r : } \ } } & { { p _ { \theta } ( z _ { t } \mid z _ { t - 1 } , a _ { t - 1 } ) } } \\ { { \mathrm { F i l t e r e d ~ P o s t e r i o r : } \ } } & { { q _ { \phi } ( z _ { t } \mid z _ { t - 1 } , a _ { t - 1 } , o _ { t } ) } } \\ { { \mathrm { R e c o n s t r u c t i o n : } \ } } & { { p _ { \theta } ( o _ { t } \mid z _ { t } ) } } \end{array} .
$$

Consistent with the Markovian structure, the joint distribution over observations and latent states factorizes as:

$$
p _ { \theta } \big ( o _ { 1 : T } , z _ { 0 : T } \mid a _ { 0 : T - 1 } \big ) = p _ { \theta } \big ( z _ { 0 } \big ) \prod _ { t = 1 } ^ { T } p _ { \theta } \big ( z _ { t } \mid z _ { t - 1 } , a _ { t - 1 } \big ) p _ { \theta } \big ( o _ { t } \mid z _ { t } \big ) .
$$

To infer the latent states, we must approximate the intractable true posterior $p _ { \theta } \big ( z _ { 0 : T } \ \big | \ o _ { 1 : T } , a _ { 0 : T - 1 } \big )$ with a time-factorized variational distribution:

$$
q _ { \phi } ( z _ { 0 : T } | o _ { 1 : T } , a _ { 0 : T - 1 } ) = q _ { \phi } ( z _ { 0 } | o _ { 1 } ) \prod _ { t = 1 } ^ { T } q _ { \phi } ( z _ { t } | z _ { t - 1 } , a _ { t - 1 } , o _ { t } ) ,
$$

which indeed reduces to the action-free case when ignoring $a$ inputs. Directly maximizing the log-likelihood $\log p _ { \theta } ( o _ { 1 : T } |$ $a _ { 0 : T - 1 } )$ is intractable. Instead, we optimize an ELBO using the approximate posterior $q _ { \phi }$ , which provides a tractable objective for learning the model parameters:

$$
\begin{array} { r l r } & { } & { \log p _ { \theta } ( o _ { 1 : T } \mid a _ { 0 : T - 1 } ) = \log \int p _ { \theta } \big ( o _ { 1 : T } , z _ { 0 : T } \mid a _ { 0 : T - 1 } \big ) d z _ { 0 : T } } \\ & { } & { \geq \mathbb { E } _ { q _ { \phi } } \Big [ \log \frac { p _ { \theta } \big ( o _ { 1 : T } , z _ { 0 : T } \mid a _ { 0 : T - 1 } \big ) } { q _ { \phi } \big ( z _ { 0 : T } \mid o _ { 1 : T } , a _ { 0 : T - 1 } \big ) } \Big ] = : \mathcal { L } ( \theta , \phi ) . } \end{array}
$$

Under the assumption of Markov factorization for both $p _ { \theta }$ and $q _ { \phi }$ , this ELBO decomposes into a reconstruction objective and a KL regularization term:

$$
\begin{array} { r l } {  { \mathcal { L } ( \theta , \phi ) = \sum _ { t = 1 } ^ { T } \mathbb { E } _ { q _ { \phi } ( z _ { t } ) } \bigl [ \log p _ { \theta } ( o _ { t } \mid z _ { t } ) \bigr ] } \quad } & { { } } \\ { - \ D _ { \mathrm { K L } } \bigl ( q _ { \phi } ( z _ { 0 : T } \mid o _ { 1 : T } , a _ { 0 : T - 1 } ) \parallel p _ { \theta } ( z _ { 0 : T } \mid a _ { 0 : T - 1 } ) \bigr ) . } \end{array}
$$

Modern world models thus adopt a reconstructionregularization training paradigm: the likelihood term $\log p _ { \theta } ( o _ { t } \quad | \quad z _ { t } )$ encourages faithful observation prediction, and $\mathrm { K L }$ regularization terms align the filtered posterior $q _ { \phi } ( z _ { t } \mid z _ { t - 1 } , a _ { t - 1 } , o _ { t } )$ with the dynamics prior $p _ { \theta } ( z _ { t } \mid z _ { t - 1 } , a _ { t - 1 } )$ . Such world models can be instantiated with recurrent models [25][27], Transformer-based architectures [28][30], or diffusion-based decoders [31][35]. In all cases, the learned latent trajectory $z _ { 1 : T }$ serves as a compact, predictive memory to support downstream policy optimization, model-predictive control, and counterfactual reasoning in embodied AI.

# III. TAXONOMY

We categorize world models along three core dimensions, which provide the foundation for the subsequent analysis in this survey. The first dimension, decision coupling, distinguishes between Decision-Coupled and General-Purpose world models. Decision-Coupled models are task-specific, learning dynamics optimized for a particular decision-making task. In contrast, General-Purpose models are task-agnostic simulators that focus on broad prediction, enabling generalization across various downstream applications. The second dimension, temporal reasoning, delineates two distinct paradigms of prediction. Sequential Simulation and Inference models dynamics in an autoregressive manner, unfolding future states one step at a time. In contrast, Global Difference Prediction directly estimates entire future states in parallel, offering greater efficiency at the potential cost of reduced temporal coherence.

![](images/1.jpg)  
the field. Figure design inspired in part by [12], [14], [22], [23].

The third dimension, spatial representation, comprises four primary strategies used in current research to model spatial states: 1) Global Latent Vector representations encode complex world states into a compact vector, enabling efficient realtime computation on physical devices. 2) Token Feature Sequence representations model world states as sequences of tokens, focusing on capturing complex spatial, temporal, and cross-modal dependencies among tokens. 3) Spatial Latent Grid representations incorporate spatial inductive biases into world models by leveraging geometric priors such as Bird's-Eye View (BEV) features or voxel grids. 4) Decomposed Rendering Representation involves decomposing 3D scenes into a set of learnable primitives, such as 3D Gaussian Splatting (3DGS) [36] or Neural Radiance Fields (NeRF) [37], and then using differentiable rendering to achieve high-fidelity novel view synthesis. The following tables apply this taxonomy to classify representative works. Tab. I reviews approaches in robotics, while Tab. II focuses on autonomous driving. Together, they provide a roadmap for the detailed analyses in the subsequent sections.

# A. Decision-Coupled World Models

1) Sequential Simulation and Inference: Global Latent Vector. Early decision-coupled world models combined sequential inference with global latent states. These approaches primarily use Recurrent Neural Networks (RNNs) for efficient real-time and long-horizon prediction. Ha and Schmidhuber [9] introduced an early world model that encodes observations into a latent space and employs an RNN to model dynamics for policy optimization. Building on this, PlaNet [38] introduced the Recurrent State-Space Model (RSSM), which fuses deterministic memory with stochastic components to enable robust long-horizon imagination. The successor models Dreamer, DreamerV2, and DreamerV3 [10] [12] further advanced this formulation, inspiring a broad line of subsequent research. Building on RSSM, several variants modify or eliminate the decoder to better capture dynamics. For example, Dreaming [110] uses contrastive learning and linear methods to mitigate state shifts, whereas DreamerPro [111] replaces the decoder with prototypes to suppress visual distractions. To further enhance robustness, HRSSM [25] was proposed, featuring a dual-branch architecture that aligns latent observations and shares information without reconstruction. Beyond architectural refinements, DisWM [112] disentangles semantic knowledge from video content, distilling it into a world model that enables cross-domain generalization.

A unifying theme across recent RSSM extensions is transferability, which reflects generalization across modalities, tasks, and embodiments for robust real-world robotics. At the representation level, PreLAR [52] learns implicit action abstractions to bridge video-pretrained representations and control finetuning. Similarly, Wang et al. [113] used optical flow as an embodiment-agnostic action representation to refine behaviorcloned policies, facilitating transfer across diverse embodiments. SENSEI [114] distilled a Vision-Language Model (VLM) to derive semantic rewards and employed an RSSM that learns to predict and propagate these rewards internally. Under limited supervision, SR-AIF [115] exploits prior preference learning and self-revision to enable adaptive learning in sparse-reward, continuous-control settings. To mitigate the Sim-to-Real (S2R) gap, ReDRAW [116] is pretrained in simulation and adapted to the real environment using a limited amount of reward-free data, applying residual corrections to the latent dynamics. To handle mismatches, AdaWM [117] identifies discrepancies between the learned dynamics and the planner and selectively fine-tunes critical components. Other methods like WMP [118] address S2R transfer for challenging tasks, and DayDreamer [43] demonstrated sample-efficient deployment on physical robots. To broaden transfer, FOUNDER [119] grounds representations from foundation models in the world-model state space, using temporal-distance prediction to handle flexible goals, and LUMOS [120] introduced a language-conditioned imitation framework that operates on-policy in latent space with intrinsic rewards, enabling zero-shot transfer to real-world robotics. TABLE I A SUMMARY OF REPRESENTATIVE WORLD MODELS IN ROBOTICS AND GENERAL-PURPOSE DOMAINS.   

<table><tr><td></td><td></td><td></td><td></td><td colspan="6">Datasets Platform</td><td></td><td></td><td></td><td>Modality</td><td></td><td></td><td></td></tr><tr><td>Paper</td><td>Publication</td><td>Taxonomy1</td><td>Characteristics2</td><td>N0 </td><td>20</td><td>S</td><td>20 </td><td>Br I</td><td>0 (e</td><td></td><td></td><td>S</td><td>0</td><td>Poudod </td><td>aenee</td><td>(</td><td>Reality4</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>PlaNet [38]</td><td>ICML&#x27;19</td><td>Dec/Seq/GLV Dec/Seq/GLV</td><td>RSSM RSSM</td><td>✓</td><td></td><td></td><td></td><td></td><td>✓</td><td>1</td><td>✓</td><td></td><td>√</td><td></td><td></td><td></td><td></td></tr><tr><td>Dreamer [10]</td><td>ICLR&#x27;20 ICLR&#x27;21</td><td>Dec/Seq/GLV</td><td>IDM</td><td>✓ ✓</td><td>✓ ✓</td><td></td><td></td><td></td><td></td><td>3 2</td><td></td><td>✓</td><td>✓</td><td></td><td></td><td></td><td></td></tr><tr><td>GLAMOR [39]</td><td>ICLR&#x27;21</td><td>Dec/Seq/GLV</td><td>RSSM</td><td>✓</td><td>✓</td><td></td><td></td><td></td><td></td><td>2</td><td></td><td>✓</td><td>✓</td><td></td><td></td><td></td><td></td></tr><tr><td>DreamerV2 [11]</td><td>arXiv&#x27;22</td><td>Dec/Seq/GLV</td><td>TSSM</td><td>✓</td><td>✓</td><td></td><td></td><td></td><td>✓</td><td>4</td><td></td><td>✓</td><td>✓</td><td></td><td></td><td></td><td></td></tr><tr><td>TransDreamer [28]</td><td>NeurIPS&#x27;22</td><td>Dec/Seq/GLV</td><td>IDM</td><td>✓</td><td></td><td></td><td></td><td></td><td>✓</td><td>4</td><td></td><td>✓</td><td>✓</td><td></td><td></td><td></td><td></td></tr><tr><td>Iso-Dream [40]</td><td>CoRL&#x27;22</td><td>Dec/Seq/TFS</td><td>RSSM</td><td>✓</td><td>✓</td><td></td><td></td><td>√</td><td></td><td>3</td><td></td><td>✓</td><td>✓</td><td></td><td></td><td></td><td></td></tr><tr><td>MWM [41]</td><td>CoRL&#x27;22</td><td>Dec/Seq/TFS</td><td>CoT</td><td></td><td></td><td></td><td></td><td></td><td>✓</td><td>3</td><td>✓</td><td>✓</td><td>✓</td><td></td><td></td><td></td><td></td></tr><tr><td>Inner Monologue [42]</td><td>CoRL&#x27;22</td><td>Dec/Seq/GLV</td><td>RSSM</td><td></td><td></td><td></td><td></td><td></td><td>✓</td><td>4</td><td>✓</td><td></td><td>✓</td><td></td><td>✓</td><td></td><td>✓ ✓</td></tr><tr><td>DayDreamer [43]</td><td>ICLR&#x27;23</td><td>Dec/Seq/TFS</td><td>Transformer</td><td></td><td>✓</td><td></td><td></td><td></td><td></td><td>1</td><td></td><td></td><td>✓</td><td>✓ ✓</td><td></td><td></td><td></td></tr><tr><td>TWM [29]]</td><td>ICLR&#x27;23</td><td>Dec/Seq/TFS</td><td>Transformer</td><td></td><td>✓</td><td></td><td></td><td></td><td></td><td>1</td><td></td><td>✓</td><td>✓</td><td></td><td></td><td></td><td></td></tr><tr><td>IRIS [44]</td><td>arXiv&#x27;24</td><td>Gen/Glo/TFS</td><td>Transformer</td><td></td><td></td><td></td><td></td><td></td><td></td><td>✓ 4</td><td>✓</td><td>✓</td><td>✓</td><td></td><td></td><td></td><td></td></tr><tr><td>WorldDreamer [45]</td><td>ICRA&#x27;24</td><td>Dec/Seq/TFS</td><td>LLM</td><td></td><td></td><td></td><td></td><td></td><td></td><td>√ 2</td><td>√</td><td></td><td>✓</td><td></td><td>✓</td><td></td><td></td></tr><tr><td>Statler [46]</td><td>arXiv&#x27;24</td><td>Gen/Seq/TFS</td><td>Video Diffusion</td><td></td><td></td><td>✓</td><td></td><td></td><td></td><td>✓ 2</td><td>✓</td><td></td><td>✓</td><td>√</td><td>✓</td><td></td><td>√</td></tr><tr><td>Pandora [47]</td><td>RSS&#x27;24</td><td>Dec/Seq/GLV</td><td>MLP</td><td></td><td></td><td></td><td></td><td></td><td></td><td>✓ 4</td><td></td><td></td><td></td><td></td><td>✓</td><td></td><td></td></tr><tr><td>DWL [48]</td><td>ICML&#x27;24</td><td>Dec/Glo/TFS</td><td>IDM</td><td></td><td></td><td>✓</td><td></td><td></td><td>✓</td><td>2</td><td>✓</td><td></td><td>✓</td><td>✓</td><td></td><td>✓</td><td>✓</td></tr><tr><td>RoboDreamer [49] Genie [50]</td><td>ICML&#x27;24</td><td>Gen/Seq/TFS</td><td>Transformer</td><td></td><td></td><td></td><td></td><td></td><td>✓</td><td>✓ 3</td><td>✓</td><td></td><td>✓ ✓</td><td></td><td>✓</td><td>✓</td><td></td></tr><tr><td>V-JEPA [51]</td><td>TMLR&#x27;24</td><td>Gen/Glo/TFS</td><td>JEPA</td><td></td><td></td><td>✓</td><td></td><td></td><td></td><td>✓ 6</td><td>✓</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>PreLAR [52]</td><td>ECCV&#x27;24</td><td>Dec/Seq/GLV</td><td>RSSM</td><td></td><td></td><td>✓ ✓</td><td></td><td>✓</td><td></td><td>3</td><td>✓</td><td></td><td>✓</td><td></td><td></td><td></td><td></td></tr><tr><td>ManiGaussian [53]</td><td>ECCV&#x27;24</td><td>Dec/Seq/DRR</td><td>3DGS</td><td></td><td></td><td>✓</td><td></td><td></td><td></td><td>1</td><td>✓</td><td></td><td>✓</td><td>✓ ✓</td><td>✓</td><td></td><td></td></tr><tr><td>ECoT [54]</td><td>CoRL&#x27;24</td><td>Dec/Glo/TFS</td><td>CoT</td><td></td><td></td><td></td><td></td><td></td><td>✓</td><td>3</td><td>✓</td><td></td><td>✓</td><td></td><td>✓</td><td></td><td>✓</td></tr><tr><td>VidMan [55]</td><td>NeurIPS&#x27;24</td><td>Dec/Glo/TFS</td><td>IDM</td><td></td><td></td><td>✓</td><td>✓</td><td></td><td>✓</td><td>4</td><td>✓</td><td></td><td>✓</td><td>✓</td><td>✓</td><td></td><td></td></tr><tr><td>iVideoGPT [56]</td><td>NeurIPS&#x27;24</td><td>Gen/Seq/TFS Dec/Seq/SLG</td><td>Transformer Video Diffusion</td><td></td><td></td><td></td><td>√ √</td><td>✓</td><td>✓ ✓ ✓</td><td>6 4</td><td>✓</td><td></td><td>✓</td><td></td><td></td><td></td><td></td></tr><tr><td>EnerVerse [34] GLAM [57]</td><td>arXiv&#x27;25 AAAI&#x27;25</td></table>

1 Taxonomy: Abbreviations for the taxonomy categories defined in III . . RSSM-based models have also been developed for autonomous driving. MILE [81] leverages offline expert data to enable imagined future states for planning. SEM2 [83] integrates semantic filtering with multi-source sampling to extract driving-relevant features and balance data distribution. Popov et al. [121] addressed covariate shift through a latent generative world model that realigns policies with expert states. TABLE II A SUMMARY OF REPRESENTATIVE WORLD MODELS FOR THE AUTONOMOUS DRIVING DOMAIN.   

<table><tr><td></td><td></td><td>Taxonomy1</td><td>Characteristics2</td><td></td><td>Datasets Platform</td><td></td><td></td><td></td><td></td><td>Input Modality</td><td></td><td></td><td>en</td><td></td></tr><tr><td>Paper</td><td>Publication</td><td></td><td></td><td>00</td><td>nuetnes omm 20</td><td>Oud 0</td><td>()o</td><td>2</td><td>S</td><td>Wonow A</td><td>00</td><td>oq puno</td><td>Ooede</td><td>(o</td></tr><tr><td>MILE [81]</td><td>NeurIPS&#x27;22</td><td>Dec/Seq/GLV</td><td>RSSM</td><td>√</td><td></td><td></td><td>1</td><td></td><td>✓</td><td>✓ ✓</td><td></td><td></td><td></td><td></td></tr><tr><td>Copilot4D [82]</td><td>ICLR&#x27;24</td><td>Gen/Seq/SLG</td><td>Video Diffusion</td><td></td><td>✓</td><td></td><td>✓</td><td>3</td><td></td><td>✓</td><td>✓</td><td></td><td></td><td></td></tr><tr><td>SEM2 [83]</td><td>TITS&#x27;24</td><td>Dec/Seq/GLV</td><td>RSSM</td><td>✓</td><td></td><td></td><td></td><td>1</td><td>✓</td><td>✓ ✓</td><td>✓</td><td></td><td></td><td></td></tr><tr><td>MagicDrive3D [84]</td><td>arXiv&#x27;24</td><td>Gen/Glo/DRR</td><td>3DGS</td><td></td><td>✓</td><td></td><td></td><td>1</td><td>✓</td><td>✓ ✓</td><td></td><td>✓ ✓</td><td></td><td>✓</td></tr><tr><td>OccSora [85]</td><td>arXiv&#x27;24</td><td>Gen/Glo/SLG</td><td>Diffusion</td><td></td><td>✓</td><td>✓</td><td></td><td>2</td><td></td><td>✓</td><td></td><td></td><td>✓</td><td></td></tr><tr><td>Delphi [86]</td><td>arXiv&#x27;24</td><td>Gen/Seq/SLG</td><td>Video Diffusion</td><td></td><td>✓</td><td></td><td></td><td>1</td><td>✓</td><td></td><td>✓</td><td>✓</td><td>✓</td><td></td></tr><tr><td>DriveWorld [87]</td><td>CVPR&#x27;24</td><td>Dec/Seq/SLG</td><td>RSSM</td><td></td><td>✓</td><td></td><td>✓</td><td>2</td><td>✓</td><td>✓</td><td></td><td></td><td>✓ ✓</td><td></td></tr><tr><td>Drive-WM [88]</td><td>CVPR&#x27;24</td><td>Dec/Glo/SLG Gen/Seq/SLG</td><td>Video Diffusion</td><td></td><td>✓</td><td></td><td></td><td>1</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td></tr><tr><td>ViDAR [89]</td><td>CVPR&#x27;24</td><td>Gen/Seq/TFS</td><td>Transformer Video Diffusion</td><td></td><td>✓ ✓</td><td>✓</td><td>✓</td><td>1</td><td>✓</td><td>✓</td><td>✓</td><td></td><td></td><td></td></tr><tr><td>GenAD [90]</td><td>CVPR&#x27;24</td><td>Dec/Seq/SLG</td><td>Transformer</td><td></td><td>✓ ✓ ✓</td><td>✓</td><td>✓</td><td>4</td><td>✓</td><td>✓</td><td></td><td></td><td>✓</td><td></td></tr><tr><td>OccLLaMA [18]</td><td>arXiv&#x27;24</td><td>Dec/Seq/SLG</td><td>GRU</td><td></td><td>✓</td><td></td><td></td><td>3 1</td><td>✓</td><td>✓</td><td></td><td></td><td>✓ ✓</td><td></td></tr><tr><td>DriveDreamer [91]</td><td>ECCV&#x27;24 ECCV&#x27;24</td><td>Dec/Seq/SLG</td><td>GRU</td><td></td><td>✓</td><td></td><td></td><td>1</td><td>√</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td></td></tr><tr><td>GenAD [92]</td><td>ECCV&#x27;24</td><td>Dec/Seq/SLG</td><td>Transformer</td><td></td><td>✓</td><td>✓</td><td></td><td>2</td><td>✓ ✓</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>OccWorld [93] DOME [94]</td><td>arXiv&#x27;24</td><td>Gen/Seq/SLG</td><td>DiT</td><td></td><td>✓</td><td>✓</td><td></td><td>2</td><td>✓</td><td>✓ ✓</td><td>✓</td><td></td><td>✓</td><td></td></tr><tr><td>TOKEN [95]</td><td>CoRL&#x27;24</td><td>Dec/Glo/TFS</td><td>Transformer</td><td></td><td>✓</td><td></td><td>✓</td><td>2</td><td>✓</td><td>✓</td><td>✓</td><td></td><td>✓</td><td></td></tr><tr><td>Vista [96]</td><td>NeurIPS&#x27;24</td><td>Gen/Seq/SLG</td><td>Video Diffusion</td><td></td><td>√</td><td>✓ ✓</td><td>√</td><td>4</td><td>✓</td><td>✓</td><td></td><td></td><td>✓</td><td></td></tr><tr><td>DriveDreamer-2 [97]</td><td>AAAI&#x27;25</td><td>Gen/Glo/SLG</td><td>Video Diffusion</td><td></td><td>✓</td><td></td><td></td><td>1</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>✓ ✓</td><td></td></tr><tr><td>DTT [98]</td><td>arXiv&#x27;25</td><td>Dec/Seq/DRR</td><td>Transformer</td><td></td><td>✓</td><td>✓</td><td></td><td>2</td><td>✓</td><td>✓</td><td></td><td></td><td>✓ ✓</td><td></td></tr><tr><td>DynamicCity [99]</td><td>ICLR&#x27;25</td><td>Gen/Glo/SLG</td><td>DiT</td><td>✓</td><td>✓</td><td>✓ ✓</td><td></td><td>4</td><td>✓</td><td>✓</td><td></td><td></td><td>✓ ✓</td><td></td></tr><tr><td>LidarDM [100]</td><td>ICRA&#x27;25</td><td>Gen/Seq/SLG</td><td>Diffusion</td><td></td><td></td><td>✓</td><td>✓</td><td>3</td><td></td><td>✓</td><td>✓</td><td></td><td></td><td></td></tr><tr><td>FutureSightDrive [101]</td><td>arXiv&#x27;25</td><td>Dec/Seq/TFS</td><td>CoT(VLM)</td><td></td><td>✓</td><td></td><td>✓</td><td>3</td><td>✓</td><td>✓</td><td></td><td></td><td>✓</td><td></td></tr><tr><td>GEM [102]</td><td>CVPR&#x27;25</td><td>Gen/Seq/SLG</td><td>Video Diffusion</td><td></td><td>✓</td><td>✓</td><td></td><td>1</td><td>✓</td><td>✓</td><td></td><td></td><td></td><td>✓</td></tr><tr><td>GaussianWorld [103]</td><td>CVPR&#x27;25</td><td>Gen/Seq/DRR</td><td>Transformer</td><td></td><td>✓</td><td></td><td>✓</td><td>2</td><td>✓</td><td>✓</td><td></td><td></td><td></td><td></td></tr><tr><td>MaskGWM [104]</td><td>CVPR&#x27;25</td><td>Gen/Glo/TFS</td><td>DiT</td><td></td><td>✓</td><td>✓ ✓</td><td></td><td>3</td><td>✓</td><td>✓</td><td></td><td></td><td>✓</td><td></td></tr><tr><td>DriveDreamer4D [105]</td><td>CVPR&#x27;25</td><td>Gen/Glo/DRR</td><td>4DGS</td><td></td><td>✓ ✓</td><td>✓</td><td>3</td><td>✓</td><td></td><td>✓</td><td>✓ ✓</td><td>✓</td><td>✓</td><td>✓</td></tr><tr><td>ReconDreamer [106]</td><td>CVPR&#x27;25</td><td>Gen/Glo/DRR</td><td>3DGS</td><td></td><td>✓</td><td>✓</td><td>✓</td><td>3</td><td>✓</td><td>✓</td><td>✓ ✓</td><td>✓</td><td></td><td></td></tr><tr><td>WoTE [107]</td><td>ICCV&#x27;25</td><td>Dec/Seq/SLG</td><td>Transformer</td><td>✓</td><td>✓</td><td></td><td>✓</td><td>2 4</td><td>✓ ✓</td><td>✓ ✓</td><td>✓</td><td>✓</td><td></td><td></td></tr><tr><td>HERMES [108] InfiniCube [22]</td><td>ICCV&#x27;25 ICCV&#x27;25</td><td>Gen/Glo/SLG Gen/Seq/DRR</td><td>LLM 3DGS</td><td></td><td>✓</td></table>

1 Taxonomy: Abbreviations for the taxonomy categories defined in $\ S \mathrm { I I I }$ . . For safety, VL-SAFE [122] supervises world models using safety scores derived from a VLM to generate safe trajectories. Finally, CALL [123] extended the RSSM framework to MultiAgent RL by introducing ego-centric information sharing to enhance planning capabilities. In contrast to RSSM, TransDreamer [28] introduced a Transformer State-Space Model (TSSM) that replaces the recurrent core in Dreamer, thereby substantially enhancing its capacity to capture long-horizon dependencies. The complementary OSVI-WM [124] employs a causal Transformer for one-shot Imitation Learning (IL), autoregressively predicting the future latent trajectory and decoding it into physical waypoints for robot control. Some approaches continue to employ RNNs to capture temporal dependencies. On the modeling side, RWM [125] introduced a dual-autoregressive, domain-agnostic neural simulator for long-horizon prediction. X-MOBILITY [126], in contrast, disentangles modeling from policy learning, using multi-head decoders for large-scale pretraining followed by supervised fine-tuning to derive strong policies. For humanoid locomotion, DWL [48] and WMR [127] adopted end-to-end (E2E) frameworks. These frameworks reconstruct states from partial observations using either denoising or a gradient-blocked state estimator, which enables zero-shot transfer across complex real-world terrains. Recently, State Space Models (SSMs), exemplified by Mamba, have emerged as alternatives to RNNs and Transformers, combining linear-time complexity with long-horizon modeling capacity. Building on this, GLAM [57] improves both fidelity and efficiency via a Mamba-based parallel framework that integrates global and local modules to capture contextual and fine-grained dynamics. Beyond forward temporal modeling, Inverse Dynamics Modeling (IDM) is a key paradigm in world model construction. An IDM infers the actions required to transition between an initial and a target state. Agrawal et al. [128] integrated a forward model with an IDM for multi-step prediction, establishing the basis for subsequent research. More recent work includes GLAMOR [39], which trains an object-conditioned IDM to predict the actions necessary to reach a specified target. In Dreamer-style agents, Iso-Dream [40] leverages IDM to decompose the world model into controllable and uncontrollable components, using the rollout of uncontrollable states to guide policy learning. Token Feature Sequence. The Token Feature Sequence paradigm centers on modeling dependencies among discrete tokens. This representation supports causal inference, multimodal integration, and the reuse of Large Language Model (LLM).

Recent RSSM-centric studies have begun to exploit tokenlevel dependencies to strengthen representation learning and temporal reasoning. For example, MWM [41] decouples visual tokens from RSSM-based dynamics via a masked autoencoder, improving both performance and data efficiency. NavMorph [129] introduced a self-evolving RSSM with a contextual evolution memory for online adaptation. For temporal abstraction, WISTER [130] employed action-conditioned contrastive predictive coding to train a TSSM that captures high-level temporal features. Similarly, TWM [29] used a Transformer to align multimodal tokens with historical states during training, while relying on a lightweight policy at inference. To handle long-horizon tasks, some approaches integrate LLMs with RSSMs to decompose objectives into subtasks. EvoAgent [131], for example, uses an LLM to guide low-level actions and regularizes RSSM updates. RoboHorizon [132], in contrast, enhances task recognition with dense rewards and leverages key task segments via a masked autoencoder. In autonomous driving, token-based sequence representations are increasingly adopted to model cross-modal interactions and spatiotemporal structures. DrivingWorld [133] pairs next-state prediction for temporal dynamics with next-token prediction for spatial structure. For multimodal control, Doe-1 [134] formulates closed-loop driving as autoregressive prediction over perception-description-action tokens, which unifies perception, prediction, and planning, and DrivingGPT [23] interleaves vision and action tokens and casts world modeling and trajectory planning as next-token prediction. To enhance diversity and safety, LatentDriver [135] models future actions as a mixture distribution and actuates the world model with planner-sampled intermediate actions. At the same time, Vasudevan et al. [136] proposed an adaptive model that predicts surrounding agents for safe navigation. The token-based paradigm also extends to broader robotics. Within RL, IRIS [44] and TWM [137] leverage discrete tokens to enable data-efficient policy learning via imagined or hybrid rollouts. DyWA [138] improves action learning by conditioning on trajectory dynamics and jointly predicting future states with single-view point-cloud and proprioceptive modalities. EgoAgent [139] interleaves state-action sequence modeling within a Transformer, enabling unified perception, prediction, and action inference. Tokenized representations unify multimodal inputs, including vision, language, and action (VLA), enabling generalist agents with cross-domain adaptability, as shown in WorldVLA [67]. Recent studies encode environmental states as discrete symbolic tokens and condition next-token prediction on action, as demonstrated by DCWM [140] and TrajWorld [141].

Recent studies have strengthened the link between tokenized representations and planning, particularly through object-centric approaches. These models, such as CarFormer [142], the work of Jeong et al. [143], and Dyn-O [69], represent scenes as a collection of slots. CarFormer autoregressively models the relationships between these slots in BEV. Jeong et al. added language-guided manipulation, while Dyn-O used a Mamba with dropout scheduling for temporal modeling and to disentangle static from dynamic elements. $\Delta$ -IRIS [144] introduced a hybrid Transformer that integrates tokens with stochastic $\Delta$ tokens to capture dynamics. $\mathrm { D ^ { 2 } P O }$ [145] employed preference learning to jointly optimize state prediction and action selection, enhancing the model's understanding of underlying dynamics. For efficiency, MineWorld [59] accelerated token generation by predicting sequences in parallel and introduced an IDM as a controllability metric. Meanwhile, PIVOT-R [146] and ReOI [147] incorporated VLMs into control. PIVOT-R parses instructions to produce waypoint-based plans that an action module decodes into low-level controls, whereas ReOI detects implausible prediction elements, reimagines the distractors, and reintegrates the corrected content. Based on tokenization, some studies employ autoregressive diffusion to achieve stable generation and long-horizon planning. Epona [148] decouples spatiotemporal modeling, handled by a Transformer, from long-horizon multimodal generation, realized through trajectory and vision Diffusion Transformers (DiTs). Goff et al. [149] used a DiT to instantiate the state transition, which enables on-policy training and multi-second closed-loop rollouts. SceneDiffuser $^ { + + }$ [150] pushes this further to city-scale traffic simulation, applying multi-tensor diffusion over agents and traffic lights to produce stable closed-loop rollouts. For navigation, NWM [66] introduced an efficient conditional DiT to simulate visual trajectories for zero-shot planning. Another emerging direction is to inject explicit reasoning into the world model using LLMs and Chain-of-Thought (CoT). NavCoT [58] decomposes navigation into imagination, filtering, and prediction, enabling parameter-efficient in-domain training, and ECoT [54] leverages a pipeline of foundation models to generate reasoning labels for training a VLA policy. Variants like MineDreamer [79] introduced Chain-of-Imagination (CoI), where a multimodal LLM imagines future observations to steer diffusion and guide actions, and FSDrive [101] generates physics-constrained future scenes and treats them as CoT supervision, enabling VLMs to function as IDMs for planning. Other approaches directly couple LLMs with world models to operationalize planning and data generation. Dyna-Think [151] fuses reasoning and acting via a distilled LLM, and RIG [152] unifies reasoning and imagination end-to-end generalist policy. In terms of explicit dynamics and long-horizon, Gkountouras et al. [153] trained a causal world-model simulator that grounds an LLM nvironment causal reasoning and planning skills, Statler [46] enables LLMs to keep a structured world-state, using a reader for planning and a writer for updating, and Inner Monologue [42] incorporates a closed-loop feedback into LLMs, enabling agents to reason and deliberate more akin to human thinking. Finally, WoMAP [154] synthesized 3DGS scenes and trained a world model that refines VLM instructions for precise execution. Spatial Latent Grid. By encoding features on geometryaligned grids or incorporating explicit spatial priors, this paradigm preserves locality, enables efficient convolutional or attention-based updates and streaming rollouts. In autonomous driving, many studies couple RNN-based dynamics with spatial grids to guide planning. For instance, DriveDreamer [91] and GenAD [92] adopt GRU-based dynamics over grid or instance-centric tokens to predict motion and decode trajectories. In contrast, DriveWorld [87] and Raw2Drive [155] instantiate RSSM dynamics on BEV tokens. DriveWorld conditions on tokens and actions for joint prediction, while Raw2Drive adopts a dual-stream design for spatiotemporal learning.

Numerous studies focus on autoregressively predicting future 3D occupancy representations to enable motion planning for autonomous driving. One strand discretizes scenes into occupancy tokens for sequential prediction, exemplified by OccWorld [93] and RenderWorld [156]. Another strand directly forecasts volumetric features or embeddings, as in Drive-OccWorld [157] and PreWorld [158]. Self-supervised variants predict future representations from current cues. For example, LAW [159] conditions on current representations and trajectories, SSR [160] compresses scenes into sparse BEV tokens for future BEV features, and NeMo [161] voxelizes multi-frame images and predicts occupancy to support imitation-based planning. Building on these representations, FASTopoWM [162] employs a unified decoder to align fast and slow systems from vehicle poses, enabling lane-topology reasoning, and WoTE [107] simulates candidate trajectories in BEV and selects among them with a reward model. Extending the paradigm, OccLLaMA [18] unifies occupancy, actions, and text within a single token vocabulary and employs a LLaMA for next token forecasting, planning, and question answering. Beyond autonomous driving, similar formulations have been extended to broader domains of robotics. WMNav [163] leverages a VLM to maintain a curiosity-driven value map and adopts phased decision making to enable zero-shot, object-driven navigation. RoboOccWorld [164] targets indoor robotics by predicting fine-grained 3D occupancy with a poseconditioned autoregressive Transformer, thereby supporting exploration and decision making. To achieve high-fidelity dynamics, EnerVerse [34] applies chunk-wise autoregressive video diffusion with a sparse memory mechanism to produce 4D latent dynamics and integrates 4DGS to mitigate the S2R gap in robotic execution. For manipulation, ParticleFormer [165] forecasts future point clouds with a Transformer-based particletized dynamics model, enabling robust handling of multi-object and multi-material interactions. At the representation level, DINOWM [70] learns dynamics in the DINOv2 feature space and predicts future states to support zero-shot planning. Decomposed Rendering Representation. This paradigm represents scenes with explicit renderable primitives such as NeRFs and 3DGS, updating them to simulate dynamics and render future observations. It provides view-consistent forecasts, object-level compositionality, and seamless integration with physics priors and digital twins, thereby supporting longhorizon rollouts.

Building on 3DGS, GAF [74] augments splats with learnable motion attributes to forecast future states and refines initial actions with diffusion. ManiGaussian [53] predicts per-point variations to generate future Gaussian scenes for manipulation under current states and actions, and ManiGaussian $^ { + + }$ [80] adds a hierarchical leader-follower design with task-oriented splats to model primitive deformations for multi-body and bimanual skills. Within simulation and digital twin coupling, DreMa [60] integrates GS with a physics simulator to build twins for data synthesis in imitation learning, Abou-Chakra et al. [166] introduced a dual GaussianParticle representation where Gaussian points are attached to particles driven by visual loss forces, DexSim2Real2 [167] builds twins of articulated objects with generative models and uses sampling based planning for precise manipulation, PIN-WM [168] combines 3DGS with differentiable physics to estimate physical parameters from limited observations and generates digital cousins for zero-shot S2R policy learning, and PWTF [169] constructs an interactive twin that simulates candidate action outcomes and uses VLMs for evaluation and selection. At the representation level, DTT [98] adopts a triplane representation with multiscale Transformers to autoregressively capture incremental changes. forming a 4D world model for prediction and planning. 2)Global Difference Prediction: Token Feature Sequence. The compact Global Latent Vector representation discards finegrained spatiotemporal detail and is therefore rarely used for global prediction. In contrast, Token Feature Sequences predict the future sequence in parallel, reducing error accumulation while enabling multimodal diversity.

On the representation side, TOKEN [95] tokenizes scenes into object-level tokens, aligning world representations with reasoning and leveraging LLMs to predict full future trajectories for long-tail scenarios. GeoDrive [170] extracts 3D representations, renders trajectory-conditioned views, and edits the position of vehicles to guide DiT in producing editable generations. For control, FLARE [171] aligns diffusion policies with latent future representations, avoiding pixel-space video generation and learning effectively from action-free videos. In a related vein, LaDi-WM [172] predicts future states through interactive diffusion in a latent space aligned with visual foundation models, integrating geometric and semantic features while iteratively refining the diffusion policy to improve performance and generalization. villa-X [76] and VidMan [55] both couple diffusion-based models with IDM for control. villaX infers latent actions, aligns them with ego-centric forward dynamics, and maps them via joint diffusion, while VidMan adapts a pretrained video-diffusion model into an IDM using a self-attention adapter for accurate action prediction. Spatial Latent Grid. Spatial grid models parallel-forecast BEV or voxel maps from ego-stabilized views, preserving locality and uncertainty while producing planner-ready maps for fast control. Diffusion-based world models are commonly used for parallel generation. EmbodiedDreamer [173] couples differentiable physics with video diffusion to render photorealistic and physically consistent futures. TesserAct [78] reconstructs 4D spatiotemporal consistent scenes by jointly generating RGB, depth, and normal videos for IDM-based action learning. DFIT-OccWorld [174] reformulates prediction as decoupled voxel warping and adopts image-assisted single-stage training for reliable and efficient dynamic scene modeling. For instruction-conditioned control, RoboDreamer [49] decomposes instructions into low-level primitives that steer video diffusion, synthesizing novel compositional scenes beyond the training distribution while grounding execution via an IDM, and ManipDreamer [175] extends this design with an action-tree prior together with depth and semantic guidance to improve instruction following and temporal consistency. On the planning side, 3DFlowAction [176] employs a pretrained 3D optical flow world model to treat future motion as a unified action cue, enabling label-free and cross-robot manipulation through closed-loop optimization. Imagine-2- Drive [177] integrates video diffusion with a multimodal diffusion policy to accelerate policy learning. Drive-WM [88] uses multi-view diffusion with image-based rewards to select safer trajectories, while World4Drive [178] leverages vision-based priors to construct intent-aware world models that support selfsupervised multi-intent imagination. COMBO [179] composes multi-agent actions with diffusion, leverages VLMs to infer purposes, and integrates tree search for online cooperative planning.

# B. General-Purpose World Models

1) Sequential Simulation and Inference: Token Feature Sequence. General-purpose models pretrain task-agnostic dynamics to capture environmental physics and generate future scenes, prioritizing transferability over specific tasks.

Some general world models increasingly pretrain on unlabeled video and use tokenized latent space for robust forecasting and generation. iVideoGPT [56] was pretrained on large-scale interaction videos for action-free forecasting and later adapted to downstream control. Genie [50] learned discrete latent actions and spatiotemporal tokens, enabling user-controllable interactive environments via autoregressive dynamics. RoboScape [180] jointly learned video generation with temporal depth and keypoint dynamics to improve physical realism. PACT [181] tokenized multimodal perception and action and trained a causal Transformer to obtain a unified representation for diverse tasks, and DINO-world [182] learns generalizable dynamics by predicting the temporal evolution of DINOv2 features from large-scale unlabeled video corpora. Building on language priors, EVA [71] introduced a Reflectionof-Generation (RoG) policy that used a VLM for iterative self-correction, strengthening long-horizon anticipation. In the same vein, Owl-1 [183] employs a VLM to forecast world dynamics conditioned on current states and generated fragments, explicitly guiding the subsequent fragments and enabling coherent long-horizon video synthesis, while World4Omni [68] employs a reflective world model, where a VLM refines subgoal images from an image generator, and integrates them with pretrained modules for zero-shot robotic manipulation. Recent work adapts video diffusion models into controllable world models that autoregressively imagine future scenes. AdaWorld [72] introduced an action-aware pretraining scheme that extracted self-supervised latent actions between adjacent frames to condition diffusion, enabling efficient transfer with minimal interaction. Vid2World [184] adapts pretrained video diffusion models into autoregressive interactive world models via causalization and a causal action-guidance mechanism. GenAD [90] employs a two-stage strategy to adapt diffusion into a general video-prediction model conditioned on text and actions, enabling large-scale driving simulation and planning. Pandora [47] uses an instruction-tuned LLM to autoregressively steer a separate video-diffusion generator for explicit, goaldirected control, while Yume [75] quantized camera motions into text tokens to guide a masked Video DiT, enabling autoregressive synthesis of dynamic 3D exploratory worlds.

To maintain geometric fidelity and long-horizon stability, recent methods couple explicit 3D priors with temporalconsistency modules in diffusion-based world models. At the geometric level, Geometry Forcing [185] aligns latent features with a geometric foundation model to inject explicit 3D priors, improving geometric consistency, while DeepVerse [64] integrates visual and geometric prediction targets and introduces a geometry-aware memory to sustain consistent long-horizon generation. For temporal stability, VRAG [186] proposed a Video Retrieval-Augmented Generation (RAG) framework that retrieves historical frames conditioned on a global state to stabilize autoregressive rollouts, StateSpaceDiffuser [63] combines Mamba with diffusion to alleviate long-term memory loss and content drift under short context windows, and InfinityDrive [187] injects memory and adopts an adaptive loss within DiT, producing minute-scale driving videos with high fidelity, temporal consistency, and diverse content. Complementing these designs, LongDWM [188] mitigates error accumulation in long-horizon video generation through distillation—where a fine-grained DiT learns continuous motion to guide a coarse model, whereas MiLA [189] adopts a coarse-to-fine strategy that predicts sparse anchor frames and refines them during interpolation to improve temporal consistency and long-term fidelity. Finally, for dynamics and conditioning, Orbis [190] employs a continuous-space flow-matching formulation that demonstrates greater robustness than discrete-token schemes for long-horizon rollouts, and DriVerse [109] leverages multimodal trajectory prompts with latent motion alignment to synthesize long-horizon driving videos from a single image and navigation trajectories. Sequential world models increasingly act as learned simulators, providing action-conditioned rollouts for policy evaluation and training. WorldGym [191] and WorldEval [192] generate action-conditioned rollouts and use VLM-based critics for evaluation, while WorldEval further leverages latent action representations to drive a DiT-based synthesizer. RLVR-World [62] fine-tunes world models with RL with Verifiable Rewards (RLVR), using explicit metrics to close the pretrainingtask objective gap. For safety risk prediction, Guan et al. [193] presented a framework for autonomous driving accident prediction that augments data with a domain-informed world model and enhances spatiotemporal reasoning using graph and temporal convolutions. Beyond diffusion, sequence models broadened the capacity for long-range consistency. Po et al. [194] integrated block-wise state space models for long-term memory with local attention for short-term coherence, enabling video generation with sustained memory and consistent dynamics. S2-SSM [61] employs a Mamba layer to model the independent evolution of object slots and a sparsity-regularized cross-attention mechanism to capture their causal interactions, enabling causal reasoning over the environment. Spatial Latent Grid. Pretraining geometry-aligned spatial maps with self-supervised spatiotemporal objectives, the spatial latent grid paradigm preserves locality and enables efficient rollouts, multimodal fusion, and transferable planner-ready maps. Building on this paradigm, structured-grid and physicsinformed methods encode geometry and dynamics for controllable rollouts. PhyDNet [195] disentangles physical priors expressed as partial differential equations from visual factors, improving prediction. ViDAR [89] unifies semantics, geometry, and dynamics through a pre-training task of point cloud forecasting and a latent rendering operator, enabling a scalable foundation for downstream autonomous driving tasks. FOLIAGE [196] models dynamics with an Accretive Graph Network and a Transformer-based predictor, executing rollouts on simulated data. Complementing these grid and physics lines, MindJourney [73] couples VLMs with a controllable world model to render egocentric rollouts along planned camera trajectories, enabling multi-view reasoning.

Building on grid-based representations, diffusion-based forecasting has become dominant for stable long-horizon generation. Within grid-centric predictors, DOME [94] encodes observations into a continuous latent space and applies a spatiotemporal DiT for scene forecasting, Copilot4D [82] tokenizes point clouds and couples a spatiotemporal Transformer with discrete diffusion to improve fidelity and coherence, and LidarDM [100] generates layout-conditioned static scenes, composes them with dynamic objects, integrating LiDAR simulation to produce controllable videos. For long-video generation, Vista [96] adopts a two-stage large-scale training regime to produce controllable, high-fidelity, driving videos, and Delphi [86] enforces long-horizon multiview consistency through shared noise and feature alignment and a failure-driven framework to synthesize targeted data for improved planning. To strengthen long-horizon stability, GEM [102] achieves controllable egovision generation through large-scale training and fine-grained control over motion, dynamics, and posture, Zhou et al. [197] maintains a persistent RGB-D 3D memory map to guide subsequent frames, and STAGE [198] introduced hierarchical temporal feature transfer with multi-stage training. Decomposed Rendering Representation. Scenes are decomposed into explicit primitives to synthesize view-consistent, simulatable trajectories over long horizons. Within this paradigm, GaussianWorld [103] models scene evolution as ego-motion, object dynamics, and newly observed regions, iteratively updating 3D Gaussian primitives to enable accurate and efficient dynamic perception. InfiniCube [22] introduced a hybrid pipeline that combines voxel-based generation, video synthesis, and dynamic Gaussian reconstruction, enabling largescale dynamic 3D driving scenes conditioned on HDmaps, bounding boxes, and text. Complementarily, Wu et al. [199] augmented a video world model with long-term spatial memory grounded in reconstructed geometry and an episodic memory, which together condition sequential generation for long-range consistency. 2) Global Difference Prediction: Token Feature Sequence. For general-purpose world models, tokenized feature sequences support global prediction via masked and generative modeling, enabling parallel long-horizon rollouts with global constraints and multimodal conditioning. Within Joint-Embedding Predictive Architecture (JEPA) [200], V-JEPA [51] extends this architecture to video by predicting latent features of occluded spatiotemporal regions, learning generalizable representations for both appearance and motion without pixel reconstruction or contrastive learning. Building on this, V-JEPA 2 [14] scales pretraining to large-scale Internet videos with larger models and incorporates limited robot interaction data for post-training, transferring to robotic planning. AD-L-JEPA [201] adapts JEPA to BEV LiDAR, predicting masked embeddings in a self-supervised manner. Beyond JEPA-style prediction, WorldDreamer [45] frames world modeling as masked visual sequence prediction to learn physics and motion for diverse video generation and editing, while MaskGWM [104] combines diffusion with masked feature reconstruction and a dual-branch masking strategy to improve long-horizon consistency and generalization.

In parallel, diffusion-based methods have become central to global-difference modeling. Sora [13] represents video as unified spacetime patches and uses a DiT to generate long, coherent sequences at scale. ForeDiff [202] decouples conditioning from denoising by adding a deterministic predictive stream and employing a pretrained predictor to guide generation, improving accuracy and consistency. For domain-specific synthesis, AirScape [203] introduces an aerial videointention dataset, applies supervised fine-tuning for controllability, and leverages VLMs to impose spatiotemporal constraints; MarsGen [204] builds a multimodal Mars dataset from NASA's sparse rover stereo imagery using geometric foundation models, then trains a controllable generator to produce visually realistic, geometry-consistent Martian videos. In clinical guidance, EchoWorld [205] proposes a motion-aware world model for echocardiography probe control, pretraining on region- and motion-outcome prediction and fine-tuning attention to fuse visual and motion cues for precise guidance. Spatial Latent Grid. Spatial-grid models forecast voxel grids in parallel and fuse multi-view visual features into a unified map, learning a general-purpose world model.

Recent work converges on unified scene understanding and future prediction. UniFuture [206] couples Dual Latent Sharing with multi-scale latent interaction to jointly model appearance and depth in future driving scenes, and HERMES [108] integrates multiview BEV features into an LLM with world queries that link scene understanding to future prediction within a single framework. BEVWorld [207] maps images and LiDAR into a compact BEV latent space through a unified tokenizer and applies a latent BEV diffusion model for synchronized multimodal forecasting. Progress in grid and occupancy forecasting includes differentiable raycasting with a proxy reformulation for sensor agnostic motion learning by Khurana et al. [208], [209] and LiDAR to range images 3D spatiotemporal convolutions by Mersch et al. [210]. Cam4DOcc [211] established the first vision-only benchmark with an E2E 3D CNN baseline, and Liu et al. [212] enhanced cross-task transfer through high-ratio compression and latent flow matching.

On the generative front, tokenized 4D representations enable controllable scene synthesis. OccSora [85] uses a 4D tokenizer to derive compact representations for trajectory conditioned diffusion, and DynamicCity [99] encodes 4D occupancy as HexPlanes representations with a VAE and employs a conditional DiT for high fidelity controllable dynamics. Fidelity and consistency improve through decoupling ego-motion with scene evolution in COME [213], physics-informed constraints in DrivePhysica [214], cross-view point map alignment in Liu et al. [215], and photometric warping-based supervision in PosePilot [216]. For controllable conditioning, DriveDreamer 2 [97] translates prompts into agent trajectories and HDMaps for customizable video generation, EOT-WM [217] encodes ego and surrounding trajectories as trajectory videos for trajectory consistent synthesis, and ORV [65] uses 4D semantic occupancy sequences to guide action conditioned video with S2R transfer. AETHER [77] unifies dynamic 4D reconstruction, action-conditioned video prediction, and vision-based planning under training on synthetic 4D data and achieves zero-shot generalization to real-world scenarios. Decomposed Rendering Representation. This paradigm performs global prediction by combining explicit 3D structure with video generative priors. A trend is combining video generation with Gaussian Splatting. DriveDreamer4D [105] exploits complex driving trajectories such as lane changes to guide video synthesis and optimize a 4DGS model, which enhances reconstruction fidelity and spatiotemporal coherence from novel viewpoints. ReconDreamer [106] introduces an online restoration module together with progressive data reuse to correct artifacts in Gaussian-rendered views and enables reliable reconstruction of large-scale trajectories. MagicDrive3D [84] generates multiview street scenes conditioned on BEV maps, 3D boxes, and text, and further converts the outputs into full 3D environments through fault-tolerant GS. In contrast, implicit-field methods replace GS with continuous neural representations. UnO [17] leverages future point clouds to learn a NeRF-style 4D occupancy field, which allows annotation-free prediction and achieves strong transfer performance beyond supervised baselines in point-cloud forecasting.

# IV. DatA ResourceS & Metrics

World models in embodied AI are required to address diverse tasks spanning manipulation, navigation, and autonomous driving, requiring heterogeneous resources and rigorous evaluation. Accordingly, we present Data Resources in $\ S _ { \mathrm { I V - A } }$ and Metrics in $\ S _ { \mathrm { I V - B } }$ , focusing on the most widely adopted platforms and evaluation measures as a unified foundation for cross-domain assessment.

# A. Data Resources

To meet the diverse demands of embodied AI, we categorize data resources into four categories: Simulation Platforms, Interactive Benchmarks, Offline Datasets, and Real-world Robot Platforms, as detailed in the following subsections. Tab. III provides a comprehensive overview of these resources. 1) Simulation Platforms: Simulation platforms provide controllable and scalable virtual environments for training and evaluating world models. MuJoCo [218] is a customizable physics engine widely adopted for its efficient robotic simulation of articulated systems and contact dynamics in robotics and control research. • NVIDIA Isaac is an E2E, GPU-accelerated simulation stack that comprises Isaac Sim, Isaac Gym [221], and Isaac Lab [222]. It offers photorealistic rendering and large-scale RL capabilities. • CARLA [219] is an open-source simulator based on Unreal Engine for urban autonomous driving, providing realistic rendering, diverse sensors, and closed-loop evaluation protocols.   
Habitat [220] is a high-performance simulator for embodied AI, specializing in photorealistic 3D indoor navigation. 2) Interactive Benchmarks: Interactive benchmarks offer   
standardized task suites and protocols for reproducible closed  
loop evaluation of world models. • DeepMind Control (DMC) [224] is a standard MuJoCobased suite for control tasks, offering a consistent basis for comparing agents that learn from state or pixel-based observations.   
Atari [223] is a suite of pixel-based, discrete-action games for evaluating agent performance. The Atari100k [239] specifically assesses sample efficiency by limiting interaction to 100k steps. •Meta-World [225] is a benchmark for multi-task and metaRL, featuring 50 diverse robotic manipulation tasks with a Sawyer arm in MuJoCo under standardized evaluation protocols.   
RLBench [226] offers 100 simulated tabletop manipulation tasks with sparse rewards and rich, multi-modal observations, designed to test complex skills and rapid adaptation. •LIBERO [228] is a benchmark for lifelong robotic manipulation, providing 130 procedurally generated tasks and human demonstrations to evaluate sample-efficient and continual learning. • nuPlan [227] is a planning benchmark for autonomous driving, using a lightweight closed-loop simulator and over $1 5 0 0 \mathrm { h }$ of real-world driving logs to evaluate long-horizon performance. 3) Offline Datasets: Offine datasets are large-scale, pre  
collected trajectories that eliminate interactive rollouts and   
provide a foundation for reproducible evaluation and data  
efficient pretraining of world models. • RT-1 [233] is a real-world dataset for robot learning, collected over 17 months with 13 Everyday Robots mobile manipulators. It contains 130 000 demonstrations spanning more than 700 tasks, pairing language instructions and image observations with discretized 11-DoF actions for the arm and mobile base. Open X-Embodiment (OXE) [235] is a corpus aggregating 60 sources from 21 institutions, spanning 22 robot embodiments, 527 skills, and over one million trajectories in a unified format for cross-embodiment training. Models trained on OXE demonstrate strong transfer beyond single-robot baselines, underscoring the effectiveness of cross-platform data sharing. TABLE III AN OVERVIEW OF DATA RESOURCES FOR TRAINING AND EVALUATING EMBODIED WORLD MODELS.   

<table><tr><td>Category</td><td>Name</td><td>Year</td><td>Task</td><td>Input</td><td>Domain</td><td>Scale</td><td>Protocol1</td></tr><tr><td rowspan="6">Ban</td><td>MuJoCo [218]</td><td>2012</td><td>Continuous control</td><td>Proprio.</td><td>Sim</td><td>-</td><td>-</td></tr><tr><td>CARLA [219]</td><td>2017</td><td>Driving simulation</td><td>RGB-D/Seg/LiDAR/Radar/GPS/IMU</td><td>Sim</td><td></td><td>✓</td></tr><tr><td>Habitat [220]</td><td>2019</td><td>Embodied navigation</td><td>RGB-D/Seg/GPS/Compass</td><td>Sim</td><td></td><td>✓</td></tr><tr><td>Isaac Gym [221]</td><td>2021</td><td>continuous control</td><td>Proprio.</td><td>Sim</td><td></td><td>-</td></tr><tr><td>Isaac Lab [222]</td><td>2023</td><td>Robot learning suites</td><td>RGB-D/Seg/LiDAR/Proprio.</td><td>Sim</td><td>-</td><td></td></tr><tr><td>Atari [223]</td><td>2013</td><td>Discrete-action game</td><td>RGB/State</td><td>Sim</td><td>55+ Games</td><td>✓</td></tr><tr><td rowspan="6">2</td><td>DMC [224]</td><td>2018</td><td>Continuous control</td><td>RGB/Proprio.</td><td>Sim</td><td>30+ Tasks</td><td>✓</td></tr><tr><td>Meta-World [225]</td><td>2019</td><td>Multi-task manipulation</td><td>RGB/Proprio.</td><td>Sim</td><td>50 tasks</td><td></td></tr><tr><td>RLBench [226]</td><td>2020</td><td>Robotic manipulation</td><td>RGB-D/Seg/Proprio.</td><td>Sim</td><td>100 tasks</td><td>✓</td></tr><tr><td>nuPlan [227]</td><td>2021</td><td>Driving planning</td><td>RGB/LiDAR/Map/Proprio.</td><td>Real</td><td>1.5k hours</td><td>✓</td></tr><tr><td>LIBERO [228]</td><td>2023</td><td>Lifelong manipulation</td><td>RGB/Text/Proprio.</td><td>Sim</td><td>130 tasks</td><td>✓</td></tr><tr><td>SSv2 [229]</td><td>2018</td><td>Video-action understanding</td><td>RGB/Text</td><td>Real</td><td>220k videos</td><td>169k/24k/27k</td></tr><tr><td rowspan="10">2</td><td>nuScenes [230]</td><td>2020</td><td>Driving perception</td><td>RGB/LiDAR/Radar/GPS/IMU</td><td>Real</td><td>1k scenes</td><td>700/150/150</td></tr><tr><td>Waymo [231]</td><td>2020</td><td>Driving perception</td><td>RGB/LiDAR</td><td>Real</td><td>1.15k scenes</td><td>798/202/150</td></tr><tr><td>HM3D [232]</td><td>2021</td><td>Indoor navigation</td><td>RGB-D</td><td>Real</td><td>1k scenes</td><td>800/100/100</td></tr><tr><td>RT-1 [233]</td><td>2022</td><td>Real-robot manipulation</td><td>RGB/Text</td><td>Real</td><td>130k+ trajectories</td><td></td></tr><tr><td>Occ3D [234]</td><td>2023</td><td>3D occupancy</td><td>RGB/LiDAR</td><td>Real</td><td>1.9k scenes</td><td>600/150/150; 798/202/-</td></tr><tr><td>OXE [235]</td><td>2024</td><td>Cross-embodiment pretraining</td><td>RGB-D/LiDAR/Text</td><td>Real</td><td>1M+ trajectories</td><td></td></tr><tr><td>OpenDV [90]</td><td>2024</td><td>Driving video pretraining</td><td>RGB/Text</td><td>Real</td><td>2k+ hours</td><td></td></tr><tr><td>VideoMix22M [14]</td><td>2025</td><td>Video pretraining</td><td>RGB</td><td>Real</td><td>22M+ samples</td><td></td></tr><tr><td>Franka Emika [236]</td><td>2022</td><td>Manipulation</td><td>Proprio.</td><td>Real</td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>20</td><td>Unitree Go1 [237] Unitree G1 [238]</td><td>2021 2024</td><td>Quadruped locomotion Humanoid manipulation</td><td>RGB-D/LiDAR/Proprio. RGB-D/LiDAR/Proprio./Audio</td><td>Real Real</td><td></td><td></td></tr></table>

Protocol: For interactive benchmarks, a check mark $( \checkmark )$ indicates available evaluation protocols. For datasets, it indicates official data splits are provided. • Habitat-Matterport 3D (HM3D) [232] is a large-scale dataset of 1 000 indoor reconstructions with $1 1 2 5 0 0 \mathrm { m } ^ { 2 }$ navigable area, substantially expanding the scope and diversity of embodied AI simulation. Released for the Habitat platform, it provides the necessary metadata and resources for seamless use. •nuScenes [230] is a large-scale multimodal driving dataset with a 360-degree sensor suite comprising six cameras, five radars, one LiDAR, and GPS/IMU. It contains 1 000 twentysecond scenes collected in Boston and Singapore with dense 3D annotations for 23 classes and HDMaps, providing a core benchmark for multimodal fusion and long-horizon prediction. • Waymo [231] is a multimodal autonomous driving benchmark with 1 150 twenty-second scenes at $1 0 \mathrm { H z }$ from San Francisco, Phoenix, and Mountain View. It includes five LiDARs and five cameras, with about 12 million 3D and 2D annotations, making it a large-scale resource for modeling traffic dynamics. •Occ3D [234] defines 3D occupancy prediction from surroundview images, providing voxel labels that distinguish free, occupied, and unobserved states. Occ3D-nuScenes contains about 40000 frames at $0 . 4 \mathrm { m }$ resolution, while Occ3DWaymo offers about 200 000 frames at $0 . 0 5 \mathrm { m }$ This voxellevel supervision enables holistic scene understanding beyond bounding boxes. • Something-Something v2 (SSv2) [229] is a video dataset for fine-grained action understanding. It contains 220 847 clips across 174 categories, collected from crowd workers following textual prompts (e.g., Putting something into something) with splits of 168 913 train, 24777 val, and 27 157 test videos. •OpenDV [90] is the largest large-scale video-text dataset for autonomous driving, proposed by GenAD, which supports video prediction and world-model pretraining. It contains 2059 hours and 65.1 million frames from YouTube and seven public datasets, covering over 40 countries and 244 cities. The dataset provides command and context annotations to enable language- and action-conditioned prediction and planning. • VideoMix22M [14] is a large-scale dataset introduced with V-JEPA 2 for self-supervised pretraining. It scales from 2 million to 22 million samples, drawn from YT-Temporal1B [240], HowTo100M [241], Kinetics [242], SSv2, and ImageNet [243]. The largest source, YT-Temporal-1B, is curated with retrieval-based filtering to suppress noise, while ImageNet images are converted into static video clips for consistency. 4) Real-world Robot Platforms: Real-world robot platforms provide physical embodiments for interaction, enabling closedloop evaluation, high-fidelity data collection, and S2R validation under real-world constraints. •Franka Emika [236] is a 7-DoF collaborative robot arm with joint torque sensors for precise force control. Through the control interface, it supports $1 \mathrm { k H z }$ torque control for contact-rich tasks, while its ROS integration makes it a versatile platform. •Unitree Go1 [237] is a cost-effective and widely adopted quadrupedal robot equipped with a panoramic depth-sensing suite, onboard computing of 1.5 TFLOPS, and a maximum speed of $4 . 7 \mathrm { m } / \mathrm { s }$ , establishing it as a standard platform for locomotion and embodied-AI research. Unitree G1 [238] is a compact humanoid robot for research, offering up to 43-DoF and knee torques of $1 2 0 ~ \mathrm { N \cdot m }$ ,with integrated 3D LiDAR and depth cameras. With multimodal sensing, onboard compute, ROS support, and swappable batteries, this low-cost platform provides a practical realrobot testbed for training and evaluating embodied world models.

# B. Metrics

Metrics evaluate the capability of world models to capture dynamics, generalize to unseen scenarios, and scale with additional resources. We organize them into three abstraction levels: §IV-B1 Pixel Prediction Quality, $\ S _ { \mathrm { I V - B } 2 }$ State-level Understanding, and §IV-B3 Task Performance, representing a progression from low-level signal fidelity to high-level goal attainment. 1) Pixel Generation Quality: At the most fundamental level, world models are evaluated by their ability to reconstruct sensory inputs and generate realistic sequences. Metrics assess image fidelity, temporal consistency, and perceptual similarity, providing quantitative measures of the extent to which models capture raw environmental dynamics. Fréchet Inception Distance (FID) [244]. FID is a metric for assessing the realism and diversity of generated images. It compares real and generated image distributions in the feature space of an ImageNet-pretrained Inception-v3 [245], modeling embeddings as Gaussians with means $\mu _ { x } , \mu _ { y }$ and covariances $\Sigma _ { x } , \Sigma _ { y }$ . Defined as a lower FID denotes a closer alignment between real and generated distributions. By comparing the first and second moments, it penalizes fidelity loss (mean shift) and mode collapse (covariance mismatch), offering a holistic measure of generative performance.

$$
\begin{array} { r } { \mathrm { F I D } ( x , y ) = \| \pmb { \mu _ { x } } - \pmb { \mu _ { y } } \| _ { 2 } ^ { 2 } + \operatorname { T r } \left( \pmb { \Sigma _ { x } } + \pmb { \Sigma _ { y } } - 2 ( \pmb { \Sigma _ { x } } \pmb { \Sigma _ { y } } ) ^ { 1 / 2 } \right) , } \end{array}
$$

Fréchet Video Distance (FVD) [246]. FVD extends FID to videos, evaluating both per-frame quality and temporal consistency. It replaces the image-based Inception network with an I3D [247] pretrained on Kinetics-400 [248]. Using the same Fréchet distance as Eq. (6) on motion-aware features, FVD yields a holistic video quality score. A lower value indicates a closer alignment of distributions in appearance and dynamics while penalizing temporal artifacts like unnatural motion or flickering. Structural Similarity Index Measure (SSIM) [249]. SSIM is a perceptual metric for image quality that compares luminance, contrast, and structure between a generated image and its reference. For two patches $x$ and $y$ with means $\mu _ { x } , \mu _ { y }$ variances $\Sigma _ { x } ^ { 2 } , \Sigma _ { y } ^ { 2 }$ , and covariance $\Sigma _ { x y }$ , SSIM is defined as

$$
\mathrm { S S I M } ( x , y ) = \frac { ( 2 \pmb { \mu } _ { x } \pmb { \mu } _ { y } + C _ { 1 } ) ( 2 \pmb { \Sigma } _ { x y } + C _ { 2 } ) } { ( \pmb { \mu } _ { x } ^ { 2 } + \pmb { \mu } _ { y } ^ { 2 } + C _ { 1 } ) ( \pmb { \Sigma } _ { x } ^ { 2 } + \pmb { \Sigma } _ { y } ^ { 2 } + C _ { 2 } ) } .
$$

The final score is obtained by averaging SSIM over sliding windows, and values closer to 1 indicate higher similarity. Peak Signal-to-Noise Ratio (PSNR) [250]. PSNR measures pixel-wise distortion between a reconstruction and its reference. Let the mean-squared error (MSE) over $N$ pixels be and let MAX denote the maximum possible pixel value(e.g., 255 for RGB or 1 for normalized images). Then

$$
\mathrm { M S E } = \frac { 1 } { N } \sum _ { i = 1 } ^ { N } \left( x _ { i } - y _ { i } \right) ^ { 2 } ,
$$

$$
\mathrm { P S N R } ( x , y ) = 1 0 \cdot \log _ { 1 0 } \left( \frac { \mathrm { M A X } ^ { 2 } } { \mathrm { M S E } } \right) .
$$

Higher PSNR values indicate lower distortion and greater fidelity. Learned Perceptual Image Patch Similarity (LPIPS) [251]. LPIPS is a metric that correlates with human judgments by $\tilde { f } _ { x } ^ { l }$ and $\hat { f } _ { y } ^ { l }$ denote the unit-normalized activations at layer $l$ for inputs $x$ and $y$ ,and $w _ { l }$ the channel-wise weights. LPIPS is defined as

$$
\mathrm { L P I P S } ( x , y ) = \sum _ { l } \frac { 1 } { H _ { l } W _ { l } } \sum _ { h , w } \left\| w _ { l } \odot \left( \hat { f } _ { h , w , x } ^ { l } - \hat { f } _ { h , w , y } ^ { l } \right) \right\| _ { 2 } ^ { 2 } .
$$

Lower LPIPS values imply greater similarity, offering enhanced fidelity compared to pixel-based metrics and robustness against distortions. VBench [252]. VBench is a comprehensive benchmark for video generation that assesses performance across 16 dimensions grouped into two categories: Video Quality (e.g., subject consistency, motion smoothness) and Video-Condition Consistency (e.g., object class, human action). It provides carefully curated prompt suites and large-scale human preference annotations to ensure strong perceptual alignment, thereby enabling fine-grained evaluation of model capabilities and limitations. 2) State-level Understanding: Beyond pixel fidelity, statelevel understanding assesses whether models capture objects, layouts, and semantics, and can predict their evolution. Metrics span semantic, BEV, and 3D segmentation, detection, occupancy, geometry, and trajectory accuracy, emphasizing structural understanding beyond appearance. mean Intersection over Union (mIoU). mIoU evaluates semantic segmentation by averaging the Intersection over Union (IoU) across classes. For class $c$ , where TP, FP, and FN denote the true positives, false positives, and false negatives. IoU quantifies overlap with the ground truth while penalizing segmentation errors. The dataset-level score is

$$
\mathrm { I o U } = \frac { \mathrm { T P } } { \mathrm { T P } + \mathrm { F P } + \mathrm { F N } } ,
$$

$$
\mathrm { m I o U } = { \frac { 1 } { | C | } } \sum _ { c \in C } \mathrm { I o U } _ { c } .
$$

A higher mIoU reflects more precise semantic scene understanding. mean Average Precision (mAP). mAP evaluates detection and instance segmentation by averaging per-class Average Precision (AP). For a class $c$ at IoU threshold $\tau$ ,predictions are ranked by confidence and matched one-to-one with ground truths when $\mathrm { I o U } \geq \tau$ with unmatched predictions counted as FP and unmatched ground truths as FN. Precision and recall are

$$
\mathrm { P r e c i s i o n } = \frac { \mathrm { T P } } { \mathrm { T P } + \mathrm { F P } } , \quad \mathrm { R e c a l l } = \frac { \mathrm { T P } } { \mathrm { T P } + \mathrm { F N } } .
$$

Let $P _ { c , \tau } ( r )$ denote the precision-recall envelope obtained via monotonic interpolation. The AP for class $c$ at threshold $\tau$ is mAP averages AP across classes and thresholds $T$ :

$$
\mathrm { A P } _ { c , \tau } = \int _ { 0 } ^ { 1 } P _ { c , \tau } ( r ) \mathrm { d } r .
$$

$$
\mathrm { m A P } = { \frac { 1 } { | C | } } \sum _ { c \in C } \left( { \frac { 1 } { | T | } } \sum _ { \tau \in T } { \mathrm { A P } } _ { c , \tau } \right) .
$$

A higher mAP indicates better instance recognition, more accurate localization, and more calibrated confidence estimates. Displacement Error. Displacement error metrics assess statelevel understanding by measuring spatial accuracy for keypoints, object centers, and trajectory waypoints. The L2 trajectory error computes the Euclidean distance between predicted and ground-truth waypoints. Common variants include Average Displacement Error (ADE), which calculates the average displacement, and Final Displacement Error (FDE), which measures the displacement at the final step. Lower values indicate more accurate localization. Chamfer Distance (CD) [253]. CD quantifies geometric similarity between a prediction $S _ { 1 }$ and ground truth $S _ { 2 }$ by summing squared nearest-neighbor distances across the two sets:

$$
\mathrm { C D } ( S _ { 1 } , S _ { 2 } ) = \sum _ { x \in S _ { 1 } } \operatorname* { m i n } _ { y \in S _ { 2 } } \left\| x - y \right\| _ { 2 } ^ { 2 } + \sum _ { y \in S _ { 2 } } \operatorname* { m i n } _ { x \in S _ { 1 } } \left\| x - y \right\| _ { 2 } ^ { 2 } .
$$

Unlike pixel-level metrics, CD captures surfaces, occupancy, BEV, and 3D structures, and its differentiability enables use as both a training loss and an evaluation metric that complements IoU. 3) Task Performance: Ultimately, the value of a world model lies in supporting effective decision-making, with task-level metrics assessing goal achievement under safety and efficiency constraints in embodied settings. Success Rate (SR). SR quantifies performance as the fraction of evaluation episodes that satisfy a predefined success condition. In navigation and manipulation, the condition is typically binary, such as reaching a target or placing an object correctly. In autonomous driving, the requirement is stricter, demanding route completion without collisions or major violations. The final SR is reported as the average of binary outcomes across all test episodes. Sample Efficiency (SE). SE quantifies the samples needed to reach target performance. It is evaluated by fixed-budget benchmarks (e.g., Atari-100k), dataperformance curves, or in robotics by the demonstrations required to achieve a given success rate. TABLE IV PERFORMANCE COMPARISON OF VIDEO GENERATION ON THE NUSCENES.   

<table><tr><td>Method</td><td>Pub.</td><td>Resolution</td><td>FID↓</td><td>FVD↓</td></tr><tr><td>MagicDrive3D [84]</td><td>arXiv&#x27;24</td><td>224 × 400</td><td>20.7</td><td>164.7</td></tr><tr><td>Delphi [86]</td><td>arXiv&#x27;24</td><td>512 × 512</td><td>15.1</td><td>113.5</td></tr><tr><td>Drive-WM [88]</td><td>CVPR&#x27;24</td><td>192 × 384</td><td>15.8</td><td>122.7</td></tr><tr><td>GenAD [90]</td><td>CVPR&#x27;24</td><td>256 × 448</td><td>15.4</td><td>184.0</td></tr><tr><td>DriveDreamer [91]</td><td>ECCV&#x27;24</td><td>128 × 192</td><td>52.6</td><td>452.0</td></tr><tr><td>Vista [96]</td><td>NeurIPS&#x27;24</td><td>576 × 1024</td><td>6.9</td><td>89.4</td></tr><tr><td>DrivePhysica [214]</td><td>arXiv&#x27;24</td><td>256 × 448</td><td>4.0</td><td>38.1</td></tr><tr><td>DrivingWorld [133]</td><td>arXiv&#x27;24</td><td>512 × 1024</td><td>7.4</td><td>90.9</td></tr><tr><td>DriveDreamer-2 [97]</td><td>AAAI&#x27;25</td><td>256 × 448</td><td>11.2</td><td>55.7</td></tr><tr><td>UniFuture [206]</td><td>arXiv&#x27;25</td><td>320 × 576</td><td>11.8</td><td>99.9</td></tr><tr><td>MiLA [189]</td><td>arXiv&#x27;25</td><td>360 × 640</td><td>4.1</td><td>14.9</td></tr><tr><td>GeoDrive [170]</td><td>arXiv&#x27;25</td><td>480 × 720</td><td>4.1</td><td>61.6</td></tr><tr><td>LongDWM [188]</td><td>arXiv&#x27;25</td><td>480 × 720</td><td>12.3</td><td>102.9</td></tr><tr><td>MaskGWM [104]</td><td>CVPR&#x27;25</td><td>288 × 512</td><td>8.9</td><td>65.4</td></tr><tr><td>GEM [102]</td><td>CVPR&#x27;25</td><td>576 × 1024</td><td>10.5</td><td>158.5</td></tr><tr><td>Epona [148]</td><td>ICCV&#x27;25</td><td>512 × 1024</td><td>7.5</td><td>82.8</td></tr><tr><td>STAGE [198]</td><td>IROS&#x27;25</td><td>512 × 768</td><td>11.0</td><td>242.8</td></tr><tr><td>DriVerse [109]</td><td>ACMMM&#x27;25</td><td>480 × 832</td><td>18.2</td><td>95.2</td></tr></table>

Reward. In RL, the reward is a signal $r _ { t }$ at timestep $t$ $G _ { t } \ =$ $\scriptstyle \sum _ { k = 0 } ^ { \infty } \gamma ^ { k } r _ { t + k + 1 }$   
or average return, often with normalization for cross-task comparison. Collision. Safety is evaluated with collision-based metrics. The primary measure, collision rate, is the proportion of evaluation episodes with at least one collision and is common in indoor navigation. In autonomous driving, exposure-normalized variants are used, such as collisions per kilometer or collisions per hour.

# V. PERFORMancE COMPaRIsoN

Given the proliferation of world-model variants and heterogeneous metrics, we organize comparisons by task objectives and rely on standardized benchmarks, reporting concise tables that highlight each method's strengths and limitations.

# A. Pixel Generation

Generation on nuScenes. Driving video generation is treated as a world-modeling task that synthesizes plausible scene dynamics in fixed-length clips. Typical protocols produce short sequences and evaluate quality with $F I D$ for appearance fidelity and $F V D$ for temporal consistency. For a fair comparison on the nuScenes validation split, recent approaches have achieved remarkable progress, as shown in Tab. IV. DrivePhysica delivers the best visual fidelity, while MiLA achieves the strongest temporal coherence, together establishing new state-of-the-art performance.

# B. Scene Understanding

4D Occupancy Forecasting on Occ3D-nuScenes. 4D occupancy forecasting is treated as a representative world modeling task. Given $2 \mathrm { s }$ of past 3D occupancy, models predict the subsequent $3 \mathrm { s }$ of scene dynamics. Evaluation follows the Occ3D-nuScenes protocol and reports mIoU and per horizon IoU. As summarized in Tab. V, we compare methods by input modality, auxiliary supervision, and ego trajectory usage to reveal design choices for spatiotemporal forecasting. Methods using occupancy inputs outperform camera-only variants, and adding auxiliary supervision with a GT ego trajectory further mitigates performance decay at $_ { 2 - 3 \mathrm { ~ s ~ } }$ Among all methods, COME (with GT ego) achieves the best average mIoU and per-horizon IoU.

# C. Control Tasks

Evaluation on DMC. Most studies probe the capacity of world models to learn control-relevant dynamics, typically adopting a pixel-based setting with $6 4 \times 6 4 \times 3$ observations. The primary metric is Episode Return, defined as the cumulative reward over $1 0 0 0$ steps, with a theoretical maximum of 1 000 given $r _ { t } \in [ 0 , 1 ]$ . For comparability, Tab. VI reports the step budget and summarizes performance by task score and task count. The results indicate improved data efficiency, with recent models reaching strong performance in far fewer training steps. However, inconsistent evaluation protocols and task subsets impede a fair assessment of generalization, and building a broadly transferable model across tasks, modalities, and datasets remains an open challenge. Evaluation on RLBench. RLBench evaluates manipulation with a 7-DoF simulated Franka arm and is widely used to assess whether world models capture task-relevant dynamics and support conditioned action generation. The primary metric is Success Rate, defined as the fraction of episodes that reach the goal within the step limit. As summarized in Tab. VII, implementations differ in episode budgets, resolution, and modalities, which complicates like-for-like comparison. Despite this heterogeneity, several trends are evident. Recent methods increasingly leverage multimodal inputs and adopt stronger backbones such as 3DGS and DiT. VidMan achieves a high average success rate on the broadest task, revealing IDM as a promising architectural direction. Planning on nuScenes. Open-loop planning is treated as a world modeling task on the nuScenes validation split, where models predict ego motion from a limited history. Methods observe $2 \mathrm { s }$ of past trajectories and forecast the next $3 \mathrm { s }$ as 2D BEV waypoints. Evaluation reports $L 2$ error and collision rate at multiple horizons, and Tab. VIII summarizes results by input modality, auxiliary supervision, and metric settings. Under this shared protocol, a clear tradeoff emerges. UniAD $^ +$ DriveWorld achieves the lowest $L 2$ with extensive auxiliary supervision, whereas SSR attains the best collision rate with competitive $L 2$ without extra supervision. Camera-based methods now surpass models that use privileged occupancy, reflecting the growing maturity of E2E planning.

# VI. CHALLENgES AND TrENdS

This section reviews the open challenges and emerging directions for world models in embodied AI. We discuss them across three dimensions: $\ S \nabla \mathrm { I }$ -A Data & Evaluation, $\ S \ V \mathbf { I - B }$ Computational Efficiency, and $\ S \mathrm { V I - C }$ Modeling Strategies.

# A. Data & Evaluation

Challenges. From a data perspective, the central challenge lies in the scarcity and heterogeneity of existing corpora. Although embodied AI spans diverse domains such as navigation, manipulation, and autonomous driving, a unified large-scale dataset remains lacking. This fragmentation constrains the capacity of world models and substantially hinders their ability to generalize. Evaluation practices face similar limitations. Metrics such as FID and FVD emphasize pixel fidelity while ignoring physical consistency, dynamics, and causality. Recent benchmarks, such as EWM-Bench [255], introduce new measures but remain task-specific and lack cross-domain standards. Future Directions. Recent initiatives such as OpenDV. 2K [90] and VideoMix22M [14] highlight the growing focus on large-scale pretraining and broader modality coverage, yet resources remain fragmented and domain specific. Future work should prioritize constructing unified multimodal, cross-domain datasets to enable transferable pretraining, while advancing evaluation frameworks that move beyond perceptual realism to assess physical consistency, causal reasoning, and long-horizon dynamics.

# B. Computational Efficiency

Challenges. Embodied AI tasks encounter significant challenges in computational efficiency, particularly in realtime applications. Although models such as Transformers and Diffusion networks exhibit strong performance, their high inference costs conflict with the real-time control demands of robotic systems. Consequently, traditional methods like RNNs and global latent vectors remain widely employed, as they offer greater computational efficiency, despite limitations in capturing long-term dependencies. Future Directions. To address this challenge, future research should focus on optimizing model architectures using techniques like quantization, pruning, and sparse computation to reduce inference latency without compromising performance. Additionally, exploring novel temporal methods, such as SSMs, could enhance long-range reasoning while maintaining realtime efficiency, offering a promising solution for robotic systems.

# C. Modeling Strategy

Challenges. Despite rapid progress, world models still struggle with long-horizon temporal dynamics and efficient spatial representations. The main difficulty lies in balancing recurrent simulation and global prediction: autoregressive designs are compact and sample-efficient but accumulate errors over time, whereas global prediction improves multi-step coherence at the cost of heavy computation and weaker closedloop interactivity. On the spatial side, efficiency remains a bottleneck. Latent vectors, token sequences, and spatial grids each present trade-offs between efficiency and expressiveness, while decomposed rendering approaches (e.g., NeRF and 3DGS) offer high fidelity yet scale poorly in dynamic scenes. Taken into closed-loop control remains challenging. Furthermore, a promising approach lies in integrating the strengths of both autoregressive and global prediction methods. Explicit memory or hierarchical planning can enhance long-horizon prediction stability, while task decomposition inspired by CoT can improve temporal consistency through intermediate goal setting. Future frameworks should prioritize optimizing long-range reasoning, computational efficiency, and generative fidelity, while seamlessly integrating temporal and spatial modeling into unified architectures that strike an effective balance between efficiency, fidelity, and interactivity. TABLE V PERFORMANCE cOMPARISON OF 4D OCCUPANCY FORECASTING ON THE OCC3D-NUSCENES BENCHMARK1.   

<table><tr><td rowspan="2">Method</td><td rowspan="2">Input</td><td rowspan="2">Aux. Sup</td><td rowspan="2">Ego traj.</td><td colspan="5">mIoU (%) ↑</td><td colspan="5">IoU (%) ↑</td></tr><tr><td>Recon.</td><td>1s</td><td>2s</td><td>3s</td><td>Avg.</td><td>Recon.</td><td>1s</td><td>2s</td><td>3s</td><td>Avg.</td></tr><tr><td>Copy &amp; Paste2</td><td>Occ</td><td>None</td><td>Pred.</td><td>66.38</td><td>14.91</td><td>10.54</td><td>8.52</td><td>11.33</td><td>62.29</td><td>24.47</td><td>19.77</td><td>17.31</td><td>20.52</td></tr><tr><td>OccWorld-O [93]</td><td>Occ</td><td>None</td><td>Pred.</td><td>66.38</td><td>25.78</td><td>15.14</td><td>10.51</td><td>17.14</td><td>62.29</td><td>34.63</td><td>25.07</td><td>20.18</td><td>26.63</td></tr><tr><td>OccLLaMA-O [18]</td><td>Occ</td><td>None</td><td>Pred.</td><td>75.20</td><td>25.05</td><td>19.49</td><td>15.26</td><td>19.93</td><td>63.76</td><td>34.56</td><td>28.53</td><td>24.41</td><td>29.17</td></tr><tr><td>RenderWorld-O [156]</td><td>Occ</td><td>None</td><td>Pred.</td><td>-</td><td>28.69</td><td>18.89</td><td>14.83</td><td>20.80</td><td>-</td><td>37.74</td><td>28.41</td><td>24.08</td><td>30.08</td></tr><tr><td>DTT-O [98]</td><td>Occ</td><td>None</td><td>Pred.</td><td>85.50</td><td>37.69</td><td>29.77</td><td>25.10</td><td>30.85</td><td>92.07</td><td>76.60</td><td>74.44</td><td>72.71</td><td>74.58</td></tr><tr><td>DFIT-OccWorld-O [174]</td><td>Occ</td><td>None</td><td>Pred.</td><td>-</td><td>31.68</td><td>21.29</td><td>15.18</td><td>22.71</td><td>-</td><td>40.28</td><td>31.24</td><td>25.29</td><td>32.27</td></tr><tr><td>COME-O [213]</td><td>Occ</td><td>None</td><td>Pred.</td><td>-</td><td>30.57</td><td>19.91</td><td>13.38</td><td>21.29</td><td></td><td>36.96</td><td>28.26</td><td>21.86</td><td>29.03</td></tr><tr><td>DOME-O [94]</td><td>Occ</td><td>None</td><td>GT</td><td>83.08</td><td>35.11</td><td>25.89</td><td>20.29</td><td>27.10</td><td>77.25</td><td>43.99</td><td>35.36</td><td>29.74</td><td>36.36</td></tr><tr><td>COME-O [213]</td><td>Occ</td><td>None</td><td>GT</td><td>-</td><td>42.75</td><td>32.97</td><td>26.98</td><td>34.23</td><td>-</td><td>50.57</td><td>43.47</td><td>38.36</td><td>44.13</td></tr><tr><td>OccWorld-T [93]</td><td>Camera</td><td>Semantic LiDAR</td><td>Pred.</td><td>7.21</td><td>4.68</td><td>3.36</td><td>2.63</td><td>3.56</td><td>10.66</td><td>9.32</td><td>8.23</td><td>7.47</td><td>8.34</td></tr><tr><td>OccWorld-S [93]</td><td>Camera</td><td>None</td><td>Pred.</td><td>0.27</td><td>0.28</td><td>0.26</td><td>0.24</td><td>0.26</td><td>4.32</td><td>5.05</td><td>5.01</td><td>4.95</td><td>5.00</td></tr><tr><td>RenderWorld-S [156]</td><td>Camera</td><td>None</td><td>Pred.</td><td>-</td><td>2.83</td><td>2.55</td><td>2.37</td><td>2.58</td><td>-</td><td>14.61</td><td>13.61</td><td>12.98</td><td>13.73</td></tr><tr><td>COME-S [213]</td><td>Camera</td><td>None</td><td>Pred.</td><td>-</td><td>25.57</td><td>18.35</td><td>13.41</td><td>19.11</td><td>-</td><td>45.36</td><td>37.06</td><td>30.46</td><td>37.63</td></tr><tr><td>OccWorld-D [93]</td><td>Camera</td><td>Occ</td><td>Pred.</td><td>18.63</td><td>11.55</td><td>8.10</td><td>6.22</td><td>8.62</td><td>22.88</td><td>18.90</td><td>16.26</td><td>14.43</td><td>16.53</td></tr><tr><td>OccWorld-F [93]</td><td>Camera</td><td>Occ</td><td>Pred.</td><td>20.09</td><td>8.03</td><td>6.91</td><td>3.54</td><td>6.16</td><td>35.61</td><td>23.62</td><td>18.13</td><td>15.22</td><td>18.99</td></tr><tr><td>OccLLaMA-F [18]</td><td>Camera</td><td>Occ</td><td>Pred.</td><td>37.38</td><td>10.34</td><td>8.66</td><td>6.98</td><td>8.66</td><td>38.92</td><td>25.81</td><td>23.19</td><td>19.97</td><td>22.99</td></tr><tr><td>DFIT-OccWorld-F [174]</td><td>Camera</td><td>Occ</td><td>Pred.</td><td>-</td><td>13.38</td><td>10.16</td><td>7.96</td><td>10.50</td><td>-</td><td>19.18</td><td>16.85</td><td>15.02</td><td>17.02</td></tr><tr><td>DTT-F [98]</td><td>Camera</td><td>Occ</td><td>Pred.</td><td>43.52</td><td>24.87</td><td>18.30</td><td>15.63</td><td>19.60</td><td>54.31</td><td>38.98</td><td>37.45</td><td>31.89</td><td>36.11</td></tr><tr><td>DOME-F [94]</td><td>Camera</td><td>Occ</td><td>GT</td><td>75.00</td><td>24.12</td><td>17.41</td><td>13.24</td><td>18.25</td><td>74.31</td><td>35.18</td><td>27.90</td><td>23.44</td><td>28.84</td></tr><tr><td>COME-F [213]</td><td>Camera</td><td>Occ</td><td>GT</td><td>-</td><td>26.56</td><td>21.73</td><td>18.49</td><td>22.26</td><td>-</td><td>48.08</td><td>43.84</td><td>40.28</td><td>44.07</td></tr></table>

TABLE VI PERFORMaNCE COMPARISON ON THE DMC BENCHMaRK.1.

<table><tr><td rowspan="2">Method</td><td rowspan="2">Step</td><td colspan="4">Episode Return↑</td><td rowspan="2">Avg. / Total</td></tr><tr><td>Reacher Easy</td><td>Cheetah Run</td><td>Finger Spin</td><td>Walker Walk</td></tr><tr><td>PlaNet [38]</td><td>5M</td><td>469</td><td>496</td><td>495</td><td>945</td><td>333/20</td></tr><tr><td>Dreamer [10]</td><td>5M</td><td>935</td><td>895</td><td>499</td><td>962</td><td>823/20</td></tr><tr><td>Dreaming [110]</td><td>500k</td><td>905</td><td>566</td><td>762</td><td>469</td><td>610/12</td></tr><tr><td>TransDreamer [28]</td><td>2M</td><td>-</td><td>865</td><td>-</td><td>933</td><td>893/4</td></tr><tr><td>DreamerPro [111]</td><td>1M</td><td>873</td><td>897</td><td>811</td><td>-</td><td>857/6</td></tr><tr><td>MWM [41]</td><td>1M</td><td>-</td><td>670</td><td></td><td>-</td><td>690/7</td></tr><tr><td>HRSSM [25]</td><td>500k</td><td>910</td><td>-</td><td>960</td><td>-</td><td>938/3</td></tr><tr><td>DisWM [112]</td><td>1M</td><td>960</td><td>820</td><td>-</td><td>920</td><td>879/5</td></tr></table>

1 Noe: Performanc comparison n the DMC.Underlined entris indicate sores aproximate fom the epecive rewar curves.Average scores (Avg.are proided as  coars dicator, given the a difficulty across tasks. TABLE VIIPERFORMANCE COMPARISON FOR MANIPULATION TASKS ON RLBENCH.  

<table><tr><td rowspan="2" colspan="2">Criteria</td><td colspan="5">Methods</td></tr><tr><td>VidMan [55]</td><td>ManiGaussian [53]</td><td>ManiGaussian++ [80]</td><td>DreMa [60]</td><td>TesserAct [78]</td></tr><tr><td rowspan="7">Segns</td><td>Episode</td><td>125</td><td>25</td><td>25</td><td>250</td><td>100</td></tr><tr><td>Pixel</td><td>224</td><td>128</td><td>256</td><td>128</td><td>512</td></tr><tr><td>Depth</td><td></td><td>✓</td><td>✓</td><td>✓</td><td>✓</td></tr><tr><td>Language</td><td>✓</td><td>✓</td><td>✓</td><td></td><td>✓</td></tr><tr><td>Proprioception</td><td>✓</td><td>✓</td><td>✓</td><td></td><td></td></tr><tr><td>Characteristic</td><td>IDM</td><td>GS</td><td>Bimanual</td><td>GS</td><td>DiT</td></tr><tr><td>Stack Blocks</td><td>48</td><td>12</td><td>-</td><td>12</td><td>-</td></tr><tr><td></td><td>Close Jar</td><td>88</td><td>28</td><td>-</td><td>51</td><td>44</td></tr><tr><td>() ee cn</td><td>Open Drawer</td><td>94</td><td>76</td><td>-</td><td>-</td><td>80</td></tr><tr><td></td><td>Sweep to Dustpan</td><td>93</td><td>64</td><td>92</td><td>-</td><td>56</td></tr><tr><td></td><td>Slide Block</td><td>98</td><td>24</td><td>-</td><td>62</td><td>-</td></tr><tr><td></td><td>Avg.1 / Total</td><td>67/18</td><td>45/10</td><td>35/10</td><td>25/9</td><td>63/10</td></tr></table>

Avg. Average scores are reported only as a coarse indicator, given varying task difficulty together, temporal and spatial modeling are still constrained by structural trade-offs that limit scalability and adaptability. Future Directions. Several promising avenues have emerged to address current bottlenecks. SSMs (e.g., Mamba), aligned with autoregressive modeling, offer linear-time scalability and strong potential for long-horizon reasoning. In contrast, masked approaches (e.g., JEPA), closer to global prediction, improve representation learning and efficiency, though their integration

# VII. CONCLUSION

This survey organizes world models in embodied AI using a novel three-part framework: functionality, temporal modeling, and spatial representation. Based on this, we review existing research, datasets, and metrics to establish a standard for comparison. However, significant challenges remain, including a lack of unified datasets and evaluation methods that overlook physical causality. A core modeling challenge involves reconciling the trade-off between efficient autoregressive approaches and robust global prediction paradigms. Future work should address these gaps by creating unified, physically-grounded benchmarks and exploring efficient architectures. Developing hybrid methods that balance fidelity, efficiency, and interactivity is key, as world models form the foundation for the next generation of embodied AI by unifying perception, prediction, and decision-making.

# REFERENCES

[1] D. Batra, A. X. Chang, S. Chernova, A. J. Davison, J. Deng, V. Koltun, S. Levine, J. Malik, I. Mordatch, R. Mottaghi et al., "Rearrangement: A challenge for embodied AI," arXiv preprint arXiv:2011.01975, 2020. 1 TABLE VIIIPERFORMANCE COMPARISON FOR OPEN-LOOP PLANNING ON THE NUSCENES VALIDATION SPLIT1.  

<table><tr><td rowspan="2">Method</td><td rowspan="2">Input</td><td rowspan="2">Aux. Sup.2</td><td colspan="4">L2 (m) ↓</td><td colspan="4">Collision Rate (%) ↓</td></tr><tr><td>1s</td><td>2s</td><td>3s</td><td>Avg.</td><td>1s</td><td>2s</td><td>3s</td><td>Avg.</td></tr><tr><td>UniAD [254]</td><td>Camera</td><td>Map &amp; Box &amp; Motion &amp; Tracklets &amp; Occ</td><td>0.48</td><td>0.96</td><td>1.65</td><td>1.03</td><td>0.05</td><td>0.17</td><td>0.71</td><td>0.31</td></tr><tr><td>UniAD+DriveWorld [87]</td><td>Camera</td><td>Map &amp; Box &amp; Motion &amp; Tracklets &amp; Occ</td><td>0.34</td><td>0.67</td><td>1.07</td><td>0.69</td><td>0.04</td><td>0.12</td><td>0.41</td><td>0.19</td></tr><tr><td>GenAD [92]</td><td>Camera</td><td>Map &amp; Box &amp; Motion</td><td>0.36</td><td>0.83</td><td>1.55</td><td>0.91</td><td>0.06</td><td>0.23</td><td>1.00</td><td>0.43</td></tr><tr><td>FSDrive [101]</td><td>Camera</td><td>Map &amp; Box &amp; QA</td><td>0.40</td><td>0.89</td><td>1.60</td><td>0.96</td><td>0.07</td><td>0.12</td><td>1.02</td><td>0.40</td></tr><tr><td>OccWorld-T [93]</td><td>Camera</td><td>Semantic LiDAR</td><td>0.54</td><td>1.36</td><td>2.66</td><td>1.52</td><td>0.12</td><td>0.40</td><td>1.59</td><td>0.70</td></tr><tr><td>Doe-1 [134]</td><td>Camera</td><td>QA</td><td>0.50</td><td>1.18</td><td>2.11</td><td>1.26</td><td>0.04</td><td>0.37</td><td>1.19</td><td>0.53</td></tr><tr><td>SSR [160]</td><td>Camera</td><td>None</td><td>0.24</td><td>0.65</td><td>1.36</td><td>0.75</td><td>0.00</td><td>0.10</td><td>0.36</td><td>0.15</td></tr><tr><td>OccWorld-S [93]</td><td>Camera</td><td>None</td><td>0.67</td><td>1.69</td><td>3.13</td><td>1.83</td><td>0.19</td><td>1.28</td><td>4.59</td><td>2.02</td></tr><tr><td>Epona [148]</td><td>Camera</td><td>None</td><td>0.61</td><td>1.17</td><td>1.98</td><td>1.25</td><td>0.01</td><td>0.22</td><td>0.85</td><td>0.36</td></tr><tr><td>RenderWorld [156]</td><td>Camera</td><td>None</td><td>0.48</td><td>1.30</td><td>2.67</td><td>1.48</td><td>0.14</td><td>0.55</td><td>2.23</td><td>0.97</td></tr><tr><td>Drive-OccWorld [157]</td><td>Camera</td><td>None</td><td>0.32</td><td>0.75</td><td>1.49</td><td>0.85</td><td>0.05</td><td>0.17</td><td>0.64</td><td>0.29</td></tr><tr><td>OccWorld-D [93]</td><td>Camera</td><td>Occ</td><td>0.52</td><td>1.27</td><td>2.41</td><td>1.40</td><td>0.12</td><td>0.40</td><td>2.08</td><td>0.87</td></tr><tr><td>OccWorld-F [93]</td><td>Camera</td><td>Occ</td><td>0.45</td><td>1.33</td><td>2.25</td><td>1.34</td><td>0.08</td><td>0.42</td><td>1.71</td><td>0.73</td></tr><tr><td>OccLLaMA-F [18]</td><td>Camera</td><td>Occ</td><td>0.38</td><td>1.07</td><td>2.15</td><td>1.20</td><td>0.06</td><td>0.39</td><td>1.65</td><td>0.70</td></tr><tr><td>DTT-F [98]</td><td>Camera</td><td>Occ</td><td>0.35</td><td>1.01</td><td>1.89</td><td>1.08</td><td>0.08</td><td>0.33</td><td>0.91</td><td>0.44</td></tr><tr><td>DFIT-OccWorld-V [174]</td><td>Camera</td><td>Occ</td><td>0.42</td><td>1.14</td><td>2.19</td><td>1.25</td><td>0.09</td><td>0.19</td><td>1.37</td><td>0.55</td></tr><tr><td>NeMo [61]</td><td>Camera</td><td>Occ</td><td>0.39</td><td>0.74</td><td>1.39</td><td>0.84</td><td>0.00</td><td>0.09</td><td>0.82</td><td>0.30</td></tr><tr><td>OccWorld-O [93]</td><td>Occ</td><td>None</td><td>0.43</td><td>1.08</td><td>1.99</td><td>1.17</td><td>0.07</td><td>0.38</td><td>1.35</td><td>0.60</td></tr><tr><td>OccLLaMA-O [18]</td><td>Occ</td><td>None</td><td>0.37</td><td>1.02</td><td>2.03</td><td>1.14</td><td>0.04</td><td>0.24</td><td>1.20</td><td>0.49</td></tr><tr><td>RenderWorld-O [156]</td><td>Occ</td><td>None</td><td>0.35</td><td>0.91</td><td>1.84</td><td>1.03</td><td>0.05</td><td>0.40</td><td>1.39</td><td>0.61</td></tr><tr><td>DTT-O [98]</td><td>Occ</td><td>None</td><td>0.32</td><td>0.91</td><td>1.76</td><td>1.00</td><td>0.08</td><td>0.32</td><td>0.51</td><td>0.30</td></tr><tr><td>DFIT-OccWorld-O [174]</td><td>Occ</td><td>None</td><td>0.38</td><td>0.96</td><td>1.73</td><td>1.02</td><td>0.07</td><td>0.39</td><td>0.90</td><td>0.45</td></tr></table>

[2] T. Gupta, W. Gong, C. Ma, N. Pawlowski, A. Hilmkil, M. Scetbon, M. Rigter, A. Famoti, A. J. Llorens, J. Gao et al., "The essential role of causality in foundation world models for embodied AI," arXiv preprint arXiv:2402.06665, 2024. 1 [3] Y. Liu, W. Chen, Y. Bai, X. Lia, G. Li, W. Gao, and L. Lin, "Ali cyber space with physical world: A comprehensive survey on embodied AI," IEEE/ASME Transactions on Mechatronics, pp. 128, 2025. 1 [4] J. Ding, Y. Zhang, Y. Shang, Y. Zhang, Z. Zong, J. Feng, Y. Yuan, H. Su, N. Li, N. Sukiennik et al. "Understanding world or preditin future? S comprehensive survey of world models," ACM Computing Surveys, 2024. 1 [5] A. Clark, Being there: Putting brain, body, and world together again. MIT press, 1998. 1 [6] L. W. Barsalou, "Perceptions of perceptual symbols," Behavioral and brain sciences, vol. 22, no. 4, pp. 637660, 1999. 1 [7] K. Friston, "The free-energy principle: a unified brain theory?" Nature reviews neuroscience, vol. 11, no. 2, pp. 127138, 2010. 1 [8] P. Fu, Y. Bachrach, A. Celikyilma, K. Chauduri, D. Chen, W. Chung, E. Dupoux, H. Jégou, A. Lazaric, A. Majumdar et al., "Embodied AI agents: Modeling the world," arXiv preprint arXiv:2506.22355, 2025. 1 [9] D. Ha and J. Schmidhuber, "Recurrent world models facilitate policy evolution," in Annu. Conf. Neur. Inform. Process. Syst., 2018, pp. 2455   
2467. 1, 3 [10] D. Hafner, T. Lillicrap, J. Ba, and M. Norouzi, "Dream to Control: Learning behaviors by latent imagination," in Int. Conf. Learn. Represent., 2020, pp. 113. 1, 3, 4, 15 [11] D. Hafner, T. P. Lillicrap, M. Norouzi, and J. Ba, "Mastering atari with discrete world models," in Int. Conf. Learn. Represent., 2021, pp. 115.   
1, 3, 4 [12] D. Hafner, J. Pasukonis, J. Ba, and T. Lillicrap, "Mastering diverse col tasks throug word models," Nature, vol. 0, no. 8, pp.   
647653, 2025. 1, 3, 4 [13] OpenAI, "Video generation models as world simulators," 2024, accessed: 2025-09-14. [Online]. Available: https://openai.com/index/ video-generation-models-as-world-simulators/ 1, 9 [14] M. Assran, A. Bardes, D. Fan, Q. Garrido, R. Howes, M. Muckley, A. Rizvi, C. Roberts, K. Sinha, A. Zholus et al., "V-JEPA 2: Selfsupervised video models enable understanding, prediction and planning," arXiv preprint arXiv:2506.09985, 2025. 1, 3, 4, 9, 11, 14 [15] A. Venkatraman, M. Hebert, and J. Bagnell, "Improving multi-step prediction of learned time series models," in AAAI Conf. Artif. Intell., vol. 29, no. 1, 2015. 1 [16] K. Asadi, D. Misra, and M. Littman, "Lipschitz continuity in modelbased reinforcement learning," in Int. Conf. Mach. Learn., 2018, pp.   
264273. 1   
[17] B. Agro, Q. Sykora, S. Casas, T. Gilles, and R. Urtasun, "UnO: Unsupervised occupancy fields for perception and forecasting," in IEEE Conf. Comput. Vis. Pattern Recog., 2024, pp. 14 48714 496. 1, 10   
[18] J. Wei, S. Yuan, P. Li, Q. Hu, Z. Gan, and W. Ding, "OccLLaMA: An occupancy-language-action generative world model for autonomous driving," arXiv preprint arXiv:2409.03272, 2024. 1, 5, 7, 15, 16   
[19] Z. Zhu, X. Wang, W. Zhao, C. Min, N. Deng, M. Dou, Y. Wang, B. Shi, K. Wang, C. Zhang et al., "Is Sora a World Simulator? A comprehensive survey on general world models and beyond," arXiv preprint arXiv:2405.03520, 2024. 1   
[20] Y. Guan, H. Liao, Z. Li, J. Hu, R. Yuan, G. Zhang, and C. Xu, "World models for autonomous driving: An initial survey," IEEE Transactions on Intelligent Vehicles, pp. 117, 2024. 1   
[21] T. Feng, W. Wang, and Y. Yang, "A survey of world models for autonomous driving," arXiv preprint arXiv:2501.11260, 2025. 1   
[22] Y. Lu, X. Ren, J. Yang, T. Shen, Z. Wu, J. Gao, Y. Wang, S. Chen, M. Chen, S. Fidler et al., "InfiniCube: Unbounded and controllable dynamic 3D driving scene generation with world-guided video models," in Int. Conf. Comput. Vis., 2025. 3, 5, 9   
[23] Y. Chen, Y. Wang, and Z. Zhang, "DrivingGPT: Unifying driving world modeling and planning with multi-modal autoregressive transformers," arXiv preprint arXiv:2412.18607, 2024. 3, 6   
[24] R. D. Smallwood and E. J. Sondik, "The optimal control of partially observable Markov processes over a finite horizon," Operations research, vol. 21, no. 5, pp. 10711088, 1973. 2   
[25] R. Sun, H. Zang, X. Li, and R. Islam, "Learning latent dynamic robust representations for world models," in Int. Conf. Mach. Learn., 2024, pp. 47 23447 260. 2, 3, 15   
[26] G. Zhai, X. Zhang, and N. Navab, "Recurrent world model with tokenized latent states," in Int. Conf. Learn. Represent. Worksh., 2025, pp. 1-7. 2   
[27] E. Aljalbout, M. Krinner, A. Romero, and D. Scaramuzza, "Accelerating model-based reinforcement learning with state-space world models," in Int. Conf. Learn. Represent. Worksh., 2025. 2   
[28] C. Chen, Y.-F. Wu, J. Yoon, and S. Ahn, "TransDreamer: Reinforcement learning with transformer world models," arXiv preprint arXiv:2202.09481, 2022. 2, 4, 5, 15   
[29] J. Robine, M. Höftmann, T. Uelwer, and S. Harmeling, "Transformerbased world models are happy with 100k interactions," in Int. Conf. Learn. Represent., 2023, pp. 113. 2, 4, 6   
[30] W. Zhang, G. Wang, J. Sun, Y. Yuan, and G. Huang, "STORM: Efficient stochastic transformer based world models for reinforcement learning," in Annu. Conf. Neur. Inform. Process. Syst., 2023, pp. 27 14727 166. 2   
[31] C. Zhu, R. Yu, S. Feng, B. Burchfiel, P. Shah, and A. Gupta, "Unified World Models: Coupling video and action diffusion for pretraining on large robotic datasets," in Robotics: Science and Systems, 2025. 2   
H.  H. A.ZuY. u  H.Y i robot policies through predictive world modeling," arXiv preprint arXiv:2502.00622, 2025. 2   
[33] J. Guo, X. Ma, Y. Wang, M. Yang, H. Liu, and Q. Li, "FlowDreamer: A RGB-D world model with flow-based motion representations for robot manipulation," arXiv preprint arXiv:2505.10075, 2025. 2   
[34] S. Huang, L. Chen, P. Zhou, S. Chen, Z. Jiang, Y. Hu, Y. Liao, P. Gao, H. Li, M. Yao et al., "EnerVerse: Envisioning embodied future space for robotics manipulation," arXiv preprint arXiv:2501.01895, 2025. 2, 4, 7   
[35] H. A. Alhaija, J. Alvarez, M. Bala, T. Cai, T. Cao, L. Cha, J. Chen, M. Chen, F. Ferroni, S. Fidler et al., "Cosmos-Transfer1: Conditional world generation with adaptive multimodal control," arXiv preprint arXiv:2503.14492, 2025. 2   
[36] B. Kerbl, G. Kopanas, T. Leimkühler, and G. Drettakis, "3D Gaussian Splatting for real-time radiance field rendering," ACM Transactions on Graphics, vol. 42, no. 4, pp. 114, 2023. 3   
[37] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng, "NeRF: Representing scenes as Neural Radiance Fields for view synthesis," in Eur. Conf. Comput. Vis., 2020, pp. 405421. 3   
[38] D. Hafner, T. Lillicrap, I. Fischer, R. Villegas, D. Ha, H. Lee, and J. Davidson, "Learning latent dynamics for planning from pixels," in Int. Conf. Mach. Learn., 2019, pp. 25552565. 3, 4, 15   
.  . A.  . , "  ixe inverse dynamics models," in Int. Conf. Learn. Represent., 2021, pp. 112. 4, 5   
[40] M. Pan, X. Zhu, Y. Wang, and X. Yang, "Iso-Dream: Isolating and leveraging noncontrollable visual dynamics in world models," in Annu. Conf. Neur. Inform. Process. Syst., 2022, pp. 23 17823 191. 4, 5   
[41] Y. Seo, D. Hafner, H. Liu, F. Liu, S. James, K. Lee, and P. Abbeel, "Masked world models for visual control," in Conf. Robot. Learn., 2023, pp. 13321344. 4, 6, 15   
[42] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch, Y. Chebotar et al., "Inner Monologue: Embodied reasoning through planning with language models," in Conf. Robot. Learn., 2023, pp. 17691782. 4, 6   
[43] P. Wu, A. Ecoela, D. Hafer, P.Abeel, and K.Glg, "DayDreamer: World models for physical robot learning," in Conf. Robot. Learn., 2023, pp. 22262240. 4   
[44] V. Micheli, E. Alonso, and F. Fleuret, "Transformers are sample-efficient world models," in Int. Conf. Learn. Represent., 2023, pp. 114. 4, 6   
[45] X. Wang, Z. Zhu, G. Huang, B. Wang, X. Chen, and J. Lu, "WorldDreamer: Towards general world models for video generation via predicting masked tokens," arXiv preprint arXiv:2401.09985, 2024. 4, 9   
[6T. Yoneda, J. Fang, P. Li, H. Zhang, T. Jiang, S. Lin, B. Picker, D. Yunis, H. Mei, and M. R. Walter, "Statler: State-maintaining language models fo  ,"   . o. Roo. u  . 1508315 091. 4, 6   
[47] J. Xiang, G. Liu, Y. Gu, Q. Gao, Y. Ning, Y. Zha, Z. Feng, T. Tao, S. Hao, Y. Shi et al., "Pandora: Towards general world model with natural language actions and video states," arXiv preprint arXiv:2406.09455, 2024. 4, 8   
[48] X. Gu, Y.-J. Wang, X. Zhu, C. Shi, Y. Guo, Y. Liu, and J. Chen, "Advancing Humanoid Locomotion: Mastering challenging terrains with denoising world model learning," in Robotics: Science and Systems, 2024. 4, 5   
[49] S. Zhou, Y. Du, J. Chen, Y. Li, D.-Y. Yeung, and C. Gan, "RoboDreamer: Learning compositional world models for robot imagination," in Int. Conf. Mach. Learn., 2024, pp. 61 88561 896. 4, 7   
[50] J. Bruce, M. D. Dennis, A. Edwards, J. Parker-Holder, Y. Shi, E. Hughes, M. Lai, A. Mavalankar, R. Steigerwald, C. Apps et al., "Genie: Generative interactive environments," in Int. Conf. Mach. Learn., 2024, pp. 46034623. 4, 8   
[51] A. Bardes, Q. Garrido, J. Ponce, X. Chen, M. Rabbat, Y. LeCun, M. Assran, and N. Ballas, "Revisiting feature prediction for learning visual representations from video," Trans. Mach. Learn. Res., vol. 2024, pp. 114, 2024. 4, 9   
[52] L. Zhang, M. Kan, S. Shan, and X. Chen, "PreLAR: World model pre-training with learnable action representation," in Eur. Conf. Comput. Vis., 2024, pp. 185201. 3, 4   
[53] G. Lu, S. Zhang, Z. Wang, C. Liu, J. Lu, and Y. Tang, "ManiGaussian: Dynamic gaussian splatting for multi-task robotic manipulation," in Eur. Conf. Comput. Vis., 2024, pp. 349366. 4, 7, 15   
[54] M. Zawalski, W. Chen, K. Pertsch, O. Mees, C. Finn, and S. Levine, "Robotic control via embodied Chain-of-Thought reasoning," in Conf. Robot. Learn., 2025, pp. 31573181. 4, 6   
[55] Y. Wen, J. Lin, Y. Zhu, J. Han, H. Xu, S. Zhao, and X. Liang, "VidMan: Exploiting implicit dynamics from video diffusion model for effective robot manipulation," in Annu. Conf. Neur. Inform. Process. Syst., 2024, pp. 4105141075. 4, 7, 15   
[56] J. Wu, S. Yin, N. Feng, X. He, D. Li, J. Hao, and M. Long, "iVideoGPT: Interactive videoGPTs are scalable world models," in Annu. Conf. Neur. Inform. Process. Syst., 2024, pp. 68 08268 119. 4, 8   
[57] Q. He, W. Liang, C. Hao, G. Sun, and J. Tian, "GLAM: Global-local variation awareness in mamba-based world model," in AAAI Conf. Artif. Intell., 2025, pp. 17 10517 113. 4, 5   
[58] B. Lin, Y. Nie, Z. Wei, J. Chen, S. Ma, J. Han, H. Xu, X. Chang, and X. Liang, "NavCot: Boosting LLM-based Vision-and-Language navigation via learning disentangled reasoning," IEEE Trans. Pattern Anal. Mach. Intell, vol. 47, no. 7, pp. 59455945, 2025. 4, 6   
[59] J. Guo, Y. Ye, T. He, H. Wu, Y. Jiang, T. Pearce, and J. Bian, "MineWorld: A real-time and open-source interactive world model on minecraft," arXiv preprint arXiv:2504.08388, 2025. 4, 6   
[60] L. Barcellona, A. Zadaianchuk, D. Allegro, S. Papa, S. Ghidoni, and E. Gavves, "Dream to Manipulate: Compositional world models empowering robot imitation learning with imagination," in Int. Conf. Learn. Represent., 2025. 4, 7, 15   
[61] F. Petri, L. Asprino, and A. Gangemi, "Learning local causal world models with state space models and attention," arXiv preprint arXiv:2505.02074, 2025. 4, 8   
[62] J. Wu, S. Yin, N. Feng, and M. Long, "RLVR-World: Training world models with reinforcement learning," arXiv preprint arXiv:2505.13934, 2025. 4, 8   
[63] N. Savov, N. Kazemi, D. Zhang, D. P. Paudel, X. Wang, and L. Van Gool, "StateSpaceDiffuser: Bringing long context to diffusion world models," arXiv preprint arXiv:2505.22246, 2025. 4, 8   
[64] J. Chen, H. Zhu, X. He, Y. Wang, J. Zhou, W. Chang, Y. Zhou, Z. Li, Z. Fu, J. Pang et al., "DeepVerse: 4D autoregressive video generation as a world model," arXiv preprint arXiv:2506.01103, 2025. 4, 8   
[65] X. Yang, B. Li, S. Xu, N. Wang, C. Ye, Z. Chen, M. Qin, Y. Ding, X. Jin, H. Zhao et al., "ORV: 4D occupancy-centric robot video generation," arXiv preprint arXiv:2506.03079, 2025. 4, 10   
[66] A. Bar, G. Zhou, D. Tran, T. Darrell, and Y. LeCun, "Navigation world models," in IEEE Conf. Comput. Vis. Pattern Recog., 2025, pp. 15 79115 801. 4, 6   
[67] J. Cen, C. Yu, H. Yuan, Y. Jiang, S. Huang, J. Guo, X. Li, Y. Song, H. Luo, F. Wang et al., "WorldVLA: Towards autoregressive action world model," arXiv preprint arXiv:2506.21539, 2025. 4, 6   
[68] H. Chen, B. Wang, J. Guo, T. Zhang, Y. Hou, X. Huang, C. Tie, and L. Shao, "World4Omni: A zero-shot framework from image generation world model to robotic manipulation," arXiv preprint arXiv:2506.23919, 2025. 4, 8   
[69] Z. Wang, K. Wang, L. Zhao, P. Stone, and J. Bian, "Dyn-O: Building structured world models with object-centric representations," arXiv preprint arXiv:2507.03298, 2025. 4, 6   
[70] G. Zhou, H. Pan, Y. LeCun, and L. Pinto, "DINO-WM: World models on pre-trained visual features enable zero-shot planning," in Int. Conf. Mach. Learn., 2025. 4, 7   
[71] X. Chi, C.-K. Fan, H. Zhang, X. Qi, R. Zhang, A. Chen, C.-M. Chan, W. Xue, Q. Liu, S. Zhang et al., "Empowering world models with reflection for embodied video prediction," in Int. Conf. Mach. Learn., 2025. 4, 8   
. Go S. ou Y. Du, J. Z n C.n, "r adaptable world models with latent actions," in Int. Conf. Mach. Learn., 2025. 4, 8   
[73] Y. Yang, J. Liu, Z. Zhang, S. Zhou, R. Tan, J. Yang, Y. Du, and C. Gan, "MindJourney: Test-time scaling with world models for spatial reasoning," arXiv preprint arXiv:2507.12508, 2025. 4, 9   
[74] Y. Chai, L. Deng, R. Shao, J. Zhang, L. Xing, H. Zhang, and Y. Liu, "GAF: Gaussian action field as a dvnamic world model for robotic mlanipulation," arXiv preprint arXiv:2506.14135, 2025. 4, 7   
[75] X. Mao, S. Lin, Z. Li, C. Li, W. Peng, T. He, J. Pang, M. Chi, Y. Qiao, and K. Zhang, "Yume: An interactive world generation model," arXiv preprint arXiv:2507.17744, 2025. 4, 8   
X.C H.  P. Z .  . g Y.o , Y. Wang, X. Xiao, L. Zhao et al., "villa-X: Enhancing latent action modeling in Vision-Language-Action models," arXiv preprint arXiv:2507.23682. 2025. 4. 7   
[77] A. Team, H. Zhu, Y. Wang, J. Zhou, W. Chang, Y. Zhou, Z. Li, J. Chen, C. Shen, J. Pang et al., "Aether: Geometric-aware unified world modeling," in Int. Conf. Comput. Vis., 2025. 4, 10   
[78 H. Zhen, Q. Sun, H. Zhag, J. Li, S. Zhou, Y. Du, and C.Gan, TesserAct: Learning D embodied world models," in Int. Conf. Comput. Vis., 2025. 4, 7, 15   
[79] E. Zhou, Y. Qin, Z. Yin, Y. Huang, R. Zhang, L. Sheng, Y. Qiao, and J. Shao, "MineDreamer: Learning to follow instructions via Chain-ofImagination for simulated-world control," in Int. Conf. Intell. Robot. Syst., 2025. 4, 6   
[80] T. Yu, G. Lu, Z. Yang, H. Deng, S. S. Chen, J. Lu, W. Ding, G. Hu, Y. Tang, and Z. Wang, "ManiGaussian++: General robotic bimanual manipulation with hierarchical gaussian world model," in Int. Conf. Intell. Robot. Syst., 2025. 4, 7, 15   
[81] A. Hu, G. Corrado, N. Griffths, Z. Murez, C. Gurau, H. Yeo, A. Kendall, R. Cipolla, and J. Shotton, "Model-based imitation learning for urban driving," in Annu. Conf. Neur. Inform. Process. Syst., 2022, pp. 20703 20716. 4, 5   
[82] L. Zhang, Y. Xiong, Z. Yang, S. Casas, R. Hu, and R. Urtasun, Copilot4D: Learning unsupervised world models for autonomous driving via discrete diffusion," in Int. Conf. Learn. Represent., 2024. 5, 9   
[83] Z. Gao, Y. Mu, C. Chen, J. Duan, P. Luo, Y. Lu, and S. E. Li, "Enhace sample efficiency and robustness of End-to-End urban autonomous driving via semantic masked world model," IEEE Trans. Intell. Transp. Syst., vol. 25, no. 10, pp. 1306713079, 2024. 4, 5   
[84] R. Gao, K. Chen, Z. Li, L. Hong, Z. Li, and Q. Xu, "MagicDrive3D: Controllable 3D generation for any-view rendering in street scenes," arXiv preprint arXiv:2405.14475, 2024. 5, 10, 13   
[85] L. Wang, W. Zheng, Y. Ren, H. Jiang, Z. Cui, H. Yu, and J. Lu, "OccSora: 4D occupancy generation models as world simulators for autonomous driving," arXiv preprint arXiv:2405.20337, 2024. 5, 9   
[86] E. Ma, L. Zhou, T. Tang, Z. Zhang, D. Han, J. Jiang, K. Zhan, P. Jia, X. Lang, H. Sun et al., "Unleashing generalization of End-to-End autonomous driving with controllable long video generation," arXiv preprint arXiv:2406.01349, 2024. 5, 9, 13   
[87] C. Min, D. Zhao, L. Xiao, J. Zhao, X. Xu, Z. Zhu, L. Jin, J. Li, Y. Guo, J. Xing et al., "DriveWorld: 4D pre-trained scene understanding via world models for autonomous driving," in IEEE Conf. Comput. Vis. Pattern Recog., 2024, pp. 15 52215 533. 5, 6, 16   
[88] Y. Wang, J. He, L. Fan, H. Li, Y. Chen, and Z. Zhang, "Driving into the Future: Multiview visual forecasting and planning with world model i 2024, pp. 14 74914 759. 5, 8, 13   
[ . , L. h Y. Sn  . Li, "Vi enables scalable autonomous driving," in IEEE Conf. Comput. Vis. Pattern Recog., 2024, pp. 14 67314 684. 5, 9   
0 Y.  . P. L l "Generalize predicive odel oonomous diig." in IEEE Conf. Comput. Vis. Pattern Recog., 2024, pp. 14 66214672. 5, 8, 11, 13, 14   
[91] X. Wang, Z. Zhu, G. Huang, X. Chen, J. Zhu, and J. Lu, "DriveDreamer: Towards real-world-drive world models for autonomous driving," in Eur. Conf. Comput. Vis., 2024, pp. 5572. 5, 6, 13   
[92] W. Zheng, R. Song, X. Guo, C. Zhang, and L. Chen, "GenAD: Generative End-to-End autonomous driving," in Eur. Conf. Comput. Vis., 2024, pp. 87104. 5, 6, 16   
[93] W. Zheng, W. Chen, Y. Huang, B. Zhang, Y. Duan, and J. Lu, "OccWorld: L   " i Eur. Conf. Comput. Vis., 2024, pp. 5572. 5, 7, 15, 16   
[94] S. Gu, W. Yin, B. Jin, X. Guo, J. Wang, H. Li, Q. Zhang, and X. Long, "DOME: Taming diffusion model into high-fidelity controllable occupancy world model," arXiv preprint arXiv:2410.10429, 2024. 5, 9, 15   
[95] R. Tian, B. Li, X. Weng, Y. Chen, E. Schmerling, Y. Wang, B. Ivanovic, and M. Pavone, "Tokenize the world into object-level knowledge to ss ong-al vets iuoou rivg," Con Robot. Lear 2025, pp. 36563673. 5, 7   
[96] S. Gao, J. Yang, L. Chen, K. Chitta, Y. Qiu, A. Geiger, J. Zhang, and H. Li "Vista: A generalizable driving world model with high fideliy and versatile controllability," in Annu. Conf. Neur. Inform. Process. Syst., 2024, pp. 91 56091 596. 5, 9, 13   
[97] G. Zhao, X. Wang, Z. Zhu, X. Chen, G. Huang, X. Bao, and X. Wang, DriveDreamer-2: LLM-enhanced world models for diverse driving video generation," in AAAI Conf. Artif. Intell, 2025, pp. 1041210420. 5, io0 12   
[98] H. Xu, P. Peng, G. Tan, Y. Chang, Y. Zhao, and Y. Tian, "DeltaTriplane transformers as occupancy world models," arXiv preprint arXiv:2503.07338, 2025. 5, 7, 15, 16   
[9] H. Bian, L. Kong, H. Xie, L. Pan, Y. Qiao, and Z. Liu, "DynaicCiy: Large-scale 4D occupancy generation from dynamic scenes," in Int. Conf. Learn. Represent., 2025, pp. 114. 5, 10   
[100] V. Zyrianov, H. Che, Z. Liu, and S. Wang, "LidarDM: Generative LiDÁR simulation in a generated world," in IEEE Int. Conf. Robot. Autom., 2025. 5, 9   
[101] S. Zeng, X. Chang, M. Xie, X. Liu, Y. Bai, Z. Pan, M. Xu, and X. Wei, "FutureSightDrive: Thinking visually with spatio-temporal CoT for autonomous driving," arXiv preprint arXiv:2505.17685, 2025. 5, 6, 16   
[102] M. Hassan, S. Stapf, A. Rahimi, P. Rezende, Y. Haghighi, D. Brüggemann, I. Katircioglu, L. Zhang, X. Chen, S. Saha et al., "GEM: A generalizable ego-vision multimodal world model for fine-grained egomotion, object dynamics, and scene composition control," in EEE Conf. Comput. Vis. Pattern Recog., 2025, pp. 224042415. 5, , 13   
[103] S. Zuo, W. Zheng, Y. Huang, J. Zhou, and J. Lu, "GaussianWorld: Gaussian world model for streaming 3D occupancy prediction," in IEEE Conf. Comput. Vis. Pattern Recog., 2025, pp. 67726781. 5, 9   
[104] J. Ni, Y. Guo, Y. Liu, R. Chen, L. Lu, and Z. Wu, "MaskGWM: A generalizable driving world model with video mask reconstruction," in IEEE Conf. Comput. Vis. Pattern Recog., 2025, pp. 22 38122 391. 5, 9, 13   
[05] G. Zhao, C. Ni, X. Wang, Z. Zhu, X. Zhag, Y. Wang, G. Huang, X.Chen, B. Wang, Y. Zhang et al., "DriveDreamer4D: World models are effective data machines for 4D driving scene representation," in IEEE Conf. Comput. Vis. Pattern Recog., 2025, pp. 1201512026. 5, 10   
[106] C. Ni, G. Zhao, X. Wang, Z. Zhu, W. Qin, G. Huang, C. Liu, Y. Chen, Y. Wang, X. Zhang et al., "ReconDreamer: Crafting world models for driving scene reconstruction via online restoration," in IEEE Conf. Comput. Vis. Pattern Recog., 2025, pp. 15591569. 5, 10   
Y. LY. W Y. L J. He, L. Fan, and Z. Za "n-- driving with online trajectory evaluation via BEV world model," in Int. Conf. Comput. Vis., 2025. 5,7   
[108] X. Zhou, D. Liang, S. Tu, X. Chen, Y. Ding, D. Zhang, F. Tan, H. Zhao, and X. Bai, "HERMES: A unified self-driving world model for simultaneous 3D scene understanding and generation," in Int. Conf. Comput. Vis., 2025. 5, 9   
[109] X. Li, C. Wu, Z. Yang, Z. Xu, D. Liang, Y. Zhang, J. Wan, and J. Wang, "DriVerse: Navigation world model for driving simulation via multimodal trajectory prompting and motion alignment," in ACM Int. Conf. Multimedia, 2025. 5, 8, 13   
[110] M. Okada and T. Taniguchi, "Dreaming: Model-based reinforcement learning by latent imagination without reconstruction," in IEEE Int. Conf. Robot. Autom., 2021, pp. 42094215. 3, 15   
[111] F. Deng, I. Jang, and S. Ahn, "DreamerPro: Reconstruction-ree modelbased reinforcement learning with prototypical representations," in Int. Conf. Mach. Learn., 2022, pp. 49564975. 3, 15   
[112] Q. Wang, Z. Zhang, B. Xie, X. Jin, Y. Wang, S. Wang, L. Zheng, X. Yang, and W. Zeng, "Disentangled world models: Learning to transfer semantic knowledge from distracting videos for renorcement leaing." in Int. Conf. Comput. Vis., 2025. 3, 15   
[] Y. Wng, M. Verhee, and J. Scider, 'Ltet polic se with embodiment-agnostic pretrained world models," arXiv preprint arXiv:2507.13340, 2025. 3   
[114] C. Sancaktar, C. Gumbsch, A. Zadaianchuk, P. Kolev, and G. Martius, SENSEI: Semantic exploration guided by foundation models to learn versatile world models," in Int. Conf. Mach. Learn., 2025. 4   
[115] V. D. Nguyen, Z. Yang, C. L. Buckley, and A. Ororbia, "SR-AIF: Solving sparse-reward robotic tasks from pixels with active inference and world models," in IEEE Int. Conf. Robot. Autom., 2025, pp. 6510 6518. 4   
[116] J. Lanier, K. Kim, A. Karamzade, Y. Liu, A. Sinha, K. He, D. Corsi, and R. Fox, "Adapting world models with latent-state dynamics residuals," arXiv preprint arXiv:2504.02252, 2025. 4   
[117] H. Wang, X. Ye, F. Tao, C. Pan, A. Mallik, B. Yaman, L. Ren, and J. Zhang, "AdaWM: Adaptive world model based planning for autonomous driving," in Int. Conf. Learn. Represent., 2025, pp. 113. 4   
H. J.Cao, J Xu, H. u Y.  T. Y.Yu., "World model-based perception for visual legged locomotion," in IEEE Int. Conf. Robot. Autom., 2025. 4   
[119] Y. Wang, R. Yu, S. Wan, L. Gan, and D.-C. Zhan, "FOUNDER: Grounding foundation models in world models for open-ended embodied decision making." in Int. Conf. Mach. Learn.. 2025. 4   
[120] I. Nematollahi, B. DeMoss, A. L. Chandra, N. Hawes, W. Burgard, and I. Posner, "LUMOS: Language-conditioned imitation learning with world models," in IEEE Int. Conf. Robot. Autom., 2025. 4   
[121] A. Popov, A. Degirmenci, D. Wehr, S. Hegde, R. Oldja, A. Kamenev, B. Douillard, D. Nistér, U. Muller, R. Bhargava et al., "Mitigatin covariate shift in imitation learning for autonomous vehicles using latent space generative world models," in IEEE Int. Conf. Robot. Autom., 2025. 4   
[122] Y. Qu, Z. Huang, Z. Sheng, J. Chen, S. Chen, and S. Labi, "VL-SAFE: Vnmodels for autonomous driving," arXiv preprint arXiv:2505.16377, 2025. 5   
[] H. Wang, D. Go, and J. Zhang, "Ego-cenric learning of communitive world models for autonomous driving," arXiv preprint arXiv:2506.08149, 2025.5   
[124] R. G. Goswami, P. Krishnamurthy, Y. LeCun, and F. Khorrami, "OSVIWM: One-shot visual imitation for unseen tasks using world-modelguided trajectory generation," arXiv preprint arXiv:2505.20425, 2025. 5   
[125] C. Li, A. Krause, and M. Hutter, "Robotic world model: A neural network simulator for robust policy optimization in robotics," arXiv preprint arXiv:2501.10100, 2025. 5   
[126] W. Liu, H. Zhao, C. Li, J. Biswas, B. Okal, P. Goyal, Y. Chang, and S. Pouya, "X-MOBILITY: End-to-end generalizable navigation via world modeling," in IEEE Int. Conf. Robot. Autom., 2025. 5   
[ W. Sun, L. Chen, Y. Su, B. Cao, Y. Liu, and Z. Xie, "Leari humanoid locomotion with world model reconstruction," arXiv preprint arXiv:2502.16230, 2025. 5   
.    e J  . to Poke by Poking: Experiential learning of intuitive physics," in Annu. Conf. Neur. Inform. Process. Syst., 2016. 5   
[129] X. Yao, J. Gao, and C. Xu, "NavMorph: A self-evolving world model for vision-and-language navigation in continuous environments," in Int. Conf. Comput. Vis., 2025. 6   
[130] M. Burchi and R. Timofte, "Learning transformer-based world models with contrastive predictive coding," in Int. Conf. Learn. Represent., 2025, pp. 114.6   
[131] T. Feng, X. Wang, Z. Zhou, R. Wang, Y. Zhan, G. Li, Q. Li, and W. Zhu, "EvoAgent: Agent autonomous evolution with continual world model for long-horizon tasks," arXiv preprint arXiv:2502.05907, 2025. 6   
[132] Z. Chen, J. Huo, Y. Chen, and Y. Gao, "RoboHorizon: An LLM-assisted multi-view world model for long-horizon robotic manipulation," arXiv preprint arXiv:2501.06605, 2025. 6   
[13] X. Hu, W. Yin, M. Jia, J. Deng, X. Guo, Q. Zhang, X. Long, and P. Tan, video gpt," arXiv preprint arXiv:2412.19505, 2024. 6, 13   
[134] W. Zheng, Z. Xia, Y. Huang, S. Zuo, J. Zhou, and J. Lu, "Doe-1: Closed-loop autonomous driving with large world model," arXiv preprint arXiv:2412.09627, 2024. 6, 16   
Xi JJ.  S.  X.  X. e .Y  J , "Learning multiple probabilistic decisions from latent world model in autonomous driving," in IEEE Int. Conf. Robot. Autom., 2025. 6   
[136] A. B. Vasudevan, N. Peri, J. Schneider, and D. Ramanan, "Planning with adaptive world models for autonomous driving," in IEEE Int. Conf. Robot. Autom., 2025, pp. 1493814 945. 6   
[137] A. Dedieu, J. Ortiz, X. Lou, C. Wendelken, J. S. Guntupalli, W. Lehrach, M. Lazaro-Gredilla, and K. P. Murphy, "Improving transformer world models for data-efficient RL," in Int. Conf. Mach. Learn., 2025. 6   
[138] J. Lyu, Z. Li, X. Shi, C. Xu, Y. Wang, and H. Wang, "DyWA: Dynamics-adaptive world action model for generalizable non-prehensile manipulation," in Int. Conf. Comput. Vis., 2025. 6   
[139] L. Chen, Y. Wang, S. Tang, Q. Ma, T. He, W. Ouyang, X. Zhou, H. Bao, and S. Peng, "EgoAgent: A joint predictive agent model in egocentric worlds," in Int. Conf. Comput. Vis., 2025.   
[140] A. Scannell, M. Nakhaeinezhadfard, K. Kujanpää, Y. Zhao, K. S. Luck, A. Solin, and J. Pajarinen, "Discrete codebook world models for continuous control," in Int. Conf. Learn. Represent., 2025, pp. 116.6   
[141] S. Yin, J. Wu, S. Huang, X. Su, X. He, J. HAO, and M. Long, "Trajectory world models for heterogeneous environments," in Int. Conf. Mach. Learn., 2025. 6   
[142] S. Hamdan and F. Güney, "CarFormer: Self-driving with learned objectcentric representations," in Eur. Conf. Comput. Vis., 2024, pp. 177193. 6   
[143] Y. Jeong, J. Chun, S. Cha, and T. Kim, "Object-centric world model for language-guided manipulation," in Int. Conf. Learn. Represent. Worksh., 2025 nn 113 6   
[144] V. Micheli, E. Alonso, and F. Fleuret, "Efficient world models with n . 35 62335 638. 6   
[145] S. Wang, Z. Fei, Q. Cheng, S. Zhang, P. Cai, J. Fu, and X. Qiu, "World Modeling Makes a Better Planner: Dual preference optimization for embodied task planning," in Annu. Meet. Assoc. Comput. Linguistics, 2025.6   
[146 K. Zhang, P. Ren, B. Lin, J. Lin, S. Ma, H. Xu, and X. Liang, "PIVOT-R: Primitive-driven waypoint-aware world model for robotic manipulation," in Annu. Conf. Neur. Inform. Process. Syst., 2024, pp. 54 10554 136.6   
[147] Y. Chen, J. Wei, C. Xu, B. Li, M. Tomizuka, A. Bajsy, and R. Tian, "Reimagination with Test-time Observation Interventions: Distractorrobust world model predictions for visual model predictive control," in Robotics: Science and Systems Worksh., 2025. 6   
[148] K. Zhang, Z. Tang, X. Hu, X. Pan, X. Guo, Y. Liu, J. Huang, L. Yuan, Q. Zhang, X.-X. Long et al., "Epona: Autoregressive diffusion world model for autonomous driving," in Int. Conf. Comput. Vis., 2025. 6, 13, 16   
[149] M. Goff, G. Hogan, G. Hotz, A. du Parc Locmaria, K. Raczy, H. Schäfer, A. Shihadeh, W. Zhang, and Y. Yousfi, "Learning to drive from a world . 19641973. 6   
[150] S. Tan, J. Lambert, H. Jeon, S. Kulshrestha, Y. Bai, J. Luo, D. Anguelov, M. Tan, and C. M. Jiang, "SceneDiffuser++: City-scale traffic simulation via a generative world model," in IEEE Conf. Comput. Vis. Pattern Recog., 2025, pp. 15701580.6   
[151] X. Yu, B. Peng, R. Xu, M. Galley, H. Cheng, S. Nath, J. Gao, and Z. Yu, "Dyna-Think: Synergizing reasoning, acting, and world model simulation in AI agents," arXiv preprint arXiv:2506.00320, 2025. 6   
[152] Z. Zhao, W. Zhang, H. Huang, K. Liu, J. Gao, G. Wang, and K. Chen, RIG Synergizing reasoning and imagination in End-to-End generalist policy," arXiv preprint arXiv:2503.24388, 2025. 6   
[153] J. Gkountouras, M. Lindemann, P. Lippe, E. Gavves, and I. Titov, "Language agents meet causalitybridging llms and causal world models," in Int. Conf. Learn. Represent., 2025, pp. 115. 6   
[4]T. Yin, Z. Mei, T. Sun, L. Zha, E. Zhou, J. Bo, M. Yamane, O. Sho, and A. Majumdar, "WoMAP: World models for embodied open-vocabulary object localization," in Robotics: Science and Systems Worksh., 2025. 6   
[155] Z. Yang, X. Jia, Q. Li, X. Yang, M. Yao, and J. Yan, "Raw2Drive: Reinforcement learning with aligned world models for End-to-End autonomous driving (in CARLA v2)," arXiv preprint arXiv:2505.16394, 2025.6   
Z. n .   .  H   H.  . , Y. Wang, F. Remondino et al., "RenderWorld: World model with selfsupervised 3D label," in IEEE Int. Conf. Robot. Autom., 2025. 7, 15, 16   
[157] Y. Yang, J. Mei, Y. Ma, S. Du, W. Chen, Y. Qian, Y. Feng, and Y. Liu, "Driving in the Occupancy World: Vision-Centric 4D occupancy forecasting and planning via world models for autonomous driving," in AAAI Conf. Artif. Intell., 2025, pp. 93279335. 7, 16   
[158] X. Li, P. Li, Y. Zheng, W. Sun, Y. Wang, and Y. Chen, "Semi-supervised vision-centric 3D occupancy world model for autonomous driving," in Int. Conf. Learn. Represent., 2025, pp. 113. 7   
JHY. End-to-End autonomous driving with latent world model," in Int. Conf. Learn. Represent., 2025, pp. 114. 7   
[160] P. Li and D. Cui, "Navigation-guided sparse scene representation for End-to-End autonomous driving," in Int. Conf. Learn. Represent., 2024, pp. 113. 7, 16   
[161] Z. Huang, J. Zhang, and E. Ohn-Bar, "Neural volumetric world models for autonomous driving," in Eur. Conf. Comput. Vis., 2024, pp. 195213. 7, 16   
. .F  X. , S. Cui, and Z. Li, "FASTopoWM: Fast-slow lane segment topology reasoning with latent world models," arXiv preprint arXiv:2507.23325, 2025.7   
X. Y.  .  n, vision-language models into world models for object goal navigation," in Int. Conf. Intell. Robot. Syst., 2025.7   
[164] Z. Zhang, Q. Zhang, W. Cui, S. Shi, Y. Guo, G. Han, W. Zhao, J. Sun, J. Cao, J. Wang et al., "Occupancy world model for robots," arXiv preprint arXiv:2505.05512, 2025. 7   
.H . X. J. ., " A 3D point cloud world model for multi-object, multi-material robotic manipulation," in Conf. Robot. Learn., 2025.   
[166] J. Abou-Chakra, K. Rana, F. Dayoub, and N. Sünderhauf, "Physically Embodied Gaussian Splatting: A realtime correctable world model for robotics," in Conf. Robot. Learn., 2025, pp. 513530. 7   
[167] T. Jiang, Y. Guan, L. Ma, J. Xu, J. Meng, W. Chen, Z. Zeng, L. Li, .Rl B p p tilatje exteu anulation," Tans. R vol. 41, pp. 43604379, 2025. 7   
[168] W. Li, H. Zhao, Z. Yu, Y. Du, Q. Zou, R. Hu, and K. Xu, "PINWM: Learning physics-informed world models for non-prehensile manipulation," in Robotics: Science and Systems, 2025. 7   
[169] C. Ning, K. Fang, and W.-C. Ma, "Prompting with the Future: Openworld model predictive control with interactive digital twins," in Robotics: Science and Systems, 2025. 7   
0h . ZY. a X.  . a .a .r, and S. Zhang, "GeoDrive: 3D geometry-informed driving world model with precise action control," arXiv preprint arXiv:2505.22421, 2025. 7, 13   
[171] R. Zheng, J. Wang, S. Reed, J. Bjorck, Y. Fang, F. Hu, J. Jang, K. Kundalia, Z. Lin, L. Magne et al., "FLARE: Robot learning with implicit world modeling," in Robotics: Science and Systems, 2025. 7   
[172] Y. Huang, J. Zhang, S. Zou, X. Liu, R. Hu, and K. Xu, "LaDi-WM: A latent diffusion-based world model for predictive manipulation," in Conf. Robot. Learn., 2025. 7   
[173] B. Wang, X. Meng, X. Wang, Z. Zhu, A. Ye, Y. Wang, Z. Yang, C. Ni, G. Huang, and X. Wang, "EmbodieDreamer: Advancing real2sim2real traner or poliytrai vi bodid worldmodeling," arXipret arXiv:2507.05198, 2025. 7   
[174] H. Zhang, Y. Xue, X. Yan, J. Zhang, W. Qiu, D. Bai, B. Liu, S. Cui, and Z. Li, "An efficient occupancy world model via decoupled dynamic flow and image-assisted training," arXiv preprint arXiv:2412.13772, 2024. 7, 15, 16   
[175] Y. Li, X. Wei, X. Chi, Y. Li, Z. Zhao, H. Wang, N. Ma, M. Lu, and S. Zhang, "ManipDreamer: Boosting robotic manipulation world model with action tree and visual guidance," arXiv preprint arXiv:2504.16464, 2025. 7   
[176] H. Zhi, P. Chen, S. Zhou, Y. Dong, Q. Wu, L. Han, and M. Tan, "3DFlowAction: Learning cross-embodiment manipulation from 3D flow world model," arXiv preprint arXiv:2506.06199, 2025. 8   
[177] A. Garg and K. Madhava Krishna, "Imagine-2-Drive: High-fidelity world modeling in carla for autonomous vehicles," in Int. Conf. Intell. Robot. Syst., 2025. 8   
. X Y. Y.   , Xia Ji l"Woi nd-on via intention-aware physical latent world model," in Int. Conf. Comput. Vis., 2025. 8   
[179] H. Zhang, Z. Wang, Q. Lyu, Z. Zhang, S. Chen, T. Shu, B. Dariush, K. Lee, Y. Du, and C. Gan, "COMBO: Compositional world models for embodied multi-agent cooperation," in Int. Conf. Learn. Represent., 2025, pp. 116. 8   
[180] Y. Shang, X. Zhang, Y. Tang, L. Jin, C. Gao, W. Wu, and Y. Li, "RoboScape: Physics-informed embodied world model," arXiv preprint arXiv:2506.23135, 2025. 8   
[181] R. Bonatti, S. Vemprala, S. Ma, F. Frujeri, S. Chen, and A. Kapoor, AC pre-training," in Int. Conf. Intell. Robot. Syst., 2023, pp. 36213627. 8   
[182] F. Baldassarre, M. Szafraniec, B. Terver, V. Khalidov, F. Massa, Y. LeCun, P. Labatut, M. Seitzer, and P. Bojanowski, "Back to the Features: DINO as a foundation for video world models," arXiv preprint arXiv:2507.19468, 2025. 8   
[183] Y. Huang, W. Zheng, Y. Gao, X. Tao, P. Wan, D. Zhang, J. Zhou, and J. Lu, "Owl-: Omni world model for consistent long video generation," arXiv preprint arXiv:2412.09600, 2024. 8   
[184] S. Huang, J. Wu, Q. Zhou, S. Miao, and M. Long, "Vid2World: Crafting video diffusion models to interactive world models," arXiv preprint arXiv:2505.14357, 2025. 8   
[85 H. Wu, D. Wu, T. He, J. Guo, Y. Ye, Y. Duan, and J. Bian, "Geory Forcing: Marrying video diffusion and 3D representation for consistent world modeling," arXiv preprint arXiv:2507.07982, 2025. 8   
[186] T. Chen, X. Hu, Z. Ding, and C. Jin, "Learning world models for interactive vide0 generation," arXiv preprint arXiv:2505.21996, 2025. 8   
[187] X. Guo, C. Ding, H. Dou, X. Zhang, W. Tang, and W. Wu, "InfinityDrive: Breaking time limits in driving world models," arXiv preprint arXiv:2412.01522, 2024. 8   
[188] X. Wang, Z. Wu, and P. Peng, "Longdwm: Cross-granularity distilo    ,   
H, . , H. Xi, H. , E. a, K.Yu, L   B. , "MiLA: Multi-view intensive-fidelity long-term video generation world model for autonomous driving," arXiv preprint arXiv:2503.15875, 2025. 8, 13   
[190] A. Mousakhan, S. Mittal, S. Galesso, K. Farid, and T. Brox, "Orbis: Overcoming challenges of long-horizon prediction in driving world models," arXiv preprint arXiv:2507.13162, 2025. 8   
[191] J. Quevedo, S. Ansh Kumar, Y. Sun, V. Suryavanshi, P. Liang, and S. Yang, "WorldGym: World model as an environment or policy evaluation," arXiv preprint arXiv:2506.00613, 2025. 8   
[192] Y. Li, Y. Zhu, J. Wen, C. Shen, and Y. Xu, "WorldEval: World model as real-world robot policies evaluator," arXiv preprint arXiv:2505.19017, 2025.8   
[193] Y. Guan, H. Liao, C. Wang, X. Liu, J. Zhang, and Z. Li, "World model-based End-to-End scene generation for accident anticipation in iCo  o  . 144, 2025. 8   
[194] R. Po, Y. Nitzan, R. Zhang, B. Chen, T. Dao, E. Shechtman, G. Wetzstein, and X. Huang, "Long-Context State-Space video world models," in Int. Conf. Comput. Vis., 2025. 8   
[195] V. L. Guen and N. Thome, "Disentangling physical dynamics from unknown factors for unsupervised video prediction," in IEEE Conf. Comput. Vis. Pattern Recog., 2020, pp. 11 47411 484. 9   
[196] X. Liu and H. Tang, "FOLIAGE: Towards physical intelligence world models via unbounded surface evolution," arXiv preprint arXiv:2506.03173, 2025. 9   
[197] S. Zhou, Y. Du, Y. Yang, L. Han, P. Chen, D.-Y. Yeung, and C. Gan, "Learning 3D persistent embodied world models," arXiv preprint arXiv:2505.05495, 2025. 9   
[198] J. Wang, Y. Yao, X. Feng, H. Wu, Y. Wang, Q. Huang, Y. Ma, and X. Zhu, "STAGE: A stream-centric generative world model for lonhorizon driving-scene simulation," in Int. Conf. Intell. Robot. Syst., 2025. 9, 13   
[199] T. Wu, S. Yang, R. Po, Y. Xu, Z. Liu, D. Lin, and G. Wetzstein, "Video world models with long-term spatial memory," arXiv preprint arXiv:2506.05284, 2025. 9   
[200] Y. LeCun, "A path towards autonomous machine intelligence," ver..o pape. va: https://openreview.net/pdf?id=BZ5a1r-kVsf9   
[201] H. Zhu, Z. Dong, K. Topollai, and A. Choromanska, "AD-L-JEPA: Self-supervised spatial world models with joint embedding predictive architecture for autonomous driving with LiDAR data," arXiv preprint arXiv:2501.04969, 2025. 9   
[202] Y. Zhang, X. Guo, H. Xu, and M. Long, "Consistent world models via foresight diffusion," arXiv preprint arXiv:2505.16474, 2025. 9   
[] B. Z R. Tag, M. , Z. Wg FMa, X. Z Y. Sh, W. Zhang, C. Gao, W. Wu et al., "AirScape: An aerial generative world model with motion controllability," arXiv preprint arXiv:2507.08885, 2025. 9   
[204] L. Li, Z. Fan, W. Cong, X. Liu, Y. Yin, M. Foutter, P. Pan, C. You, Y. Wang, Z. Wang et al., "Martian World Models: Controllable video synthesis with physically accurate 3D reconstructions," arXiv preprint arXiv:2507.07978, 2025. 9   
Y. Yue, Y. W H. Jiag P. Lu,S. S an G. Hu "EcW: Learning motion-aware world models for echocardiography probe guidance," in IEEE Conf. Comput. Vis. Pattern Recog., 2025, pp. 25 993 26003.9   
[206] D. Liang, D. Zhang, X. Zhou, S. Tu, T. Feng, X. Li, Y. Zhang, M. Du, X. Tan, and X. Bai, "Seeing the Future, Perceiving the Future: A unified driving world model for future generation and perception," arXiv preprint arXiv:2503.13587, 2025. 9, 13   
[207] Y. Zhang, S. Gong, K. Xiong, X. Ye, X. Tan, F. Wang, J. Huang, H. Wu, and H. Wang, "BEVWorld: A multimodal world model for autonomous driving via unified BEV latent space," arXiv e-prints, pp. arXiv2407, 2024. 9   
[208] T. Khurana, P. Hu, A. Dave, J. Ziglar, D. Held, and D. Ramanan, Differentiable raycasting for self-supervised occupancy forecasting," in Eur. Conf. Comput. Vis., 2022, pp. 353369. 9   
[209] T. Khurana, P. Hu, D. Held, and D. Ramanan, "Point cloud forecasting as a proxy for 4D occupancy forecasting," in IEEE Conf. Comput. Vis. Pattern Recog., 2023, pp. 11161124. 9   
[210] B. Mersch, X. Chen, J. Behley, and C. Stachniss, "Self-supervised point cloud prediction using 3D spatio-temporal convolutional networks," in Conf. Robot. Learn., 2022, pp. 14441454. 9   
[211] J. Ma, X. Chen, J. Huang, J. Xu, Z. Luo, J. Xu, W. Gu, R. Ai, and H. Wang, "Cam4DOcc: Benchmark for camera-only 4D occupancy forecasting in autonomous driving applications," in IEEE Conf. Comput. Vis. Pattern Recog., 2024, pp. 21 48621 495. 9   
[212] T. Liu, S. Zhao, and N. Rhinehart, "Towards foundational LiDAR world models with efficient latent flow matching," arXiv preprint arXiv:2506.23434, 2025. 9   
[213] Y. Shi, K. Jiang, Q. Meng, K. Wang, J. Wang, W. Sun, T. Wen, M. Yang, and D. Yang, "COME: Adding scene-centric forecasting control to occupancy world model," arXiv preprint arXiv:2506.13260, 2025. 10, 15   
[214] Z. Yang, X. Guo, C. Ding, C. Wang, and W. Wu, "Physical informed driving world model," arXiv preprint arXiv:2412.08410, 2024. 10, 13   
[215] Z. Liu, S. Li, E. Cousineau, S. Feng, B. Burchfiel, and S. Song, "Geometry-aware 4D video generation for robot manipulation," arXiv preprint arXiv:2507.01099, 2025. 10   
[216] B. Jin, W. Li, B. Yang, Z. Zhu, J. Jiang, H.-a. Gao, H. Sun, K. Zhan, H. Hu, X. Zhang et al., "PosePilo: Steering camera pose for generative world models with self-supervised depth," in Int. Conf. Intell. Robot. Syst., 2025. 10   
[217] J. Zhu, Z. Jia, T. Gao, J. Deng, S. Li, F. Liu, P. Jia, X. Lang, and X. Sun, "Other Vehicle Trajectories Are Also Needed: A driving world model unifies ego-other vehicle trajectories in video latent space," arXiv preprint arXiv:2503.09215, 2025. 10   
[218] E. Todorov, T. Erez, and Y. Tassa, "MuJoCo: A physics engine for mode-based control," in Int. Conf. Intell. Robot. Syst., 2012, pp. 5026 5033. 10, 11   
[219] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun, "CARLA: An open urban driving simulator," in Conf. Robot. Learn., 2017. 10, 11   
[220] M. Savva, A. Kadian, O. Maksymets, Y. Zhao, E. Wijmans, B. Jain, JStraub, J. Li V. Koltun, J. Malik et al. "Habita: plator o embodied AI research," in Int. Conf. Comput. Vis., 2019, pp. 93399347. 10, 11   
[221] V. Makoviychuk, L. Wawrzyniak, Y. Guo, M. Lu, K. Storey, M. Macklin, D.Heller, N. Rudin, A. Allshire, A. Handa, and G. State, "Isaac Gym: H    " in Annu. Conf. Neur. Inform. Process. Syst. Worksh., 2021. 10, 11   
[222] M. Mittal, C. Yu, Q. Yu, J. Liu, N. Rudin, D. Hoeller, J. L. Yuan, R. Singh, Y. Guo, H. Mazhar et al., "Orbit: A unified simulation framework for interactive robot learning environments," IEEE Robot. Autom. Lett., vol. 8, no. 6, pp. 37403747, 2023. 10, 11   
[223] M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling, "The arcade learning environment: An evaluation platform for general agents," Journal of artificial intelligence research, vol. 47, pp. 253279, 2013. 10, 11   
[224] Y. Tassa, Y. Doron, A. Muldal, T. Erez, Y. Li, D. d. L. Casas, D. Budden, A. Abdolmaleki, J. Merel, A. Lefrancq et al., "Deepmind control suite.," arXiv preprint arXiv:1801.00690, 2018. 10, 11   
[25] T. Yu, D. Quillen, Z. He, R. Julian, K. Hausman, C. Finn, and S. Levine, Meta-World: A benchmark and evaluation for multi-task and meta reinforcement learning," in Conf. Robot. Learn., 2020, pp. 10941100. 10, 11   
[226] S. James, Z. Ma, D. R. Arrojo, and A. J. Davison, "RLBench: The robot learning benchmark & learning environment," IEEE Robot. Autom. Lett, vol. 5, no. 2, pp. 30193026, 2020. 10, 11   
[] H. Ce, J. Kabzn, . S. Tan, W. K. Fog, E. Wol A. Lag, L. Fletcher, O. Beijbom, and S. Omari, "NuPlan: A closed-loop MLbased planning benchmark for autonomous vehicles," arXiv preprint arXiv:2106.11810, 2021. 10, 11   
[8 B. Liu, Y. Zhu, C. Gao, Y. Feng, Q. Liu, Y. Zhu, and P. Stone, "LIBERO: Benchmarking knowledge transfer for lifelong robot learning," in Annu. Conf. Neur. Inform. Process. Syst., 2023, pp. 44 77644 791. 10, 11   
[229] R. Goyal, S. Ebrahimi Kahou, V. Michalski, J. Materzynska, S. Westphal, H.Ki V. Haeel, I. Frund, P. Yanios, M. Mueller-Freitag l "The "Something Something" video database for learning and evaluating visual common sense," in Int. Conf. Comput. Vis., 2017, pp. 58425850. 11   
[230] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan, Y. Pan, G. Baldan, and O. Beijbom, "nuScenes: A multimodal dataset for autonomous driving," in IEEE Conf. Comput. Vis. Pattern Recog., 2020, pp. 11 62111 631. 11   
[1] P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V. Patnaik, P. Tsui, J.Guo Y. Zhou, Y.Chai B.Caine et l "Scalability in Perceptin or Autonomous Driving: Waymo Open Dataset," in IEEE Conf. Comput. Vis. Pattern Recog., 2020, pp. 24462454. 11   
.. A. E.  . A. .  .  . .X. M.Savva, Y. Zhao, and D. Batra, "Habitat-Matterport 3D Dataset (HM3D): 1000 large-scale 3D environments for embodied AI," in Annu. Conf. Neur. Inform. Process. Syst. Worksh., 2021. 11   
[233] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog, J. Hsu et al., "RT-1: Robotics transformer for real-world control at scale," arXiv preprint arXiv:2212.06817, 2022. 10, 11   
[234] X. Tian, T. Jiang, L. Yun, Y. Mao, H. Yang, Y. Wang, Y. Wang, and H. Zhao, "Occ3D: A large-scale 3D occupancy prediction benchmark for autonomous driving," in Annu. Conf. Neur. Inform. Process. Syst., 2023, pp. 64 31864 330. 11   
[235] A. O'Neill, A. Rehman, A. Maddukuri, A. Gupta, A. Padalkar, A. Lee, A. Pooley, A. Gupta, A. Mandlekar, A. Jain et al., "Open X-Embodiment: Robotic learning datasets and RT-X models : Open X-embodiment collaboration," in IEEE Int. Conf. Robot. Autom., 2024, pp. 68926903. 10, 11   
[36] S. Haddadin, S. Parusel, L. Johannsmeier, S. Golz, S. Gabl, F. Walch, M. Sabaghian, C. Jähne, L. Hausperger, and S. Haddadin, "The Franka Emika Robot: A reference platform for robotics research and education," IEEE Robotics & Automation Magazine, vol. 29, no. 2, pp. 4664, 2022. 11   
[237] Unitree Robotics, "Unitree Go1," 2021, accessed: 2025-09-25. [Online]. Available:https://www.unitree.com/cn/go111   
[238] Unitree G1," 2024, accessed: 2025-09-25. [Online]. Available: https://www.unitree.com/cn/g111   
[239] L. Kaiser, M. Babaeizadeh, P. Milos, B. Osiski, R. H. Campbell, K. Czechowski, D. Erhan, C. Finn, P. Kozakowski, S. Levine et al., Model based reinforcement learning for atari," in Int. Conf. Learn. Represent., 2020, pp. 114. 10   
[240] R. Zellers, J. Lu, X. Lu, Y. Yu, Y. Zhao, M. Salehi, A. Kusupati, J. Hessel, A. Farhadi, and Y. Choi, "MERLOT Reserve: Neural script kolee roug ion nd ngg nd und,"   Co. Comput. Vis. Pattern Recog., 2022, pp. 1637516 387. 11   
[241] A. Miech, D. Zhukov, J.-B. Alayrac, M. Tapaswi, I. Laptev, and J. Sivic, "HowTo100M: Learning a text-video embedding by watching hundred million narraed video clips,"in Int.Con.Comput. s.  pp. 26302640. 11   
[242] J. Carreira, E. Noland, C. Hillier, and A. Zisserman, "A short note on the kinetics-700 human action dataset," arXiv preprint arXiv:1907.06987, 2019. 11   
[243] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, "ImageNet: A large-scale hierarchical image database," in IEEE Conf. Comput. Vis. Pattern Recog., 2009, pp. 248255. 11   
[244] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter, "GANs trained by a two time-scale update rule converge to a local nash equilibrium," in Annu. Conf. Neur. Inform. Process. Syst., 2017. 12   
..o J . the inception architecture for computer vision," in IEEE Conf. Comput. Vis. Pattern Recog., 2016, pp. 28182826. 12   
[246] T. Unterthiner, S. Van Steenkiste, K. Kurach, R. Marinier, M. Michalski, and S. Gelly, "Towards Accurate Generative Models of Video: A new metric & challenges," arXiv preprint arXiv:1812.01717, 2018. 12   
[247] J. Carreira and A. Zisserman, "Quo Vadis, Action Recognition? A new model and the kinetics dataset," in IEEE Conf. Comput. Vis. Pattern Recog., 2017, pp. 62996308. 12   
[248] W. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier, S. Vijayanarasimhan, F. Viola, T. Green, T. Back, P. Natsev et al., "The Kinetics human action video dataset," arXiv preprint arXiv:1705.06950, 2017. 12   
[249] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, "Image quality assessment: from error visibility to structural similarity," IEEE Trans. Image Process., vol. 13, no. 4, pp. 600612, 2004. 12   
[250] A. Hore and D. Ziou, "Image quality metrics: PSNR vs. SSIM," in Int. Conf. Pattern Recog., 2010, pp. 23662369. 12   
[251] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, "The unreasonable effectiveness of deep features as a perceptual metric," in IEEE Conf. Comput. Vis. Pattern Recog., 2018, pp. 586595. 12   
[252] Z. Huang, Y. He, J. Yu, F. Zhang, C. Si, Y. Jiang, Y. Zhang, T. Wu, Q. Jin, N. Chanpaisit et al., "VBench: Comprehensive benchmark suite for video generative models," in IEEE Conf. Comput. Vis. Pattern Recog., 2024, pp. 21 80721 818. 12   
[253] H. Fan, H. Su, and L. J. Guibas, "A point set generation network for 3D object reconstruction from a single image," in IEEE Conf. Comput. Vis. Pattern Recog., 2017, pp. 605613. 13   
[254] Y. Hu, J. Yang, L. Chen, K. Li, C. Sima, X. Zhu, S. Chai, S. Du, T. Lin, W. Wang et al., "Planning-Oriented autonomous driving," in CVPR, 2023, pp. 17 85317 862. 16

[255] H. Yue, S. Huang, Y. Liao, S. Chen, P. Zhou, L. Chen, M. Yao, and G. Ren, "EWMBench: Evaluating scene, motion, and semantic quality in embodied world models," arXiv preprint arXiv:2505.09694, 2025. 14