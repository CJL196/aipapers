# MotionLCM: Real-time Controllable Motion Generation via Latent Consistency Model

Wenxun Dai $^ { 1 , 2 } \oplus$ - Ling-Hao $\mathrm { C h e n ^ { 2 \star } } \oplus$ , Jingbo Wang $\mathfrak { I P } _ { \mathbb { \oplus } }$ - Jinpeng Liu $^ { 1 , 2 } \oplus$ Bo Dai3 Yansong Tang $^ { 1 , 2 } \oplus$ $\bot$ Shenzhen Key Laboratory of Ubiquitous Data Enabling, Tsinghua Shenzhen International Graduate School 2Tsinghua University 3Shanghai AI Laboratory {wxdai2001, thu.lhchen, wangjingbo1219, liu.jinpeng.55}@gmail.com {doubledaibo, tangyansong15}@gmail.com Project page: https://dai-wenxun.github.io/MotionLCM-page

![](images/1.jpg)  
Fig. 1: We propose MotionLCM, a real-time controllable motion latent consistency model. Our model uses the last few frames of the previous motion as temporal control signals to autoregressively generate the next motion in real-time under different text prompts. Green blocks denote the junctions. The numbers in red are the inference time.

Abstract. This work introduces MotionLCM, extending controllable motion generation to a real-time level. Existing methods for spatialtemporal control in text-conditioned motion generation suffer from significant runtime inefficiency. To address this issue, we first propose the motion latent consistency model (MotionLCM) for motion generation, building on the motion latent diffusion model. By adopting one-step (or few-step) inference, we further improve the runtime efficiency of the motion latent diffusion model for motion generation. To ensure effective controllability, we incorporate a motion ControlNet within the latent space of MotionLCM and enable explicit control signals (i.e., initial motions) in the vanilla motion space to further provide supervision for the training process. By employing these techniques, our approach can generate human motions with text and control signals in real-time. Experimental results demonstrate the remarkable generation and controlling capabilities of MotionLCM while maintaining real-time runtime efficiency. Keywords: Text-to-Motion $\cdot$ Real-time Control · Consistency Model

# 1 Introduction

Text-to-motion generation (T2M) has attracted increasing attention [1, 15, 43, 49, 65] due to its important roles in many applications [70, 72]. Previous attempts mainly focus on GANs [1, 38], VAEs [3, 19, 48, 49] and diffusion models [9, 11, 39, 56, 65, 78, 83] via pairwise textmotion data [17, 26, 45, 52, 53, 57, 63, 74] and achieve impressive generation results. Existing approaches [9, 65, 83] mainly take diffusion models [20, 46, 55, 60] as a base generative model, owing to their powerful ability to model motion distribution. However, these diffusion fashions inevitably require considerable sampling steps for motion synthesis during inference, even with some sampling acceleration methods [61]. Specifically, MDM [65] and MLD [9] require $\cdots$ 24s and ${ \sim } 0 . 2 \mathrm { s }$ to generate a high-quality motion sequence. Such low efficiency blocks the applications of generating high-quality motions in various real-time scenarios.

![](images/2.jpg)  
Fig.2: Comparison of the inference time costs on HumanML3D [17]. We compare the AITS and FID metrics with five SOTA methods. The closer the model is to the origin the better. Diffusion-based models are indicated by the blue dashed box. Our MotionLCM achieves real-time inference speed while ensuring high-quality motion generation.

In addition to the language description itself serving as a coarse control signal, another line of research focuses on controlling the motion generation with spatial-temporal constraints [29, 56, 73]. Although these attempts enjoy impressive controlling ability in the T2M task, there still exists a significant gap towards real-time applications. For example, OmniControl [73] exhibits a relatively long inference time, $\sim$ 81s per sequence. Therefore, trading-off between generation quality and efficiency is a challenging problem. As a result, in this paper, we target the real-time controllable motion generation research problem. Recently, the concept of consistency models [44, 62] has been introduced in image generation, resulting in significant progress by enabling efficient and highfidelity image synthesis with a minimal number of sampling steps (e.g., 4 steps vs. 50 steps). These properties perfectly align with our goal of accelerating motion generation without compromising generation quality. Therefore, we propose MotionLCM (Motion Latent Consistency Model) distilled from the motion latent diffusion model, MLD [9], to tackle the low-efficiency problem in diffusion sampling. To the best of our knowledge, we introduce consistency distillation into the motion generation area for the first time and accelerate motion generation to a real-time level via latent consistency distillation [44].

Here, in MotionLCM, we are facing another challenge on how to control motions with spatial-temporal signals (i.e., initial motions) in the latent space. Previous methods [56, 73] model human motions in the vanilla motion space and can manipulate the motion directly in the denoising process. However, for our latent-diffusion-based MotionLCM, it is non-trivial to feed the control signals into the latent space. This is because the latent has no explicit motion semantics, which cannot be manipulated directly by the control signals. Inspired by the notable success of [82] in controllable image generation [55], we introduce a motion ControlNet to control motion generation in the latent space. However, the naïve motion ControlNet is not sufficient to provide supervision for the control signals. The main reason is the lack of explicit supervision in the motion space. Therefore, during the training phase, we decode the predicted latent through the frozen VAE [30] decoder into the vanilla motion space to provide explicit control supervision on the generated motion. Thanks to the powerful one-step inference capability of MotionLCM, the latent generated by MotionLCM can significantly facilitate control supervision both in the latent space and motion space for training the motion ControlNet compared to MLD [9]. With our key designs, our proposed MotionLCM successfully enjoys the balance between generation quality and efficiency in controllable motion generation. Before delivering into detail, we sum up our core contributions as follows. We propose the Motion Latent Consistency Model (MotionLCM) via consistency distillation on the motion latent diffusion model extending controllable motion generation to a real-time level. Building upon our achievement of real-time motion generation, we introduce a motion ControlNet, enabling high-quality controllable motion generation. Extensive experimental results show that MotionLCM enjoys a good balance of generation quality, controlling capability, and real-time efficiency.

# 2 Related Work

Generating human motions can be divided into three main fashions according to inputs: motion synthesis (1) without any condition [54,65,77,84], (2) with some given multi-modal conditions, such as action labels [6, 12, 19, 31, 48, 75], textual description [15, 7, 911, 1315, 17, 18, 24, 27, 29, 3739, 43, 4951, 56, 64, 65, 6870, 72,78,81,83,85,86], audio or music [3235,59,66], (3) with user-defined trajectories [22,23,29,36,56, 58,68,71, 73]. To generate diverse, natural, and high-quality human motions, many generative models have been explored by [2,38, 49,79,80]. Recently, diffusion-based models significantly improved the motion generation performance and diversity [8,9,11,16,65,76,83] with stable training. Specifically, MotionDiffuse [83] represents the first text-based motion diffusion model that provides fine-grained instructions on body parts and achieves arbitrary-length motion synthesis with time-varied text prompts. MDM [65] introduces a motion diffusion model that operates on raw motion data, enabling both high-quality generation and generic conditioning that together comprise a good baseline for new motion generation tasks. Based on MDM [65], OmniControl [73] integrates flexible spatial-temporal control signals across different joints by combining analytic spatial guidance and realism guidance into the diffusion model, ensuring that the generated motion closely conforms to the input control signals. The work most relevant to ours is MLD [9], which introduces a motion latent-based diffusion model to enhance generation quality and reduce computational resource requirements. The key idea is training a VAE [30] for motion embedding, followed by implementing latent diffusion [55] within the learned latent space. However, these diffusion fashions inevitably require considerable sampling steps for motion synthesis during inference, even with some sampling acceleration methods [61]. Thus, we propose MotionLCM, which not only guarantees high-quality controllable motion generation but also achieves real-time runtime efficiency.

# 3 Method

In this section, we first briefly introduce preliminaries about latent consistency models in Sec. 3.1. Then, we describe how to conduct latent consistency distillation for motion generation in Sec. 3.2, followed by our implementation of motion control in latent space in Sec. 3.3. The overall pipeline is illustrated in Fig. 4.

# 3.1 Preliminaries

The Consistency Model (CM) [62] introduces a kind of efficient generative model designed for efficient one-step or few-step generation. Given a Probability Flow ODE (a.k.a. PF-ODE) that smoothly converts data to noise, the CM is to learn the function $f ( \cdot , \cdot )$ that maps any points on the ODE trajectory to its origin distribution (i.e., the solution of the PF-ODE). The consistency function is formally defined as $f : ( \mathbf { x } _ { t } , t ) \longmapsto \mathbf { x } _ { \epsilon }$ , where $t \in [ \epsilon , T ]$ , $T > 0$ is a fixed constant and $\epsilon$ is a small positive number to avoid numerical instability. According to [62], the consistency function should satisfy the self-consistency property:

$$
\begin{array} { r } { \pmb { f } ( \mathbf { x } _ { t } , t ) = \pmb { f } ( \mathbf { x } _ { t ^ { \prime } } , t ^ { \prime } ) , \forall t , t ^ { \prime } \in [ \epsilon , T ] . } \end{array}
$$

![](images/3.jpg)  
Fig. 3: The training objective of consistency distillation is to learn a consistency function $f _ { \Theta }$ , initialized with the parameters of a pre-trained diffusion model (e.g., MLD [9]). This function $f _ { \Theta }$ should projects any points (i.e., $\mathbf { z } _ { t }$ ) on the ODE trajectory to its solution $( i . e . , \mathbf { z } _ { 0 } )$ . Once the pre-trained model [9] is distilled, unlike the traditional denoising model [65, 83] that requires considerable sampling steps, our MotionLCM can generate high-quality motion sequences with one-step sampling and further improve the generation quality through multi-step inference.

As shown in Eq. (1), the self-consistency property indicates that for arbitrary pairs of $\left( \mathbf { x } _ { t } , t \right)$ on the same PF-ODE trajectory, the outputs of the model should be consistent. The goal of a parameterized consistency model $f _ { \Theta }$ is to learn a consistency function from data by enforcing the self-consistency property in Eq. (1). To ensure that $f _ { \Theta } ( \mathbf { x } , \epsilon ) = \mathbf { x }$ , the consistency model $f _ { \Theta }$ is parameterized as, where $c _ { \mathrm { s k i p } } ( t )$ and $\boldsymbol { c } _ { \mathrm { o u t } } ( t )$ are differentiable functions with $c _ { \mathrm { s k i p } } ( \epsilon ) = 1$ and $c _ { \mathrm { o u t } } ( \epsilon ) = 0$ , and $F _ { \Theta } ( \cdot , \cdot )$ is a deep neural network to learn the self-consistency. The CM trained from distilling the knowledge of pre-trained diffusion models is called Consistency Distillation. The consistency loss is defined as follows, where $d ( \cdot , \cdot )$ is a chosen metric function for measuring the distance between two samples. $f _ { \Theta } ( \cdot , \cdot )$ and $f _ { \Theta ^ { - } } ( \cdot , \cdot )$ are referred to as "online network" and "target network" according to [62]. Besides, $\Theta ^ { - }$ is updated with the exponential moving average (EMA) of the parameters of $\Theta$ 1. In Eq. (3), $\hat { \mathbf { x } } _ { t _ { n } } ^ { \Phi }$ is a one-step estimation of $\mathbf { x } _ { t _ { n } }$ from $\mathbf { x } _ { t _ { n + 1 } }$ , which is formulated as, where $\Phi$ is a one-step ODE solver applied to PF-ODE.

$$
f _ { \Theta } ( \mathbf x , t ) = c _ { \mathrm { s k i p } } ( t ) \mathbf x + c _ { \mathrm { o u t } } ( t ) F _ { \Theta } ( \mathbf x , t ) ,
$$

$$
\begin{array} { r } { \mathcal { L } ( \boldsymbol { \Theta } , \boldsymbol { \Theta } ^ { - } ; \boldsymbol { \Phi } ) = \mathbb { E } \left[ d \left( \boldsymbol { f } _ { \boldsymbol { \Theta } } ( \mathbf { x } _ { t _ { n + 1 } } , t _ { n + 1 } ) , \boldsymbol { f } _ { \boldsymbol { \Theta } ^ { - } } ( \hat { \mathbf { x } } _ { t _ { n } } ^ { \boldsymbol { \Phi } } , t _ { n } ) \right) \right] , } \end{array}
$$

$$
\begin{array} { r } { \hat { \mathbf { x } } _ { t _ { n } } ^ { \Phi } \gets \mathbf { x } _ { t _ { n + 1 } } + ( t _ { n } - t _ { n + 1 } ) \Phi ( \mathbf { x } _ { t _ { n + 1 } } , t _ { n + 1 } , \emptyset ) , } \end{array}
$$

![](images/4.jpg)  
Fig. 4: The overview of MotionLCM. (a) Motion Latent Consistency Distillation (Sec. 3.2). Given a raw motion sequence $\mathbf { x } _ { 0 } ^ { 1 : N }$ , a pre-trained VAE [30] encoder first compresses it into the latent space, then a forward diffusion operation is performed to add $n { \mathrel { + { k } } }$ steps of noise. Then, the noisy ${ \mathbf z } _ { n + k }$ is fed into the online network and teacher network to predict the clean latent. The target network takes the $k$ -step estimation results of the teacher output to predict the clean latent. To learn self-consistency, a loss is applied to enforce the output of the online network and target network to be consistent. (b) Motion Control in Latent Space (Sec. 3.3). With the powerful MotionLCM trained in the first stage, we incorporate a motion ControlNet into the MotionLCM to achieve controllable motion generation. Furthermore, we leverage the decoded motion to explicitly supervise the spatial-temporal control signals (i.e., initial poses $\mathbf { g } ^ { 1 : \tau }$ ).

The Latent Consistency Model (LCM) [44] learns the self-consistency property in the latent space $D _ { \mathbf { z } } = \{ ( \mathbf { z } , \mathbf { c } ) | \mathbf { z } = { \mathcal { E } } ( \mathbf { x } ) , ( \mathbf { x } , \mathbf { c } ) \in D \}$ , where $D$ denotes the dataset, $\mathbf { c }$ is the given condition, and $\varepsilon$ is the pre-trained encoder. Compared to CMs [62] using the numerical continuous PF-ODE solver [28], LCMs [44] adopt the discrete-time schedule [41, 42,61] to adapt to Stable Diffusion [55]. Instead of ensuring consistency between adjacent time steps $t _ { n + 1 } \to t _ { n }$ , LCMs [44] are designed to ensure consistency between the current time step and $k$ -step away, i.e., $t _ { n + k } \ \to \ t _ { n }$ , thereby significantly reducing convergence time costs. As classifier-free guidance (CFG) [21] plays a crucial role in synthesizing highquality text-aligned images, LCMs integrate CFG into the distillation as follows, where $w$ denotes the CFG scale which is uniformly sampled from $[ w _ { \mathrm { m i n } } , w _ { \mathrm { m a x } } ]$ and $k$ is the skipping interval. To efficiently perform the above $k$ -step guided distillation, LCMs augment the consistency function to $f : ( \mathbf { z } _ { t } , t , w , \mathbf { c } ) \longmapsto \mathbf { z } _ { 0 }$ , which is also the form adopted by our MotionLCM.

$$
\hat { \mathbf { z } } _ { t _ { n } } ^ { \Phi , w } \gets \mathbf { z } _ { t _ { n + k } } + ( 1 + w ) \Phi ( \mathbf { z } _ { t _ { n + k } } , t _ { n + k } , t _ { n } , \mathbf { c } ) - w \Phi ( \mathbf { z } _ { t _ { n + k } } , t _ { n + k } , t _ { n } , \emptyset ) ,
$$

# 3.2 MotionLCM: Motion Latent Consistency Model

Motion compression into the latent space. Motivated by [44, 62], we propose MotionLCM (Motion Latent Consistency Model) to tackle the low-efficiency problem in motion diffusion models [65, 83], unleashing the potential of LCM in the motion generation task. Similar to MLD [9], our MotionLCM adopts a consistency model in the motion latent space. We choose MLD [9] as the underlying diffusion model to distill from. We aim to achieve few-step (2\~4) and even onestep inference without compromising motion quality. In MLD, an autoencoder $( \mathcal { E } , \mathcal { D } )$ is first trained to compress a high dimensional motion into low dimensional latent vectors $\mathbf { z } = { \mathcal { E } } ( \mathbf { x } )$ , which are then decoded to reconstruct the motion as $\hat { \mathbf { x } } = \mathcal { D } ( \mathbf { z } )$ . Training diffusion models in the motion latent space greatly reduces the computational resources compared to the vanilla diffusion models trained on raw motion sequences (i.e., motion space) and speeds up the inference process. Thus, we effectively leverage the motion latent space for consistency distillation. Motion latent consistency distillation. An overview of our motion latent ct stilation is cibe Fi. (a). A  oti ${ \bf x } _ { 0 } ^ { 1 : N } =$ $\{ \mathbf { x } ^ { i } \} _ { i = 1 } ^ { N }$ is a sequence of human poses, where $N$ is the number of frames. We follow [17] to use the redundant motion representation for our experiments, which is widely used in previous work [9, 65, 83]. As shown in Fig. 4(a), given a raw motion sequence $\mathbf { x } _ { 0 } ^ { 1 : N }$ the latent space, ${ \bf z } _ { 0 } = \mathcal { E } ( { \bf x } _ { 0 } )$ . Then, a forward diffusion operation with $n + k$ steps is conducted to add noise on $\mathbf { z } _ { 0 }$ , where $k$ is the skipping interval illustrated in Sec. 3.1. The noisy ${ \mathbf z } _ { n + k }$ is fed to the frozen teacher network and trainable online network to predict the clean $\hat { \mathbf { z } } _ { 0 } ^ { \ast }$ , and $\hat { \mathbf { z } } _ { 0 }$ . The target network uses the cleaner $\hat { \mathbf { z } } _ { n }$ obtained by a $k$ -step ODE solver $\Phi$ , such as DDIM [61] to predict the $\hat { \mathbf { z } } _ { 0 } ^ { - }$ . Since the classifier-free guidance (CFG) [21] is essential for condition alignment in diffusion models [9, 55, 65], we integrate CFG into the distillation, where $\mathbf { c }$ is the text condition and $w$ denotes the guidance scale. To ensure the self-consistency property defined in Eq. (1), the latent consistency distillation loss $\mathcal { L } _ { \mathrm { L C D } }$ is designed as follows, where $d ( \cdot , \cdot )$ is a distance measuring function, such as L2 loss or Huber loss [25]. As discussed in Sec. 3.1, the target network $\Theta ^ { - }$ is updated with the exponential moving average (EMA) of the trainable parameters of the online network $\Theta$ . Here we define the teacher network $\Theta ^ { * }$ as the pre-trained motion latent diffusion model, i.e., MLD [9]. According to [44], the online network and target network are initialized with the parameters of the teacher network. During the inference phase, as shown in Fig. 5, our MotionLCM can generate high-quality motions with one-step sampling and achieve the fastest runtime ( $\sim$ 30ms per motion sequence) compared to other motion diffusion models [9, 65].

$$
\hat { \mathbf { z } } _ { n } \gets \mathbf { z } _ { n + k } + ( 1 + w ) \Phi ( \mathbf { z } _ { n + k } , t _ { n + k } , t _ { n } , \mathbf { c } ) - w \Phi ( \mathbf { z } _ { n + k } , t _ { n + k } , t _ { n } , \emptyset ) ,
$$

$$
\mathcal { L } _ { \mathrm { L C D } } ( \boldsymbol { \Theta } , \boldsymbol { \Theta } ^ { - } ) = \mathbb { E } \left[ d \left( f _ { \boldsymbol { \Theta } } ( \mathbf { z } _ { n + k } , t _ { n + k } , w , \mathbf { c } ) , \boldsymbol { f } _ { \boldsymbol { \Theta } ^ { - } } ( \hat { \mathbf { z } } _ { n } , t _ { n } , w , \mathbf { c } ) \right) \right] ,
$$

# 3.3 Controllable Motion Generation in Latent Space

After addressing the low-efficiency issue in the motion latent diffusion model [9], we delve into another exploration of real-time motion control. Inspired by the great success of ControlNet [82] in controllable image generation [55], we introduce a motion ControlNet $\Theta ^ { a }$ in the latent space of MotionLCM and initialize the motion ControlNet with a trainable copy of MotionLCM. Specifically, each layer in the motion ControlNet is appended with a zero-initialized linear layer for eliminating random noise in the initial training steps. To achieve an autoregressive motion generation paradigm, as depicted in Fig. 1, we define the motion control task as generating motions given the initial $\tau$ poses and textual description. As shown in Fig. 4(b), the initial $\tau$ poses are defined by the trajectories of $K$ control joints, $\mathbf { g } ^ { 1 : \tau } = \{ \mathbf { g } ^ { i } \} _ { i = 1 } ^ { \tau }$ , where $\mathbf { g } ^ { i } \in \mathbb { R } ^ { K \times 3 }$ denotes the global absolute locations of each control joint. In our motion control pipeline, we design a Trajectory Encoder $\Theta ^ { b }$ consisting of stacked transformer [67] layers to encode the trajectory signals. We append a global token (i.e., [CLS]) before the start of the trajectory sequence as the output feature of the encoder, which is added to the noisy $\mathbf { z } _ { n }$ and fed into the trainable motion ControlNet $\Theta ^ { a }$ . Under the guidance of motion ControlNet, MotionLCM predicts the denoised $\hat { \mathbf { z } } _ { 0 }$ through the consistency function $f _ { \Theta ^ { s } }$ , where $\Theta ^ { s }$ is the combination of $\Theta ^ { a }$ , $\Theta ^ { b }$ and $\Theta$ . The following reconstruction loss $\mathcal { L } _ { \mathrm { r e c o n } }$ optimizes the motion ControlNet $\Theta ^ { a }$ and Trajectory Encoder $\Theta ^ { b }$ , where $\mathbf { c } ^ { * }$ includes the text condition and control guidance from the Trajectory Encoder and the motion ControlNet. However, during training, the sole reconstruction supervision in the latent space is insufficient. We argue this is because the controllable motion generation requires more detailed constraints, which cannot be effectively provided solely by the reconstruction loss in the latent space. Unlike previous methods like OmniControl [73], which directly diffuse in the motion space, allowing explicit supervision of control signals, effectively supervising control signals in the latent space is non-trivial. Therefore, we utilize the frozen VAE [30] decoder $\mathcal { D }$ to decode the latent $\hat { \mathbf { z } } _ { 0 }$ into the motion space, obtaining the predicted motion $\hat { \mathbf { x } } _ { 0 }$ , thereby introducing the control loss ${ \mathcal { L } } _ { \mathrm { c o n t r o l } }$ as follows, where $R ( \cdot )$ converts the joint local positions to global absolute locations and $m _ { i j } \in \{ 0 , 1 \}$ is the binary joint mask at frame $_ i$ for the joint $j$ . Then we optimize the motion ControlNet $\Theta ^ { a }$ and Trajectory Encoder $\Theta ^ { b }$ with the overall objective, where $\lambda$ is the weight to balance the two losses. This design enables explicit control signals in the vanilla motion space to further provide supervision for the generation process. Extensive experiments demonstrate that the introduced supervision is beneficial in improving motion control performance, which will be introduced in the following section.

$$
\mathcal { L } _ { \mathrm { r e c o n } } ( \Theta ^ { a } , \Theta ^ { b } ) = \mathbb { E } \left[ d \left( f _ { \Theta ^ { s } } \left( \mathbf { z } _ { n } , t _ { n } , w , \mathbf { c } ^ { * } \right) , \mathbf { z } _ { 0 } \right) \right] ,
$$

$$
\mathcal { L } _ { \mathrm { c o n t r o l } } ( \Theta ^ { a } , \Theta ^ { b } ) = \mathbb { E } \left[ \frac { \sum _ { i } \sum _ { j } m _ { i j } | | R ( \hat { \mathbf { x } } _ { 0 } ) _ { i j } - R ( \mathbf { x } _ { 0 } ) _ { i j } | | _ { 2 } ^ { 2 } } { \sum _ { i } \sum _ { j } m _ { i j } } \right] ,
$$

$$
\begin{array} { r } { \Theta ^ { a } , \Theta ^ { b } = \underset { \Theta ^ { a } , \Theta ^ { b } } { \arg \operatorname* { m i n } } ( \mathcal { L } _ { \mathrm { r e c o n } } + \lambda \mathcal { L } _ { \mathrm { c o n t r o l } } ) , } \end{array}
$$

# 4 Experiments

In this section, we first present the experimental setup details in Sec. 4.1. Subsequently, we provide quantitative and qualitative comparisons to evaluate the effectiveness of our proposed MotionLCM framework in Sec. 4.2 and Sec. 4.3. Finally, we conduct comprehensive ablation studies on MotionLCM in Sec. 4.4.

# 4.1 Experimental setup

Datasets. We experiment on the popular HumanML3D [17] dataset, featuring 14,616 unique human motion sequences with 44,970 textual descriptions. For a fair comparison with previous methods [9, 17, 49, 65, 83], we take the redundant motion representation, including root velocity, root height, local joint positions, velocities, rotations in root space, and the foot contact binary labels.

Evaluation metrics. We extend the evaluation metrics of previous works [9, 17, 73]. (1) Time cost: We follow [9] to report the Average Inference Time per Sentence (AITS) to evaluate the inference efficiency of models. (2) Motion quality: Frechet Inception Distance (FID) is adopted as a principal metric to evaluate the feature distributions between the generated and real motions. The feature extractor employed is from [17]. (3) Motion diversity: MultiModality (MModality) measures the generation diversity conditioned on the same text and Diversity calculates variance through features [17]. (4) Condition matching: Following [17], we calculate the motion-retrieval precision (R-Precision) to report the text-motion Top-1/2/3 matching accuracy and Multimodal Distance (MM Dist) calculates the mean distance between motions and texts. (5) Control error: Trajectory error (Traj. err.) quantifies the ratio of unsuccessful trajectories, characterized by any control joint location error surpassing a predetermined threshold. Location error (Loc. err.) represents the unsuccessful joints. Average error (Avg. err.) denotes the mean location error of the control joints.

Implementation details. Our baseline motion diffusion model is based on MLD [9]. We reproduce MLD with higher performance. Unless otherwise specified, all our experiments are conducted on this model. For MotionLCM, we employ the AdamW [40] optimizer for 96K iterations using a cosine decay learning rate scheduler and 1K iterations of linear warm-up. A batch size of 256 and a learning rate of 2e-4 are used. We set the training guidance scale range as $[ w _ { \mathrm { m i n } } , w _ { \mathrm { m a x } } ] = [ 5 , 1 5 ]$ , with the testing guidance scale set to 7.5, and adopt the EMA rate $\mu = 0 . 9 5$ by default. We use the DDIM [61] solver with skipping interval $k = 2 0$ and choose the Huber [25] loss as the distance measuring function $d$ . For motion ControlNet, we use the AdamW [40] optimizer for 192K iterations with 1K iterations of linear warm-up. The batch size and learning rate are set to 128 and 1e-4. The learning rate scheduler is the same as the first stage. For the training objective, we employ $d$ as the L2 loss and set the control loss weight $\lambda$ to 1.0 by default. We set the control ratio $\tau$ as 0.25 and the number of control joints as $K = 6$ (i.e., Pelvis, Left foot, Right foot, Head, Left wrist, and Right wrist) in both training and testing. We implement our model using PyTorch [47] with training on an NVIDIA RTX 4090 GPU and testing on a Tesla V100 GPU.

Table 1: Comparison of text-conditional motion synthesis on HumanML3D [17] dataset. We compute suggested metrics following [17]. We repeat the evaluation 20 times for each metric and report the average with a $9 5 \%$ confidence interval. " indicates that the closer to the real data, the better. Bold and underline indicate the best and the second best result. “\*" denotes the reproduced version of MLD [9]. The MotionLCM in one-step inference $\mathbf { ( 3 0 m s ) }$ surpasses all state-of-the-art models.   

<table><tr><td rowspan="2">Methods</td><td rowspan="2">AITS ↓</td><td colspan="3">R-Precision ↑</td><td rowspan="2">FID ↓</td><td rowspan="2">MM Dist ↓</td><td rowspan="2"></td><td rowspan="2">Diversity → MModality ↑</td></tr><tr><td>Top 1</td><td>Top 2</td><td>Top 3</td></tr><tr><td>Real</td><td>-</td><td>0.511±.003</td><td>0.703±.003</td><td>0.797±.002</td><td>0.002±.000</td><td>2.974±.008</td><td>9.503±.065</td><td>-</td></tr><tr><td>Seq2Seq [37]</td><td>-</td><td>0.180±.002</td><td>0.300±.002</td><td>0.396±.002</td><td>11.75±.035</td><td>5.529±.007</td><td>6.223±.061</td><td>-</td></tr><tr><td>JL2P [2]</td><td></td><td>0.246±.002</td><td>0.387±.002</td><td>0.486±.002</td><td>11.02±.046</td><td>5.296±.008</td><td>7.676±.058</td><td>-</td></tr><tr><td>T2G [5]</td><td></td><td>0.165±.001</td><td>0.267±.002</td><td>0.345±.002</td><td>7.664±.030</td><td>6.030±.008</td><td>6.409±.071</td><td></td></tr><tr><td>Hier [14]</td><td></td><td>0.301±.002</td><td>0.425±.002</td><td>0.552±.004</td><td>6.532±.024</td><td>5.012±.018</td><td>8.332±.042</td><td></td></tr><tr><td>TEMOS [49]</td><td>0.017</td><td>0.424±.002</td><td>0.612±.002</td><td>0.722±.002</td><td>3.734±.028</td><td>3.703±.008</td><td>8.973±.071</td><td>0.368±.018</td></tr><tr><td>T2M [17]</td><td>0.038</td><td>0.457±.002</td><td>0.639±.003</td><td>0.740±.003</td><td>1.067±.002</td><td>3.340±.008</td><td>9.188±.002</td><td>2.090±.083</td></tr><tr><td>MDM [65]</td><td>24.74</td><td>0.320±.005</td><td>0.498±.004</td><td>0.611±.007</td><td>0.544±.044</td><td>5.566±.027</td><td>9.559±.086</td><td>2.799±.072</td></tr><tr><td>MotionDiffuse [83]</td><td>14.74</td><td>0.491±.001</td><td>0.681±.001</td><td>0.782±.001</td><td>0.630±.001</td><td>3.113±.001</td><td>9.410±.049</td><td>1.553±.042</td></tr><tr><td>MLD [9]</td><td>0.217</td><td>0.481±.003</td><td>0.673±.003</td><td>0.772±.002</td><td>0.473±.013</td><td>3.196±.010</td><td>9.724±.082</td><td>2.413±.079</td></tr><tr><td>MLD* [9]</td><td>0.225</td><td>0.504±.002</td><td>0.698±.003</td><td>0.796±.002</td><td>0.450±.011</td><td>3.052±.009</td><td>9.634±.064</td><td>2.267±.082</td></tr><tr><td>MotionLCM (1-step)</td><td>0.030</td><td>0.502±.003</td><td>0.701±.002</td><td>0.803±.002</td><td>0.467±.012</td><td>3.022±.009</td><td>9.631±.066</td><td>2.172±.082</td></tr><tr><td>MotionLCM (2-step)</td><td>0.035</td><td>0.505±.003</td><td>0.705±.002</td><td>0.805±.002</td><td>0.368±.011</td><td>2.986±.008</td><td>9.640±.052</td><td>2.187±.094</td></tr><tr><td>MotionLCM (4-step)</td><td>0.043</td><td>0.502±.003</td><td>0.698±.002</td><td>0.798±.002</td><td>0.304±.012</td><td>3.012±.007</td><td>9.607±.066</td><td>2.259±.092</td></tr></table>

# 4.2 Comparisons on Text-to-motion

In the following part, we first evaluate our MotionLCM on the text-to-motion (T2M) task. We compare our method with some T2M baselines on HumanML3D [17] with suggested metrics [17] under the $9 5 \%$ confidence interval from 20 times running. As MotionLCM is based on MLD, we mainly focus on the performance compared with MLD. For evaluating time efficiency, we compare the Average Inference Time per Sentence (AITS) with TEMOS [49], T2M [17], MDM [65], MotionDiffuse [83] and MLD [9]. The results are borrowed from MLD [9]. The deterministic methods [2, 5, 14,37], are unable to produce diverse results from a single input text and thus we leave their MModality metrics empty. For the quantitative results, as shown in Tab. 1, our MotionLCM boasts an impressive real-time runtime efficiency, averaging around 30ms per motion sequence during inference. This performance exceeds that of previous diffusion-based methods [9,65,83] and even surpasses MLD [9] by an order of magnitude. Furthermore, despite employing only one-step inference, our MotionLCM can approximate or even surpass the performance of MLD [9] (DDIM [61] 50 steps). With two-step inference, we achieve the best R-Precision and MM Dist metrics, while increasing the sampling steps to four yields the best FID. The above results demonstrate the effectiveness of latent consistency distillation. For the qualitative results, as shown in Fig. 5, MotionLCM not only accelerates motion generation to real-time speed but also delivers high-quality outputs, closely aligning with the textual descriptions.

# 4.3 Comparisons on Controllable Motion Generation

As shown in Tab. 2, we compare our MotionLCM with OmniControl [73] and MLD [9]. We observe that OmniControl struggles with multi-joint control and falls short in both generation quality and control performance compared to MotionLCM. To verify the effectiveness of the latent generated by our MotionLCM for training motion ControlNet, we conducted the following two sets of experiments: "LC" and "MC", which indicate introducing control supervision in the latent space and motion space. It can be observed that under the same experimental settings, MotionLCM maintains higher fidelity and significantly outperforms MLD [9] in motion control performance. This demonstrates that the latent generated by MotionLCM is more effective for training motion ControlNet compared to MLD [9]. In terms of inference speed, MotionLCM (1-step) is $\mathbf { 1 9 2 9 } \times$ faster compared to OmniControl [73] and $\mathbf { 1 3 \times }$ faster than MLD [9]. For qualitative results, as shown in Fig. 6, OmniControl fails to control the initial poses in the second example and does not generate motion that aligns with the text in the third case. However, our MotionLCM not only adheres to the control of the initial poses but also generates motions that match the textual descriptions.

![](images/5.jpg)  
Fig. 5: Qualitative comparison of the state-of-the-art methods in the text-to-motion task. With only one-step inference, MotionLCM achieves the fastest motion generation while producing high-quality movements that closely match the textual descriptions.

# 4.4 Ablation Studies

Impact of the hyperparameters of training MotionLCM. We conduct a comprehensive analysis of the training hyperparameters of MotionLCM, including the training guidance scale range $[ w _ { \mathrm { m i n } } , w _ { \mathrm { m a x } } ]$ , EMA rate $\mu$ , skipping interval $k$ , and the type of loss. We summarize the evaluation results based on one-step inference in Tab. 3. We find out that using a dynamic training guidance scale (e.g., $w \in \left[ 5 , 1 5 \right]$ ) during training leads to an improvement in model performance compared to using a static training guidance scale (e.g., $w = 7 . 5$ ). Additionally, an excessively large range for the training guidance scale can also negatively impact the performance of the model (e.g., $w \in \left[ 2 , 1 8 \right]$ ). Regarding the EMA rate $\mu$ , we observe that the larger the value of $\mu$ , the better the performance of the model. This indicates that maintaining a slower update rate for the target network $\Theta ^ { - }$ helps improve the performance of latent consistency distillation. When the skipping interval $k$ continues to increase, the performance of the distillation model improves progressively, but larger values of $k$ (e.g., $k = 5 0$ ) may result in inferior results. As for the type of loss, the Huber loss [25] significantly outperforms the L2 loss, demonstrating its superior robustness.

Table 2: Comparison of motion control on HumanML3D [17] dataset. Bold indicates the best result. Our MotionLCM outperforms OmniControl [73] and MLD [9] regarding generation quality, control performance, and inference speed. "LC" and "MC" refer to the control supervision introduced in the latent space and motion space.   

<table><tr><td>Methods</td><td>AITS↓</td><td>FID ↓</td><td>R-Precision ↑ Top 3</td><td>Diversity →</td><td>Traj. err. ↓ (50cm)</td><td>Loc. err. ↓ (50cm)</td><td>Avg. err. ↓</td></tr><tr><td>Real</td><td>-</td><td>0.002</td><td>0.797</td><td>9.503</td><td>0.0000</td><td>0.0000</td><td>0.0000</td></tr><tr><td>OmniControl [73]</td><td>81.00</td><td>2.328</td><td>0.557</td><td>8.867</td><td>0.3362</td><td>0.0322</td><td>0.0977</td></tr><tr><td>MLD [9] (LC)</td><td>0.552</td><td>0.469</td><td>0.723</td><td>9.476</td><td>0.4230</td><td>0.0653</td><td>0.1690</td></tr><tr><td>MotionLCM (1-step, LC)</td><td>0.042</td><td>0.319</td><td>0.752</td><td>9.424</td><td>0.2986</td><td>0.0344</td><td>0.1410</td></tr><tr><td>MotionLCM (2-step, LC)</td><td>0.047</td><td>0.315</td><td>0.770</td><td>9.427</td><td>0.2840</td><td>0.0328</td><td>0.1365</td></tr><tr><td>MotionLCM (4-step, LC)</td><td>0.063</td><td>0.328</td><td>0.745</td><td>9.441</td><td>0.2973</td><td>0.0339</td><td>0.1398</td></tr><tr><td>MLD [9] (LC&amp;MC)</td><td>0.552</td><td>0.555</td><td>0.754</td><td>9.373</td><td>0.2722</td><td>0.0215</td><td>0.1265</td></tr><tr><td>MotionLCM (1-step, LC&amp;MC)</td><td>0.042</td><td>0.419</td><td>0.756</td><td>9.390</td><td>0.1988</td><td>0.0147</td><td>0.1127</td></tr><tr><td>MotionLCM (2-step, LC&amp;MC)</td><td>0.047</td><td>0.397</td><td>0.759</td><td>9.469</td><td>0.1960</td><td>0.0143</td><td>0.1092</td></tr><tr><td>MotionLCM (4-step, LC&amp;MC)</td><td>0.063</td><td>0.444</td><td>0.753</td><td>9.355</td><td>0.2089</td><td>0.0172</td><td>0.1140</td></tr></table>

Table 3: Ablation study on different training guidance scale ranges $[ w _ { \mathrm { m i n } } , w _ { \mathrm { m a x } } ]$ , EMA rates $\mu$ , skipping intervals $k$ and types of loss. We use metrics in Tab. 1 and adopt a one-step inference setting with the testing CFG scale of 7.5 for fair comparison.   

<table><tr><td>Methods</td><td>R-Precision ↑ Top 1</td><td>FID ↓</td><td>MM Dist ↓</td><td></td><td>Diversity → MModality ↑</td></tr><tr><td>Real</td><td>0.511±.003</td><td>0.002±.000</td><td>2.974±.008</td><td>9.503±.065</td><td>-</td></tr><tr><td>MotionLCM (w  [5, 15])</td><td>0.502±.003</td><td>0.467±.012</td><td>3.022±.009</td><td>9.631±.066</td><td>2.172±.082</td></tr><tr><td>MotionLCM (w  [2, 18])</td><td>0.497±.003</td><td>0.481±.009</td><td>3.030±.010</td><td>9.644±.073</td><td>2.226±.091</td></tr><tr><td>MotionLCM (w = 7.5)</td><td>0.486±.002</td><td>0.479±.009</td><td>3.094±.009</td><td>9.610±.072</td><td>2.320±.097</td></tr><tr><td>MotionLCM (µ = 0.95)</td><td>0.502±.003</td><td>0.467±.012</td><td>3.022±.009</td><td>9.631±.066</td><td>2.172±.082</td></tr><tr><td>MotionLCM (µ = 0.50)</td><td>0.498±.003</td><td>0.478±.009</td><td>3.022±.010</td><td>9.655±.071</td><td>2.188±.087</td></tr><tr><td>MotionLCM (µ = 0)</td><td>0.499±.003</td><td>0.505±.008</td><td>3.018±.009</td><td>9.706±.070</td><td>2.123±.085</td></tr><tr><td>MotionLCM (k = 50)</td><td>0.488±.003</td><td>0.547±.011</td><td>3.096±.010</td><td>9.511±.074</td><td>2.324±.091</td></tr><tr><td>MotionLCM (k = 20)</td><td>0.502±.003</td><td>0.467±.012</td><td>3.022±.009</td><td>9.631±.066</td><td>2.172±.082</td></tr><tr><td>MotionLCM (k = 10)</td><td>0.497±.003</td><td>0.449±.009</td><td>3.017±.010</td><td>9.693±.075</td><td>2.133±.086</td></tr><tr><td>MotionLCM (k = 5)</td><td>0.488±.003</td><td>0.438±.009</td><td>3.044±.009</td><td>9.647±.074</td><td>2.147±.083</td></tr><tr><td>MotionLCM (k = 1)</td><td>0.442±.002</td><td>0.635±.011</td><td>3.255±.008</td><td>9.384±.080</td><td>2.146±.075</td></tr><tr><td>MotionLCM (w/ Huber)</td><td>0.502±.003</td><td>0.467±.012</td><td>3.022±.009</td><td>9.631±.066</td><td>2.172±.082</td></tr><tr><td>MotionLCM (w/ L2)</td><td>0.486±.002</td><td>0.622±.010</td><td>3.114±.009</td><td>9.573±.069</td><td>2.218±.086</td></tr></table>

![](images/6.jpg)  
Fig. 6: Qualitative comparison of the state-of-the-art methods in the motion control task. We provide the visualized motion results and real references from five prompts. Compared to OmniControl [73], MotionLCM with ControlNet not only generates the initial poses that accurately follow the given multi-joint trajectories (i.e., the green poses in real references) but also produces motions that closely align with the texts.

Impact of control loss weights $\lambda$ . To verify the impact of different control loss weights $\lambda$ on the control performance of MotionLCM, we report the experimental results in Tab. 4. We also include experiments of MotionLCM without ControlNet (i.e., only text-to-motion) for comparison. We found a significant improvement in control-related metrics (e.g., Loc. err.) after introducing motion ControlNet (i.e., $\lambda = 0$ ). Furthermore, control performance can be further improved by introducing control loss (i.e., $\lambda > 0$ ). Increasing the weight $\lambda$ enhances control performance but leads to a decline in the generation quality, which is reflected in higher FID scores. To balance these two aspects, we adopt $\lambda = 1$ as our default setting for training motion ControlNet. Impact of different control ratios $\tau$ and number of control joints $K$ In Tab. 5, we present the results of all models with the testing control ratio as 0.25 and keep the number of control joints $K$ equal in both training and testing. We found that the model with the fixed training control ratio (i.e., $\tau = 0 . 2 5$ ) performs better compared to a dynamic ratio (e.g., $\tau \in [ 0 . 1 , 0 . 5 ]$ ), and we discover that our model maintains good performance when incorporating additional redundant control signals, such as whole-body joints with $K = 2 2$ .

Table 4: Ablation study on different control loss weights $\lambda$ . We present the results of (1, 2, 4)-step inference. We add the MotionLCM without ControlNet for comparison.   

<table><tr><td>Methods</td><td>FID ↓</td><td>R-Precision ↑ Top 3</td><td>Diversity → Traj. err. ↓</td><td>(50cm)</td><td>(50cm)</td><td>Loc. err. ↓ Avg. err. ↓</td></tr><tr><td>Real</td><td>0.002</td><td>0.797</td><td>9.503</td><td>0.0000</td><td>0.0000</td><td>0.0000</td></tr><tr><td>MotionLCM (1-step, w/o control)</td><td>0.467</td><td>0.803</td><td>9.631</td><td>0.7605</td><td>0.2302</td><td>0.3493</td></tr><tr><td>MotionLCM (2-step, w/o control)</td><td>0.368</td><td>0.805</td><td>9.640</td><td>0.7646</td><td>0.2214</td><td>0.3386</td></tr><tr><td>MotionLCM (4-step, w/o control)</td><td>0.304</td><td>0.798</td><td>9.607</td><td>0.7739</td><td>0.2207</td><td>0.3359</td></tr><tr><td>MotionLCM (1-step, λ = 0)</td><td>0.319</td><td>0.752</td><td>9.424</td><td>0.2986</td><td>0.0344</td><td>0.1410</td></tr><tr><td>MotionLCM (2-step, λ = 0)</td><td>0.315</td><td>0.770</td><td>9.427</td><td>0.2840</td><td>0.0328</td><td>0.1365</td></tr><tr><td>MotionLCM (4-step, λ = 0)</td><td>0.328</td><td>0.745</td><td>9.441</td><td>0.2973</td><td>0.0339</td><td>0.1398</td></tr><tr><td>MotionLCM (1-step, λ = 0.1)</td><td>0.344</td><td>0.753</td><td>9.386</td><td>0.2711</td><td>0.0275</td><td>0.1310</td></tr><tr><td>MotionLCM (2-step, λ = 0.1)</td><td>0.324</td><td>0.759</td><td>9.428</td><td>0.2631</td><td>0.0256</td><td>0.1268</td></tr><tr><td>MotionLCM (4-step, λ = 0.1)</td><td>0.357</td><td>0.743</td><td>9.415</td><td>0.2713</td><td>0.0268</td><td>0.1309</td></tr><tr><td>MotionLCM (1-step, λ = 1.0)</td><td>0.419</td><td>0.756</td><td>9.390</td><td>0.1988</td><td>0.0147</td><td>0.1127</td></tr><tr><td>MotionLCM (2-step, λ = 1.0)</td><td>0.397</td><td>0.759</td><td>9.469</td><td>0.1960</td><td>0.0143</td><td>0.1092</td></tr><tr><td>MotionLCM (4-step, λ = 1.0)</td><td>0.444</td><td>0.753</td><td>9.355</td><td>0.2089</td><td>0.0172</td><td>0.1140</td></tr><tr><td>MotionLCM (1-step, λ = 10.0)</td><td>0.636</td><td>0.744</td><td>9.479</td><td>0.1465</td><td>0.0097</td><td>0.0967</td></tr><tr><td>MotionLCM (2-step, λ = 10.0)</td><td>0.551</td><td>0.757</td><td>9.569</td><td>0.1590</td><td>0.0107</td><td>0.0987</td></tr><tr><td>MotionLCM (4-step, λ = 10.0)</td><td>0.568</td><td>0.742</td><td>9.486</td><td>0.1723</td><td>0.0132</td><td>0.1045</td></tr></table>

Table 5: Ablation study on different control ratios $\tau$ and number of control joints $K$ We report the results of (1, 2, 4)-step inference. "\* is the default training setting.   

<table><tr><td>Methods</td><td></td><td>FID ↓</td><td>Top 3</td><td>R-Precision ↑ Diversity → Traj. err. ↓</td><td>(50cm)</td><td>(50cm)</td><td>Loc. err. ↓ Avg. err. ↓</td></tr><tr><td>Real</td><td></td><td>0.002</td><td>0.797</td><td>9.503</td><td>0.0000</td><td>0.0000</td><td>0.0000</td></tr><tr><td>MotionLCM*</td><td>(1-step, τ = 0.25, K = 6)</td><td>0.419</td><td>0.756</td><td>9.390</td><td>0.1988</td><td>0.0147</td><td>0.1127</td></tr><tr><td>MotionLCM*</td><td>(2-step, τ = 0.25, K = 6)</td><td>0.397</td><td>0.759</td><td>9.469</td><td>0.1960</td><td>0.0143</td><td>0.1092</td></tr><tr><td>MotionLCM*</td><td> (4-step, τ = 0.25, K = 6)</td><td>0.444</td><td>0.753</td><td>9.355</td><td>0.2089</td><td>0.0172</td><td>0.1140</td></tr><tr><td></td><td>MotionLCM (1-step, τ  [0.1, 0.25])</td><td>0.456</td><td>0.757</td><td>9.477</td><td>0.2821</td><td>0.0234</td><td>0.1214</td></tr><tr><td></td><td>MotionLCM (2-step, τ  [0.1, 0.25])</td><td>0.409</td><td>0.769</td><td>9.592</td><td>0.2707</td><td>0.0230</td><td>0.1179</td></tr><tr><td></td><td>MotionLCM (4-step, τ  [0.1, 0.25])</td><td>0.457</td><td>0.757</td><td>9.540</td><td>0.2928</td><td>0.0256</td><td>0.1228</td></tr><tr><td></td><td>MotionLCM (1-step, τ  [0.1, 0.5])</td><td>0.448</td><td>0.763</td><td>9.538</td><td>0.2390</td><td>0.0182</td><td>0.1182</td></tr><tr><td></td><td>MotionLCM (2-step, τ  [0.1, 0.5])</td><td>0.413</td><td>0.768</td><td>9.517</td><td>0.2349</td><td>0.0180</td><td>0.1153</td></tr><tr><td></td><td>MotionLCM (4-step, τ  [0.1, 0.5])</td><td>0.446</td><td>0.753</td><td>9.498</td><td>0.2517</td><td>0.0199</td><td>0.1196</td></tr><tr><td>MotionLCM (1-step, K = 12)</td><td></td><td>0.412</td><td>0.753</td><td>9.412</td><td>0.2072</td><td>0.0110</td><td>0.1029</td></tr><tr><td>MotionLCM (2-step, K = 12)</td><td></td><td>0.410</td><td>0.758</td><td>9.509</td><td>0.1979</td><td>0.0108</td><td>0.1000</td></tr><tr><td>MotionLCM (4-step, K = 12)</td><td></td><td>0.442</td><td>0.755</td><td>9.380</td><td>0.2169</td><td>0.0132</td><td>0.1048</td></tr><tr><td></td><td>MotionLCM (1-step, K = 22(whole-body))</td><td>0.436</td><td>0.748</td><td>9.379</td><td>0.2143</td><td>0.0083</td><td>0.0914</td></tr><tr><td></td><td>MotionLCM (2-step, K = 22(whole-body))</td><td>0.413</td><td>0.758</td><td>9.492</td><td>0.2061</td><td>0.0082</td><td>0.0881</td></tr><tr><td></td><td>MotionLCM (4-step, K = 22(whole-body))</td><td>0.461</td><td>0.745</td><td>9.459</td><td>0.2173</td><td>0.0097</td><td>0.0918</td></tr></table>

# 5 Conclusion

This work proposes an efficient controllable motion generation framework, MotionLCM. By introducing latent consistency distillation, MotionLCM enjoys the trade-off between runtime efficiency and generation quality. Moreover, thanks to the motion ControlNet manipulation in the latent space, our method obtains excellent controlling ability with given conditions. Extensive experimental results show the effectiveness of the proposed method. As the VAE of MLD lacks explicit temporal modeling, the MotionLCM cannot achieve a good temporal explanation. Therefore, our future work will lie in developing a more explainable compression architecture for efficient motion control.

# Acknowledgements

The research is supported by Shenzhen Ubiquitous Data Enabling Key Lab under grant ZDSYS20220527171406015 and CCF-Tencent Rhino-Bird Open Research Fund. This project is also supported by Shanghai Artificial Intelligence Laboratory. The author team would like to acknowledge Yiming Xie, Zhiyang Dou, and Shunlin Lu for their helpful technical discussions and suggestions.

# References

1. Ahn, H., Ha, T., Choi, Y., Yoo, H., Oh, S.: Text2action: Generative adversarial synthesis from language to action. In: ICRA. pp. 59155920 (2018) 2. Ahuja, C., Morency, L.P.: Language2pose: Natural language grounded pose forecasting. In: 3DV. pp. 719728 (2019) 3. Athanasiou, N., Petrovich, M., Black, M.J., Varol, G.: Teach: Temporal action composition for 3d humans. In: 3DV. pp. 414423 (2022) 4. Barquero, G., Escalera, S., Palmero, C.: Seamless human motion composition with blended positional encodings. In: CVPR. pp. 457469 (2024) 5. Bhattacharya, U., Rewkowski, N., Banerjee, A., Guhan, P., Bera, A., Manocha, D.: Text2gestures: A transformer-based network for generating emotive body gestures for virtual agents. In: VR. pp. 110 (2021) 6. Cervantes, P., Sekikawa, Y., Sato, I., Shinoda, K.: Implicit neural representations for variable length human motion generation. In: ECCV. pp. 356372 (2022) 7. Chen, L.H., Lu, S., Zeng, A., Zhang, H., Wang, B., Zhang, R., Zhang, L.: Motionllm: Understanding human behaviors from human motions and videos. arXiv preprint arXiv:2405.20340 (2024) 8. Chen, L.H., Zhang, J., Li, Y., Pang, Y., Xia, X., Liu, T.: Humanmac: Masked motion completion for human motion prediction. In: ICCV. pp. 95449555 (2023) 9. Chen, X., Jiang, B., Liu, W., Huang, Z., Fu, B., Chen, T., Yu, G.: Executing your commands via motion diffusion in latent space. In: CVPR. pp. 1800018010 (2023) 10. Cong, P., Dou, Z.W., Ren, Y., Yin, W., Cheng, K., Sun, Y., Long, X., Zhu, X., Ma, Y.: Laserhuman: Language-guided scene-aware human motion generation in free environment. arXiv preprint arXiv:2403.13307 (2024) 11. Dabral, R., Mughal, M.H., Golyanik, V., Theobalt, C.: Mofusion: A framework for denoising-diffusion-based motion synthesis. In: CVPR. pp. 97609770 (2023) 12. Dou, Z., Chen, X., Fan, Q., Komura, T., Wang, W.: C. ase: Learning conditional adversarial skill embeddings for physics-based characters. In: SIGGRAPH Asia. pp. 111 (2023) 13. Fan, K., Tang, J., Cao, W., Yi, R., Li, M., Gong, J., Zhang, J., Wang, Y., Wang, C., Ma, L.: Freemotion: A unified framework for number-free text-to-motion synthesis. arXiv preprint arXiv:2405.15763 (2024) 14. Ghosh, A., Cheema, N., Oguz, C., Theobalt, C., Slusallek, P.: Synthesis of compositional animations from textual descriptions. In: ICCV. pp. 13961406 (2021) 15. Guo, C., Mu, Y., Javed, M.G., Wang, S., Cheng, L.: Momask: Generative masked modeling of 3d human motions. In: CVPR. pp. 19001910 (2024) 16. Guo, C., Mu, Y., Zuo, X., Dai, P., Yan, Y., Lu, J., Cheng, L.: Generative human motion stylization in latent space. arXiv preprint arXiv:2401.13505 (2024) 17. Guo, C., Zou, S., Zuo, X., Wang, S., Ji, W., Li, X., Cheng, L.: Generating diverse and natural 3d human motions from text. In: CVPR. pp. 51525161 (2022) 18. Guo, C., Zuo, X., Wang, S., Cheng, L.: Tm2t: Stochastic and tokenized modeling for the reciprocal generation of 3d human motions and texts. In: ECCV. pp. 580 597 (2022) 19. Guo, C., Zuo, X., Wang, S., Zou, S., Sun, Q., Deng, A., Gong, M., Cheng, L.: Action2motion: Conditioned generation of 3d human motions. In: ACMMM. pp. 20212029 (2020)   
Ho J. Jain A. beel . D iff balcels.NeurP . 68406851 (2020) 21. Ho, J., Salimans, T.: Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598 (2022) 22. Holden, D., Komura, T., Saito, J.: Phase-functioned neural networks for character control. TOG 36(4), 113 (2017) 23. Holden, D., Saito, J., Komura, T.: A deep learning framework for character motion synthesis and editing. TOG 35(4), 111 (2016) 24. Huang, Y., Yang, H., Luo, C., Wang, Y., Xu, S., Zhang, Z., Zhang, M., Peng, J.: Stablemofusion: Towards robust and efficient diffusion-based motion generation framework. arXiv preprint arXiv:2405.05691 (2024) 25. Huber, P.J.: Robust estimation of a location parameter. The Annals of Mathematical Statistics 35(1), 73101 (1964) 26. Ji, Y., Xu, F., Yang, Y., Shen, F., Shen, H.T., Zheng, W.S.: A large-scale rgb-d database for arbitrary-view human action recognition. In: ACMMM. pp. 15101518 (2018) 27. Jiang, B., Chen, X., Liu, W., Yu, J., Yu, G., Chen, T.: Motiongpt: Human motion as a foreign language. NeurIPS (2024)   
28Karras, T., Aittala, M., Aila, T., Laine, S.: Elucidating the design space of diffusionbased generative models. NeurIPS pp. 2656526577 (2022) 29. Karunratanakul, K., Preechakul, K., Suwajanakorn, S., Tang, S.: Guided motion diffusion for controllable human motion synthesis. In: CVPR. pp. 21512162 (2023) 30. Kingma, D.P., Welling, M.: Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114 (2013) 31. Lee, T., Moon, G., Lee, K.M.: Multiact: Long-term 3d human motion generation from multiple action labels. In: AAAI. pp. 12311239 (2023) 32. Li, B., Zhao, Y., Zhelun, S., Sheng, L.: Danceformer: Music conditioned 3d dance generation with parametric motion transformer. In: AAAI. pp. 12721279 (2022) 33. Li, R., Zhang, Y., Zhang, Y., Zhang, H., Guo, J., Zhang, Y., Liu, Y., Li, X.: Lodge: A coarse to fine diffusion network for long dance generation guided by the characteristic dance primitives. In: CVPR. pp. 15241534 (2024) 34. Li, R., Zhao, J., Zhang, Y., Su, M., Ren, Z., Zhang, H., Tang, Y., Li, X.: Finedance: A fine-grained choreography dataset for 3d full body dance generation. In: ICCV. pp. 1023410243 (2023) 35. Li, R., Yang, S., Ross, D.A., Kanazawa, A.: Ai choreographer: Music conditioned 3d dance generation with aist++. In: ICCV. pp. 1340113412 (2021) 36. Li, T., Qiao, C., Ren, G., Yin, K., Ha, S.: Aamdm: Accelerated auto-regressive motion diffusion model. In: CVPR. pp. 18131823 (2024) 37. Lin, A.S., Wu, L., Corona, R., Tai, K., Huang, Q., Mooney, R.J.: Generating animated videos of human activities from natural language descriptions. Learning 1(2018), 1 (2018) 38. Lin, X., Amer, M.R.: Human motion modeling using dvgans. arXiv preprint arXiv:1804.10652 (2018) 39. Liu, J., Dai, W., Wang, C., Cheng, Y., Tang, Y., Tong, X.: Plan, posture and go: Towards open-world text-to-motion generation. arXiv preprint arXiv:2312.14828 (2023) 40. Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 (2017)   
4Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., Zhu, J.: Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. NeurIPS pp. 57755787 (2022) 42. Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., Zhu, J.: Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095 (2022) 43. Lu, S., Chen, L.H., Zeng, A., Lin, J., Zhang, R., Zhang, L., Shum, H.Y.: Humantomato: Text-aligned whole-body motion generation. arXiv preprint arXiv:2310.12978 (2023) 44. Luo, S., Tan, Y., Huang, L., Li, J., Zhao, H.: Latent consistency models: Synthesizing high-resolution images with few-step inference. arXiv preprint arXiv:2310.04378 (2023) 45. Mahmood, N., Ghorbani, N., Troje, N.F., Pons-Moll, G., Black, M.J.: Amass: Archive of motion capture as surface shapes. In: ICCV. pp. 54425451 (2019) 46. Nichol, A.Q., Dhariwal, P.: Improved denoising diffusion probabilistic models. In: ICML. pp. 81628171 (2021) 47. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al.: Pytorch: An imperative style, highperformance deep learning library. NeurIPS (2019) 48. Petrovich, M., Black, M.J., Varol, G.: Action-conditioned 3d human motion synthesis with transformer vae. In: ICCV. pp. 1098510995 (2021) 49. Petrovich, M., Black, M.J., Varol, G.: Temos: Generating diverse human motions from textual descriptions. In: ECCV. pp. 480497 (2022) 50. Petrovich, M., Black, M.J., Varol, G.: Tmr: Text-to-motion retrieval using contrastive 3d human motion synthesis. In: ICCV. pp. 94889497 (2023)   
5Petrovich, M. Liany, O., Iqbal, U., Black, M.J., Varol, G., Bin Peng, X. Rempe, D.: Multi-track timeline control for text-driven 3d human motion generation. In: CVPRW. pp. 19111921 (2024)   
5 Plappert, M., Mandery, C., Asfour, T.: The kit motion-language dataset. Big data 4(4), 236252 (2016) 53. Punnakkal, A.R., Chandrasekaran, A., Athanasiou, N., Quiros-Ramirez, A., Black, M.J.: Babel: Bodies, action and behavior with english labels. In: CVPR. pp. 722 731 (2021) 54. Raab, S., Leibovitch, I., Li, P., Aberman, K., Sorkine-Hornung, O., Cohen-Or, D.: Modi: Unconditional motion synthesis from diverse data. In: CVPR. pp. 13873 13883 (2023) 55. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models. In: CVPR. pp. 1068410695 (2022) 56. Shafir, Y., Tevet, G., Kapon, R., Bermano, A.H.: Human motion diffusion as a generative prior. In: ICLR (2024) 57. Shahroudy, A., Liu, J., Ng, T.T., Wang, G.: Ntu rgb+ d: A large scale dataset for 3d human activity analysis. In: CVPR. pp. 10101019 (2016) 58. Shi, Y., Wang, J., Jiang, X., Dai, B.: Controllable motion diffusion model. arXiv preprint arXiv:2306.00416 (2023) 59. Siyao, L., Yu, W., Gu, T., Lin, C., Wang, Q., Qian, C., Loy, C.C., Liu, Z.: Bailando: 3d dance generation by actor-critic gpt with choreographic memory. In: CVPR. pp. 1105011059 (2022) 60. Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., Ganguli, S.: Deep unsupervised learning using nonequilibrium thermodynamics. In: ICML. pp. 22562265 (2015) 61. Song, J., Meng, C., Ermon, S.: Denoising difusion implicit models. In: ICLR (2021) 62. Song, Y., Dhariwal, P., Chen, M., Sutskever, I.: Consistency models. In: ICML (2023)   
63.Tang, Y., Liu, J., Liu, A., Yang, B., Dai, W., Rao, Y., Lu, J., Zhou, J., Li, X.: Fl A civiy ta  a n. InCVPR. . 2210622117 (2023) 64. Tevet, G., Gordon, B., Hertz, A., Bermano, A.H., Cohen-Or, D.: Motionclip: Exposing human motion generation to clip space. In: ECCV. pp. 358374 (2022) 65. Tevet, G., Raab, S., Gordon, B., Shafir, Y., Cohen-Or, D., Bermano, A.H.: Human motion diffusion model. In: ICLR (2022)   
66Tseng, J., Castellon, R., Liu, K.: Edge: Editable dance generation from music. In: CVPR. pp. 448458 (2023)   
Vasan A. hazeer, N. Prar, N. Uzrei, J. Joes, L.Goe, A.. K L., Polosukhin, I.: Attention is all you need. NeurIPS (2017) 68. Wan, W., Dou, Z., Komura, T., Wang, W., Jayaraman, D., Liu, L.: Tlcontrol: Trajectory and language control for human motion synthesis. arXiv preprint arXiv:2311.17135 (2023)   
Y   , W., Huang, S.: Move as you say interact as you can: Language-guided human motion generation with scene affordance. In: CVPR. pp. 433444 (2024)   
7 Wang, Z., Chen, Y., Liu, T., Zhu, Y., Liang, W., Huang, S.: Humanise: Languageconditioned human motion generation in 3d scenes. NeurIPS pp. 1495914971 (2022)   

71. Wang, Z., Wang, J., Lin, D., Dai, B.: Intercontrol: Generate human motion interactions by controlling every joint. arXiv preprint arXiv:2311.15864 (2023)   
7 Xiao, Z., Wang, T. Wang, J., Cao, J., Zhag, W. Dai, B., Lin, D., Pa J.: Unified human-scene interaction via prompted chain-of-contacts. In: ICLR (2024)   
73.Xie, Y., Jampani, V., Zhong, L., Sun, D., Jiang, H.: Omnicontrol: Control any joint at any time for human motion generation. In: ICLR (2024)   
v X.Y. Ji X. . Xu CY. Y. F. X., et al.: Inter-x: Towards versatile human-human interaction analysis. In: CVPR. pp. 2226022271 (2024) 75. Xu, L., Song, Z., Wang, D., Su, J., Fang, Z., Ding, C., Gan, W., Yan, Y., Jin, X., Yang, X., et al.: Actformer: A gan-based transformer towards general actionconditioned 3d human motion generation. In: ICCV. pp. 22282238 (2023) 76. Xu, L., Zhou, Y., Yan, Y., Jin, X., Zhu, W., Rao, F., Yang, X., Zeng, W.: Regennet: Towards human action-reaction synthesis. In: CVPR. pp. 17591769 (2024)   
7Yan, S., Li, Z., Xiong, Y., Yan, H., Lin, D.: Convolutional sequence generatin for skeleton-based action synthesis. In: ICCV. pp. 43944402 (2019) 78. Yuan, Y., Song, J., Iqbal, U., Vahdat, A., Kautz, J.: Physdiff: Physics-guided human motion diffusion model. In: ICCV. pp. 1601016021 (2023) 79. Zhang, B., Cheng, Y., Wang, C., Zhang, T., Yang, J., Tang, Y., Zhao, F., Chen, D., Guo, B.: Rodinhd: High-fidelity 3d avatar generation with diffusion models. arXiv preprint arXiv:2407.06938 (2024) 80. Zhang, B., Cheng, Y., Yang, J., Wang, C., Zhao, F., Tang, Y., Chen, D., Guo, B.: Gaussiancube: Structuring gaussian splatting using optimal transport for 3d generative modeling. arXiv preprint arXiv:2403.19655 (2024) 81. Zhang, J., Zhang, Y., Cun, X., Zhang, Y., Zhao, H., Lu, H., Shen, X., Shan, Y.: Generating human motion from textual descriptions with discrete representations. In: CVPR. pp. 1473014740 (2023) 82. Zhang, L., Rao, A., Agrawala, M.: Adding conditional control to text-to-image diffusion models. In: ICCV. pp. 38363847 (2023) 83. Zhang, M., Cai, Z., Pan, L., Hong, F., Guo, X., Yang, L., Liu, Z.: Motiondiffuse: Text-driven human motion generation with diffusion model. arXiv preprint arXiv:2208.15001 (2022) 84. Zhao, R., Su, H., Ji, Q.: Bayesian adversarial human motion synthesis. In: CVPR. pp. 62256234 (2020) 85. Zhong, L., Xie, Y., Jampani, V., Sun, D., Jiang, H.: Smoodi: Stylized motion diffusion model. arXiv preprint arXiv:2407.12783 (2024) 86. Zhou, W., Dou, Z., Cao, Z., Liao, Z., Wang, J., Wang, W., Liu, Y., Komura, T., Wang, W., Liu, L.: Emdm: Efficient motion diffusion model for fast, high-quality motion generation. arXiv preprint arXiv:2312.02256 (2023)

# MotionLCM: Real-time Controllable Motion Generation via Latent Consistency Model

Supplementary Material In this supplementary material, we provide additional details and experiments not included in the main paper due to limitations in space. Appendix A: Additional experiments. Appendix B: Supplementary quantitative results. Appendix C: Details of the evaluation metrics.

# A Additional Experiments

# A.1 Comparison to other ODE Solvers

To validate the effectiveness of latent consistency distillation, we compare three ODE solvers (DDIM [61], DPM [41], DPM $^ { + + }$ [42]). The quantitative results shown in Tab. 6 demonstrate that our MotionLCM notably outperforms baseline methods. Moreover, unlike DDIM [61], DPM [41], and DPM $^ { + + }$ [42], requiring more peak memory per sampling step when using CFG [21], MotionLCM only requires one forward pass, saving both time and memory costs.

Table 6: Quantitative results with the testing CFG scale $w = 7 . 5$ MotionLCM notably outperforms baseline methods [41, 42,61] on HumanML3D [17] dataset, demonstrating the effectiveness of latent consistency distillation. Bold indicates the best result.   

<table><tr><td rowspan="2">Methods</td><td colspan="3">R-Precision (Top 3) ↑</td><td colspan="3">FID ↓</td></tr><tr><td>1-Step</td><td>2-Step</td><td>4-Step</td><td>1-Step</td><td>2-Step</td><td>4-Step</td></tr><tr><td>DDIM [61]</td><td>0.651±.003</td><td>0.691±.002</td><td>0.765±.003</td><td>4.022±.043</td><td>2.802±.038</td><td>0.966±.018</td></tr><tr><td>DPM [41]</td><td>0.651±.003</td><td>0.691±.002</td><td>0.777±.003</td><td>4.022±.043</td><td>2.798±.038</td><td>0.727±.015</td></tr><tr><td>DPM++ [42]</td><td>0.651±.003</td><td>0.691±.002</td><td>0.777±.003</td><td>4.022±.043</td><td>2.798±.038</td><td>0.684±.015</td></tr><tr><td>MotionLCM</td><td>0.803±.002</td><td>0.805±.002</td><td>0.798±.002</td><td>0.467±.012</td><td>0.368±.011</td><td>0.304±.012</td></tr></table>

# A.2 Impact of different testing CFGs

As shown in Fig. 7, we provide an extensive ablation study on the testing CFG [21]. It can be observed that, under different testing CFGs, increasing the number of inference steps continuously improves the performance. However, further increasing the inference steps results in comparable performance but significantly increases the time cost.

![](images/7.jpg)  
Fig. 7: Comparison of testing CFGs

# B More Qualitative Results

In this section, we provide more qualitative results of our MotionLCM. Fig. 8 presents more generation results on the text-to-motion task. Fig. 9 displays additional visualization results on the motion control task. All videos shown in the figures can be found in the supplementary video (i.e., supp.mp4).

![](images/8.jpg)  
Fig. 8: More qualitative results of MotionLCM on the text-to-motion task.

# C Metric Definitions

Time cost: To assess the inference efficiency of models, we follow [9] to report the Average Inference Time per Sentence (AITS) measured in seconds. We calculate AITS on the test set of HumanML3D [17] by setting the batch size to 1 and excluding the time cost for model and dataset loading parts. Motion quality: Frechet Inception Distance (FID) measures the distributional difference between the generated and real motions, calculated using the feature extractor associated with a specific dataset, e.g., HumanML3D [17].

![](images/9.jpg)  
Fig. 9: More qualitative results of MotionLCM on the motion control task.

Motion diversity: Following [18,19], we report Diversity and MultiModality to evaluate the generated motion diversity. Diversity measures the variance of the generated motions across the whole set. Specifically, two subsets of the same size $S _ { d }$ are randomly sampled from all generated motions with their extracted motion feature vectors $\{ \mathbf { v } _ { 1 } , . . . , \mathbf { v } _ { S _ { d } } \}$ and $\{ \mathbf { v } _ { 1 } ^ { ' } , . . . , \mathbf { v } _ { S _ { d } } ^ { ' } \}$ . Diversity is defined as follows,

$$
\mathrm { D i v e r s i t y } = \frac { 1 } { S _ { d } } \sum _ { i = 1 } ^ { S _ { d } } | | \mathbf { v } _ { i } - \mathbf { v } _ { i } ^ { ' } | | _ { 2 } .
$$

Different from Diversity, MultiModality (MModality) measures how much the generated motions diversify within each textual description. Specifically, a set of textual descriptions with size $C$ is randomly sampled from all descriptions. Then we randomly sample two subsets with the same size $I$ from all generated motions conditioned by the $c$ -th textual description, with extracted feature vectors $\{ \mathbf { v } _ { c , 1 } , . . . , \mathbf { v } _ { c , I } \}$ and $\{ \mathbf { v } _ { c , 1 } ^ { ' } , . . . , \mathbf { v } _ { c , I } ^ { ' } \}$ . MModality is formalized as follows,

$$
\mathrm { M M o d a l i t y } = \frac { 1 } { C \times I } \sum _ { c = 1 } ^ { C } \sum _ { i = 1 } ^ { I } | | \mathbf { v } _ { c , i } - \mathbf { v } _ { c , i } ^ { ' } | | _ { 2 } .
$$

Condition matching: [17] provides motion/text feature extractors to generate geometrically closed features for matched text-motion pairs and vice versa. Under this feature space, evaluating motion-retrieval precision (R-Precision) involves mixing the generated motion with 31 mismatched motions and then calculating the text-motion Top-1/2/3 matching accuracy. Multimodal Distance (MM Dist) calculates the mean distance between the generated motions and texts. Control error: Following [73], we report Trajectory error, Location error, and Average error to assess the motion control performance. Trajectory error (Traj. err.) is defined as the proportion of unsuccessful trajectories, i.e., if a control joint in the generated motion exceeds a certain distance threshold from the corresponding joint in the given control trajectory, it is considered a failed trajectory. Similar to the Trajectory error, Location error (Loc. err.) is defined as the ratio of unsuccessful joints. In our experiments, we adopt 50cm as the distance threshold to calculate the Trajectory error and Location error. Average error (Avg. err.) denotes the mean distance between the control joint positions in the generated motion and those in the given control trajectory.