# 统一多模态预训练中的新兴特性

蔡锐 邓*1, 德耀 ${ \pmb z } { \hslash } { \pmb u } ^ { * 1 }$ , 昆长 ${ \bf L } \breve { \bf I } ^ { * 2 \ddagger }$ , 辰辉 ${ \tt G o u ^ { * 3 \ddagger } }$ , 冯李*4† 泽宇 王5‡, 胡钟1, 威皓 $\pmb { \mathsf { Y } } \pmb { \mathsf { u } } ^ { 1 }$ , 萧楠 聂1, 赟翔 宋1, 广石1 浩奇 范**

1字节跳动，2深圳先进技术研究院，3莫纳什大学，4香港科技大学，$^ { 5 } \mathsf { U C }$ 圣克鲁斯

*等量贡献，§通讯作者，†项目负责人

# 摘要

统一多模态理解和生成在尖端专有系统中展现了令人印象深刻的能力。在这项工作中，我们介绍了BAGEL，一个开源基础模型，原生支持多模态理解和生成。BAGEL是一个统一的、仅解码器模型，经过预训练，使用了来自大规模交织的文本、图像、视频和网络数据的万亿个文本标记。当以如此多样的多模态交织数据进行扩展时，BAGEL展现出在复杂多模态推理方面的新兴能力。因此，它在标准基准测试中，显著超越了开源统一模型，在多模态生成和理解方面表现出色，同时展示了先进的多模态推理能力，如自由形态图像处理、未来帧预测、3D处理和世界导航。为了促进更多多模态研究的机会，我们分享了关键发现、预训练细节、数据创建协议，并向社区发布我们的代码和检查点。

日期：2025年7月29日 联系邮箱：shiguang.sg@bytedance.com 项目页面：https://bagel-ai.org/

# 1 引言

统一多模态理解与生成领域的兴趣激增，许多研究项目[57, 66, 68, 70, 73, 79, 83, 89]展示了通过精心设计的统一架构在共同优化生成和理解基准方面的有希望的结果。尽管一些努力[1, 11, 70]尝试扩展他们的统一模型，但它们仍然主要在标准图像生成和理解任务中的图像-文本配对数据上进行训练。最近的研究[10]揭示了学术模型与像GPT-40和Gemini 2.0这样的专有系统之间在统一多模态理解与生成方面存在显著差距，后者的底层技术仍未披露。我们认为，弥合这一差距的关键在于以经过精心构建的多模态交错数据进行扩展——集成文本、图像、视频和网络源。我们的实验揭示了随着交错多模态预训练的扩展而出现的新特性。除了增强核心多模态理解和生成能力外，扩展还促进了复杂的组合能力，例如自由形式的视觉操作和具有长上下文推理的多模态生成，为广泛的高级功能铺平了道路。

![](images/1.jpg)

一辆由小车组成的汽车。

<想法> 模型应该生成一幅图像，其中许多小汽车组合成一辆大汽车的形状和结构。精细化的提示是：一幅高度详细且富有创意的图像，展示一辆完全由许多小汽车组成的大汽车，细致地排列以复制全尺寸车辆的形状和结构。这些小汽车紧密相连，形成一个无缝且逼真的外观，具有准确的比例和细节。环境中性且干净，柔和的漫射光线强调构图的复杂性，没有任何干扰…… /想法结束

![](images/2.jpg)

![](images/3.jpg)

他被发明寿司的国家的国花环绕。

<think> 用户想要将雕像的周围环境替换为樱花盛开的场景，同时保持雕像的主体不变。首先，我必须确保雕像的细节，如肌肉线条、面部表情和衣物褶皱，保持清晰完整。接下来，需要在雕像周围添加樱花元素，使其符合“发明寿司的国家的国花”的设定。樱花... </think>

![](images/4.jpg)

![](images/5.jpg)

您能展示出根据这个设计制作的雕塑吗？

<think> 问题图像展示了一个由10岁的孩子Blake W. Torney创作的章鱼设计，标题为“章鱼”。描述中提到，这个雕塑是由1964年出生的美国玻璃艺术家Dante Marioni于2008年制作的，采用吹制和热雕刻的玻璃，以及应用的玻璃片。这个雕塑是…… 问题文本要求展示一件受到这个设计启发的雕塑。根据描述，最终的答案图像应展示Dante Marioni创作的实际玻璃雕塑，反映出孩子画作的设计。雕塑应具有绿色的身体和红色的触手，与孩子的描绘相似…… </think>

![](images/6.jpg)

![](images/7.jpg)  
Figure 1 Showcase of the versatile abilities of the BAGEL model.

为了实现这一愿景，我们建立了一种新的协议，用于可扩展的数据来源、过滤和高质量多模态交错数据的构建。除了网络来源，我们还结合了视频数据，这些数据自然提供像素级、概念级、时间级和物理连续性，这为规模化获取全面的知识提供了独特的信号。此外，我们的交错数据还包括多模态对话、文本到图像/视频和图像处理等任务，实现了多样化生成数据的无缝整合。受到DeepSeek-R1的启发，我们进一步充实交错数据，加入以推理为导向的内容，以促进多模态推理，从而实现理解和生成过程之间的无缝知识转移。因此，精心挑选的数据捕获了丰富的世界知识和细致的跨模态交互内容，使模型具备了上下文预测、世界建模和复杂多模态推理的基础能力。

关于架构设计，我们的主要目标是最大化模型的容量。基于这一设计理念，我们采用了混合变压器专家（MoT）架构，该架构通过选择性激活特定模态的参数。与一些先前的方法不同，这些方法在生成和理解模块之间引入了瓶颈连接，而我们的设计通过共享自注意力操作实现了多模态理解与生成之间的长上下文交互。这种无瓶颈的设计使训练数据和步骤的有效扩展成为可能，使模型的全部潜力信号得以呈现，而不受架构限制的干扰或遮蔽。

我们介绍了可扩展的生成认知模型（BAGEL），这是一个开源的多模态基础模型，具有70亿活跃参数（总计140亿），在大规模交错多模态数据上进行训练。BAGEL在标准的多模态理解排行榜上超越了当前顶级开源VLM，并且在图像生成质量上与领先的公共生成器如SD3和FLUX.1-dev相竞争。此外，BAGEL在经典图像编辑场景中表现出持续优越的定性结果，超过了领先的开源模型。更重要的是，它扩展到了自由形式的视觉操作、多视角合成和世界导航，这些能力构成了超出之前图像编辑模型范围的“世界建模”任务。我们在图1中展示了定性性能。

随着BAGEL在交错多模态预训练中扩展，我们观察到一个明显的模式：基本的多模态理解和高保真生成首先收敛；接下来，复杂的编辑和自由形式的生成也逐渐融合，这表明先前独立的基本技能在不同模态间协同作用，形成组合推理。这些新兴的能力不仅得到了公共基准的支持，更在我们提出的InteligentBench中得到了更加明显的体现，并通过定性观察进行了进一步验证。这些观察强调，尽管理解和生成的优化景观仍然部分解耦，但它们可以共同分享注意力上下文，利用同一模型，从而在一个开源系统中产生丰富的能力谱。

# 2 模型

如图2所示，BAGEL采用了MoT架构，包含两个变压器专家——一个专注于多模态理解，另一个专注于多模态生成。因此，该模型使用两个独立的视觉编码器：一个面向理解的编码器和一个面向生成的编码器。这两个变压器专家在每一层通过共享自注意力操作在同一令牌序列上进行操作。当预测文本令牌时，BAGEL遵循下一个令牌预测的范式，符合自回归语言模型的既定优势。对于视觉令牌预测，BAGEL采用了Rectified Flow方法，遵循视觉生成领域的最佳实践。在本节的其余部分，我们将分享塑造这些设计选择的见解和动机。

# 2.1 模型设计空间

统一多模态生成和理解模型的典型设计选择包括：

![](images/8.jpg)  
Figure  We use woTransformer experts to proces understanding and generation inormation, and all tokes do sharemultmodal seattention acTransormerblock.Wedopt tistincencoders  eparately apue semantic content and low-level pixel information for image understanding and generation tasks.

量化自回归。使用离散视觉标记器的自回归视觉生成。这一系列方法利用了文本和视觉标记生成的下一个标记预测范式，实施起来比较简单，因为可以直接利用现有的大型语言模型基础设施。不幸的是，自回归模型的视觉生成质量在经验上低于扩散模型。此外，由于自回归方法的顺序特性，推理延迟受到影响。

外部扩散器。LLM骨干结合了外部扩散模块[18, 23, 57, 69, 73]。该设计通过轻量级的可训练适配器将预训练的LLMs/VLMs与扩散模型连接起来。通常，语言骨干以自回归方式生成一组潜在标记作为“语义条件”信号，然后由扩散模块用于生成图像。这种设置通常表现出快速收敛，消耗数据极少，并且在多个模态生成和理解的既定基准上可能也能取得竞争性的表现[57]。然而，其主要缺点是将LLM上下文压缩成相对较少的潜在标记。这在理解和生成模块之间引入了显式瓶颈，冒着大量信息丢失的风险——尤其是在长上下文的多模态推理中。这种限制可能与大型基础模型的扩展哲学相悖。

集成变换器。统一集成语言模型和扩散模型于单一变换器中 [40,50, 6, 104]。该方法利用自回归变换器（强大的理解/推理能力）和扩散变换器（强大的视觉生成能力）的互补优势，使用它们共同的模型架构实现两种范式之间的无缝切换。与外部扩散器解决方案相比，它对训练计算的要求显著更高。然而，它通过在所有变换器块中保持瓶颈自由上下文，提供了显著优势，从而使生成模块与理解模块之间实现无损的互动，并更易于扩展。

在这项工作中，我们认为统一模型有能力从大规模交织的多模态数据中学习更丰富的多模态能力——这些新兴能力并未被传统基准捕获。为此，我们选择了无瓶颈的集成变换器解决方案，我们相信它在大规模训练环境中具有更大的潜力，并可能更好地作为长上下文多模态推理和强化学习的基础模型。

# 2.2 架构

我们的主干模型继承自一种仅解码器的 transformer 架构的 LLM。我们选择 Qwen2.5 LLM [93] 作为初始化，因为它卓越的性能 [21] 和公开可用性。它采用 RMSNorm [98] 进行规范化，SwiGLU [65] 进行激活，RoPE [67] 进行位置编码，并使用 GQA [2] 实现 KV 缓存减少。此外，我们在每个注意力块中添加了 QK-Norm [15]，这符合图像/视频生成模型 [19, 35, 63] 中的常规做法，对稳定训练过程有效。

视觉信息从两个方面进行表示：

•为了视觉理解，我们利用ViT编码器将原始像素转换为标记。我们采用SigLIP2-so400m/14 [75]，以固定的384分辨率作为ViT编码器的初始化。在此基础上，我们首先插值位置嵌入，并设置$9 8 0 \times 9 8 0$作为最大输入大小，然后整合NaViT [16]以实现图像按其原始纵横比的处理。采用两层MLP连接器以匹配ViT标记和LLM隐藏状态的特征维度。•对于视觉生成，我们使用来自FLUX [35]的预训练VAE模型将图像从像素空间转换到潜在空间，反之亦然。潜在表示的下采样比例为8，潜在通道为16，随后通过$2 \times 2$的补丁嵌入层处理，以减少空间大小并匹配LLM骨干网络的隐藏维度。VAE模型在训练期间保持不变。

我们的框架在将ViT和VAE令牌整合到LLM主干之前，应用了2D位置编码。对于扩散时间步编码，我们遵循[17]，直接将时间步嵌入添加到VAE令牌的初始隐藏状态中，而不是像传统的扩散变换器[19, 35, 82]那样使用AdaLN。这一修改保持了性能，同时使架构更加清晰。在LLM内部，来自理解和生成任务的文本、ViT和VAE令牌根据输入的模态结构交错排列。对于属于同一样本的令牌，我们采用了一种广义的因果注意力机制。这些令牌首先被划分为多个连续的分割，每个分割包含来自单一模态的令牌（例如，文本、ViT或VAE）。一个分割中的令牌可以关注所有前面分割中的令牌。在每个分割内部，我们对文本令牌采用因果注意力，并对视觉令牌保持双向注意力。

# 2.3 广义因果注意力

在训练过程中，交错的多模态生成样本可能包含多张图像。对于每张图像，我们准备了三组视觉标记：

•噪声VAE令牌：受扩散噪声影响的VAE潜变量，仅用于修正流训练；MSE损失是基于这一组计算的。

清洁VAE令牌：原始（无噪声）潜变量，在生成后续图像或文本令牌时作为条件。

Vi tokens：从SigLIP2编码器获得，帮助统一交错生成和理解数据的输入格式，并在实证中提升交错生成质量。

对于交错的图像或文本生成，后续的图像或文本标记可以关注前面的干净VAE标记和ViT标记，但不能关注它们的噪声VAE对应标记。

对于交错多图像生成，我们采用扩散强制策略[8]，该策略为不同图像添加独立的噪声水平，并将每个图像基于前面图像的噪声表示进行条件处理。此外，为了增强生成的一致性，我们随机对连续图像进行分组，遵循[17]，并在每组内应用全注意力。每组内的噪声水平是相同的。

我们使用PyTorch FlexAttention实现了广义因果注意力 [72]，相比于简单的缩放点积注意力实现了约2倍的加速。在推理过程中，广义因果结构允许我们缓存生成的多模态上下文的键值（KV）对，从而加速多模态解码。只有干净的变分自编码器（VAE）令牌和视觉变换器（ViT）令牌的KV对被存储；一旦图像完全生成，相应的KV对就会被替换。在交错推理中，我们分别以0.1、0.5和0.1的概率随机丢弃文本、ViT和干净VAE令牌。广义因果注意力的示意图如图15所示。

# 2.4 变换器设计

根据集成变压器解决方案的原则，我们比较了几种变压器变体：标准稠密变压器、专家混合（MoE）变压器以及变压器混合（MoT）架构。

![](images/9.jpg)  
Figure 3 Loss curves of various designs.CE loss and MSE loss arecomputed on multimodal understanding and etin sk epetivyblati expermt  ca 5 LThe splnrator and understanding data is set at 4:1.

•MoE变体：我们只在每个Qwen2.5 LLM块中复制前馈网络（FFN），作为生成专家的初始化。

MoT变体：我们复制Qwen2.5 LLM的所有可训练参数，以创建一个全尺寸生成专家。这种架构已被现有工作采用[40, 66]。

我们模型中的MoE和MoT都使用硬路由：新复制的生成专家专门处理VAE令牌，而原始参数——理解专家——处理文本和ViT令牌，遵循Qwen-VL系列的策略 [4, 77]。尽管MoE和MoT架构在总参数数量上比稠密基线大约增加了一倍，但三种模型变体在训练和推理期间的FLOPs是相同的。

我们对1.5B Qwen-2.5 LLM进行了受控实验，保持相同的超参数和数据配置，以便能够与其他架构进行比较。结果显示，MoT变体在多模态生成任务上始终优于稠密和MoE设计，差距在此任务上最为明显。MSE损失（生成）表现出平滑且单调递减的轨迹，其中MoT不仅收敛最快，而且达到了最低的最终损失。相反，CE损失（理解）表现出更大的逐步波动——这是异构数据交错的预期结果——但MoT在整体上仍保持最佳性能。这些发现强调了将生成所需的参数与优化理解的参数解耦的明显优势，这表明这两个目标可能会将模型引导到参数空间的不同区域——至少在这里考察的1.5B规模下。简言之，为多模态理解和生成分配单独的容量可以减轻来自竞争性模态特定学习目标的优化挑战。

# 3 数据

由于数据定义了大型基础模型的知识边界，BAGEL在多个模态的数据集上进行了训练，包括语言、图像、视频和网络数据，使其能够通过统一的多模态接口执行多模态推理、上下文预测、物理动态建模和未来帧预测。除了标准的视觉-语言(VLM)、文本到图像(T2I)和大规模语言建模(LLM)数据集，我们还从网络和视频源构建了新的视觉-文本交错数据集，以进一步增强模型的顺序多模态推理能力。在表中，我们总结了不同模态下训练数据的规模和组成。在接下来的部分中，我们详细介绍了我们的数据集来源、准备协议和数据混合策略。

<table><tr><td>数据来源</td><td># 数据 (百万)</td><td># 代币 (千)</td></tr><tr><td>文本数据</td><td>400</td><td>0.4</td></tr><tr><td>图像-文本配对理解数据</td><td>500</td><td>0.5</td></tr><tr><td>图像-文本配对生成数据</td><td>1600</td><td>2.6</td></tr><tr><td>交错理解数据</td><td>100</td><td>0.5</td></tr><tr><td>交错生成数据：视频</td><td>45</td><td>0.7</td></tr><tr><td>交错生成数据：网络</td><td>20</td><td>0.4</td></tr></table>

您经过培训的数据截至2023年10月。

# 3.1 仅文本数据

为了保持基础大型语言模型的语言建模能力，我们通过增加一系列高质量的纯文本数据来补充我们的训练语料库。这些数据经过精心策划，以支持广泛的语言覆盖，并在通用文本任务中实现强大的推理和生成能力。

# 3.2 视觉-文本配对数据

文本-图像配对数据在多模态学习中发挥着核心作用，为视觉语言模型（VLMs）和文本到图像（T2I）生成提供了大规模的视觉监督。在我们的设置中，我们将视觉-文本配对数据根据其下游用途组织成两个子集：一个用于VLM预训练，另一个用于T2I生成。

VLM图像-文本对。我们利用大规模的图像-文本对进行VLM训练，涵盖广泛的视觉概念，主要来源于网络替代文本和说明。数据经过基于CLIP的相似性过滤、分辨率和纵横比约束、文本长度检查以及去重，以确保质量和多样性。为了应对长尾分布，采用概念感知抽样，以提高对稀有类别的覆盖率。此外，还包括来自OCR文档、图表和基础注释的结构化监督，以增强模型在阅读和空间理解方面的能力。

T2I图像-文本对。我们结合了高质量的图像-文本对，以及来自现有T2I模型的最少合成数据。这些数据不仅具有多样化的标题风格，如艺术性、文本性和超现实标题，还具有高质量的图像，这些图像在清晰度、结构完整性和样式多样性上得到了提升。这些例子共同增强了我们T2I训练语料库的视觉质量和风格多样性。

# 3.3 视觉-文本交替数据

尽管视觉-文本配对数据提供了有用的监督，但在支持涉及多个图像和中间文本的复杂上下文推理方面，它仍然不足。基于这些数据训练的模型往往难以捕捉跨模态的视觉和语义关系，导致生成内容不够连贯。为了应对这些限制，我们将大规模视觉-文本交错数据纳入训练。为了改善多模态理解，我们利用VLM交错数据集。对于视觉生成，我们引入了一种统一的构建视觉-文本交错数据的协议，通过结合多样化的来源以支持更丰富的多模态互动，具体如下所述。

# 3.3.1 数据来源

为了全面涵盖多样化的ra-worl场景并提供可扩展的数据供应，我们的训练语料库整合了两个主要来源，这些来源为多模态推理提供了足够的知识：视频数据和网页数据。

视频数据通过直接捕捉来自真实世界的时空动态，提供了丰富的世界知识——这是最大和最自然的模拟器。它保留了精细的视觉细节，保持了跨帧的身份一致性，并建模复杂的运动，使其在年龄编辑、导航和三维操作等任务中特别有效。我们使用公开可用的在线视频资源以及两个开源数据集构建了我们的视频数据集：Koala36M [78]，提供了大规模的指导和互动丰富的内容，以及MVImgNet2.0 [28]，其中包含从不同摄像机视角捕捉的物体，以支持多视图空间理解。

表格质量过滤规则适用于网络文档，每种过滤类型都有特定的过滤阈值或方法。

<table><tr><td>过滤类型</td><td>描述</td></tr><tr><td>界面移除</td><td>移除包含诸如图标或小部件等子字符串的图像 URL</td></tr><tr><td>分辨率</td><td>要求宽度和高度在 [150, 20000] 之间，宽高比在 [1/2, 2] 之间</td></tr><tr><td>图像清晰度</td><td>使用清晰度操作器移除模糊或低质量的图像</td></tr><tr><td>文本密度</td><td>丢弃检测到超过 100 个 OCR 文本标记的文档样式图像</td></tr><tr><td>相关性</td><td>根据 CLIP 相似度移除冗余或无关的图像</td></tr><tr><td>文档修剪</td><td>通过大型语言模型移除无关的页眉和页脚</td></tr><tr><td>图像数量</td><td>保持包含 38 张图像的文档，以获得平衡的上下文</td></tr></table>

网络数据捕捉复杂的现实世界多模态结构，并提供跨越广泛主题的多样知识，包括视觉教程和其他丰富的文档。这种交错的格式为训练模型执行多模态推理提供了丰富的监督。我们基于OmniCorpus [39]进行构建，这是一个从Common Crawl [14]预处理的大规模数据集，提供了大量交错文本和图像的网络文档。此外，我们还包括开源图像编辑数据集作为结构化交错数据 [3, 22, 32, 80, 88, 101]，这些数据集教授细粒度的编辑行为，并增强模型进行精确多模态推理和逐步生成的能力。

# 3.3.2 数据过滤器

视频数据的过滤。我们遵循T2V视频处理管道[63]协议对视频进行预处理，通过时间分割、空间裁剪和质量过滤，首先使用轻量级镜头检测将视频分割成短而连贯的片段，并根据视觉相似性选择性地合并相关片段。然后，我们使用裁剪检测和逐帧边界框聚合去除黑边和叠加元素，如徽标或文本。为了确保质量，我们通过长度、分辨率、清晰度和运动稳定性过滤片段，并使用基于CLIP的相似性进行去重。这个过程产生了一个干净且多样化的视频数据集，适合于多模态训练。

DaFiergr We DatToatehig-ualitereavedatromargecorpu ede fltering pipeline 目标文档，如教程、百科条目和设计内容，其中文本和图像具有强语义对齐。受到 DeepSeekMath [64] 的启发，我们首先应用轻量级的主题选择过程：LLM 被提示对一小部分文档进行分类，所得到的标签用于训练 fastText [34] 分类器以实现高效的大规模推断。选定的数据然后再次通过 LLM 分类器进行精细过滤。我们采用 14B 变体的 Qwen2.5 模型 [93]，因为它在性能和效率之间取得了平衡。为了进一步提高数据质量，我们应用了一组基于规则的过滤器，针对图像清晰度、相关性和文档结构，如表 2 所总结。

# 3.3.3 数据构建

视频中的交错数据。为了从视频中构建图像-文本交错序列，我们生成连续帧之间视觉变化的文本描述——捕捉物体运动、动作转变和场景变化。这些帧间标题作为学习视觉动态的时间监督。虽然大型视觉语言模型可以产生高质量的变化描述，但其推理成本限制了可扩展性。因此，我们基于Qwen2.5-VL-7B [4] 提炼了一个轻量级的标题生成模型，并在一小组高质量的帧间示例上进行了微调。为了减少幻觉，我们将标题长度限制在30个标记。对于每个视频片段，我们平均采样四帧，并为每对帧生成标题，从而生成400万个时间上有依据的交错序列。图4a展示了数据流程及一个示例。

来自网页的交错数据。为了从网页文档构建高质量的交错序列，我们旨在减少由于图像、其伴随文本以及周围视觉上下文之间的弱对齐造成的图像生成难度。为了为每张图像提供更具本地化和相关性的提示，我们采用了先生成标题的策略：对于每张图像，我们使用Qwen2.5-VL-7B [4] 生成一个简洁的描述，并将其直接插入到图像之前作为概念支架。这使得模型能够形成一个概念草图。

(a) 来自视频的交错数据的数据管道。

![](images/10.jpg)  
Furnteavatconstruction s.)Wensrucnteeavevodat reron feoheialy ts  l stil alarge LM.) For web data, we build n Omniorpus [39] and perfor a two-stage topic selection ollowed by qualiterng n captioning o produc trucure sequnce. Data example from bot pipelines are hown.

![](images/11.jpg)  
(b) Data pipeline for interleaved data from webs.

目标图像基于前文上下文和插入的标题生成。在生成标题以指导模型在图像中应该期待什么时，这种方法减轻了与松散相关或模棱两可输入相关的问题。此外，我们使用LLM摘要工具重写超过300个token的图像之间文本段，以提高上下文密度。这些步骤产生了一个更干净、更结构化的2000万交错网页文档数据集。数据管道和示例如图b所示。

# 3.3.4 推理增强数据

受到最近模型如O1 [33]和DeepSeek-R1 [26]的启发，我们利用长上下文的思维链数据进行多模态理解。此外，我们假设在图像生成之前引入基于语言的推理步骤有助于明确视觉目标并改善规划。为了探索这一点，我们构建了500k个增强推理的示例，涵盖基于输入和输出之间结构关系的四个类别：文本到图像生成、自由形式图像处理和抽象编辑。

文本到图像生成。我们首先手动创建一组简短而含糊的T2I查询，每个查询都配有简单的生成指导。通过上下文学习，我们提示Qwen2.5-72B [93] 生成额外的查询-指导对和相应的详细提示，然后将这些传递给FLUX.1-dev [35] 以生成目标图像。这个过程产生了查询、推理轨迹（指导 $^ +$ 详细提示）和图像的训练三元组，使模型能够在基于语言的推理中扎根于图像生成。

自由形式图像处理。我们通过使用源图像、目标图像、用户查询和来自DeepSeek-R1的推理追踪示例来提示VLM，从而生成增强推理的示例。R1示例准确地结合了源图像的描述、用户查询和推理步骤。表11和表12展示了生成推理追踪的VLM提示。我们主要从两个来源中采样源图像和目标图像对：开放源代码编辑数据集，如OmniEdit，以及交错视频数据，这提供了一套丰富的自然发生的编辑场景，具有显著的运动、视角变化和人类互动，同时保持时空一致性。

概念编辑。概念编辑针对需要高级概念推理而非简单局部像素修改的图像处理案例，例如将物体转变为设计草图。对于这些任务，我们使用网络交错数据集，从每个序列中抽取候选图像对，并应用三阶段的VLM管道构建高质量的QA示例。首先，给定一系列图像，我们提示VLM识别一个合理的输入-输出对。接下来，我们提示模型基于选择的对生成相应的文本问题。最后，我们使用VLM评估问题的质量，并与输入和输出图像进行对比，筛选出低质量示例。接受的示例随后传递给VLM，提示其使用来自DeepSeek-R1 [26] 的推理轨迹示例，以生成预期转变的有根据解释，如表13所示。这种设置帮助模型学习从多样的文本指令中解释复杂的视觉目标。

# 4 训练

如表3所示，我们采用多阶段训练策略，使用上述整理数据的动态混合——具体来说，初始化VLM连接器的对齐阶段、大规模预训练的预训练阶段、增加分辨率和交错数据比例的持续训练阶段，以及高质量微调的监督微调阶段：

•阶段：对齐。在这个阶段，我们通过仅训练 MLP 连接器，将 SigLIP2 ViT 编码器与 Qwen2.5 LLM 对齐，同时保持视觉编码器和语言模型不变。在这个阶段，仅使用图像文本配对数据来进行图像描述，每张图像被调整为固定分辨率 $3 7 8 \times 3 7 8$ 以匹配预训练的 SigLIP2 的输入大小。

•阶段：预训练（PT）。在此阶段，我们向LLM添加QK-Norm，除了VAE的模型参数外，所有模型参数都是可训练的。训练语料库包含2.5T的tokens，包括文本、图像-文本对、多模态对话、网页交错和视频交错数据。我们采用了一种原生分辨率策略，用于多模态理解和生成，限制每个图像的最大长边和最小短边。

阶段：持续训练（CT）。与PT相比，我们在CT阶段提高了视觉输入分辨率，这对多模态生成和理解性能都很重要。我们进一步战略性地增加交错数据的采样比率，以强调学习跨模态推理，因为模型的核心理解和生成能力变得更加稳定和可靠。CT阶段消耗了大约$2 . 6 \mathrm { T }$个标记。

•阶段：监督微调（SFT）。在SFT阶段，对于多模态生成，我们从图像-文本配对数据集和交错生成数据集中构建一个高质量的子集。对于多模态理解，我们从LLaVA-OV [37]和Mammoth-VL [27]指令微调数据中过滤出一个子集。此阶段的训练令牌总数为727亿。

表3 BAGEL的训练配方。多模态交错数据以灰色突出显示

<table><tr><td></td><td>对齐</td><td>PT</td><td>CT</td><td>SFT</td></tr><tr><td>超参数 学习率 LR调度器 权重衰减</td><td colspan="4">1 × 10−3 1.0 × 10−4 余弦 恒定 0.0 0.0 AdamW (β1 = 0.9, β2 = 0.95,  = 1.0 × 10−15)</td></tr><tr><td>梯度范数剪切 优化器 损失权重 (CE : MSE) 预热步骤 训练步骤 EMA比率 每个排名的序列长度 (最小, 最大) # 训练见到的标记 最大上下文窗口 生成分辨率 (最小短边, 最大长边) 降噪分辨率 (最小短边, 最大长边) 扩散时间步移位</td><td>1.0 - 250 5K - (32K, 36K) 4.9B 16K - (378, 378)</td><td>1.0 0.25 : 1 2500 200K 0.9999 (32K, 36K) 2.5T 16k (256, 512) (224, 980) 1.0</td><td>0.0 1.0 0.25 : 1 2500 100k 0.9999 (40K, 45K) 2.6T 40k (512, 1024) (378, 980)</td><td>0.0 1.0 0.25 : 1 500 15K 0.995 (40K, 45K) 72.7B 40k (512, 1024) (378, 980)</td></tr><tr><td>数据采样比例 文本 图像-文本配对 (T2I) 图像-文本配对 (I2T)</td><td>0.0 0.0 1.0</td><td>0.05 0.6 0.1</td><td>4.0 0.05 0.4</td><td>4.0 0.05 0.3</td></tr><tr><td>交错理解 交错生成：视频 交错生成：网页</td><td>0.0 0.0 0.0</td><td>0.1 0.1 0.05</td><td>0.1 0.15 0.15</td><td>0.05 0.2 0.2</td></tr></table>

![](images/12.jpg)  
Figu5 Los crves  iffent dataratis.blai expt ar  ut  .5B LLM. "gumen that the sampling ratio for generation and understanding data is set at 1:1.

![](images/13.jpg)  
Figure 6 Loss curves of different learning rates. Ablation experiments are carried out on a 1.5B LLM. The sampling ratio for generation and understanding data is set at 1:1.

在所有训练阶段，我们使用AdamW优化器，$\beta _ { 1 } = 0 . 9$，$\beta _ { 2 } = 0 . 9 5$。受到[52]的启发，我们设置$\epsilon = 1 . 0 \times 1 0 ^ { - 1 5 }$来抑制oss峰值。当提高分辨率或生成时，我们还将扩散时间步从1.0增加到4.0，以确保适当的噪声水平分布。我们对PT、CT和SFT阶段采用恒定学习率，这样可以在不重新启动训练过程的情况下轻松扩展训练数据[30]。为确保不同排名之间的负载平衡，我们将每个排名的序列打包成一个较窄的长度范围（对齐和PT阶段为32K到36K个标记，CT和SFT阶段为40K到45K个标记）。

与独立的视觉语言模型或文本到图像模型的预训练不同，统一的多模态预训练需要仔细调整两个关键超参数——数据采样比例和学习率，以平衡理解和生成任务的信号。下面，我们描述了指导这些选择的经验性见解，这些选择进一步塑造了在表3中总结的训练协议。

# 4.1 数据采样比例

为了选择统一预训练期间每个数据源的采样比例，我们对1.5B Qwen2.5 LLM进行了系列控制研究，通过调整多模态生成数据与多模态理解数据的比例。如图所示，将生成数据的采样比例从$50\%$（"1g1u"）提高到$80\%$（"4g1u"），MSE损失稳步降低，绝对减少了$0.4\%$——对于已校正的模型而言，这是一个相当大的幅度。相反，交叉熵（CE）损失在采样比例之间没有表现出一致的模式；在第14,000步观察到的最大间隙为0.07，即在$" 4g1u "$与$" 2g1u "$之间，对下游基准的影响微乎其微。这些发现表明生成实例的采样频率应显著高于理解实例——这一启发式原则贯穿于总结在表3中的训练协议。

# 4.2 学习率

接下来，我们进行了一个控制实验，与4.1节中的设置相同，唯一不同的是学习率的设置。如图6所示，两个损失的表现相反：较大的学习率使MSE损失收敛得更快，而较小的学习率则有利于CE损失。为了调和这种权衡，我们为两个目标分配了单独的权重因子，如表3所列。

# 5 评估

为了全面评估统一模型，我们借鉴了针对定义良好的技能的既定基准，例如多模态理解、图像生成和经典图像编辑。然而，对于需要强大多模态推理和复杂任务组合的能力，有效的评估策略仍然缺乏。在接下来的部分中，我们将说明在评估中使用的可用基准，然后介绍一个用于自由形式图像处理（包括概念编辑）的新评估套件，旨在揭示模型在多模态推理和复杂组合任务中的能力。

多模态理解。我们采用六个广泛使用的基准——MME [20]、MMBench (1.0-EN) [46]、MMVet [96]、MMMU [97]、MathVista [49] 和 MMVP [74]。它们共同提供了一个简明但全面的测试平台，涵盖感知、认知和多模态推理，同时保持强大的区分能力，以便对最新的模型进行排名。

文本到图像生成。我们遵循[11, 57]并在流行的GenEval [25]基准上报告结果。我们还采用了最近提出的WISE基准[53]，该基准提供了对文本到图像生成中复杂语义理解和世界知识整合的全面评估。此外，我们还包括与最先进模型的定性比较，作为对这些自动评估指标的补充。

图像编辑。我们采用GEdit-Bench [44]作为我们的主要评估套件，因为它与现实世界相关且具有多样化的编辑任务。GEdit-Bench基于从网络抓取的真实用户请求构建，紧密反映了实际编辑需求。性能评分由GPT-4.1 [54]自动生成，同时我们也用定性示例补充这些分数，以提供更细致的评估。

智能法师编辑。我们提出了智能基准作为评估自由形式图像编辑能力的代理任务，这需要复杂的多模态推理和任务组合。智能基准的初始发布包含350个示例，每个示例由一个问题图像、问题文本和一个参考答案图像组成。评估使用GPT-4o（版本：gpt-4o-2024-11-20）进行，该模型审核完整的四元组——问题图像、问题文本、参考答案图像和模型生成的图像。评估标准包括请求满足、视觉一致性和知识创造力，基准重点关注任务的正确性和推理的深度。每个答案的评分范围是0到2。模型的最终得分通过将所有单个得分相加并归一化到100分的范围内计算。详细评估提示见附录表14。在智能基准的帮助下，我们可以评估模型在推理和整合世界知识进行图像编辑方面的表现。一些在智能基准上的展示和定性结果可以在图12中找到。

# 6 个新兴特性

新出现的特性在大型视觉或语言模型的背景下得到了广泛研究[7, 81]。在这项工作中，我们围绕统一的多模态基础模型的范围，采用了更为具体的定义来描述新出现的特性：

如果某项能力在早期训练阶段不存在，但在后来的预训练中出现，那么这项能力正在逐渐显现。

这种定性现象，常被称为相变，表示在行为模式中出现的突然而剧烈的变化，这种变化无法通过从训练损失曲线的外推来预测。 有趣的是，我们在统一多模态缩放中观察到了类似的现象，损失曲线并未明确指示新能力的出现。因此，我们通过在历史检查点上评估一系列任务的表现来研究模型能力的出现。具体而言，我们报告在标准VLM基准上的平均表现作为多模态理解的代理，生成能力的GenEval分数，以及分别评估模型在简单和复杂多模态推理中的能力的GEdit分数和IntelligentBench分数。

![](images/14.jpg)  
(a) Average score on Image Understanding tasks.

![](images/15.jpg)  
(b) GenEval score on Image Generation task

![](images/16.jpg)

![](images/17.jpg)  
(d) IntelligentBench Score on Intelligent Editing task.

(c) GEdit在经典图像编辑任务上的总体得分

图7 新兴曲线。BAGEL在不同任务上的预训练性能曲线。较浅的区域显示BAGEL在训练标记数量增加时表现出一致的性能提升。性能与训练规模之间的关系可总结如下：i）BAGEL在不同任务上持续改善；不同能力在合成任务中表现出不同的能力；采用A和B两种方法的效果；与MME-S、MMBench、MMMU、MMVet、MathVista和MMVP的得分进行比较。所有性能评估均在禁用BAGEL的思考模式下进行。

有趣的是，不同的任务表现出不同的学习动态和饱和行为。我选择了达到 $85\%$ 峰值性能所需的已见标记数量作为指标，如图7所示，我们发现传统的理解和生成基准相对较早达到饱和：分别在大约 0.18T 和 0.68T 的标记。然而，编辑任务需要理解和生成的能力，表现出更慢的收敛，仅在 2.64T 标记后才达到 $85\%$ 的性能。

最值得注意的是，智能编辑任务旨在消除简单编辑案例，并强调复杂的多模态推理，需要3.61T的token才能达到85%的性能，展示出与[81]中描述的突现行为类似的模式。在这种情况下，模型最初表现较差，但在3T的可见token之后逐渐显著改善。虽然传统编辑任务在3T token的分辨率增加上影响不大，但智能编辑的性能显著提高，从15提升到45，在后期训练阶段增长了三倍，突显了其对统一多模态推理的依赖。我们发现，不可理解的特别是输入在去除ViT token时对GEdit-Bench影响最小，但在智能编辑中造成了$1 6 \%$的下降，这突显了在复杂编辑任务中视觉-语义推理的重要性。

![](images/18.jpg)  
FigureComparison of models with different amounts o training tokens. W present cases Text-o-mage generation and image editing.

虽然评估指标可能无法完全捕捉到模型的真实能力——尽管这不太可能导致新的出现现象——我们进一步通过检查不同训练检查点的生成输出来研究定性的出现行为。如图8所示，我们观察到与性能曲线一致的趋势：在1.5T已见令牌之前，生成质量已经很强，在使用更高分辨率训练后，在3.0T已见令牌后有小幅质量提升。对于文本渲染，正确生成“hello”和“BAGEL”的能力在1.5T到4.5T令牌之间才出现。

新出现的行为也在Fiur的定性可视化智能编辑任务中被观察到。与图8中仅对输入图像进行部分修改的传统编辑不同，智能编辑通常需要基于多模态推理生成全新的概念。在处理3.5T令牌之前，模型倾向于以最小的变化重现输入图像——当任务未完全理解时的回退策略。然而，在看到3.5T令牌之后，模型开始展示出明确的推理，生成连贯且语义适当的编辑，与图7中观察到的新兴行为相一致。

# 问题

你能展示这个编织项目完成后的样子吗？

夜间灯光下该地点的外观如何？

你能分享一张这个角色惊讶的图片吗？

你能给这些杯子蛋糕加一些配料吗？

你能展示一下这件礼服的后面吗？

一旦搅拌好，你能展示一下果昔吗？

什么方法可以帮助将面糊添加到甜甜圈模具中？

![](images/19.jpg)  
Figurarionme w iffenntatkr ig that requires strong multimodal reasoning abilities.

有模特穿戴项圈的图片吗？

您能给我展示汽车后置发动机的详细视图吗？

# 7个主要结果

在本节中，我们提供定量和定性评估，以检验BAGEL的多模态能力。我们首先在已建立的基准上展示基本能力，包括第7.1节的图像理解和第7.2节的图像生成。然后，我们在第7.3节报告现有图像编辑基准和IntelligentBench的性能。在第7.4节中，我们探讨了具有明确推理的生成和编辑。在这种设置下，BAGEL被允许生成中间思考步骤，然后再输出最终结果。我们发现这种推理显著提升了性能。最后，在第7.5节中，我们提供了定性可视化，展示了BAGEL的世界建模能力，包括世界导航和视频生成。

# 7.1 图像理解

表4 在视觉理解基准上的与最新技术的比较。MME-S指MME-P和MME-C的总结。对于MoE模型，我们报告它们的激活参数/总参数。$^ \dagger$ MetaQuery [57] 采用Qwen2.5-VL [4]的预训练模型，并在训练过程中被冻结。$^ { * * }$ : 部分结果来自MetaMorph [73]或MetaQuery [57]。

<table><tr><td>类型 模型</td><td></td><td># LLM 参数 MME-P MME-S↑ MMBench↑ MMMU↑ MM-Vet↑ MathVista↑ MMVP↑</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td rowspan="12">h </td><td>InternVL2 [13]</td><td>1.8B</td><td>1440</td><td>1877</td><td>73.2</td><td>34.3</td><td>44.6</td><td>46.4</td><td>35.3</td></tr><tr><td>InternVL2.5 [12]</td><td>1.8B</td><td>-</td><td>2138</td><td>74.7</td><td>43.6</td><td>60.8</td><td>51.3</td><td>-</td></tr><tr><td>Qwen2-VL[77]</td><td>1.5B</td><td>-</td><td>1872</td><td>74.9</td><td>41.1</td><td>49.5</td><td>43.0</td><td></td></tr><tr><td>Qwen2.5-VL[4]</td><td>3B</td><td>-</td><td>2157</td><td>79.1</td><td>53.1</td><td>61.8</td><td>62.3</td><td></td></tr><tr><td>BLIP-3 [91]</td><td>4B</td><td>-</td><td>-</td><td>76.8</td><td>41.1</td><td>-</td><td>39.6</td><td></td></tr><tr><td>LLava-OV [37]</td><td>7B</td><td>1580</td><td>-</td><td>80.8</td><td>48.8</td><td>57.5</td><td>63.2</td><td>-</td></tr><tr><td>InternVL2 [13]</td><td>7B</td><td>1648</td><td>2210</td><td>81.7</td><td>49.3</td><td>54.2</td><td>58.3</td><td>51.3</td></tr><tr><td>InternVL2.5 [12]</td><td>7B</td><td>-</td><td>2344</td><td>84.6</td><td>56.0</td><td>62.8</td><td>64.4</td><td>-</td></tr><tr><td>Qwen2-VL [77]</td><td>7B</td><td>-</td><td>2327</td><td>83.0</td><td>54.1</td><td>62.0</td><td>58.2</td><td>-</td></tr><tr><td>Qwen2.5-VL[4]</td><td>7B</td><td>-</td><td>2347</td><td>83.5</td><td>58.6</td><td>67.1</td><td>68.2</td><td>-</td></tr><tr><td>Emu3-Chat** [79]</td><td>8B</td><td>1244</td><td>-</td><td>58.5</td><td>31.6</td><td>37.2</td><td>-</td><td>36.6</td></tr><tr><td>Kimi-VL [71]</td><td>2.8B/16B</td><td>-</td><td>-</td><td>-</td><td>57.0</td><td>66.7</td><td>68.7</td><td>-</td></tr><tr><td></td><td>DeepSeek-VL2 [87]</td><td>4.1B/28B</td><td>-</td><td></td><td>-</td><td>51.1</td><td>60.0</td><td>62.8</td><td>-</td></tr><tr><td>Janus [83]</td><td>Show-0512 [89]</td><td>1.3B</td><td>1097</td><td></td><td>-</td><td>26.7</td><td>-</td><td>-</td><td></td></tr><tr><td></td><td></td><td>1.5B</td><td>1338</td><td></td><td>69.4</td><td>30.5</td><td>34.3</td><td></td><td></td></tr><tr><td>BAGEL</td><td>Janus-Pro [11]</td><td>1.5B</td><td>1444</td><td>-</td><td>75.5</td><td>36.3</td><td>39.8</td><td></td><td></td></tr><tr><td rowspan="9">p</td><td></td><td>1.5B MoT</td><td>1610</td><td>2183</td><td>79.2</td><td>43.2</td><td>48.2</td><td>63.4</td><td>54.7</td></tr><tr><td>ILLUME [76]</td><td>7B</td><td>1445</td><td></td><td>75.1</td><td>38.2</td><td>37.0</td><td></td><td>-</td></tr><tr><td>VILA-U256 [85]</td><td>7B</td><td>1336</td><td></td><td>66.6</td><td>32.2</td><td>27.7</td><td></td><td>22.0</td></tr><tr><td>Chameleon** [70]</td><td>7B</td><td>-</td><td></td><td>35.7</td><td>28.4</td><td>8.3</td><td></td><td>0.0</td></tr><tr><td>Janus-Pro [11]</td><td>7B</td><td>1567</td><td></td><td>79.2</td><td>41.0</td><td>50.0</td><td></td><td>-</td></tr><tr><td>MetaQuery-XL† [57]</td><td>7B</td><td>1685</td><td></td><td>83.5</td><td>58.6</td><td>66.6</td><td></td><td>-</td></tr><tr><td>LlamaFusion** [66]</td><td>8B</td><td>1604</td><td></td><td>72.1</td><td>41.7</td><td>-</td><td></td><td>-</td></tr><tr><td>MetaMorph [73]</td><td>8B</td><td>-</td><td></td><td>75.2</td><td>41.8</td><td>-</td><td></td><td>48.3</td></tr><tr><td>SEED-X [23]</td><td>13B</td><td>1457</td><td></td><td>70.1</td><td>35.6</td><td>43.0</td><td></td><td>-</td></tr><tr><td>TokenFlow-XL [59]</td><td></td><td>13B</td><td>1546</td><td></td><td>68.9</td><td>38.7</td><td>40.7</td><td></td><td>-</td></tr><tr><td>MUSE-VL [90]</td><td></td><td>32B</td><td></td><td></td><td>81.8</td><td>50.1</td><td></td><td>55.9</td><td></td></tr><tr><td colspan="2">BAGEL</td><td>7B MoT</td><td>1687</td><td>2388</td><td>85.0</td><td>55.3</td><td>67.2</td><td>73.1</td><td>69.3</td></tr></table>

我们对BAGEL进行了广泛的基准测试，比较了其与最新的开源多模态模型的表现，包括专门的视觉理解模型和通用统一模型。我们的评估涵盖了一系列丰富的公共基准，以确保对模型能力的全面评估。

视觉理解结果总结在表4中。在可比较的激活参数规模为7B时，BAGEL在理解任务上优于现有的统一模型。例如，在MMMU和MM-Vet上，BAGEL分别比Janus-Pro [11]提高了14.3和17.1分。值得注意的是，MetaQuery-XL [57]依赖于冻结的、预训练的Qwen2.5-VL [4]骨干，限制了其适应性。此外，与专门的理解模型如Qwen2.5-VL和InternVL2.5 [12]相比，BAGEL在大多数基准测试中提供了更卓越的性能，证明我们的MoT设计有效减轻了任务冲突，同时保持强大的视觉理解能力。

TlEvalan ex tblityn Glarkel 的生成模型，“Unified”指的是具有理解和生成能力的模型。$^ \dagger$ 指使用 LLM 重写器的方法。

<table><tr><td>类型</td><td>模型</td><td></td><td>单对象 双对象</td><td>计数</td><td>颜色</td><td>位置</td><td>颜色属性</td><td>总体↑</td></tr><tr><td rowspan="8">he n</td><td>PixArt-α [9]</td><td>0.98</td><td>0.50</td><td>0.44</td><td>0.80</td><td>0.08</td><td>0.07</td><td>0.48</td></tr><tr><td>SDv2.1 [61]</td><td>0.98</td><td>0.51</td><td>0.44</td><td>0.85</td><td>0.07</td><td>0.17</td><td>0.50</td></tr><tr><td>DALL-E 2 [60]</td><td>0.94</td><td>0.66</td><td>0.49</td><td>0.77</td><td>0.10</td><td>0.19</td><td>0.52</td></tr><tr><td>Emu3-Gen [79]</td><td>0.98</td><td>0.71</td><td>0.34</td><td>0.81</td><td>0.17</td><td>0.21</td><td>0.54</td></tr><tr><td>SDXL [58]</td><td>0.98</td><td>0.74</td><td>0.39</td><td>0.85</td><td>0.15</td><td>0.23</td><td>0.55</td></tr><tr><td>DALL-E 3 [5]</td><td>0.96</td><td>0.87</td><td>0.47</td><td>0.83</td><td>0.43</td><td>0.45</td><td>0.67</td></tr><tr><td>SD3-Medium [19]</td><td>0.99</td><td>0.94</td><td>0.72</td><td>0.89</td><td>0.33</td><td>0.60</td><td>0.74</td></tr><tr><td>FLUX.1-dev† [35]</td><td>0.98</td><td>0.93</td><td>0.75</td><td>0.93</td><td>0.68</td><td>0.65</td><td>0.82</td></tr><tr><td rowspan="14"></td><td>变色龙 [70]</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td></td></tr><tr><td>LWM [42]</td><td>0.93</td><td>0.41</td><td>0.46</td><td>0.79</td><td>0.09</td><td>0.15</td><td>0.39 0.47</td></tr><tr><td>SEED-X [23]</td><td>0.97</td><td>0.58</td><td>0.26</td><td>0.80</td><td>0.19</td><td>0.14</td><td>0.49</td></tr><tr><td>TokenFlow-XL [59]</td><td>0.95</td><td>0.60</td><td>0.41</td><td>0.81</td><td>0.16</td><td>0.24</td><td>0.55</td></tr><tr><td>ILLUME [76]</td><td>0.99</td><td>0.86</td><td>0.45</td><td>0.71</td><td>0.39</td><td>0.28</td><td>0.61</td></tr><tr><td>Janus [83]</td><td>0.97</td><td>0.68</td><td>0.30</td><td>0.84</td><td>0.46</td><td>0.42</td><td>0.61</td></tr><tr><td>Transfusion [104]</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>0.63</td></tr><tr><td>Emu3-Gen†[79]</td><td>0.99</td><td>0.81</td><td>0.42</td><td>0.80</td><td>0.49</td><td>0.45</td><td>0.66</td></tr><tr><td>Show-o [89]</td><td>0.98</td><td>0.80</td><td>0.66</td><td>0.84</td><td>0.31</td><td>0.50</td><td>0.68</td></tr><tr><td>Janus-Pro-7B [11]</td><td>0.99</td><td>0.89</td><td>0.59</td><td>0.90</td><td>0.79</td><td>0.66</td><td>0.80</td></tr><tr><td>MetaQuery-XL† [57]</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>0.80</td></tr><tr><td>BAGEL</td><td>0.99</td><td>0.94</td><td>0.81</td><td>0.88</td><td>0.64</td><td>0.63</td><td>0.82</td></tr><tr><td>AGEL*</td><td>0.98</td><td>0.95</td><td>0.84</td><td>0.95</td><td>0.78</td><td>0.77</td><td>0.88</td></tr></table>

表6 WIE上词汇知识推理的比较。WIS考察复杂的语义理解和世界知识或T2I生成。'Gen.Only'代表仅图像生成模型，'Unified'表示同时具有理解和生成能力的模型。$^{**}$: GPT-4o的结果由[92]测试。

<table><tr><td>类型</td><td>模型</td><td>文化</td><td>时间</td><td>空间</td><td>生物</td><td>物理</td><td>化学</td><td>总体↑</td></tr><tr><td></td><td>SDv1.5 [61]</td><td>0.34</td><td>0.35</td><td>0.32</td><td>0.28</td><td>0.29</td><td>0.21</td><td>0.32</td></tr><tr><td></td><td>SDXL [58]</td><td>0.43</td><td>0.48</td><td>0.47</td><td>0.44</td><td>0.45</td><td>0.27</td><td>0.43</td></tr><tr><td></td><td>SD3.5-large [19]</td><td>0.44</td><td>0.50</td><td>0.58</td><td>0.44</td><td>0.52</td><td>0.31</td><td>0.46</td></tr><tr><td></td><td>PixArt-Alpha [9]</td><td>0.45</td><td>0.50</td><td>0.48</td><td>0.49</td><td>0.56</td><td>0.34</td><td>0.47</td></tr><tr><td>h un</td><td>playground-v2.5 [38]</td><td>0.49</td><td>0.58</td><td>0.55</td><td>0.43</td><td>0.48</td><td>0.33</td><td>0.49</td></tr><tr><td></td><td>FLUX.1-dev [35]</td><td>0.48</td><td>0.58</td><td>0.62</td><td>0.42</td><td>0.51</td><td>0.35</td><td>0.50</td></tr><tr><td></td><td>Janus [83]</td><td>0.16</td><td>0.26</td><td>0.35</td><td>0.28</td><td>0.30</td><td>0.14</td><td>0.23</td></tr><tr><td></td><td>VILA-U [85]</td><td>0.26</td><td>0.33</td><td>0.37</td><td>0.35</td><td>0.39</td><td>0.23</td><td>0.31</td></tr><tr><td></td><td>Show-o-512 [89]</td><td>0.28</td><td>0.40</td><td>0.48</td><td>0.30</td><td>0.46</td><td>0.30</td><td>0.35</td></tr><tr><td></td><td>Janus-Pro-7B [11]</td><td>0.30</td><td>0.37</td><td>0.49</td><td>0.36</td><td>0.42</td><td>0.26</td><td>0.35</td></tr><tr><td></td><td>Emu3 [79]</td><td>0.34</td><td>0.45</td><td>0.48</td><td>0.41</td><td>0.45</td><td>0.27</td><td>0.39</td></tr><tr><td></td><td>MetaQuery-XL [57]</td><td>0.56</td><td>0.55</td><td>0.62</td><td>0.49</td><td>0.63</td><td>0.41</td><td>0.55</td></tr><tr><td></td><td>GPT-40**</td><td>0.81</td><td>0.71</td><td>0.89</td><td>0.83</td><td>0.79</td><td>0.74</td><td>0.80</td></tr><tr><td></td><td>BAGEL</td><td>0.44</td><td>0.55</td><td>0.68</td><td>0.44</td><td>0.60</td><td>0.39</td><td>0.52</td></tr><tr><td></td><td>BAGEL w/ Self-CoT</td><td>0.76</td><td>0.69</td><td>0.75</td><td>0.65</td><td>0.75</td><td>0.58</td><td>0.70</td></tr></table>

我们在两个基准上评估视觉生成性能：GenEval 和 WISE。如表 5 所示，在与 MetaQuery-XL 相同的评估设置下，BAGEL 获得了 $88 \%$ 的整体得分，超越了两种专业生成模型（FLUX-1-dev: $82 \%$，SD3-Medium: $74 \%$）和统一模型（Janus-Pro: $80 \%$，MetaQuery-XL: $80 \%$）。即使没有 LLM 重写器，BAGEL 仍然达到了 $82 \%$，超过了先前的 SOTA 统一模型 Janus-Pro-7B。在 WISE 基准上，BAGEL 超越了除领先的私人模型 GPT-4o 之外的所有先前模型。这表明，BAGEL 具有强大的推理能力和世界知识。

我们对BAGEL和Janus-Pro 7B、SD3-medium以及GPT-4o进行了定性比较。如图10所示，BAGEL生成的图像质量明显高于Janus-Pro 7B，并且也超越了广泛使用的专业文本到图像模型SD3-medium。此外，它原生支持中文和英文的提示，并允许以任意长宽比进行生成。

# 提示

# 百吉饼

书封面，一幅超现实的双重曝光肖像，将女性的面孔与美丽的海景融合在一起。整体氛围梦幻而神秘，色彩丰富，细节精致。

1:1

一部名为“指挥”的电影海报。海报上有一个穿着深色西装的人，手持指挥棒，左手抬起，似乎在引导或指挥。背景昏暗且有些抽象，隐约显示出舞台或表演的环境。“指挥”这个标题以粗体白色大写字母醒目地展示在顶部。标题下方，副标题“身体的音乐”以较小的白色字体书写。整体设计简洁且专业，突出指挥的角色以及音乐和表演的主题。

4:3

一幅逼真的特写图像，展示两艘海盗船在一杯咖啡中交战。

A4R,###E, #5, \*RR, MMXNE , −— ′,A## #WRE,#4 E, \$#), ##≠, 3D

9:16

一位女性角色扮演者扮演着一个飘渺的仙女或精灵，穿着一袭由柔弱神秘色彩如翡翠绿和银色制成的飘逸裙子。她有尖尖的耳朵，脸上带着温柔迷人的神情，服装上装饰着闪闪发光的珠宝和复杂的图案。背景是一片神奇的森林，生长着发光的植物，栖息着神秘的生物，氛围宁静宜人。

![](images/20.jpg)  
Figure 10T qualitative comparison. Note that SD3-medium cannot take Chinese prompts so we translate them inEnglish.For GPT-4, wecontrol the aspet ratiovi text promt. JanusPro can nl enerate squareages.

在火星上，红褐色的土壤和尖锐的岩石在淡粉色的天空下延展出一片崎岖的景观。远处，一座高耸的火山巍然屹立，山顶笼罩在轻微的烟雾中。附近，一条深 canyon中有复杂的侵蚀壁，切割穿越地形。一个小型机器人探测车缓慢地在表面移动，留下了细微的火星尘埃痕迹。这个场景捕捉了红色星球的鲜明美丽和异世界的氛围。

表7 GEdit-Bench比较。所有指标均以“越高越好”（↑）报告。G_SC、G_PQ和G_O是指由GPT-4.1评估的指标。

<table><tr><td rowspan="2">类型</td><td rowspan="2">模型</td><td colspan="3">GEdit-Bench-EN（完整集合）↑</td><td colspan="3">GEdit-Bench-CN（完整集合）↑</td></tr><tr><td>G_SC</td><td>G_PQ</td><td>G_O</td><td>G_SC</td><td>G_PQ</td><td>G_O</td></tr><tr><td rowspan="2">私有</td><td>Gemini 2.0 [24]</td><td>6.73</td><td>6.61</td><td>6.32</td><td>5.43</td><td>6.78</td><td>5.36</td></tr><tr><td>GPT-4o [55]</td><td>7.85</td><td>7.62</td><td>7.53</td><td>7.67</td><td>7.56</td><td>7.30</td></tr><tr><td rowspan="6">开源</td><td>Instruct-Pix2Pix [6]</td><td>3.58</td><td>5.49</td><td>3.68</td><td>-</td><td>-</td><td>-</td></tr><tr><td>MagicBrush [99]</td><td>4.68</td><td>5.66</td><td>4.52</td><td>-</td><td></td><td></td></tr><tr><td>AnyEdit [95]</td><td>3.18</td><td>5.82</td><td>3.21</td><td></td><td></td><td></td></tr><tr><td>OmniGen [88]</td><td>5.96</td><td>5.89</td><td>5.06</td><td>-</td><td></td><td></td></tr><tr><td>Step1X-Edit [43]</td><td>7.09</td><td>6.76</td><td>6.70</td><td>7.20</td><td>6.87</td><td>6.86</td></tr><tr><td>BAGEL</td><td>7.36</td><td>6.83</td><td>6.52</td><td>7.34</td><td>6.85</td><td>6.50</td></tr></table>

<table><tr><td>类型</td><td>模型</td><td>得分↑</td></tr><tr><td rowspan="2">私有</td><td>GPT-40** [55]</td><td>78.9</td></tr><tr><td>Gemini 2.0** [24]</td><td>57.6</td></tr><tr><td rowspan="3">开源</td><td>Step1X-Edit [43]</td><td>14.9</td></tr><tr><td>BAGEL</td><td>44.9</td></tr><tr><td>BAGEL w/ Self-CoT</td><td>55.3</td></tr></table>

表8 智能性对比。智能基准测试复杂推理能力与上下文。$^ { * * }$：结果仅在回答的案例子集上报告（一些回答被拒绝）。GPT-40回答了350个问题中的318个，而Gemini 2.0回答了349个问题。

我们进一步使用 GEdit-Bench [44] 评估 BAGEL 的经典图像编辑能力。如表 7 所示，BAGEL 的结果与当前领先的专业图像编辑模型 Step1X-Edit [44] 相媲美，并且超越了 Gemini 2.0。此外，我们在表 8 中报告了我们新提议的 IntelligentBench 的结果，其中 BAGEL 的表现达到了 4.9，显著超过了现有的开源 Step1X-Edit 模型 30 个百分点。

我们还在图11和图12中提供了一组多样化图像编辑场景的定性比较，基准测试BAGEL与Gemini 2.0、GPT-4o、Step1X-Edit和IC-Edit [100]。如图所示，BAGEL始终表现出优于Step1X-Edit和IC-Edit的性能，并且超越了Gemini 2.0的能力。尽管GPT-4o成功处理了这些场景，但它往往会对源图像引入意外的修改，而BAGEL有效地避免了这个问题。

# 7.4 生成/编辑与思考

在本节中，我们从定量和定性两个角度验证了增强推理生成在各种基准测试中的有效性。

带有思考的生成。对于文本到图像的任务，我们在生成之前对WISE上的Bagel进行明确的思维链（CoT）推理过程评估。如表6所示，带有CoT的BAGEL得分为0.70，比其非CoT对应项高出0.18，同时还显著超过了所有现有的开源模型（之前的SOTA：MetaQuery-XL为0.55）。除了定量评估，我们在图13a中提供了可视化，显示当仅给出短提示时，BAGEL未能生成正确图像，但在使用基于CoT的思维范式时则能成功。

思考中的编辑。如表8所示，将CoT融入BAGEL将其智能分数从44.9提高到55.3。这个性能提升主要归因于推理的加入，这使得模型能够利用世界知识并提供详细的编辑指导。在RISEBench [103]（表9，从6.1到11.9）和KRIS-Bench [86]（表10，从56.21到60.18）上也观察到了持续的改进。我们在图13b中进一步展示了来自IntelligentBench的几个代表性案例，这些任务需要一般知识或多步骤推理。在这些场景中，在思考内容的指导下，BAGEL显著提高了图像编辑能力。

# 问题

给她做一个Jellycat毛绒玩具。

更改为吉卜力风格。

移除背景中的路人

将图像转换为彩色。给它加上一个小丑鼻子，并将文本更改为英语“Will I Lose”。

将图片中的文字更改为“孤独的熊”。

根据这张英俊男子的照片生成一个3D动作玩偶模型，大胆展现他的肌肉线条和潇洒姿态，细节展示出动态力量。将动作玩偶放置在一张木桌上。

他蹲下身，摸了一下狗的头。

![](images/21.jpg)  
Figure 11 Comparison on editing and manipulation tasks.

他在富士山顶自拍了一张照片。

她在图书馆里看一本书。这本书的封面是《如何使用贝果》。

question /'kwestʃәn/
n. 问题, 询问
v. 询问, 审问, 怀疑
[计] 询问
Definition: n. an instance of questioning
n. the subject matter at issue
n. a sentence of inquiry that asks for a reply
n. an informal reference to a marriage proposal

您能提供这辆车内饰的图片吗？

是否有与衬衫设计匹配的手机壳可供选择？您能展示一些吗？

您能提供佩戴项链的视觉效果吗？

我可以看到这个角落架概念的真实例子吗？

装饰后，饼干会是什么样子？

请描述顶部中间的图像。

你能展示一下这个车间内部吗？

![](images/22.jpg)  
Figure 12 Comparison on IntelligentBench. The results demonstrate that (i) BAGEL achieves performance caral G.ivp auln orSe-icnpaos the input image, which may be attributed to its lmited reasonig capabiltis. Note that BAGEL results here re generated in thinking mode.

你能给我看一下这辆车的引擎吗？

你能展示这块布展开后的样子吗？

<想法> 模型应该生成一幅图像，其中许多小汽车组合在一起形成一辆更大汽车的形状和结构。

精炼后的提示是：一幅高度详细和富有创意的图像，展示了一辆大型汽车，完全由许多小汽车组成， meticulously arranged to replicate the shape and structure of a full-sized vehicle. 小汽车紧密相连，创造出无缝和真实的外观，比例和细节准确。背景干净中性，柔和的散射光强调作品的复杂性，没有任何干扰。图像以真实感风格捕捉概念，专注于精度和工艺，突显比例和形状的创新使用。

![](images/23.jpg)

一只猫的体型是旁边的狗的两倍。

![](images/24.jpg)

<think> 模型应生成一幅图像，其中猫的体型明显大于狗，以突出大小的差异。

精炼的提示是：一幅高度详细和逼真的图像，一只庞大而威严的猫站在一只较小的狗旁边，猫的体型与狗相比显得尤为突出。猫的毛发光滑，色彩鲜艳，姿态自信高贵，而狗则显得较小，带着友好且略显好奇的表情。背景是一个户外环境，柔和的自然光线、温暖的色调和温馨的氛围。视角略微低角度，捕捉到两种动物之间的大小对比，构成视觉上引人注目的画面，猫的焦点清晰，而背景则有微妙的景深。

![](images/25.jpg)

一个男人正站在一只雌性棕熊和它的幼崽旁边。生成一幅图像来展示可能发生的事情。

![](images/26.jpg)

<think> 模型应生成一幅画面，展示一个男人与雌性棕熊及其幼崽的近距离接触，可能表现出男人的紧张互动或谨慎行为，因为熊通常对幼崽非常保护。

精炼后的提示是：一个男人站在一只雌性棕熊及其幼崽附近，画面中展现出紧张却自然的互动，熊表现出保护的姿态，幼崽则依偎在旁。场景捕捉了这一瞬间，男人保持着谨慎的距离，流露出关切或警惕，身体略微前倾，神情警觉。背景是一片崎岖的森林区域，茂密的绿色植被和土壤色调，柔和的自然光透过树梢洒落下来。气氛紧张却真实，强调人类与野生动物之间的原始联系，仔细描绘了熊毛发的细致纹理、男人穿着的衣物和周围的植物。超现实，专注于逼真的解剖结构和环境，捕捉情感和环境的动态。

![](images/27.jpg)

![](images/28.jpg)

问题输入结果包含思考k o SUV。问题图像显示一个卡车车斗，里面有一个大型橄榄绿色袋子，上面标注着“保持牛犊袋子凉爽的高级冷却”。这个袋子似乎是为存储或运输设计的，可能是用来存放需要温度控制的物品。问题文本要求将袋子移到SUV上，暗示用户希望在不同的上下文或车辆中可视化该袋子。为了解决这个问题，我会考虑袋子的大小和形状，确保它在SUV的货物区域内合适。移转袋子可能会描绘袋子整齐地放置在SUV内部，可能在地板上或以防止运输过程中移动的方式固定住。在形成回答文本时，我会确认转移并描述袋子在SUV中的放置情况。答案图像将显示SUV的内部，袋子清晰可见并且正确放置，强调它的功能和实用性。这一种方法确保明确并直观地满足用户的请求。

问题文本要求可视化一个带抽屉的柜子，抽屉打开，内部可见。问题图像显示柜子以标准角度打开抽屉，展示内部表面，柜子应保持在同一位置以保持清晰。答案文本将确认柜子现在展示为抽屉打开，满足用户对内部视觉检查的请求。

问题涉及从正在制作或维修的吉他琴身表面移除胶带。问题图像显示吉他琴身的边缘贴了黄色胶带，可能是为了在特定过程中保护木材，突出其自然木纹和表面处理。在没有胶带的工作空间中，展示吉他琴身干净抛光的表面和任何可见的工艺痕迹，比如木纹图案和琴身的平滑曲线。这将满足用户对吉他琴身明确查看的请求。

# 7.5 世界建模

为了提高BAGEL在长序列视觉生成中的世界建模能力，我们通过增加训练配方中视频和导航数据的比例来微调模型。对于导航，我们从视频交叉序列中构建数据集，使用ParticleSfM [102] 注释相机轨迹。在图14中，我们展示了BAGEL的世界建模能力，包括世界导航、旋转和多帧生成。

从图中可以看出，BAGEL展现了强大的世界理解和模拟能力。它可以根据输入指令生成动态数量的图像，用于导航和旋转输入图像等任务，或者根据给定的提示生成多幅图像。此外，BAGEL在世界理解方面展现出强大的泛化能力。例如，尽管仅在现实世界街道导航上进行训练，但它能够无缝扩展到墨迹画、卡通和视频游戏等多样领域。

# 7.6 更多定性结果

BAGEL-1.5B的表现。图16比较了BAGEL-1.5B（激活参数为$1.5~ \mathrm{B}$）与JanusPro-7B和Step1X-Edit（12B）在文本到图像（T2I）和图像编辑任务上的表现。尽管BAGEL-1.5B显著较小，但在这两项任务的定性比较中超过了这两个更大的模型。而且，BAGEL-1.5B与BAGEL-7B之间的差距凸显了模型扩展所带来的收益，表明更大的BAGEL变体有更大的潜力。

失败案例。在图17中，我们展示了BAGEL与其他最先进模型的代表性失败案例。需要特殊IP生成、复杂文本渲染、精细的人体姿态生成或同时生成多个实例的任务对于当代文本到图像系统仍然具有挑战性。对于图像编辑而言，诸如交换对象位置或同时修改大量实例等操作也对大多数现有模型构成挑战。在一些复杂场景中，BAGEL和Gemini 2.0在准确遵循给定指令方面都表现出类似的困难。相比之下，GPT-40在所有示例中提供了最一致的成功结果。仅通过使用包含额外文本的图像扩展数据、增加模型容量或在最终后训练阶段应用RLHF [56]，就可以简单地增强BAGEL的性能。

# 8 结论

我们提出了BAGEL，一个统一的多模态理解和生成模型，在统一预训练规模扩大时显示出新兴的能力。BAGEL在标准的多模态理解和生成基准测试中表现出色，并进一步以强大的世界建模和推理能力脱颖而出。希望能够解锁多模态研究的进一步机会，我们将BAGEL开源给研究社区。

# 9 认可

我们要感谢魏子乾、陈浩理、徐胜扬、李晨、秦宇佳、林怡、黄文浩、严申、肖晓军、吴岩、吴刚、李国栋、雷康、陶良伟、杨奇帆、易百仁、陈秀丽、曹锐、王雅婷、周宇峰、徐明迪、张婷婷、熊雪寒、程天恒、王赞博、张恒、彭扬华、吴法铭、冯嘉识、张剑锋、李绣对BAGEL项目的贡献。

![](images/29.jpg)

突出猫的橙色毛发。画面清晰且锐利，景深浅。

![](images/30.jpg)

3D颜色

![](images/31.jpg)  
Figure 14 Examples of BAGEL in navigation, rotation, and multi-image generation.