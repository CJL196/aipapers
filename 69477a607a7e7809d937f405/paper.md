# SAM 3D Body: Robust Full-Body Human Mesh Recovery

X Ynge ukreoPinksukaSagaThaFanJi ar SinkaoJiawei LiuNicolasUgrivicatFeizlJitena MaliPiotrDollarKri Meta Superintelligence Labs $\star$ Core Contributor, Intern, $^ \dagger$ Project Lead

We introduce SAM 3D Body (3DB), a promptable model for single-image full-body 3D human mesh recovery (HMR) that demonstrates state-of-the-art performance, with strong generalization and consistent accuracy in diverse in-the-wild conditions. 3DB estimates the human pose of the body, feet, and hands. It is the first model to use a new parametric mesh representation, Momentum Human Rig (MHR), which decouples skeletal structure and surface shape. 3DB employs an encoder-decoder architecture and supports auxiliary prompts, including 2D keypoints and masks, enabling userguided inference similar to the SAM family of models. We derive high-quality annotations from a multi-stage annotation pipeline that uses various combinations of manual keypoint annotation, differentiable optimization, multi-view geometry, and dense keypoint detection. Our data engine efficiently selects and processes data to ensure data diversity, collecting unusual poses and rare imaging conditions. We present a new evaluation dataset organized by pose and appearance categories, enabling nuanced analysis of model behavior. Our experiments demonstrate superior generalization and substantial improvements over prior methods in both qualitative user preference studies and traditional quantitative analysis. Both 3DB and MHR are open-source.

![](images/1.jpg)  
Figure 1 Human mesh recovery results using SAM 3D Body (3DB). Our model demonstrates robust performance in esati hallengi oscivervwoint nd produ crae od nd han pstiations a unified framework.

# 1 Introduction

Estimating 3D human pose (skeleton pose and structure) and shape (soft body tissue) from images is an essential capability orvision and embodied AI systems to understand andinteract with peopl.Despite notable progress in human mesh recovery (HMR) (9; 7; 33; 50; 51), existing approaches stil exhibit unsatisfactory robustness when applied to in-the-wild images, which limits their applicability to real-world scenarios such as robotics (37; 3; 47) and biomechanics (36). In particular, current models often fail on individuals presening challenging poses, severe oclusion, or captured from uncommon viewpoints. They also struggle to reliably estte bottheverall by posnheetai thean ndfulbody. We argue that the primary challenges in developing a robust full-body human mesh recovery model stem from both the data and model aspects. First, collecting large-scale and diverse human pose datasets with high-quality mesh annotations is inherently diffcult and computationally costly. Most existing datasets either suffer from low diversity due to laboratory capture settings (4; 12; 13) or from low mesh quality resulting from pseudo-labeling (48; 1). Second, current HMR architectures do not adequately address the distinct optimization mechanisms required for body and hand pose estimation, nor do they incorporate effective training strategies to handle uncertainty and ambiguity from monocular images. In this work, we present SAM 3D Body (3DB), a robust full-body HMR model fueled by large-scale, high-quality human pose data curated by our data engine.

Robust Full-body HMR Model. We make three main contributions to improve model performance on both body and hand pose estimation. (i) We propose a novel promptable encoder-decoder architecture (17; 39) that enables the model to condition on optional 2D keypoints, masks or camera information for controllable pose estimation. This promptable design naturally facilitates interactive guidance in ambiguous or challenging scenarios during training, and provides a coherent approach to integrate hand and body predictions. (ii) Our model utilizes a shared image encoder and two separate decoders for the body and hands. This two-waydecoder design effectively alleviates conficts in optimizing body and hand pose estimation, which arise from differences in input resolution, camera estimation, and supervision objectives. (ii) Unlike most prior work that relies on the SMPL (26) human mesh model, we build 3DB on a new parametric mesh representation, MHR (8), which decouples skeletal pose and body shape, providing richer control and interpretability for full-body reconstruction.

Data Engine for Diverse Human Pose and High-quality Annotation. HMR methods have increasingly turned to large-scale training data for higher performance (9; 3; 54). However, high-quality 3D supervision remains scarce, and existing in-the-wild datasets are still limited in scale and diversity. To this end, we design a new data creation pipeline that features: (i) Data Quality: Our annotation pipeline combines various combinations of components such as geometric constraints, parametric priors, and dense keypoint regression, which automatically yields high-quality 3D human mesh annotations. (ii) Data Quantity: We curate data from large licensed stock photo repositories, multiple multi-view capture datasets, and syntheticdata. We create a large scale of 7million images with high-quality annotation. (ii) Data Diversty: Our data is diversified using a VLM-based data engine that mines for in-the-wild challenging images and routes them for annotation. This ensures coverage of rare poses, diffiult viewpoints, and varied appearances, providing a more diverse dataset for supervision. Together, the data engine and ful-body HMR model enable 3DB to recover high-fidelity full-body human meshes from a single image. 3DB achieves state-of-the-art performance across both body and hand pose estimation. Extensive experiments demonstrate that 3DB consistently outperforms prior HMR methods on standard metrics, generalizes better to unseen datasets, and is preferred by users in a study of 7,800 participants, achieving a significant $5 : 1$ win rate in visual quality. To our knowledge, it is the first single model telves peornboyecalie ncparable peoanandspe models, while providing interactive control and strong robustness under challenging poses and in-the-wild scenarios.

# 2 Related Work

Human Mesh Models: The most widely used human mesh model is SMPL (26), which parameterizes human body into pose and shape. SMPL-X (34) goes further to include hands (MANO (40)) and faces (FLAME (21)). SMPL models intertwine the skeletal structure and soft-tissue mass within the shape space, which can limit interpretability (e.g., the parameters do not always map directly to bone lengths) and controllability. Alternatively, Momentum Human Rig (8), an enhancement of ATLAS (31), explicitly decouples the skeletal structure and body shape, and we adopt it as our representation of the human body. Human Mesh Recovery (HMR): Early HMR methods like HMR 2.0 (9) were body-only methods that predicted the body without articulated hands or feet (18; 22; 7). Instead, 3DB follows the more recent paradigm of full-body methods (2; 5; 41; 3; 51) that estimate body+hands+feet. There are also part-specific hand mesh recovery methods (35; 38) that only estimate the pose and shape of the hands, which usually have more accurate performance compared to full-body methods. In contrast, 3DB shows strong performance on both hand and full-body estimation.

![](images/2.jpg)  
Figure2 SAM 3D Body Model Architecture. We employ a promptable encoder-decoder architecture with a shared image encoder and separate decoders for body and hand pose estimation.

Promptable Inference: Promptable inference, popularized by the SAM family (17; 39), enables user or systemprovided prompts (such as 2D keypoints or masks) to guide model predictions. Similarly to (51), our approach supports various prompt types, including 2D keypoints and masks, and by integrating prompt tokens directly into the transformer architecture, enables user-guided mesh recovery. DataQualityandAotationPpelines:majorbottleneck inHMR is the qualityf trainig dataMany atasets rely on pseudo-ground-truth (pGT) meshes obtained from monocular fitting (18; 14), which often contain systematic errors in pose, shape, and camera parameters (33). Recent work (7; 50) highlights the impact of annotation noise on reported metrics and generalization.To address this, multi-view datasets (28; 16; 30) and syntheticdata have been used inour work to provide higher-fidelity supervision. Our method builds on these insights by employing a scalable data engine that mines challenging cases using vision-language models, and by leveraging a multi-stage annotation pipeline that combines dense keypoint detection, strong parametric priors, and robust optimization.

# 3 SAM 3D Body Model Architecture

Our goal is to recover 3D human meshes (i.e., MHR parameters) accurately, robustly and interactively from a single image. To this end, we design 3DB as a promptable encoderdecoder architecture (see Figure 2) with a rich set of prompt tokens. 3DB is designed to be interactive as it can accept 2D keypoints or masks, allowing users or downstream systems to guide inference.

# 3.1 Image Encoder

The human-cropped image $I$ is normalized and passed through a vision backbone to produce a dense feature map $F$ . An optional set of hand crops $I _ { \mathrm { h a n d } }$ can also be provided to obtain hand crop feature maps $F _ { \mathrm { h a n d } }$ .

$$
\begin{array} { r } { F = \mathrm { I m g E n c o d e r } ( I ) , \phantom { x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x } } \\ { F _ { \mathrm { h a n d } } = \mathrm { I m g E n c o d e r } ( I _ { \mathrm { h a n d } } ) . } \end{array}
$$

3DB considers two optional prompts: 2D keypoints and segmentation masks. Keypoint prompts are encoded by positional encodings summed with learned embeddings and are provided as additional tokens for the pose decoder. Mask prompts are embedded using convolutions and summed element-wise with the image embedding (17).

# 3.2 Decoder Tokens

3DB has two decoders: The body decoder outputs the full-body human rig and an optional hand decoder can provide enhanced hand pose results. The pose decoders take a set of query tokens as input to predict the parameters of MHR and camera parameters. There are four types of query tokens: MHR+camera, 2D keypoint prompt, auxiliary 2D/3D keypoint tokens and optional hand position tokens. MHR+Camera Token: The initial estimate of MHR and (optionally) camera parameters is embedded as a learnable token for MHR parameter estimation:

$$
\begin{array} { r l } & { T _ { \mathrm { p o s e } } = \mathrm { R i g E n c o d e r } ( E _ { \mathrm { i n i t } } ) \in \mathbb { R } ^ { 1 \times D } , } \\ & { E _ { \mathrm { i n i t } } \in \mathbb { R } ^ { d _ { \mathrm { i n i t } } } . } \end{array}
$$

2D Keypoint Prompt Tokens: If 2D keypoint prompts $K$ are provided (e.g., from a user or detector), they are encoded as:

$$
\begin{array} { r l } & { T _ { \mathrm { p r o m p t } } = \mathrm { P r o m p t E n c o d e r } ( K ) \in \mathbb { R } ^ { N \times D } , } \\ & { \qquad K \in \mathbb { R } ^ { N \times 3 } , } \end{array}
$$

where each keypoint is represented by $( x , y , \mathrm { l a b e l } )$ . Hand Position Tokens: The hand token, $T _ { \mathrm { h a n d } } \in \mathbb { R } ^ { 2 \times D }$ , isue  the body decder tlocate thehan posiions inside the human images. This set of tokens is optional, without which 3DB can still produce a full-body human rig because the output from body decoder already includes hands. AuilayKeyponTokensTourtherehanciteractiviy nd mode capacity weinclueearabletokes fo all 2D and 3D keypoints.

$$
\begin{array} { r } { T _ { \mathrm { k e y p o i n t 2 D } } \in \mathbb { R } ^ { J _ { 2 D } \times D } , } \\ { T _ { \mathrm { k e y p o i n t 3 D } } \in \mathbb { R } ^ { J _ { 3 D } \times D } . } \end{array}
$$

These tokens allow the model to reason about specific joints and support downstream tasks such as keypoint prediction or uncertainty estimation.

# 3.3 MHR Decoder

All tokens are concatenated to form the full set of queries:

$$
T = [ T _ { \mathrm { p o s e } } , T _ { \mathrm { p r o m p t } } , T _ { \mathrm { k e y p o i n t 2 D } } , T _ { \mathrm { k e y p o i n t 3 D } } , T _ { \mathrm { h a n d } } ]
$$

This fexible assembly enables the model to operate in both fully automatic and user-guided modes, adapting to the available prompts. The body decoder attends to both the query tokens $T$ , the full-body image features $F ^ { \prime }$ ,

$$
O = \mathrm { D e c o d e r } ( T , F ) \in \mathbb { R } ^ { ( 3 + N + J _ { 2 D } + J _ { 3 D } ) \times D } .
$$

Through cross-attention, the body decoder fuses prompt information with visual context, enabling robust and editable mesh recovery. Optionally, the hand decoder can take the same prompt information while attends to the hand crop features $F _ { \mathrm { h a n d } }$ to provide another output token $O _ { \mathrm { h a n d } }$ . The first output token of $O$ is passed through an MLP to regress the final mesh parameters: $\theta = \mathrm { M L P } ( O _ { 0 } ) \in$ $\mathbb { R } ^ { d _ { \mathrm { o u t } } }$ , where $\boldsymbol { \theta } = \{ { \bf P } , { \bf S } , { \bf C } , { \bf S } _ { k } \}$ are the predicted MHR parameters: pose, shape, camera pose and skeleton, respectively. Another set of outputs can be computed from $O _ { \mathrm { h a n d } }$ for a pair of MHR hands, which can be merged to the body output to improve the estimation of the hand.

# 4 Model Training and Inference

Model Training. 3DB is trained with a comprehensive multi-task loss terms, $\begin{array} { r } { \mathcal { L } _ { \mathrm { t r a i n } } = \sum _ { i } \lambda _ { i } \mathcal { L } _ { i } } \end{array}$ , where each $\mathcal { L } _ { i }$ is a task-specific loss targeting a specific prediction head or anatomical structure. $\lambda _ { i }$ are hyper-parameters s epiricaly.To stabilize traini certain loss terms (e.g. 3D keypoints) are introduce with a war-up schedule, gradually increasing their weights over the course of training. We also simulate an interactive setup (17; 46) for training by randomly sampling prompts in multiple rounds per sample. This multi-task, prompt-aware loss design provides strong supervision across al outputs. We describe the losses in details below. 2D/3D Keypoint Loss: We supervise 2D/3D joint locations using an $L _ { 1 }$ loss, incorporating learnable per-joint uncertainty to modulate the loss based on prediction confidence. For 3D body and hand keypoints, we normalize them with their respective pelvis and wrist locations before computing the loss. Hand keypoints are weighted according to annotation availability. 2D keypoints are supervised in the cropped image spaces, and we upweight the loss for the user-provided keypoint to encourage prompt consistency when keypoint prompts are available. Parameter Losses: MHR parameters (pose, shape) are supervised with $L _ { 2 }$ regression losses, and joint limit penalties are imposed to discourage anatomically implausible poses. Hand Detection Loss: 3DB can localize the hand position by a built-in hand detector. We apply GIoU loss and $L _ { 1 }$ loss to supervise the hand box regression. We also predict the uncertainty of hand boxes and turn off the hand decoder on hand-occluded samples during inference. Model Inference. For the model inference, we use the output from the body decoder as the default. However, if the hand detector head finds hands inside the input image, we can choose to merge the output from the hand decoder to body output to enhance the hand pose estimation quality. At this stage, we typically use the wrist and elbow keypoint estimation from the hand decoder and the body decoder separately as prompting keypoints to align and combine the predictions from the two decoders. Finally, the predicted local MHR parameters are merged to a full-body configuration following the kinematic tree of the mesh model.

# 5 Data Engine for Diversity

Obtaining highly accurate human mesh annotations paired with the images can be computationally costly. Instead, one common strategy is to annotate a large video collection and leveraging temporal constraints to gmorereliable pseudoanotatiosWhile  is possible toe alargenumbr traini mage fromvideos, the poses, appearance, imaging conditions, and background might be very similar. In order to increase the diversity of our training dataset, we implemented an automated data engine that selectively routes diffult images for annotation, enabling scalable and efficient dataset curation. At the core of our data engine is a Vision-Language Model (VLM) driven mining strategy. Rather than relying on simple heuristics or random sampling, we leverage VLMs to automatically generate and update mining rules that identiy high-valueimages orannotation. The VLM identifes imagesexhibiting challenging scenarios for pose estimation, including oclusion (where the human subject is partially hidden byobjects or other people, unusual poses (rare or complex body configurations such as acrobatics or dance), interaction (human-object or human-human activities like holding tools or group actions), extreme scale (subjects appearing at atypical distances from the camera), low visibility (poor lighting, motion blur, or partial visibility), and hand-body coordination (tight coupling of hand and body poses, as in sign language or sports). Mining rules are automatically update iteratively based on failure analysis of the current model, allowing the engine to adaptively focus on the most challenging o informative samples. Failure analysis is performed semi-manually, by evaluating 3DB on the current set of annotated images, visualizing the most challenging images using keypoint location error, and then manually annotating the image with a few words. These words and images are used to create text prompt for the VLM. New images selected by the VLM are then routed for manual annotation. By focusing annotation eforts on the most informative samples, our data engine enables effcient search through tens of millionsof images, whilemaximizing the value and diversity of each annotated image. By collecting a highly diverse dataset, it provides the basis on which to build a very robust HMR model that works on a wide range of in-the-wild images.

![](images/3.jpg)  
Fiur Let:GUI ofour annotation tool or annotating 2 keypoints.Right:Comparison f the dense (thi) and sparse (thick) keypoints for pseudo annotation.

![](images/4.jpg)  
Figure 4 Example of single-image MHR mesh fitting for ITW datasets. Source: SA-1B (17).

# 6 Data Annotation and Mesh Fitting

In addition to the robustness enabled by the data diversity derived from our data engine, the accuracy  our model depends heavily on the quality of our annotations. To this end, we designed a multi-stage annotation pipeline that produces accurate 3D mesh pseudo-ground truth from both in-the-wild single image datasets and a variety of multi-view datasets, using various combinations of manual 2D keypoint annotation, sparse and dense keypoint detection, geometric constraints, temporal constraints, strong parametric priors, and robust optimization methods.

# 6.1 Manual Annotation

Given a set of images selected by the data engine, we use a current version of 3DB to estimate initial 2D joint positions. Then, a team of trained annotators review and manually correct the estimated joint locations if needed, as shown in Figure 3(a). The annotators also assign a per-joint visibility label according to a strict rubric. Joints with substantial occlusion or other factors that would prevent accurate placement (e.g., 50% occlusion, motion blur) are marked as not visible.

# 6.2 Single-Image Mesh Fitting

For each image, we predict 595 dense 2D keypoints using a high-capacity keypoint detector that is conditioned on the sparse 2D keypoints obtained from the manual annotation step described above, as illustrated in Figure 3(b). Building upon prior dense keypoint detection framework that did not exploit cues other than pixels (33; 11; 6), our approach predicts accurate 2D dense keypoints from in-the-wild images (Figure 5b) by jointly leveraging image cues and sparse keypoint guidance. We then initialize MHR using a current version of 3DB's predictions for pose, shape, and camera intrinsics, which is used as an initialization for mesh optimization. MHR fitting is then performed via gradient-based refinement of the model parameters, minimizing a composite fitting loss $\begin{array} { r } { \mathcal { L } _ { \mathrm { f i t } } = \sum _ { j } \lambda _ { j } \mathcal { L } _ { j } } \end{array}$ , where each ${ \mathcal { L } } _ { j }$ is a task-specific loss including 2D keypoint loss, initialization-anchored regularization and priors. Hyper-parameters $\lambda _ { j }$ are set via cross-validation. We apply several loss terms and priors to make the ftting goal: 2D Keypoint Loss is the L2 distance between projected and detected dense 2D keypoints, to ensure minimal 2D reprojection error. Initialization-Anchored Regularization penalizes deviation from the initial prediction by applying L2 losses on both the Momentum Human Rig parameters and their coresponding 3D keypoints, thereby preventing model drift. Poseand Shape Prior enforces anatomical plausibility via a learned Gaussian Mixture prior and L2 regularization. Following the pipeline above, we derive the image to MHR fittings as training supervision as in Figure 4.

![](images/5.jpg)  
u5Exampl f MHR mesh fitting results.) Multi-view esh fttiSource:EgoExo4 (0). Scaase mesh fitting. Source: Re:Interhand (29).

# 6.3 Multi-View Mesh Fitting

Thoug single-view mesh fitting is effective or a large anddiverse set ofmages, the annotation quality tends to be lower fidelity due to the depth ambiguities and naturalocclusion. Therefore, we also exploit multi-view mesh ftting on suitable datasets. For multi-view video datasets, we further extend the pipeline to jointly fit mesh across allframes and camera views, leveraging both spatial and temporal cues. Synchronized 2D keypoints are extracted for each camera and frame, then triangulated to obtain sparse 3D keypoints.

The meshmode isnitializefromthesetriangulate points nameparameters an refinevsecondorder optimization-based update of the model parameters, minimizing a composite ftting loss, $\begin{array} { r } { \mathcal { L } _ { \mathrm { m u l t i } } = \sum _ { k } \lambda _ { k } \mathcal { L } _ { k } } \end{array}$ where each $\mathcal { L } _ { k }$ is a task-specific loss including the 2D keypoint loss and the regularization and priors as single-view mesh fitting, together with additional 3D keypoint loss and temporal smoothness: 3DKeypoint Loss is the L2 distance between mesh joints and triangulated 3D keypoints obtained from multi-view geometry, providing strong spatial supervision. Temporal Smoothness Loss encourages estimated pose parameters to temporally smooth, penalizing abrupt changes in motion and promoting realistic temporal dynamics. $\lambda _ { k }$ are set via cross-validation. Optimization alternates between updating camera parameters, shape, skeleton, and pose, with robust keypoint filtering (e.g., robust losses, RANSAC, smoothing). Body specific parameters (e.g., shape, skeleton parameters) are optimized jointly across frames. The mesh fitting happens on bodyfull-body data and hand data as shown in Figure 5.

# 7 Training Datasets

We train our model on a mix of single-view, multi-view, and synthetic datasets listed in Table 1, covering general body pose, hands, interactions, and "in-the-wild' conditions to ensure the quality, quantity and diversity of training data. Single-viewin-the-wild: We utilize datasets that captures people in unconstrained environments with diverse appearance, pose, and scene conditions. For this, we use AIChallenger (53), MS COCO (25), MPII (1), 3DPW (48), and a subset of SA-1B (17). Multi-view consistent: To incorporate geometric consistency for more reliable annotations, we use multi-view data from Ego-Exo4D (10), Harmony4D (16), EgoHumans (15), InterHand2.6M (30), DexYCB (4) and Goliath (28). Table 1 List of 3DB training datasets. $\star$ denotes the datasets providing samples to train the hand decoder.   

<table><tr><td>Dataset</td><td># Images/Frames</td><td># Subjects</td><td># Views</td></tr><tr><td>MPII human pose (1)</td><td>5K</td><td>5K+</td><td>1</td></tr><tr><td>MS COCO (25)</td><td>24K</td><td>24K+</td><td>1</td></tr><tr><td>3DPW (48)</td><td>17K</td><td>7</td><td>1</td></tr><tr><td>AIChallenger (53)</td><td>172K</td><td>172K+</td><td>1</td></tr><tr><td>SA-1B (17)</td><td>1.65M</td><td>1.65M+</td><td>1</td></tr><tr><td>Ego-Exo4D (10)</td><td>1.08M</td><td>740</td><td>4+</td></tr><tr><td>DexYCB (4)</td><td>291K</td><td>10</td><td>8</td></tr><tr><td>EgoHumans (15)</td><td>272K</td><td>50+</td><td>15</td></tr><tr><td>Harmony4D (16)</td><td>250K</td><td>24</td><td>20</td></tr><tr><td>InterHand (30)*</td><td>1.09M</td><td>27</td><td>66</td></tr><tr><td>Re:Interhand (29)*</td><td>1.50M</td><td>10</td><td>170</td></tr><tr><td>Goliath (28)*</td><td>966K</td><td>120+</td><td>500+</td></tr><tr><td>Synthetic*</td><td>1.63M</td><td></td><td></td></tr></table>

High-fidelity synthetic:We use a photorealistic synthetic extension of the Goliath dataset (28). It provides millions of frames with ground-truth MHR parameters across diverse identities, clothing, and contexts. Synthetic data ensures acurate supervision for human mesh recovery, complementing real-world datasets that prioritize diversity over quality. Hand datasets: These datasets (marked with $\star$ in Table 1), such as Re:Interhand (29), are used to train both the body and hand decoder. We provide wrist-truncated hand samples to train the hand decoder.

# 8 Evaluation

We follow prior HMR work and report standard pose and shape evaluation metrics: MPJPE (27), PA-MPJPE (55), PVE (20), and PCK (55). To evaluate on SMPL-based datasets, a MHR mesh is mapped to the SMPL mesh format.

# 8.1 Evaluating Performance on Common Datasets

We first evaluate 3DB on five standard benchmark datasets in Table 2, comparing with a wide variety of stateof-the-art (SoTA) mesh recovery methods. We present results with two variants of the model; 3DB-H leverages the commonly used ViT-H (632M) backbone, and 3DB-DINOv3 uses the recent DINOv3 (840M) (45) encoder. We use an of-the-shelf field-of-view (FOV) estimator (49) to provide camera intrinsics for model inference. 3DB outperforms all other single-image methods and is even competitive with video-based approaches that additionally leverage temporal information. In particular, our model achieves superior results in the EMDB and RICH datasets, which are out-of-domain (i.e., not included in the training set), indicating better generalization than previous SoTA methods. 3DB exceeds the second best model, NLF, on al datasets in terms of 3D metrics except for RICH which dataset NLF uses in training while our model does not. 3DB is also state-of-the-art on PCK for 2D evaluation on the COCO and LSPET datasets, demonstrating strong 2D alignment.

# 8.2 Evaluating Performance on New Datasets

Throughout our experiments, we found that mesh recovery models are particularly fragile in out-of-domain settings due to camera, appearance, and pose differences. To understand how methods perform on new, unseen data distributions, we additionally evaluate on five new datasets (38.6K images) in Table 3. The five new datasets include (1) Ego-Ex04D (10), (2) Harmony4D (16), (3) Goliath (28), (4) in-house synthetic data and (5) SA1B-Hard. Ego-Exo4D captures humans in diverse, skilled activities, divided into physical (EE4D-Phys) and procedural (EE4D-Proc) domains. Harmony4D focuses on close multi-human interaction in dynamic sports settings. Goliath offers diverse motions in a precise, studio environment. The synthetic dataset consists of single-human images with diverse camera angles and parameters. SA1B-Hard is a subset of 2.6K images extracted from SA1B using our data engine. Together, these five new datasets present a challenging new testbed for mesh recovery methods. Table2 Comparison on five common benchmarks.The best results are highlighted in bold, while the second-best reul dnResuvalaus publiy hecponot oeraiusRICH denoted by \*.   

<table><tr><td rowspan="2"></td><td rowspan="2">Models</td><td colspan="3">3DPW (14)</td><td colspan="3">EMDB (24)</td><td colspan="3">RICH (24)</td><td>COCO</td><td>LSPET</td></tr><tr><td>PA-MPJPE ↓</td><td>MPJPE ↓</td><td>PVE↓</td><td>PA-MPJPE ↓</td><td>MPJPE↓</td><td>PVE↓</td><td>PA-MPJPE↓</td><td>MPJPE ↓</td><td>PVE ↓</td><td>PCK@0.05 ↑</td><td>PCK@0.05 ↑</td></tr><tr><td></td><td>HMR2.0b (9)</td><td>54.3</td><td>81.3</td><td>93.1</td><td>79.2</td><td>118.5</td><td>140.6</td><td>48.1†</td><td>96.0†</td><td>110.9†</td><td>86.1</td><td>53.3</td></tr><tr><td>0</td><td>CameraHMR (33)</td><td>35.1</td><td>56.0</td><td>65.9</td><td>43.3</td><td>70.3</td><td>81.7</td><td>34.0</td><td>55.7</td><td>64.4</td><td>80.5†</td><td>49.1†</td></tr><tr><td></td><td>PromptHMR (51)</td><td>36.1</td><td>58.7</td><td>69.4</td><td>41.0</td><td>71.7</td><td>84.5</td><td>37.3</td><td>56.6</td><td>65.5</td><td>79.2†</td><td>55.6†</td></tr><tr><td></td><td>SMPLerX-H (3)</td><td>46.6†</td><td>76.7†</td><td>91.8†</td><td>64.5†</td><td>92.7†</td><td>112.0†</td><td>37.4†</td><td>62.5†</td><td>69.5†</td><td>—</td><td></td></tr><tr><td></td><td>NLF-L+ft* (43)</td><td>33.6</td><td>54.9</td><td>63.7</td><td>40.9</td><td>68.4</td><td>80.6</td><td>28.7†</td><td>51.0t</td><td>58.2t</td><td>74.9†</td><td>54.9†</td></tr><tr><td>0</td><td>WHAM (44)</td><td>35.9</td><td>57.8</td><td>68.7</td><td>50.4</td><td>79.7</td><td>94.4</td><td>−</td><td></td><td>—</td><td>−</td><td>−</td></tr><tr><td></td><td>TRAM (52)</td><td>35.6</td><td>59.3</td><td>69.6</td><td>45.7</td><td>74.4</td><td>86.6</td><td>−</td><td>−</td><td>−</td><td></td><td></td></tr><tr><td></td><td>GENMO (19)</td><td>34.6</td><td>53.9</td><td>65.8</td><td>42.5</td><td>73.0</td><td>84.8</td><td>39.1</td><td>66.8</td><td>75.4</td><td></td><td></td></tr><tr><td></td><td>3DB-H (Ours)</td><td>33.2</td><td>54.8</td><td>64.1</td><td>38.5</td><td>62.9</td><td>74.3</td><td>31.9</td><td>55.0</td><td>61.7</td><td>86.8</td><td>68.9</td></tr><tr><td></td><td>3DB-DINOv3 (Ours)</td><td>33.8</td><td>54.8</td><td>63.6</td><td>38.2</td><td>61.7</td><td>72.5</td><td>30.9</td><td>53.7</td><td>60.3</td><td>86.5</td><td>67.8</td></tr></table>

TableComparison n ive newbencmardatasets.The bes results are highlighte  bol whil the seconbe results are underlined. MPJPE is computed on 24 SMPL keypoints.

<table><tr><td>Models</td><td colspan="2">EE4D-Phy</td><td colspan="2">EE4D-Proc</td><td colspan="2">Harmony4D</td><td colspan="2">Goliath</td><td colspan="2">Synthetic</td><td>SA1B-Hard</td></tr><tr><td></td><td>PVE↓</td><td>MPJPE↓</td><td>PVE ↓</td><td>MPJPE ↓</td><td>PVE↓</td><td>MPJPE ↓</td><td>PVE ↓</td><td>MPJPE ↓</td><td>PVE ↓</td><td>MPJPE ↓</td><td>Avg-PCK ↑</td></tr><tr><td>CameraHMR (33)</td><td>71.1</td><td>58.8</td><td>70.3</td><td>60.2</td><td>84.6</td><td>70.8</td><td>66.7</td><td>54.5</td><td>102.8</td><td>87.2</td><td>63.0</td></tr><tr><td>PromptHMR (51)</td><td>74.6</td><td>63.4</td><td>72.0</td><td>62.6</td><td>91.9</td><td>78.0</td><td>67.2</td><td>56.5</td><td>92.7</td><td>80.7</td><td>59.0</td></tr><tr><td>NLF (43)</td><td>75.9</td><td>68.5</td><td>85.4</td><td>77.7</td><td>97.3</td><td>84.9</td><td>66.5</td><td>58.0</td><td>97.6</td><td>86.5</td><td>66.5</td></tr><tr><td>3DB-H Leave-one-out (Ours)</td><td>49.7</td><td>44.3</td><td>52.9</td><td>47.4</td><td>63.5</td><td>54.0</td><td>54.2</td><td>46.5</td><td>85.6</td><td>75.5</td><td>73.1</td></tr><tr><td>3DB-H Full dataset (Ours)</td><td>37.0</td><td>31.6</td><td>41.9</td><td>36.3</td><td>41.0</td><td>33.9</td><td>34.5</td><td>28.8</td><td>55.2</td><td>47.2</td><td>76.6</td></tr></table>

As it is diffcult to compare methods using the exact same training data and methodology due to prohibitive datausage licenses, unclear descriptions of training data, and lack of training code (CameraHMR, PromptHMR, and NLF are trained on 6, 9, and 48 datasets, respectively), we fairly test the generalization ability of 3DB by using a leave-one-out training procedure.This ensures a fair comparison with prior work which have also not seen these datasets. To serve as an in-domain, upper bound comparison, we also show the performance of 3DB when trained on the full dataset (i.e., training data is also sampled from these new datasets). For both the baselines and our model, we use ground truth camera intrinsics for model inference for all 3D datasets, except for SA1B-Hard which we used FOV estimated by MoGe-2 (49). We present the results in Table 3. Despite being trained on a large number of datasets, we find that prior work still struggle with these five domains, incurring a significant drop in performance. In contrast, our leave-one-out model shows strong generalization, owing to our more diverse data distribution and stronger training framework. Interestingly, we notice that existing methods constantly trade places for second across difert datasets, reectin sronatase-specbiassThiindicateshat each baselinverft toarow slice of the underlying data distribution. Table4Comparison n Freihand for hand pose estimation. Methods using Freihand for training are denoted by .   

<table><tr><td>Method</td><td>PA-MPVPE ↓</td><td>PA-MPJPE ↓</td><td>F@5 ↑</td><td>F@15 ↑</td></tr><tr><td>LookMa (11)</td><td>8.1</td><td>8.6</td><td>0.653</td><td>-</td></tr><tr><td>METRO (24)†</td><td>6.3</td><td>6.5</td><td>0.731</td><td>0.984</td></tr><tr><td>HaMeR (35)†</td><td>5.7</td><td>6.0</td><td>0.785</td><td>0.990</td></tr><tr><td>MaskHand (42)†</td><td>5.4</td><td>5.5</td><td>0.801</td><td>0.991</td></tr><tr><td>WiLoR (38)†</td><td>5.1</td><td>5.5</td><td>0.825</td><td>0.993</td></tr><tr><td>3DB-H (Ours)</td><td>6.3</td><td>5.5</td><td>0.735</td><td>0.988</td></tr><tr><td>3DB-DINOv3 (Ours)</td><td>6.2</td><td>5.5</td><td>0.737</td><td>0.988</td></tr></table>

Table 5 2D categorical performance analysis on the SA-1B Hard dataset.

<table><tr><td rowspan="2"></td><td colspan="2">CameraHMR (33)</td><td colspan="2">PromptHMR (51)</td><td colspan="2">3DB</td></tr><tr><td>APCK(body)</td><td>APCK(feet)</td><td>APCK(body)</td><td>APCK(feet)</td><td>APCK(body)</td><td>APCK(feet)</td></tr><tr><td>Body _shape - In-the-wild</td><td>87.64</td><td>78.56</td><td>85.73</td><td>77.87</td><td>90.76</td><td>92.12</td></tr><tr><td>Camera_view - Back or side view</td><td>59.69</td><td>46.64</td><td>61.92</td><td>47.74</td><td>76.27</td><td>66.81</td></tr><tr><td>Camera_view - Bottom-up view</td><td>55.18</td><td>34.84</td><td>46.56</td><td>29.25</td><td>69.62</td><td>55.35</td></tr><tr><td>Camera_view - Others</td><td>51.48</td><td>33.80</td><td>54.39</td><td>38.55</td><td>76.62</td><td>71.52</td></tr><tr><td>Camera _ view - Overhead view</td><td>55.08</td><td>39.46</td><td>43.65</td><td>24.63</td><td>73.33</td><td>66.94</td></tr><tr><td>Hand - Crossed or overlapped fingers</td><td>73.20</td><td>62.85</td><td>72.48</td><td>62.43</td><td>81.36</td><td>84.04</td></tr><tr><td>Hand - Holding objects</td><td>76.73</td><td>72.11</td><td>73.57</td><td>68.92</td><td>83.40</td><td>85.92</td></tr><tr><td>Hand - Self-occluded hands</td><td>73.22</td><td>58.06</td><td>72.43</td><td>56.19</td><td>80.07</td><td>80.82</td></tr><tr><td>Multi_people - Contact or interaction</td><td>63.23</td><td>51.65</td><td>61.77</td><td>47.60</td><td>74.81</td><td>69.92</td></tr><tr><td>Multi_people - Overlapped</td><td>53.11</td><td>41.88</td><td>57.17</td><td>41.43</td><td>70.82</td><td>64.71</td></tr><tr><td>Pose - Contortion or bending</td><td>47.08</td><td>32.78</td><td>42.61</td><td>20.98</td><td>65.20</td><td>53.04</td></tr><tr><td>Pose - Crossed legs</td><td>63.95</td><td>32.24</td><td>56.15</td><td>27.35</td><td>76.40</td><td>58.80</td></tr><tr><td>Pose - Inverted body</td><td>46.12</td><td>30.01</td><td>39.83</td><td>24.64</td><td>78.18</td><td>72.19</td></tr><tr><td>Pose - Leg or arm splits</td><td>57.51</td><td>31.43</td><td>54.76</td><td>33.11</td><td>83.69</td><td>72.49</td></tr><tr><td>Pose - Lotus pose</td><td>63.19</td><td>14.38</td><td>54.85</td><td>12.87</td><td>74.53</td><td>57.97</td></tr><tr><td>Pose - Lying down</td><td>51.29</td><td>35.88</td><td>44.59</td><td>26.88</td><td>71.35</td><td>66.53</td></tr><tr><td>Pose - Sitting on or riding</td><td>79.66</td><td>71.65</td><td>70.15</td><td>61.16</td><td>84.85</td><td>81.51</td></tr><tr><td>Pose - Sports or athletic activities</td><td>78.93</td><td>69.34</td><td>73.62</td><td>60.37</td><td>85.10</td><td>82.80</td></tr><tr><td>Pose - Squatting or crouching or kneeling</td><td>62.74</td><td>41.47</td><td>54.41</td><td>33.84</td><td>72.85</td><td>61.85</td></tr><tr><td>Visibility - Occlusion (foot cues)</td><td>62.93</td><td>26.83</td><td>58.00</td><td>30.81</td><td>75.43</td><td>54.74</td></tr><tr><td>Visibility - Occlusion (hand cues)</td><td>61.01</td><td>53.89</td><td>58.55</td><td>51.13</td><td>76.04</td><td>72.01</td></tr><tr><td>Visibility - Truncation (lower-body truncated)</td><td>39.27</td><td></td><td>46.50</td><td>-</td><td>61.95</td><td></td></tr><tr><td>Visibility - Truncation (others)</td><td>79.18</td><td>74.82</td><td>77.06</td><td>74.99</td><td>84.23</td><td>86.72</td></tr><tr><td>Visibility - Truncation (upper-body truncated)</td><td>62.37</td><td>54.90</td><td>56.01</td><td>49.28</td><td>64.49</td><td>70.99</td></tr></table>

# 8.3 Evaluating Hand Pose Estimation Performance

One significant characteristic of 3DB is its strong performance in estimating hand shape and pose. Previous full-body human pose estimation methods (3; 2; 23) revealed a notable gap in hand pose accuracy compared to hand-only pose estimation methods (35; 38). This performance gap arises from two main factors. First, hand-only methods can leverage large-scale datasets of hand poses, whereas full-body methods cannot utilize these datasets becausef the absence offulbodyimages and annotations.Second, afree-moving wrisallow hand pose models to more easily fit finger poses with 2D and 3D alignment, while for full-body methods, wrist rotation and position are highly constrained by the body's pose and position. Despite these challenges, 3DB demonstrates strong hand pose accuracy. 3DB benefits from the flexible model training design that incorporates both hand and body data and the hand decoder. Additionally, being promptable, 3DB provides a natural mechanism to align the wrists of the body prediction with those of the hands. We evaluate 3DB's hand estimation on the representative FreiHand (56) benchmark in Table 4. For fair comparison against hand-only models, we use the output from our hand decoder for evaluation. Despite not training on the Freihand dataset, which gives a strong in-domain boost, 3DB's hand pose estimation accuracy is already comparable to SoTA hand pose estimation methods that include Freihand alongside many other hand-centric datasets

# 8.4 Evaluating 2D Categorical Performance

To better understand the strengths and weaknesses of models on a variety of image types, we compare the performance across our 24 categories defined over SA1B-Hard (17). Our proposed evaluation set is designed to capture a broad spectrum of human appearance and activity in images, ensuring robust evaluation across real-world scenarios. It consists of 24 total categories, which are organized under several high-level groups: Body Shape, Camera View, Hand, Multi-person, Pose and Visibility. We use the PCK (Percentage of Correct Keypoints) metric for 17 body keypoints and 6 feet keypoints. Results are reported using Avg-PCK, which is PCK averaged over a range of thresholds (i.e. 0.01, 0.025, 0.05, 0.075, 0.1 of the human bounding box size). Results in Table 5 show that 3DB outperforms all baselines on all categories. Qualitative examples are given in Figure 6. Onenotable anc s or ateoriVisliy Truaio wherehemodelhows ifican vane than CameraHMR or PromptHMR. Essentially, 3DB has learned a much stronger pose prior when dealing with body truncation in images. Other rows with the large improvements are Pose - Inverted body and Pose - Leg or ar splits.We largelyattribute theseprovements to the increased distributionof hard poses elecd by the data engine. Table 6 3D categorical performance analysis.   

<table><tr><td rowspan="2"></td><td colspan="3">CameraHMR (33)</td><td colspan="3">PromptHMR (51)</td><td colspan="3">3DB</td></tr><tr><td>PVE</td><td>MPJPE</td><td>PA-MPJPE</td><td>PVE</td><td>MPJPE</td><td>PA-MPJPE</td><td>PVE</td><td>MPJPE</td><td>PA-MPJPE</td></tr><tr><td>aux:depth _ambiguous</td><td>126.25</td><td>102.25</td><td>81.33</td><td>109.58</td><td>91.77</td><td>69.24</td><td>64.38</td><td>52.72</td><td>39.85</td></tr><tr><td>aux:orient_ambiguous</td><td>84.26</td><td>71.77</td><td>45.07</td><td>83.79</td><td>72.93</td><td>46.17</td><td>42.35</td><td>36.64</td><td>25.16</td></tr><tr><td>aux:scale _ ambiguous</td><td>118.18</td><td>104.77</td><td>50.93</td><td>112.95</td><td>102.28</td><td>47.26</td><td>58.64</td><td>51.16</td><td>27.67</td></tr><tr><td>fov:medium</td><td>82.88</td><td>68.81</td><td>46.86</td><td>76.31</td><td>64.84</td><td>42.85</td><td>43.58</td><td>36.97</td><td>25.57</td></tr><tr><td>fov:narrow</td><td>82.15</td><td>69.82</td><td>49.73</td><td>90.41</td><td>77.95</td><td>53.49</td><td>52.14</td><td>43.89</td><td>36.18</td></tr><tr><td>fov:wide</td><td>71.55</td><td>60.05</td><td>38.66</td><td>74.98</td><td>64.55</td><td>42.87</td><td>37.97</td><td>33.06</td><td>22.44</td></tr><tr><td>interaction:close_interaction</td><td>107.59</td><td>90.95</td><td>57.62</td><td>115.19</td><td>98.12</td><td>64.87</td><td>54.23</td><td>44.98</td><td>29.76</td></tr><tr><td>interaction:mild_interaction</td><td>89.98</td><td>75.28</td><td>52.93</td><td>106.55</td><td>90.38</td><td>62.74</td><td>42.63</td><td>34.65</td><td>27.16</td></tr><tr><td>pose_2d:hard</td><td>117.91</td><td>107.74</td><td>77.16</td><td>117.73</td><td>110.64</td><td>79.16</td><td>62.93</td><td>57.50</td><td>45.58</td></tr><tr><td>pose_2d:very_hard</td><td>150.20</td><td>140.61</td><td>92.66</td><td>150.15</td><td>145.07</td><td>95.40</td><td>62.22</td><td>56.84</td><td>42.39</td></tr><tr><td>pose−3d:hard</td><td>133.89</td><td>121.11</td><td>84.21</td><td>129.30</td><td>118.59</td><td>81.82</td><td>71.42</td><td>63.68</td><td>49.10</td></tr><tr><td>pose_3d:very_hard</td><td>213.66</td><td>206.34</td><td>143.23</td><td>186.35</td><td>179.46</td><td>129.51</td><td>114.20</td><td>110.62</td><td>86.43</td></tr><tr><td>pose_prior:average_pose</td><td>68.52</td><td>56.70</td><td>37.22</td><td>70.32</td><td>59.73</td><td>39.42</td><td>36.06</td><td>30.95</td><td>21.35</td></tr><tr><td>pose_prior:easy_pose</td><td>57.83</td><td>47.31</td><td>29.92</td><td>62.85</td><td>53.58</td><td>32.80</td><td>29.53</td><td>24.66</td><td>17.20</td></tr><tr><td>pose_prior:hard_pose</td><td>94.64</td><td>80.04</td><td>54.53</td><td>88.12</td><td>76.19</td><td>51.15</td><td>51.65</td><td>44.24</td><td>31.09</td></tr><tr><td>shape:average_bmi</td><td>70.35</td><td>58.07</td><td>38.08</td><td>71.01</td><td>60.25</td><td>39.90</td><td>36.58</td><td>31.41</td><td>21.31</td></tr><tr><td>shape:high_bmi</td><td>84.52</td><td>69.96</td><td>47.55</td><td>79.49</td><td>67.83</td><td>43.04</td><td>43.33</td><td>36.49</td><td>22.45</td></tr><tr><td>shape:low_bmi</td><td>80.93</td><td>65.70</td><td>42.71</td><td>69.92</td><td>58.76</td><td>37.30</td><td>38.74</td><td>32.73</td><td>21.82</td></tr><tr><td>shape:very_high_bmi</td><td>87.18</td><td>72.91</td><td>47.54</td><td>81.17</td><td>69.05</td><td>44.03</td><td>48.51</td><td>41.11</td><td>24.80</td></tr><tr><td>shape:very_low_bmi</td><td>108.16</td><td>91.25</td><td>47.26</td><td>94.16</td><td>81.12</td><td>38.64</td><td>51.76</td><td>45.69</td><td>22.97</td></tr><tr><td>truncation:left_body</td><td>135.30</td><td>113.17</td><td>87.98</td><td>127.53</td><td>110.67</td><td>91.33</td><td>91.28</td><td>76.46</td><td>62.23</td></tr><tr><td>truncation:lower _body</td><td>127.81</td><td>97.84</td><td>75.82</td><td>151.52</td><td>118.65</td><td>83.79</td><td>92.87</td><td>67.10</td><td>60.77</td></tr><tr><td>truncation:right _body</td><td>110.28</td><td>91.58</td><td>71.17</td><td>115.71</td><td>98.43</td><td>72.15</td><td>75.04</td><td>62.84</td><td>50.62</td></tr><tr><td>truncation:severe</td><td>230.51</td><td>213.64</td><td>124.01</td><td>186.57</td><td>168.22</td><td>122.70</td><td>126.53</td><td>113.66</td><td>88.42</td></tr><tr><td>truncation:upper _body</td><td>85.59</td><td>79.68</td><td>56.36</td><td>86.06</td><td>80.88</td><td>56.94</td><td>50.83</td><td>48.79</td><td>38.39</td></tr><tr><td>viewpoint:average _view</td><td>75.61</td><td>62.69</td><td>41.90</td><td>74.17</td><td>62.80</td><td>41.81</td><td>41.25</td><td>35.22</td><td>24.41</td></tr><tr><td>viewpoint:bottomup_view</td><td>89.83</td><td>72.25</td><td>53.00</td><td>95.46</td><td>78.87</td><td>55.57</td><td>56.50</td><td>47.07</td><td>34.03</td></tr><tr><td>viewpoint:topdown_view</td><td>101.69</td><td>91.13</td><td>59.15</td><td>104.29</td><td>97.92</td><td>63.39</td><td>42.84</td><td>38.78</td><td>27.90</td></tr></table>

# 8.5 Evaluating 3D Categorical Performance

Categorical 3D analysis using existing single view datasets is challenging as the underlying pseudo ground truth are low-fidelity approximations of the real geometry. In order to perform a more detailed categorical analysis of HMR methods, we constructed an evaluation dataset using a mix of synthetic and real data from multi-view datasets with high camera counts (more than 100 cameras). To comprehensively evaluate 3D human mesh reconstruction performance for HMR, we define a set of 34 distinccategorbasnnterpretable cneansubjetributes, sulusion,truation, v, pose diffculty, shape, and interaction. Unlike the manual classification used for 2D categories, these 3D categories are automatically generated using rule-based criteria applied to metadata and geometric cues. This systematic approach enables consistent, scalable, and objective analysis of model performance across diverse real-world conditions. Based on results from Table 6, 3DB demonstrates superior performance in challenging scenarios. Particularly within the very hard pose categories, 3DB consistently outperforms both CameraHMR and PromptHMR in the pose_3d:very_hard category and in pose_2d:very_hard. These results indicate that 3DB possesses inherent strengths in accurately estimating poses under the most challenging conditions. Additionally, 3DB exhibits a significant advantage in handling the truncation:severe scenario in comparison to CameraHMR and achieves better performance in the viewpoint:topdown_ view category in comparison to PromptHMR.

# 8.6 Qualitative Results

In addition to quantitative gains, our model shows clear qualitative improvements over baselines. Figure 6 compares SAM 3D Body to six state-of-the-art methods on the SA1B-Hard dataset, highlighting challenging cases with complex poses, shapes, and oclusions. As shown, SAM 3D Body consistently achieves more accurate body pose and shape recovery, especially for fine details like limbs and hands. The 2D overlays in Fure urtherllustrate bettealment wi pu mage mstratin therbusne uapp even under difficult conditions. When we focus on hand-crop images where the human body is invisible or truncated out of images, we demonstrate the effectiveness of model as in Figure7. Here, we only visualize the mesh output by the hand decoder for simplicity and clearness.

![](images/6.jpg)  
Figure 6 Qualitative comparison of 3DB against state-of-the-art HMR methods. Source: SA-1B (17).

![](images/7.jpg)  
Figure 7 Qualitative results of hand estimation using the hand decoder of 3DB. Source: Freihand (56).

# 8.7 Human Preference Study

We conducted a large-scale user preference study to evaluate the perceptual quality of human reconstructions produced by 3DB compared with existing approaches on the SA1B-Hard dataset. While quantitative metrics capture geometric and numeric accuracy, they do not always align with the human perception accuracy.

We designed six independent pairwise comparison studies, each comparing 3DB against one baseline method: HMR2.0b (9), CameraHMR (33), NLF (43), PromptHMR (51), SMPLer-X (3), and SMPLest-X (54). The study encompassed 7, 800 unique participants ( $1 , 3 0 0$ unique per comparison) resulting in over $2 0 , 0 0 0$ total responses. Each participant was presented with a vido stimul. The lft andright sides of thevideo displayed reconstructions from the two methods, and a video transition effect as used to fade-in the reconstruction result over the image. Participants were instructed to choose which 3D reconstruction better matched the original image by answering: "Which 3D model of the person better matches the original image, left or right?". We quantify results using win rate and vote share. Win rate is the percentage of stimuli for which 3DB received more votes than the baseline. As summarized in Figure 8, 3DB consistently outperforms all baselines. Focusing on the strongest baseline, NLF, 3DB achieves a win rate of $8 3 . 8 \%$ .

![](images/8.jpg)  
Figure 8 Comparison of 3DB win rate against baselines for human preference study. Win rate ( $\%$ ) and number of wins out of 80.

# 9 Conclusion

We have presented 3DB, a robust HMR model for body and hands. Our approach leverages the Momentum Human Rig parametric body model, employs a flexible encoder-decoder architecture, and supports optional prompts such as 2Dkeypoints or masks to guide inference.A central advance of our work s in the supervision pipeline.Instead f relying on nois monocular pseudo-ground-truth, we leverage multi-view capturesystems, synthetic sources, and a scalable data engine that actively mines and annotates challenging samples. This strategy yields cleaner and more diverseraining signals, supportingeneralization beyond curate benchmarks. At the same time, 3DB employs a separate hand decoder to enhance the hand pose estimation with hand crops as input which makes it comparable to SoTA hand pose estimation methods.

# Acknowledgements

We gratefully acknowledge the following individuals for their contributions and support:Vivian Lee, Gorge Orlin, Nikhila Ravi, Andrew Westbury, Jyun-Ting Song, Zejia Weng, Xizi Zhang, Yuting Ye, Federica Bogo, Ronald Mallet, Ahmed Osman, Rawal Khirodkar, Javier Romero, Carsten Stoll, Shunsuke Saito, Jean-Charles Bazin, Sofien Bouaziz, Yuan Dong, Su Zhaoen, Alexander Richard, Michael Zollhoefer, Roman Radle, Sasha Mitts, Michelle Chan, Yael Yungster, Azita Shokrpour, Helen Klein, Mallika Malhotra, Ia Cheng, Eva Galper.

References   
[1] Mykhaylo Andriluka, Leonid Pishchulin, Peter Gehler, and Bernt Schiele. 2D human pose estimation: New benchmark and state of the art analysis. In Computer Vision and Pattern Recognition (CVPR), 2014.   
[2] Fabien Baradel, Matthieu Armando, Thomas Lucas, Romain Brégier, Philippe Weinzaepfel, and Grégory Rogez. Multi-HMR:Multi-person whole-body human mesh recovery in a single shot. In European Conference n Computer Vision (ECCV), 2024.   
[3] Zhongang Cai, Wanqi Yin, Ailing Zeng, Chen Wei, Qingping Sun, Wang Yanjun, Hui En Pang, Haiyi Mei, Mingyuan Zhang, Lei Zhang, Chen Change Loy, Lei Yang, and Ziwei Liu. SMPLer-X: Scaling up expressive human pose and shape estimation. In Advances in Neural Information Processing Systems (NeurIPS), 2023.   
[4] Yu-Wei Chao, Wei-Cheng Yang, Yu Xiang, et al. DexYCB: A benchmark for capturing hand grasping of objects. In Computer Vision and Pattern Recognition (CVPR), 2021.   
[5] Vasileios Choutas, Georgios Pavlakos, Timo Bolkart, Dimitrios Tzionas, and Michael J. Black. Monocular expressive body regression through body-driven attention (ExPose). In European Conference on Computer Vision (ECCV), 2020.   
[6] Hanz Cuevas-Velasquez, Anastasios Yiannakidis, Soyong Shin, Giorgio Becherini, Markus Höschle, Joachim Tesch, Taylor Obersat, Tsvetelina Alexiadis, and Michael J. Black. Mamma: Markerless automatic multi-person motion action capture, 2025.   
[7] Sai Kumar Dwivedi, Yu Sun, Priyanka Patel, Yao Feng, and Michael J. Black. TokenHMR: Advancing human mesh recovery with a tokenized pose representation. In Computer Vision and Pattern Recognition (CVPR), 2024.   
[8] Aaron Ferguson, Ahmed A. A. Osman, Berta Bescos, Carsten Stoll, Chris Twigg, Christoph Lassner, David Otte, Eric Vignola, Federica Bogo, Igor Santesteban, Javier Romero, Jenna Zarate, Jeongseok Lee, Jinhyung Park, Jinlong Yang, John Doublestein, Kishore Venkateshan, Kris Kitani, Ladislav Kavan, Marco Dal Farra, Matthew Hu, Matthew Cioffi, Michael Fabris, Michael Ranieri, Mohammad Modarres, Petr Kadlecek, Rinat Abdrashitov, Romain Prévost, Roman Rajbhand ari, Ronald Mallet, Russel Pearsall, Sand y Kao, Sanjeev Kumar, Scot Parrish, Te-Li Wang, Tony Tung, Yuan Dong, Yuhua Chen, Yuanlu Xu, Yuting Ye, and Zhongshi Jiang. Mhr: Momentum human rig. arXiv Preprint, 2025. arXiv preprint; identifier to be added.   
[9] Shubham Goel, Georgios Pavlakos, Jathushan Rajasegaran, Angjoo Kanazawa, and Jitendra Malik. Humans in 4D: Reconstructing and tracking humans with transformers. In International Conference on Computer Vision (ICCV), 2023. Includes HMR 2.0.   
[0] risten Grauma al. Ego-Ex4D: Understandig skllehumctivy omfrsnd third-person perspeive. In Computer Vision and Pattern Recognition (CVPR), 2024.   
[11] CharlieHewitt, Fatemeh Saleh, Sadeg Aliakbarian, Lohit Petikam, Shidh Rezaeiar, Louis Florentin, Zafiah Hosenie, Thomas J Cashman, Julien Valentin, Darren Cosker, and Tadas Baltruaitis. Look ma, no markers: holistic performance capture without the hassle. ACM Transactions on Graphics (TOG), 43(6), 2024.   
[12] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian Sminhisescu. Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2014.   
[13] Hanbyul Joo, Tomas Simon, Xulong Li, Hao Liu, Lei Tan, Lin Gui, Sean Banerjee, Timothy Scot Godisart, Bart Nabbe, Iain Matthews, Takeo Kanade, Shohei Nobuhara, and Yaser Sheikh. Panoptic studio: A massively multiview ystem or socialinteraction capture.IEEE Transactionson PatternAnalysis and Machine Intelgence, 2017.   
[] AngKanzaw, Michael J.Black, Davi W Jacobs, and Jitenda Malik.End-to-enrecovey u hape and pose. In Computer Vision and Pattern Recognition (CVPR), pages 71227131, 2018.   
[15] Rawal Khirodkar, Aayush Bansal, Lingni Ma, Richard Newcombe, Minh Vo, and Kris Kitani. EgoHumans: An egocentric 3D multi-human benchmark. In International Conference on Computer Vision (ICCV), pages 40154026, 2023.   
[16] Rawal Khirokar, Jyun-Ting Song, Jinkun Cao, Zhengyi Luo, and Kris Kitani. Harmony4D:A video datase for in-the-wild close human interactions. NeurIPS Datasets and Benchmarks, 2024.   
[7] AlexanrKirilov,EricMintu, NikilRavi, Hanzi Mao, ChloRolland, LurGustaon, Tte Xio, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dolár, and Ross Girshick. Segment anything. In Interational Conference on Computer Vision (ICCV), pages 40154026, October 2023.   
[18] Nikos Kolotouros, Georgios Pavlakos, Michel Black,and Kostas Daniilidis.Learnigt reconsruct 3hun pose and shape via model-ftting in the loop. In Proceedings of the IEEE/CVF international conference on computer vision, pages 22522261, 2019.   
[9] Jiefeng Li Jinkuao, Haoi Zhang, Davis Rempe, Jan Kautz, Umar Ibal, and eYuan.Geno: ealit model for human motion. In International Conference on Computer Vision (ICCV), 2025.   
[0 Jieng Li, Cha Xu, Zhic Chen, Syn Bian, Lixin Yang, andCew Lu. Hybrik:hybrid analyia-ural inverse kinematics solution for 3d human pose and shape estimation. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 33833393, 2021.   
[1] Tianye Li, Timo Bolkart, Michael J. Black, Hao Li, and Javier Romer. Learni amodel of facial shape and expression from 4D scans (FLAME). In ACM Transactions on Graphics (TOG), SIGGRAPH Asia, 2017.   
[Zhi J Lu Zhe ZhnS Xu, ndYoln.arryoao in ful frames into human pose and shape estimation. In European Conference on Computer Vision, pages 590606. Springer, 2022.   
[23] Jing Lin, Ailing Zeng, Haoqian Wang, Lei Zhang, and Yu Li. One-stage 3d whole-body mesh recovery with component aware transformer. In Computer Vision and Pattern Recognition (CVPR), pages 2115921168, 2023.   
[24] Kevin Lin, Ljuan Wang, and Zicheng Lu. End-to-nd human pose and mesh reconstruction with transormers. In Computer Vision and Pattern Recognition (CVPR), pages 19541963, 2021.   
[25] Tung-Yi Lin, Michel Maire, Serge Belongie, Luboir Bourdev, Ross Girshic, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollár. Microsoft COCO: Common objects in context. In European Conference on Computer Vision (ECCV), 2014.   
[26] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Mol, and Michael J. Black. SMPL: A skinned multi-person linear model. In ACM Transactions on Graphics (TOG), SIGGRAPH Asia, pages 248:1248:16, 2015.   
[JulMarRaya H JavRo an Jame Litip eecivbsee pose estimation. In Proceedings of the IEEE international conference on computer vision, pages 26402649, 2017.   
[28] Meta. Goliath dataset, 2025. Partial Release.   
[29] Gyeongsik Moon, Shunsuke Saito, Weipeng Xu, Rohan Joshi, JuliaBufalini, Harley Bellan, Nicholas Matthew Rosen, Jesse Richardson, Mallorie Mize, Philippe De Bree, et al. A Dataset of Relighted 3D Interacting Hands. In NeurIPS 2023 Datasets and Benchmarks Track, 2023.   
[30] Gyeongsik Moon, Shoou-i Yu, He Wen, Takaaki Shiratori, and Kyoung Mu Lee. Interhand2.6M: A dataset and baseline for 3D interacting hand pose estimation from a single RGB image. In European Conference on Computer Vision (ECCV), 2020.   
[31] Jinhyung Park, Javier Romero, Shunsuke Saito, Fabian Prada, Takaaki Shiratori, Yichen Xu, Federica Bogo, Shoou-I Yu, Kris Kitani, and Rawal Khirodkar. Atlas: Decoupling skeletal and shape parameters for expressive parametric human modeling. In International Conference on Computer Vision (ICCV), pages 65086518, 2025.   
[32use W Rc  JieMal. internet videos. arXiv preprint arXiv:2211.13225, 2022.   
[33] Priyanka Patel and Michael J Black. Camerahmr: Aligning people with perspective. In 2025 International Conference on 3D Vision (3DV), pages 15621571. IEEE, 2025.   
[34] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, and Michael J. Black. Expressive body capture: 3D hands, face, and body from a single image (SMPL-X). In Computer Vision and Pattern Recognition (CVPR), 2019.   
[35] Georgios Pavlakos, Dandan Shan, Iija Radosavovic, Angjoo Kanazawa, David Fouhey, and Jitendra Malik. Reconstructing hands in 3D with transformers. In Computer Vision and Pattern Recognition (CVPR), 2024.   
[ r Syog hi  Gou Sar Beri an Hall. usie data via dynamic optimization of a biomechanical model. Journal of biomechanics, 155:111617, 2023.   
[7 X in eg, A Kanazaa, Jten Mali, Prbbe, and Sergey Levie v: Reinorment lar of physical skills from videos. ACM Transactions On Graphics (TOG), 37(6):114, 2018.   
[38] Rolandos Alexandros Potamias, Jinglei Zhang, Jiankang Deng, and Stefanos Zafeiriou. Wilor: End-to-end 3d hand localization and reconstruction in-the-wild. In Computer Vision and Pattern Recognition (CVPR), pages 1224212254, 2025.   
[39] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr Dolár, and Christoph Feichtenhofer. SAM 2: Segment anything in images and videos. In International Conference on Learning Representations (ICLR), 2025. Oral.   
[40] Javier Romero, Dimitrios Tzionas, and Michael J. Black. Modeling and capturing hands and bodies together (MANO). In ACM Transactions on Graphics (TOG), SIGGRAPH Asia, 2017.   
[41] Yu Rong, TakaakiShiratori, and Hanbyul Joo. FrankMocap: A monocular 3D whole-body posestimatin ystem via regression and integration. In International Conference on Computer Vision (ICCV) Workshops, 2021.   
[42] Muhammad Usama Saleem, Ekkasit Pinyoanuntapong, Mayur Jagdishbhai Patel, Hongfei Xue, Ahmed Helmy, Srijan Das, and Pu Wang. Maskhand: Generative masked modeling for robust hand mesh reconstruction in the wild. In Computer Vision and Pattern Recognition (CVPR), pages 83728383, 2025.   
[43] ItSnd  Ger ons-Mol Neualcalir el oru 3 psen hapetn. In Advances in Neural Information Processing Systems (NeurIPS), 2024.   
[] Syong Shin, Juyong Kim, Eni Halila, and Michael J. Black. WHAM: Reconstructing worldgroundedhumns with accurate 3D motion. In Computer Vision and Pattern Recognition (CVPR), pages 20702080, June 2024.   
[45] Oriane Siéoni, Huy V Vo, Maximilan Seitzer, Federico Baldassarre, Maxie Oquab, Cijo Jose, Vasil Khaliov, Marc Szafraniec, Seungeun Yi, Michaël Ramamonjisoa, et al. Dinov3. arXiv preprint arXiv:2508.10104, 2025.   
[46] Konstantin Sfiuk, Iya A Petrov, and Anton Konushin.Reviving iterative trainig with mask guidncefor interactive segmentation. In 2022 IEEE international conference on image processing (ICIP), pages 31413145. IEEE, 2022.   
[7] Vasils Vasiopoulos, Gorgi avlakos, Sean Bowan, ieoCaporale, Kosts anilis, George appas, and Danil E Koditschek. Reactive semantic planning in unexplored semantic environments using deep perceptual feedback. IEEE Robotics and Automation Letters, 5(3):44554462, 2020.   
[48] Tmo von Marcar, Gerar ons-MoMicha J. Black,  l. Recoveg accrate 3D hun pos  the wild using IMUs and a moving camera. In European Conference on Computer Vision (ECCV), 2018.   
[49] Ruicheng Wang, Sicheng Xu, Yue Dong, Yu Deng, Jianfeng Xiang, Zelong Lv, Guangzhong Sun, Xin Tong, and Jiolong Yang. Moge-2: Accurate monocular geometry with metric scale and sharp details. arXiv preprint arXiv:2507.02546, 2025.   
[50] Shenze Wang Jiefeng Li, Tianye Li, Ye Yuan, Henry Fuchs, KokiNagano, Shalini De Mello, and Michael tengel. BLADE: Single-view body mesh estimation through accurate depth estimation. In Computer Vision and Pattern Recognition (CVPR), pages 2199122000, 2025.   
[51] Yufu Wang, Yu Sun, Priyana Patel, Kostas Danilis, Michael J. Black, and Muham Kocabas. romptHMR: Promptable human mesh recovery. In Computer Vision and Pattern Recognition (CVPR), 2025.   
[2 Wag, Z Wag Lg Li,  Kosas Danis.TRAM: Gobal ajean mo 3D us from in-the-wild videos. In European Conference on Computer Vision (ECCV), 2024.   
[53] Jiahong Wu, He Zheng, Bo Zhao, Yixin Li, Baoming Yan, Rui Liang, Wenjia Wang, Shipei Zhou, Guosen Lin, Yanw Fu,t al. Large-scale datasets or going deeper inimageunderstanding. In Interational Conferenc Multimedia and Expo (ICME), pages 14801485. IEEE, 2019.   
[54] Wanqi Yin, Zhongang Cai, Ruisi Wang, Ailing Zng, Chen Wei, Qingping un, Haiyi Mei, Yanju Wang, Hui En Pang, Mingyuan Zhang, Lei Zhang, Chen Change Loy, Atsushi Yamashita, Lei Yang, and Ziwei Liu. Smplest-x: Ultimate scaling for expressive human pose and shape estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 117, 2025.   
[5] Jianeg Zhang Xuechengi, nd Jishi Feng.Inference stagotiizaion or cocnaro 3hu pose estimation. Advances in neural information processing systems (NeurIPS), 33:24082419, 2020.