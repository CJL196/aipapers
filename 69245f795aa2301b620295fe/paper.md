# Semi-Supervised Subspace Clustering via Tensor Low-Rank Representation

Yuheng Jia, Guanxing Lu, Hui Liu, Junhui Hou, Senior Member, IEEE

Abstract—In this letter, we propose a novel semi-supervised subspace clustering method, which is able to simultaneously augment the initial supervisory information and construct a discriminative affinity matrix. By representing the limited amount of supervisory information as a pairwise constraint matrix, we observe that the ideal affinity matrix for clustering shares the same low-rank structure as the ideal pairwise constraint matrix. Thus, we stack the two matrices into a 3-D tensor, where a global low-rank constraint is imposed to promote the affinity matrix construction and augment the initial pairwise constraints synchronously. Besides, we use the local geometry structure of input samples to complement the global low-rank prior to achieve better affinity matrix learning. The proposed model is formulated as a Laplacian graph regularized convex low-rank tensor representation problem, which is further solved with an alternative iterative algorithm. In addition, we propose to refine the affinity matrix with the augmented pairwise constraints. Comprehensive experimental results on eight commonly-used benchmark datasets demonstrate the superiority of our method over state-of-the-art methods. The code is publicly available at https://github.com/GuanxingLu/Subspace-Clustering. Index Terms—tensor low-rank representation, semi-supervised learning, subspace clustering, pairwise constraints.

# I. INTRODUCTION

High-dimensional data are ubiquitously in many areas like image processing, DNA microarray technology, etc. The highdimensional data can often be well approximated by a set of linear subspaces, but the subspace membership of a certain sample is unknown [1], [2]. Subspace clustering aims to divide the data samples into different subspaces, which is an important tool to model the high-dimensional data. The state-of-the-art subspace clustering methods [3], [4] are based on self-expressiveness, which represent high-dimensional data by the linear combination of itself, and enforce a subspacepreserving prior on the self-representation matrix. The representation coefficients capture the global geometric relationships of samples and can act as an affinity matrix, and the subspace segmentation can be obtained by applying spectral This work was supported in part by the National Natural Science Foundation of China under Grant 62106044, in part by the Natural Science Foundation of Jiangsu Province under Grant BK20210221, in part by the Hong Kong University Grants Committee under Grant UGC/FDS11/E02/22, in part by the ZhiShan Youth Scholar Program from Southeast University 2242022R40015, and in part sponsored by CCF-DiDi GAIA Collaborative Research Funds for Young Scholars. Corresponding author: Hui Liu. Y. Jia is with the School of Computer Science and Engineering, Southeast University, Nanjing 210096, China; G. Lu is with the Chien-Shiung Wu College, Southeast University, Nanjing 211102, China; H. Liu is with the School of Computing Information Sciences, Caritas Institute of Higher Education, Hong Kong; J. Hou is with the Department of Computer Science, City University of Hong Kong, Kowloon, Hong Kong (e-mail: yhjia@seu.edu.cn; guanxing@seu.edu.cn; hliu99-c@my.cityu.edu.hk; jh.hou $@$ cityu.edu.hk).

![](images/1.jpg)  
Fig. 1: Illustration of the proposed method, which adaptively learns the affinity and enhances the pairwise constraints simultaneously by using their identical global low-rank structure.

clustering on the generated affinity matrix. The most wellknown subspace clustering methods include sparse subspace clustering [3] and low-rank representation [4].

In many real-world applications, some supervisory information is available, e.g., the label information of a dataset, and the relationships between two samples. Generally, those supervisory information can be represented by two kinds of pairwise constraints, i.e., must-link constraints and cannot-link constraints indicating whether two samples belong to the same category or not. As the supervisory information is widespread and provides a discriminative description of the samples, many semi-supervised subspace clustering methods [5][13] were proposed to incorporate them. Based on the type of the supervisory information, we roughly divide these methods into three classes. The first kind of methods include the must-link constraints. For example, [5], [6] incorporated the must-links as hard constraints, which restricted the samples with a mustlink to have exactly the same representation. The second kind of methods integrate the cannot-link constraints. For instance, [7] required the affinity relationship of two samples with a cannot-link to be O. Liu et al. [8] first enhanced the initial cannot-links by a graph Laplacian term, and then inhibited the affinity of two samples with a cannot-link. The third kind of methods can incorporate both the must-links and cannotlinks, which generally assume two samples with a must-link should have a higher affinity, while those with a cannot-link should have a lower affinity [9][13]. However, the abovementioned semi-supervised subspace clustering methods exploit the supervisory information from a local perspective, but overlook the global structure of the pairwise constraints, which is also important to semi-supervised affinity matrix learning. In other words, the previous methods under-use the supervisory information to some extent.

To this end, we propose a new semi-supervised subspace clustering method shown in Fig. 1, which explores the supervisory information from a global manner. Specifically, in the ideal case, the pairwise constraint matrix is low-rank, as if all the pairwise relationships of samples are available, we could encode the pairwise constraint matrix as a binary low-rank matrix. Meanwhile, the ideal affinity matrix is also low-rank, as a sample should only be represented by the samples from the same class. More importantly, they share an identical low-rank structure. Based on such an observation, we stack them into a 3-dimensional tensor, and regulate a global low-rank constraint to the formed tensor. By seeking the tensor low-rank representation, we can refine the affinity matrix with the available pairwise constraints, and at the same time, augment the initial pairwise constraints with the learned affinity matrix. Besides, we encode the local geometry structure of the data samples to complement the global lowrank prior. The proposed model is formulated as a convex optimization model, which can be solved efficiently. Finally, we use the augmented pairwise constraint matrix to further refine the affinity matrix. Extensive experiments on 8 datasets w.r.t. 2 metrics demonstrate that our method outperforms the state-of-the-art semi-supervised subspace clustering methods to a large extent.

# II. PRELIMINARY

In this letter, we denote tensors by boldface Euler script letters, e.g., $\mathcal { A }$ , matrices by boldface capital letters, e.g., A, vectors by boldface lowercase letters, e.g., a, and scalars by lowercase letters, e.g., ( $\boldsymbol { \imath } . \parallel \cdot \parallel _ { 2 , 1 } , \parallel \cdot \parallel _ { \infty } , \parallel \cdot \parallel _ { F }$ and $\| \cdot \| _ { * }$ are the $\ell _ { 2 , 1 }$ norm, the infinity norm, and the Frobenius norm, and the nuclear norm (i.e., the sum of the singular values) of a matrix. ${ \bf X } = [ { \bf x } _ { 1 } , { \bf x } _ { 2 } , . . . , { \bf x } _ { n } ] \in \mathbb { R } ^ { d \times n }$ is the data matrix, where $\mathbf { x } _ { i } \in \mathbb { R } ^ { d \times 1 }$ specifies the $d$ -dimensional vectorial representation of the $i$ -th sample, and $n$ is the number of samples. Let $\Omega _ { m } = \{ ( i , j ) \mid \mathbf { x } _ { i }$ and $\mathbf { x } _ { j }$ belong to the same class} and $\pmb { \Omega } _ { c } = \{ ( i , j ) \mid \mathbf { x } _ { i }$ and $\mathbf { x } _ { j }$ belong to different classes} stand for the available must-link set and cannot-link set. We can encode the pairwise constraints as a matrix $\mathbf { B } \in \mathbb { R } ^ { n \times n }$ :

# Algorithm 1 Solve Eq. (4) by ADMM nput: data $\mathbf { X }$ , pairwise constraints $\Omega _ { m } , \Omega _ { c }$ , hyper-parameters $\lambda , \beta$ .   
1: Initialize: $\mathcal { C } ^ { ( 0 ) } = \mathcal { V } _ { 2 } ^ { ( 0 ) } = 0$ ${ \bf B } ^ { ( 0 ) } = { \bf Z } ^ { ( 0 ) } = { \bf D } ^ { ( 0 ) } = { \bf E } ^ { ( 0 ) } =$ ${ \bf Y } _ { 1 } ^ { ( 0 ) } = { \bf Y } _ { 3 } ^ { ( 0 ) } = 0 , \rho = 1 . 1 , \mu ^ { ( 0 ) } = \mathrm { 1 e } - 3 , \mu _ { \mathrm { m a x } } = \mathrm { 1 e 1 0 } .$ b   
2: repeat   
3: Update $\mathcal { C }$ by $\begin{array} { r } { \mathcal { C } ^ { ( k + 1 ) } = \mathcal { S } _ { \frac { 1 } { \mu _ { * } ^ { ( k ) } } } ( \mathcal { M } ^ { ( k ) } { + } \mathcal { y } _ { 2 } ^ { ( k ) } / \mu ^ { ( k ) } ) } \\ { . } \end{array}$ , where $s$ is the tensor singular välue thresholding operator [14];   
4: Update $\mathbf { Z }$ by $\mathbf { Z } ^ { ( k + 1 ) } = \left( \mathbf { I } + \mathbf { X } ^ { \top } \mathbf { X } \right) ^ { - 1 } \left( \mathbf { X } ^ { \top } ( \mathbf { X } - \mathbf { E } ^ { ( k ) } ) + \right.$ $\mathcal C ^ { ( k ) } ( : , : , 2 ) + ( \mathbf X ^ { \top } \mathbf Y _ { 1 } ^ { ( k ) } - \mathcal y _ { 2 } ^ { ( k ) } ( : , : , 2 ) ) / \mu ^ { ( k ) } )$ ;   
5: Update $\mathbf { B }$ by $\mathbf { B } ^ { ( k + 1 ) } = ( \boldsymbol { \mu } ^ { ( k ) } ( \mathcal { C } ^ { ( k ) } ( : , : , 2 ) + \mathbf { D } ^ { ( k ) } ) - ( \mathcal { V } _ { 2 } ^ { ( k ) } ( :$ ${ \bf \Xi } , { \bf \Xi } , 2 ) + { \bf Y } _ { 3 } ^ { ( k ) } ) ) / ( \beta ( { \bf L } + { \bf L } ^ { \top } ) + 2 \mu ^ { ( k ) } { \bf I } )$ ;   
6: Update $\mathbf { D }$ by $\mathbf { D } _ { i j } ^ { ( k + 1 ) } = \left\{ \begin{array} { l l } { s , ~ \mathrm { i f } ~ ( i , j ) \in \Omega _ { m } } \\ { - s , ~ \mathrm { i f } ~ ( i , j ) \in \Omega _ { c } } \\ { \mathbf { B } _ { i j } ^ { ( k ) } + \mathbf { Y } _ { 3 i j } ^ { ( k ) } / \mu ^ { ( k ) } , ~ \mathrm { o t h e r w i s e ; } } \end{array} \right.$

7: Update $\mathbf { E }$ by

$$
\mathbf { e } _ { j } ^ { ( k + 1 ) } = \left\{ \begin{array} { l l } { \displaystyle \frac { \left\| \mathbf { q } _ { j } ^ { ( k ) } \right\| _ { 2 } - \lambda / \mu ^ { ( k ) } } { \left\| \mathbf { q } _ { j } ^ { ( k ) } \right\| _ { 2 } } \mathbf { q } _ { j } ^ { ( k ) } , } & { \mathrm { i f } \left\| \mathbf { q } _ { j } ^ { ( k ) } \right\| _ { 2 } \ge \lambda / \mu ^ { ( k ) } } \\ { 0 , } & { \mathrm { o t h e r w i s e } ; } \end{array} \right.
$$

8: Update $\mathbf { Y } _ { 1 } , \mathbf { \mathcal { Y } } _ { 2 } , \mathbf { \mathcal { Y } } _ { 3 }$ , and $\mu$ by 9: until convergence

$$
\left\{ \begin{array} { l } { \mathbf { Y } _ { 1 } ^ { ( k + 1 ) } = \mathbf { Y } _ { 1 } ^ { ( k ) } + \boldsymbol { \mu } ^ { ( k ) } \left( \mathbf { X } - \mathbf { X } \mathbf { Z } ^ { ( k + 1 ) } - \mathbf { E } ^ { ( k + 1 ) } \right) } \\ { \mathcal { Y } _ { 2 } ^ { ( k + 1 ) } ( : , : , 1 ) = \mathcal { Y } _ { 2 } ^ { ( k ) } ( : , : , 1 ) + \boldsymbol { \mu } ^ { ( k ) } \left( \mathbf { Z } ^ { ( k + 1 ) } - \mathcal { C } ^ { ( k + 1 ) } ( : , : , 1 ) \right) } \\ { \mathcal { Y } _ { 2 } ^ { ( k + 1 ) } ( : , : , 2 ) = \mathcal { Y } _ { 2 } ^ { ( k ) } ( : , : , 2 ) + \boldsymbol { \mu } ^ { ( k ) } \left( \mathbf { B } ^ { ( k + 1 ) } - \mathcal { C } ^ { ( k + 1 ) } ( : , : , 2 ) \right) } \\ { \mathbf { Y } _ { 3 } ^ { ( k + 1 ) } = \mathbf { Y } _ { 3 } ^ { ( k ) } + \boldsymbol { \mu } ^ { ( k ) } \left( \mathbf { B } ^ { ( k + 1 ) } - \mathbf { D } ^ { ( k + 1 ) } \right) } \\ { \boldsymbol { \mu } ^ { ( k + 1 ) } = \operatorname* { m i n } \left( \boldsymbol { \rho } \boldsymbol { \mu } ^ { ( k ) } ; \boldsymbol { \mu } _ { \operatorname* { m a x } } \right) ; } \end{array} \right.
$$

$$
\mathbf { B } _ { i j } = \left\{ \begin{array} { l l } { \mathrm { 1 , ~ i f ~ } ( i , j ) \in \Omega _ { m } } \\ { - 1 , \mathrm { ~ i f ~ } ( i , j ) \in \Omega _ { c } . } \end{array} \right.
$$

Subspace clustering aims to segment a set of samples into a group of subspaces. Particularly, self-expressive-based subspace clustering methods have attracted great attention, which learn a self-representation matrix to act as the affinity. For example, Liu et al. [4] proposed to learn a low-rank affinity matrix by optimizing where $\mathbf { Z } \in \mathbb { R } ^ { n \times n }$ is the representation matrix, $\textbf { E } \in \ \mathbb { R } ^ { d \times n }$ denotes the reconstruction error, and $\lambda ~ > ~ 0$ is a hyperparameter.

$$
\operatorname* { m i n } _ { \mathbf { Z } , \mathbf { E } } \quad \| \mathbf { Z } \| _ { * } + \lambda \| \mathbf { E } \| _ { 2 , 1 } \mathrm { s . t . } \mathbf { X } = \mathbf { X } \mathbf { Z } + \mathbf { E } ,
$$

Recently, many semi-supervised subspace clustering methods were proposed by incorporating the pairwise constraints [5][13]. How to include the supervisory information is the crux of semi-supervised subspace clustering. Generally, the existing methods incorporate the pairwise constraints from a local perspective, i.e., expand (resp. reduce) the value of $\mathbf { Z } _ { i j }$ $i f \mathbf { x } _ { i }$ and $\mathbf { x } _ { j }$ has a must-link (resp. cannot-link).

# III. PROPOSED METHOD

# A. Model Formulation

As aforementioned, existing semi-supervised subspace clustering methods usually impose the pairwise constraints on the affinity matrix in a simple element-wise manner, which underuses the supervisory information to some extent. As studied in previous works [4], [8], the ideal affinity matrix $\mathbf { Z }$ is low-rank as a sample should be only reconstructed by the samples within the same class. Meanwhile, the ideal pairwise constraint matrix $\mathbf { B }$ is also low-rank, as it records the pairwise relationship among samples. Moreover, we observe that their low-rank structures should be identical. Accordingly, if we stack them to form a 3-D tensor $\mathcal { C } \in \mathbb { R } ^ { n \times n \times 2 }$ ,i.e., ${ \mathcal { C } } ( : , : , 1 ) = \mathbf { Z }$ ,and $\mathcal { C } ( : , : , 2 ) = \mathbf { B }$ , the formed tensor $\mathcal { C }$ should be low-rank ideally. Therefore, we use a global tensor low-rank norm to exploit this prior and preliminarily formulate the problem as

$$
\begin{array} { r l } { \underset { \mathcal { C } , \mathbf { E } , \mathbf { B } , \mathbf { Z } } { \mathrm { m i n } } } & { \| \mathcal { C } \| _ { \mathfrak { P } } + \lambda \| \mathbf { E } \| _ { 2 , 1 } } \\ { \mathrm { s . t . } } & { \mathbf { X } = \mathbf { X } \mathbf { Z } + \mathbf { E } , \mathcal { C } ( : , : , 1 ) = \mathbf { Z } , \mathcal { C } ( : , : , 2 ) = \mathbf { B } , } \\ & { \mathbf { B } _ { i j } = s , ( i , j ) \in \Omega _ { m } , \mathbf { B } _ { i j } = - s , ( i , j ) \in \Omega _ { c } . } \end{array}
$$

![](images/2.jpg)

![](images/3.jpg)

TABLE I: Detailed Comparisions of Accuracy and NMI under $30 \%$ Initial Labels.

<table><tr><td>Accuracy</td><td>ORL</td><td>YaleB</td><td>COIL20</td><td>Isolet</td><td>MNIST</td><td>Alphabet</td><td>BF0502</td><td>Notting-Hill</td><td>Average</td></tr><tr><td>LRR</td><td>0.7405</td><td>0.6974</td><td>0.6706</td><td>0.6699</td><td>0.5399</td><td>0.4631</td><td>0.4717</td><td>0.5756</td><td>0.6036</td></tr><tr><td>DPLRR</td><td>0.8292</td><td>0.6894</td><td>0.8978</td><td>0.8540</td><td>0.7442</td><td>0.7309</td><td>0.5516</td><td>0.9928</td><td>0.7862</td></tr><tr><td>SSLRR</td><td>0.7600</td><td>0.7089</td><td>0.7159</td><td>0.7848</td><td>0.6538</td><td>0.5294</td><td>0.6100</td><td>0.7383</td><td>0.6876</td></tr><tr><td>L-RPCA</td><td>0.6568</td><td>0.3619</td><td>0.8470</td><td>0.6225</td><td>0.5662</td><td>0.5776</td><td>0.4674</td><td>0.3899</td><td>0.5612</td></tr><tr><td>CP-SSC</td><td>0.7408</td><td>0.6922</td><td>0.8494</td><td>0.7375</td><td>0.5361</td><td>0.5679</td><td>0.4733</td><td>0.5592</td><td>0.6445</td></tr><tr><td>SC-LRR</td><td>0.7535</td><td>0.9416</td><td>0.8696</td><td>0.8339</td><td>0.8377</td><td>0.6974</td><td>0.7259</td><td>0.9982</td><td>0.8322</td></tr><tr><td>CLRR</td><td>0.8160</td><td>0.7853</td><td>0.8217</td><td>0.8787</td><td>0.7030</td><td>0.6837</td><td>0.7964</td><td>0.9308</td><td>0.8020</td></tr><tr><td>Proposed Method</td><td>0.8965</td><td>0.9742</td><td>0.9761</td><td>0.9344</td><td>0.8747</td><td>0.8355</td><td>0.8697</td><td>0.9934</td><td>0.9193</td></tr><tr><td>NMI</td><td>ORL</td><td>YaleB</td><td>COIL20</td><td>Isolet</td><td>MNIST</td><td>Alphabet</td><td>BF0502</td><td>Notting-Hill</td><td>Average</td></tr><tr><td>LRR</td><td>0.8611</td><td>0.7309</td><td>0.7742</td><td>0.7677</td><td>0.4949</td><td>0.5748</td><td>0.3675</td><td>0.3689</td><td>0.6175</td></tr><tr><td>DPLRR</td><td>0.8861</td><td>0.7205</td><td>0.9258</td><td>0.8853</td><td>0.7400</td><td>0.7477</td><td>0.5388</td><td>0.9748</td><td>0.8024</td></tr><tr><td>SSLRR</td><td>0.8746</td><td>0.7409</td><td>0.7986</td><td>0.8337</td><td>0.6373</td><td>0.6070</td><td>0.4810</td><td>0.5949</td><td>0.6960</td></tr><tr><td>L-RPCA</td><td>0.8038</td><td>0.3914</td><td>0.9271</td><td>0.7834</td><td>0.5805</td><td>0.6590</td><td>0.4329</td><td>0.2294</td><td>0.6009</td></tr><tr><td>CP-SSC</td><td>0.8705</td><td>0.7224</td><td>0.9583</td><td>0.8127</td><td>0.5516</td><td>0.6459</td><td>0.4453</td><td>0.4733</td><td>0.6850</td></tr><tr><td>SC-LRR</td><td>0.8924</td><td>0.9197</td><td>0.9048</td><td>0.8362</td><td>0.7803</td><td>0.7316</td><td>0.7068</td><td>0.9931</td><td>0.8456</td></tr><tr><td>CLRR</td><td>0.9028</td><td>0.7895</td><td>0.8568</td><td>0.8892</td><td>0.6727</td><td>0.7091</td><td>0.6970</td><td>0.8293</td><td>0.7933</td></tr><tr><td>Proposed Method</td><td>0.9337</td><td>0.9548</td><td>0.9716</td><td>0.9218</td><td>0.7825</td><td>0.8107</td><td>0.7693</td><td>0.9771</td><td>0.8902</td></tr></table>

In Eq. (3), we adopt the nuclear norm $\lVert \cdot \rVert _ { \circledast }$ defined on tensorSVD [14] to seek the low-rank representation, and other kinds of tensor low-rank norms are also applicable, e.g., [15]. We introduce a scalar $s$ to constrain the maximum and minimum values of $\mathbf { B }$ promoting that $\mathbf { B }$ has a similar scale to $\mathbf { Z }$ . Empirically, $s$ is set to the largest element of the learned affinity by LRR. By solving Eq. (3), the affinity matrix $\mathbf { Z }$ and pairwise constraint matrix $\mathbf { B }$ are jointly optimized according to the nuclear norm on $\mathcal { C }$ , i.e., the supervisory information is transferred to $\mathbf { Z }$ , and at the same time, the learned affinity matrix can also augment the initial pairwise constraints from a global perspective. TABLE II: Ablation Study.   

<table><tr><td>Percentage</td><td>Accuracy</td><td>ORL</td><td>YaleB</td><td>COIL20</td><td>Isolet</td><td>MNIST</td><td>Alphabet</td><td>BF0502</td><td>Notting-Hill</td><td>Average</td></tr><tr><td rowspan="5">10</td><td>SSLRR</td><td>0.7223</td><td>0.6965</td><td>0.6874</td><td>0.6107</td><td>0.5121</td><td>0.4278</td><td>0.4150</td><td>0.5747</td><td>0.5808</td></tr><tr><td>CLRR</td><td>0.7193</td><td>0.7032</td><td>0.6309</td><td>0.7424</td><td>0.5435</td><td>0.5120</td><td>0.5165</td><td>0.6728</td><td>0.6301</td></tr><tr><td>Eq. (3)</td><td>0.7298</td><td>0.7838</td><td>0.6744</td><td>0.8599</td><td>0.5224</td><td>0.5022</td><td>0.5786</td><td>0.8079</td><td>0.6824</td></tr><tr><td>Eq. (4)</td><td>0.7298</td><td>0.7838</td><td>0.8708</td><td>0.8424</td><td>0.7659</td><td>0.6640</td><td>0.5779</td><td>0.9573</td><td>0.7740</td></tr><tr><td>Eq. (5)</td><td>0.7523</td><td>0.8696</td><td>0.9171</td><td>0.8665</td><td>0.7879</td><td>0.6862</td><td>0.5915</td><td>0.9576</td><td>0.8036</td></tr><tr><td rowspan="5">20</td><td>SSLRR</td><td>0.7390</td><td>0.6998</td><td>0.6966</td><td>0.6651</td><td>0.5308</td><td>0.4672</td><td>0.4750</td><td>0.6363</td><td>0.6137</td></tr><tr><td>CLRR</td><td>0.7808</td><td>0.7130</td><td>0.6971</td><td>0.8176</td><td>0.6401</td><td>0.6064</td><td>0.6863</td><td>0.8598</td><td>0.7251</td></tr><tr><td>Eq. (3)</td><td>0.7860</td><td>0.9194</td><td>0.8101</td><td>0.9012</td><td>0.6661</td><td>0.6443</td><td>0.7554</td><td>0.9378</td><td>0.8025</td></tr><tr><td>Eq. 4</td><td>0.7860</td><td>0.9194</td><td>0.9364</td><td>0.9065</td><td>0.8366</td><td>0.7511</td><td>0.8077</td><td>0.9817</td><td>0.8657</td></tr><tr><td>Eq. (5)</td><td>0.8325</td><td>0.9548</td><td>0.9569</td><td>0.9078</td><td>0.8439</td><td>0.7772</td><td>0.8223</td><td>0.9831</td><td>0.8848</td></tr><tr><td rowspan="5">30</td><td>SSLRR</td><td>0.7600</td><td>0.7089</td><td>0.7159</td><td>0.7848</td><td>0.6538</td><td>0.5294</td><td>0.6100</td><td>0.7383</td><td>0.6876</td></tr><tr><td>CLRR</td><td>0.8160</td><td>0.7853</td><td>0.8217</td><td>0.8787</td><td>0.7030</td><td>0.6837</td><td>0.7964</td><td>0.9308</td><td>0.8020</td></tr><tr><td>Eq. (3)</td><td>0.8893</td><td>0.9664</td><td>0.9096</td><td>0.9222</td><td>0.8370</td><td>0.7671</td><td>0.8083</td><td>0.9661</td><td>0.8832</td></tr><tr><td>Eq. (4)</td><td>0.8893</td><td>0.9664</td><td>0.9710</td><td>0.9300</td><td>0.8745</td><td>0.8244</td><td>0.8631</td><td>0.9917</td><td>0.9138</td></tr><tr><td>Eq. (5)</td><td>0.8965</td><td>0.9742</td><td>0.9761</td><td>0.9344</td><td>0.8747</td><td>0.8355</td><td>0.8697</td><td>0.9934</td><td>0.9193</td></tr></table>

Besides, if two samples ${ \bf { x } } _ { i }$ and $\mathbf { x } _ { j }$ ) are close to each other in the feature space, we can expect they have a similar pairwise relationship, i.e., $\mathbf { B } ( : , i )$ is close to $\mathbf { B } ( : , j )$ . To encode this prior, we first construct a $k \mathbf { N N }$ graph $\mathbf { W } \in \mathbb { R } ^ { n \times n }$ to capture the local geometric structure of samples, and use the local Laplacian regularization $\mathrm { T r } ( \mathbf { B L B } ^ { \top } )$ to replenish the global low-rank term, where $ { \mathbf { L } } =  { \mathbf { D } } -  { \mathbf { W } }$ is the Laplacian matrix with $\textstyle \mathbf { D } _ { i i } = \sum _ { j } \mathbf { W } _ { i j }$ [16]. Therefore, our model is finally formulated as

$$
\begin{array} { r l } { \underset { \mathcal { C } , \mathbf { E } , \mathbf { B } , \mathbf { Z } } { \operatorname* { m i n } } } & { \| \mathcal { C } \| _ { \mathfrak { S } } + \lambda \| \mathbf { E } \| _ { 2 , 1 } + \beta \operatorname { T r } ( \mathbf { B } \mathbf { L } \mathbf { B } ^ { \top } ) } \\ { \mathrm { s . t . } } & { \mathbf { X } = \mathbf { X } \mathbf { Z } + \mathbf { E } , \mathcal { C } ( : , : , 1 ) = \mathbf { Z } , \mathcal { C } ( : , : , 2 ) = \mathbf { B } , } \\ & { \mathbf { B } _ { i j } = s , ( i , j ) \in \Omega _ { m } , \mathbf { B } _ { i j } = - s , ( i , j ) \in \Omega _ { c } . } \end{array}
$$

After solving Eq. (4), we first normalize each column of $\mathbf { Z }$ to [0, 1], and normalize $\mathbf { B }$ by $\mathbf { B } \gets \mathbf { B } / s$ . Then, we use the augmented pairwise constraint matrix $\mathbf { B }$ to repair $\mathbf { Z }$ ,i.e.,

$$
\mathbf { Z } _ { i j }  \{ \begin{array} { l l } { 1 - ( 1 - \mathbf { B } _ { i j } ) ( 1 - \mathbf { Z } _ { i j } ) , \mathrm { i f } \mathbf { B } _ { i j } \geq 0 } \\ { ( 1 + \mathbf { B } _ { i j } ) \mathbf { Z } _ { i j } , \mathrm { i f } \mathbf { B } _ { i j } < 0 . } \end{array} 
$$

When $\mathbf { B } _ { i j }$ is larger than 0, $\mathbf { x } _ { i }$ and $\mathbf { x } _ { j }$ are likely to belong to the same class, Eq. (5) will increase the corresponding element of $\mathbf { Z }$ Similarly, when $\mathbf { B } _ { i j }$ is less than 0, $\mathbf { Z } _ { i j }$ will be depressed. Therefore, Eq. (5) further enhances the affinity matrix by the augmented pairwise constraints. Finally, we apply spectral clustering [17] on $\mathbf { W } = ( | \mathbf { Z } | + | \mathbf { Z } ^ { \top } | ) / 2$ to get the subspace segmentation.

# B. Optimization Algorithm

As Eq. (4) contains multiple variables and constraints, we solve it by alternating direction method of multipliers (ADMM) [18]. Algorithm 1 summarizes the whole pseudo code. Due to the page limitation, the detailed derivation process can be found in the supplementary file.

The computational complexity of Algorithm 1 is dominated by steps 3-5. Specifically, the step 3 solves the t-SVD of an $n { \times } n { \times } 2$ tensor with the complexity of $\mathcal { O } ( 2 n ^ { 2 } \log 2 \mathrm { + } 2 n ^ { 3 } )$ [14]. The steps 4-5 involve matrix inverse and matrix multiplication operations with the complexity of $\mathcal { O } ( n ^ { 3 } )$ . Note that in step 4, the to be inversed matrix $\left( \mathbf { I } + \mathbf { X } ^ { \top } \mathbf { X } \right)$ is unchanged, which only needs to be calculated once in advanced. In summary, the overall computational complexity of Algorithm 1 is $\mathcal { O } ( n ^ { 3 } )$ in one iteration.

![](images/4.jpg)  
Fig. 4: Visual comparison of the affinity matrices learned by different methods on MNIST. The learned affinity matrices were normalized to [0,1]. Zoom in the figure for a better view.

![](images/5.jpg)  
Fig. 5: Influence of the hyper-parameters on clustering performance.

# IV. EXPERIMENTS

In this section, we evaluated the proposed model on 8 commonly-used benchmark datasets, including ORL, YaleB, COIL20, Isolet, MNIST, Alphabet, BF0502 and Notting-Hill1. Those datasets cover face images, object images, digit images, spoken letters, and videos. To generate the weakly-supervisory information, following [7], we inferred the pairwise constraints from the randomly selected labels.

We compared our method with LRR [4] and six state-of-theart semi-supervised subspace clustering methods, including DPLRR [8], SSLRR [7], L-RPCA [19], CP-SSC [20], SCLRR [9] and CLRR [5]. We performed standard spectral clustering in [17] on all the methods to generate the clustering result. We adopted the clustering accuracy and normalized mutual information (NMI) to measure their performance. For both metrics, the larger, the better. For fair comparisons, we carefully tuned the hyper-parameters of the compared methods through exhaustive grid search to achieve the best result. To comprehensively evaluate the different methods, for each dataset, we randomly selected various percentages of initial labels $\left( \left\{ 5 \% , 1 0 \% , 1 5 \% , 2 0 \% , 2 5 \% , 3 0 \% \right\} \right)$ to infer the pairwise constraints. We used the same label information for all the compared methods in every case. To reduce the influence of the random selection, we repeated the experiments 10 times with the randomly selected labels each time, and reported the average performance.

![](images/6.jpg)  
Cvegenc beavcaron  ifent to hatasetsTheondnal isora

![](images/7.jpg)  
Fig. 7: Running time comparisons of different methods on eight datasets.

# A. Comparison of Clustering Accuracy

Figs. 2-3 compare the clustering accuracy and NMI of different methods under various numbers of pairwise constraints, and Table I shows the clustering performance of different methods with $30 \%$ initial labels of each datasets as the supervisory information. From those figures and table, we can draw the following conclusions. 1) With more pairwise constraints, all the semi-supervised subspace clustering methods generally perform better, which indicates the effectiveness of including supervisory information in subspace clustering. 2) The proposed method outperforms the other methods significantly. For example our method improves the accuracy value from 0.61 to 0.78 on MNIST and the NMI value from 0.72 to 0.89 on YaleB when compared with the best companions. According to Table I, the proposed method improves the average clustering accuracy of the best compared methods from 0.83 to 0.92. Moreover, the proposed method almost always achieves the best clustering performance with varied supervisory information. 3) The compared methods may be sensitive to different datasets (e.g., SC-LRR achieves the second-best performance on YaleB and MNIST, but performs relatively bad on ORL and COIL20), and sensitive to diverse clustering metrics (e.g., CP-SSC performs well in NMI but poorly in clustering accuracy). On the contrary, the proposed method is robust to distinct datasets and metrics. Besides, we visualized the affinity matrices learned by different methods on MNIST in Fig. 4, where it can be seen that our method produces more dense and correct connections, leading to the most salient block diagonal affinity. This is owing to the used global tensor low-rank regularization, and further explains the good clustering results reported in Figs. 2-3 and Table I.

# B. Hyper-Parameter Analysis

Fig. 5 illustrates how the two hyper-parameters $\lambda$ and $\beta$ affect the performance of our method on COIL20, MNIST and Alphabet. It can be seen that the proposed model is relatively robust to hyper-parameters around the optimal. To be specific, we recommend to set $\lambda { = } 0 . 0 1$ and $\beta = 1 0$ .

# C. Convergence Speed

Fig. 6 shows the convergence behaviors of all the compared algorithms on all the datasets. Note that the convergence criteria of all the methods are the same, i.e., the residual error of a variable in two successive steps is less than 1e—3. We can conclude that L-PRCA usually converges fastest among all the methods. While the proposed algorithm also converges fast compared with other methods like SSLRR and DPLRR. Moreover, the proposed algorithm gets converged within 130 iterations on all the eight datasets. Fig. 7 compares the average running time of all eight methods on each dataset. Note that we implemented all the methods with MATLAB on a Windows desktop with a 2.90 GHz Intel(R) i5-10400F CPU and 16.0 GB memory. We can observe that the proposed method takes an average time of 95.97s each run on all the eight datasets. It is slightly higher than LRR, but significantly lower than SSLRR. We also need to point out that the proposed method performs much better than the compared methods in clustering performance.

# D. Ablation Study

We investigated the effectiveness of the priors/processes involved in our model by comparing the clustering accuracy of Eqs. (3)-(5). The compared methods include two well-known element-wise semi-supervised subspace clustering methods SSLRR and CLRR. As Table II shows, the results of Eq. (3) outperform SSLRR and CLRR significantly on all the datasets, which demonstrates the advantage of the global tensor lowrank prior over the element-wisely fusion strategy. Besides, Eqs. (4) and (5) improve the performance of the proposed model successively, which indicates that both the graph regularization and the post-refinement processing contribute to our model.

# V. CONCLUSION

We have proposed a novel semi-supervised subspace clustering model. We first stacked the affinity matrix and pairwise constraint matrix to form a tensor, and then imposed a tensor low-rank prior on it to learn the affinity matrix and augment the pairwise constraints simultaneously. In addition to the global tensor low-rank term, we added a Laplacian regularization term to model the underlying local geometric structure. Furthermore, the learned affinity matrix was refined by the augmented pairwise constraints. The proposed model was formulated as a convex problem, and solved by ADMM. The experimental results demonstrated that our model outperforms other semi-supervised subspace clustering methods significantly. In the future, we will investigate how to incorporate our work with the existing semi-supervised learning neural networks [21][24]. For example, we can use the proposed pairwise constraint enhancement as a loss function to train the neural networks in an end-to-end manner. Moreover, we will improve our method by solving the noisy pairwise constraints problem.

# REFERENCES

[1] R. Vidal, "Subspace clustering," IEEE SPM, vol. 28, no. 2, pp. 5268, 2011.   
[2] X. Peng, Y. Li, I. W. Tsang, H. Zhu, J. Lv, and J. T. Zhou, "Xai beyond cass Itepeabneul luste. op. 2022.   
[3] E. Elhamifar and R. Vidal, "Sparse subspace clustering: Algorithm, theory, and applications," IEEE TPAMI, vol. 35, no. 11, pp. 27652781, 2013.   
[4] G. Liu, Z. Lin, S. Yan, J. Sun, Y. Yu, and Y. Ma, "Robust recovery o subspace structures by low-rank representation," IEEE TPAMI, vol. 35, no. 1, pp. 171184, 2013.   
[5] J. Wang, X. Wang, F. Tian, C. H. Liu, and H. Yu, "Constrained low- rank representation for robust subspace clustering," IEEE TCYB, vol. 47, no. 12, pp. 45344546, 2017.   
[6] C. Yang, M. Ye, S. Tang, T. Xiang, and Z. Liu, "Semi-supervised lowrank representation for image classification," VP, vol. 11, no. 1, pp. 7380, 2017.   
[L. Zhua, Z. Zhou, S. Gao, J. Yin, Z. Lin, andY. Ma, "Label iormin guided graph construction for semi-supervised learning," IEEE TIP, vol. 26, no. 9, pp. 41824192, 2017.   
[8] H. Liu, Y. Jia, J. Hou, and Q. Zhang, "Learning low-rank graph with enhanced supervision," IEEE TCSVT, vol. 32, no. 4, pp. 25012506, 2022.   
[9] K. Tang, R. Liu, Z. Su, and J. Zhang, "Structure-constrained low-rank representation," IEEE TNNLS, vol. 25, no. 12, pp. 21672179, 2014.   
C.Zhou . Z X.  .hi  X. o, i 16.   
[11] C.-G. Li and R. Vidal, "Structured sparse subspace clustering: A unified optimization framework," in Proc. IE CVPR, 2015, pp. 277286.   
[12] Z. Zhang, Y. Xu, L. Shao, and J. Yang, "Discriminative block-diagonal representation learning for image recognition," IEEE TNNLS, vol. 29, no. 7, pp. 31113125, 2018. with side-information," in Proc. IEEE ICPR, 2018, pp. 20932099.   
[14] C. Lu, J. Feng, Y. Chen, W. Liu, Z. Lin, and S. Yan, "Tensor robust principal component analysis with a new tensor nuclear norm," IEEE TPAMI, vol. 42, no. 4, pp. 925938, 2020.   
[15] S. Wang, Y. Chen, L. Zhang, Y. Cen, and V. Voronin, "Hyper-laplacian regularized nonconvex low-rank representation for multi-view subspace clustering," IEEE TSIPN, vol. 8, pp. 376388, 2022.   
[16] Y. Chen, X. Xiao, and Y. Zhou, "Multi-view subspace clustering via smultaneousy learnng the representation tensor and afnity matrix," PR, vol. 106, p. 107441, 2020.   
[17] B. Peng, J. Lei, H. Fu, C. Zhang, T.-S. Chua, and X. Li, "Unsupervised video action clustering via motion-scene interaction constraint," IEEE TCSVT, vol. 30, no. 1, pp. 131144, 2020.   
[18 Y.-P. Zhao, L. Chen, and C. L. P. hen, "Laplacian regularized noative representation for clustering and dimensionality reduction," IEEE T, vol. 31, no. 1, pp. 114, 2021.   
[19] D. Zeng, Z. Wu, C. Ding, Z. Ren, Q. Yang, and S. Xie, "Labeled-robust regression: Simultaneous data recovery and classification," IEEE TCYB, vol. 52, no. 6, pp. 50265039, 2022.   
[20] K. Somandepalli and S. Narayanan, "Reinforcing self-expressive representation with constraint propagation for ace clustering in movies," in Proc. IEEE ICASSP, 2019, pp. 40654069.   
[21] W. Zhang, Q. M. J. Wu, and Y. Yang, "Semisupervised manifold regularization via a subnetwork-based representation learning model," IEEE TCYB, pp. 114, 2022.   
[22] H. Zhao, J. Zheng, W. Deng, and Y. Song, "Semi-supervised broad learning system based on manifold regularization and broad network," IEEE TCÁS-I, vol. 67, no. 3, pp. 983994, 2020.   
[23] Y. Li, P. Hu, Z. Liu, D. Peng, J. T. Zhou, and X. Peng, "Contrastive clustering," in Proc. AAAI, vol. 35, no. 10, 2021, pp. 85478555.   
[24] Y. Li, M. Yang, D. Peng, T. Li, J. Huang, and X. Peng, "Twin contrastive learning for online clustering," IJCV, vol. 130, no. 9, pp. 22052221, 2022