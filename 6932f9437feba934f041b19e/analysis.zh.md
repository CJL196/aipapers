# 1. 论文基本信息

## 1.1. 标题
FlexiAct: Towards Flexible Action Control in Heterogeneous Scenarios (FlexiAct: 迈向异构场景下的灵活动作控制)

论文标题直接点明了研究的核心：`FlexiAct` 是一个旨在实现**灵活动作控制**的技术。关键词 `Heterogeneous Scenarios` (异构场景) 是理解本文关键创新的入口，它强调该技术能够处理参考主体与目标主体在空间结构（如布局、骨骼、视角）上存在显著差异的情况。

## 1.2. 作者
*   SHIYI ZHANG, JUNHAO ZHUANG, YANSONG TANG: 来自清华大学深圳国际研究生院。
*   ZHAOYANG ZHANG, YING SHAN: 来自腾讯 ARC Lab。

    作者团队由学术界（清华大学）和工业界顶级研究实验室（腾讯 ARC Lab）的研究人员组成，这种组合通常意味着研究工作既有学术深度，又关注实际应用价值。

## 1.3. 发表期刊/会议
论文拟发表于 **SIGGRAPH Conference Papers '25**。

*   **SIGGRAPH (Special Interest Group on Computer Graphics and Interactive Techniques)** 是计算机图形学领域最顶级的国际会议，被公认为该领域的最高殿堂。能在 SIGGRAPH 发表论文，代表了该研究工作在创新性、技术深度和影响力方面得到了国际顶尖同行的认可。

## 1.4. 发表年份
2025年（预印本于2025年5月6日发布）。

## 1.5. 摘要
论文提出了一种名为 `FlexiAct` 的新方法，用于解决<strong>动作定制 (Action customization)</strong> 任务。该任务的目标是生成一段视频，让视频中的主体执行由输入控制信号指定的动作。现有方法（如基于姿态或全局运动的方法）存在局限，它们要求参考视频和目标图像在空间结构（如布局、骨骼、视角）上高度一致，这大大降低了它们在多样化场景中的适应性。

为了克服这些限制，`FlexiAct` 能够将一个参考视频中的动作迁移到一个**任意**的目标图像上，即使参考视频中的主体与目标图像中的主体在布局、视角和骨骼结构上存在差异，也能保持目标主体的身份一致性。

为实现这一目标，论文提出了两个核心创新：
1.  **RefAdapter**: 一个轻量级的图像条件适配器，它擅长在保持外观一致性的同时，实现对不同空间结构的自适应，解决了外观一致性与结构灵活性之间的平衡问题。
2.  <strong>FAE (Frequency-aware Action Extraction, 频率感知动作提取)</strong>: 该方法基于一个关键观察——去噪过程在不同时间步对运动（低频信息）和外观细节（高频信息）的关注度不同。FAE 利用这一特性，在去噪过程中直接提取动作，而无需依赖独立的时空处理架构。

    实验证明，`FlexiAct` 能够有效地将动作迁移到具有不同布局、骨骼和视角的各类主体上。

## 1.6. 原文链接
*   **原文链接:** https://arxiv.org/abs/2505.03730
*   **PDF 链接:** https://arxiv.org/pdf/2505.03730v1.pdf
*   **发布状态:** 论文是提交给 SIGGRAPH 2025 的预印本 (Preprint)。

# 2. 整体概括

## 2.1. 研究背景与动机
### 2.1.1. 核心问题
论文要解决的核心问题是<strong>异构场景下的视频动作迁移 (Action Transfer in Heterogeneous Scenarios)</strong>。具体来说，就是给定一个包含特定动作的**参考视频** (Reference Video) 和一个包含目标主体的**静态图像** (Target Image)，如何生成一段新视频，让目标主体在新视频中执行参考视频里的动作，同时保持其原有的外观和身份。

下图（原文 Figure 1）直观展示了该任务的输入和输出。

![该图像是示意图，展示了FlexiAct方法在不同布局和结构下将参考视频中的动作转移到多个目标图像的效果。每一对参考视频和目标图像都展示了动作的适应性，包括不同的视角和姿势变化。](images/1.jpg)
*该图像是示意图，展示了FlexiAct方法在不同布局和结构下将参考视频中的动作转移到多个目标图像的效果。每一对参考视频和目标图像都展示了动作的适应性，包括不同的视角和姿势变化。*

### 2.1.2. 现有挑战 (Gap)
当前主流的动作迁移或视频生成控制方法主要有两类，但都存在明显短板：
1.  <strong>基于预定义信号的方法 (Predefined signal-based methods):</strong> 这类方法通常使用骨骼姿态 (Pose)、深度图等作为中间控制信号。它们的优点是控制精度高，但缺点是**约束过严**。它们要求目标主体的形状、骨骼结构、视角等必须与提供控制信号的源视频严格对齐。这在现实世界中很难满足，比如，我们无法轻易地将一个人类的舞蹈动作迁移到一个卡通角色或一只猫身上，因为它们的骨骼结构完全不同。此外，对于非人类主体，获取准确的姿态信息本身就是一大难题。

2.  <strong>基于全局运动的方法 (Global motion methods):</strong> 这类方法学习参考视频的整体运动模式（如相机移动、物体轨迹），但通常无法将动作精确地绑定到**特定主体**上。它们生成视频的整体布局往往与参考视频相似，难以实现将一个视频中的主体动作“嫁接”到另一个完全不同场景中的主体身上。

    因此，当前研究存在一个明显的空白（Gap）：<strong>缺乏一种能够跨越空间结构差异（异构性）的、灵活且保持身份一致性的动作迁移方法。</strong>

### 2.1.3. 创新思路
`FlexiAct` 的创新思路在于**解耦动作信息与空间结构**。它不依赖于严格对齐的骨骼姿态，而是试图在模型的生成过程中，动态地、智能地提取纯粹的“动作”信息，并将其“适应”到目标主体独特的空间结构上。

为实现这一点，论文从两个方面着手：
1.  **空间适应与外观保持:** 设计一个模块 (`RefAdapter`)，让模型既能参考目标图像的外观，又不过分受其初始姿态的束缚，从而为动作的“植入”留出空间。
2.  **精准动作提取:** 提出一种新颖的动作提取机制 (`FAE`)，它利用了扩散模型在去噪过程中不同阶段关注点不同的特性，从而在不需要额外复杂网络结构的情况下，实现对动作信息的精准捕捉。

## 2.2. 核心贡献/主要发现
本文的核心贡献可以总结为以下三点：
1.  **提出 `FlexiAct` 框架:** 这是首个据作者所知，能够在布局、骨骼、视角等空间结构存在巨大差异的**异构场景**下，实现灵活、高质量动作迁移的方法，同时保证了动作的准确性和主体外观的一致性。
2.  **提出 `RefAdapter` 模块:** 一个轻量级的图像条件适配器。它以少量可训练参数，实现了对不同空间结构的自适应和精细的外观一致性保持，成功地在**结构灵活性**和**外观一致性**之间取得了平衡。
3.  <strong>提出 `FAE` (频率感知动作提取) 方法:</strong> 一种新颖的动作提取与控制机制。它利用了扩散模型去噪过程中不同时间步对不同频率信息（低频运动 vs. 高频外观）的关注差异，实现了在采样过程中直接、精准地提取动作，摆脱了对独立时空架构的依赖。

# 3. 预备知识与相关工作

## 3.1. 基础概念
### 3.1.1. 扩散模型 (Diffusion Models)
扩散模型是近年来在图像和视频生成领域取得巨大成功的一类生成模型。其核心思想分为两个过程：
*   <strong>前向过程 (Forward Process):</strong> 对一张真实的图像或视频，逐步、多次地添加少量高斯噪声，直到其完全变成纯粹的随机噪声。这个过程是固定的，不需要学习。
*   <strong>反向过程 (Reverse Process) / 去噪过程 (Denoising Process):</strong> 这是模型学习的关键。训练一个神经网络（通常是 U-Net 结构），让它学会如何从一个充满噪声的输入中，预测出所添加的噪声，从而逐步地“去噪”，最终从纯随机噪声还原出一张清晰的图像或视频。
    `FlexiAct` 正是基于一个视频扩散模型构建的，其核心创新 `FAE` 也与这个去噪过程紧密相关。

### 3.1.2. 文生视频 (Text-to-Video) 与 图生视频 (Image-to-Video, I2V)
*   <strong>文生视频 (T2V):</strong> 指根据一段文字描述生成视频。
*   <strong>图生视频 (I2V):</strong> 指根据一张输入图像生成一段视频，通常视频的首帧或内容与输入图像高度相关。
    `FlexiAct` 属于 I2V 的范畴，但它比传统的 I2V 更复杂，因为它不仅需要参考一张图像，还需要从另一个视频中提取动作信息。论文使用的基础模型 `CogVideoX-I2V` 就是一个强大的 I2V 模型。

### 3.1.3. LoRA (Low-Rank Adaptation)
LoRA 是一种<strong>参数高效微调 (Parameter-Efficient Fine-Tuning, PEFT)</strong> 技术。对于一个预训练好的大型模型（如扩散模型），我们通常不希望为了适应新任务而重新训练所有参数（成本极高）。LoRA 的做法是冻结原始模型的大部分权重，而在模型的某些层（如注意力层）旁边，插入两个小的、低秩的矩阵（A 和 B）。在微调时，我们只训练这两个小矩阵的参数。由于小矩阵的参数量远小于原始权重，因此训练成本大大降低。`FlexiAct` 中的 `RefAdapter` 就利用了 LoRA 来高效地微调基础模型。

### 3.1.4. ControlNet / ReferenceNet
*   **ControlNet:** 一种给预训练扩散模型增加额外条件控制的技术。它通过复制一部分模型结构并用零卷积连接，使得模型可以在保持原有生成能力的同时，接收新的控制信号（如边缘图、姿态骨骼图）。
*   **ReferenceNet:** 类似于 ControlNet，但其设计目标是让模型参考另一张图像的外观特征。它也通过复制一部分网络层来实现细粒度的特征注入。
    `FlexiAct` 在介绍 `RefAdapter` 时将 `ReferenceNet` 作为对比，指出 `RefAdapter` 在实现类似细粒度控制的同时，参数量更小，训练成本更低。

## 3.2. 前人工作
论文将相关工作分为两大类：

### 3.2.1. 全局运动定制 (Global Motion Customization)
这类方法旨在从参考视频中提取**全局的运动动态**，如相机移动、物体运动轨迹等，并生成具有相似运动模式的视频。
*   **代表工作:** `MotionDirector`, `Motion Inversion`, `VMC`。
*   **实现方式:** 通常通过解耦外观和运动信息来实现。例如，`MotionDirector` 使用时空 `LoRA` 来分离外观和运动；`Motion Inversion` 设计了两种不同的嵌入来分别表示外观和运动。
*   **局限性:** 这些方法主要关注视频整体的动态，难以将动作精确地应用到**某个特定的主体**上，并且生成视频的布局通常会模仿参考视频，无法处理本文关注的异构场景。

### 3.2.2. 基于预定义信号的动作定制 (Predefined signal-based Action Customization)
这类方法通过提取参考视频中的**明确结构化信号**（如人体骨骼姿态、深度图、边缘图等）来驱动目标图像中的主体运动。
*   **代表工作:** `MagicAnimate`, `AnimateAnyone`, `ControlNet`。
*   **实现方式:** 通常采用一个编码器提取参考图像的外观特征，另一个网络（如 `PoseNet`）处理姿态序列，最后在扩散模型的 U-Net 中融合这两部分信息来生成视频。
*   **局限性:**
    1.  **强依赖对齐:** 它们要求目标图像中的主体与姿态序列在**空间结构上严格一致**。如果目标主体是不同物种（如动物）、不同画风（如卡通角色），或者视角、体型差异巨大，这些方法就会失效。
    2.  **信号获取困难:** 对于人类以外的生物，准确的姿态估计本身就是一个非常困难且尚未完全解决的问题。

## 3.3. 技术演进
动作控制的视频生成技术演进路线大致如下：
1.  <strong>早期 (GAN-based):</strong> 使用生成对抗网络 (GANs) 进行图像动画化，如 `First Order Motion Model`。这些方法在面部表情和简单动作上效果不错，但对于全身大幅度运动和复杂场景，生成质量和稳定性有限。
2.  <strong>中期 (Diffusion-based, 强约束):</strong> 随着扩散模型的兴起，`ControlNet`, `MagicAnimate` 等工作将扩散模型与姿态等强约束信号结合，极大地提升了生成视频的质量和动作控制的精确度。但这也带来了前述的**异构性难题**。
3.  <strong>近期 (Diffusion-based, 弱约束/解耦):</strong> `MotionDirector` 等工作尝试从更宏观的层面解耦运动和外观，摆脱对姿态等信号的依赖，但牺牲了对特定主体的精细控制。

    `FlexiAct` 正是处在这一技术脉络的最新前沿，它试图融合后两者的优点：既要摆脱对强约束信号的依赖，实现**异构场景下的灵活性**；又要实现对特定主体的**精细动作控制**和**身份保持**。

## 3.4. 差异化分析
与相关工作相比，`FlexiAct` 的核心差异化创新在于：
*   <strong>vs. 预定义信号方法 (`MagicAnimate`等):</strong> `FlexiAct` **不依赖**于骨骼姿态等需要严格对齐的中间信号，因此可以处理人类到动物、真实到卡通等跨物种、跨领域的动作迁移，解决了“异构性”难题。
*   <strong>vs. 全局运动方法 (`MotionDirector`等):</strong> `FlexiAct` 能够将动作**精确地绑定到目标主体**上，并保持其身份一致性，而不是仅仅模仿参考视频的整体布局和动态。
*   <strong>vs. 条件注入方法 (`ReferenceNet`, `IP-Adapter`):</strong> `FlexiAct` 提出的 `RefAdapter` 在实现 `ReferenceNet` 级别细粒度外观控制的同时，参数量远小于 `ReferenceNet`；与 `IP-Adapter` 这种粗粒度注入方法相比，又能更好地保持外观一致性。
*   **在动作提取上:** 现有方法通常设计独立的网络模块（如时空注意力）来提取运动。`FlexiAct` 的 `FAE` 则是**首次**提出在去噪过程中，利用不同时间步的内在特性来动态提取动作，是一种全新的、更高效的思路。

# 4. 方法论

## 4.1. 方法原理
`FlexiAct` 的核心思想是，将一个复杂的异构动作迁移任务分解为两个子问题，并分别设计模块来解决：
1.  **如何让模型适应空间结构的差异，同时忠实地还原目标主体的外观？** -> **`RefAdapter`**
2.  **如何从参考视频中精准地“剥离”出纯粹的动作信息，并注入到生成过程中？** -> **`FAE`**

    这两个模块基于一个强大的图生视频 (I2V) 扩散模型 `CogVideoX-I2V` 构建，并通过一个两阶段的训练策略进行优化，确保各模块功能明确，互不干扰。

下图（原文 Figure 3）展示了 `FlexiAct` 的整体框架，包括两个阶段的训练过程和最终的推理过程。

![该图像是示意图，展示了FlexiAct方法的工作流程，包括输入图像、动作提取和生成过程。图中各个模块如DiT Block和RefAdapter的功能被标注，强调了频率感知嵌入在动作提取过程中的重要性。](images/3.jpg)
*该图像是示意图，展示了FlexiAct方法的工作流程，包括输入图像、动作提取和生成过程。图中各个模块如DiT Block和RefAdapter的功能被标注，强调了频率感知嵌入在动作提取过程中的重要性。*

## 4.2. 核心方法详解 (逐层深入)

### 4.2.1. 基础模型：CogVideoX-I2V
`FlexiAct` 建立在 `CogVideoX-I2V` 模型之上。我们首先需要理解这个基础模型的工作方式。

*   **模型结构:** `CogVideoX-I2V` 是一个基于 `MMDiT` (Multi-modal Diffusion Transformer) 架构的潜在扩散模型 (Latent Diffusion Model)。它在 VAE 编码器压缩后的<strong>潜在空间 (Latent Space)</strong> 中进行去噪操作。
*   **输入处理:**
    *   **视频输入:** 原始视频 $\mathbf{V} \in \mathbb{R}^{T \times H \times W \times 3}$ (T帧，高H，宽W，3通道) 通过一个 3D VAE 编码器 $\epsilon$ 被压缩成潜在表示 $L_{video} \in \mathbb{R}^{\frac{T}{4} \times \frac{H}{8} \times \frac{W}{8} \times C}$。
    *   **图像输入:** 条件图像 $\mathbf{I} \in \mathbb{R}^{H \times W \times 3}$ (单帧) 也通过 VAE 编码器被压缩成 $L_{image} \in \mathbb{R}^{1 \times \frac{H}{8} \times \frac{W}{8} \times C}$。
*   **条件注入:** 在标准的 `CogVideoX-I2V` 中，为了让生成的视频内容与输入图像一致，模型会将 $L_{image}$ 在时间维度上进行填充 (zero-padding) 以匹配 $L_{video}$ 的时间维度，然后将它与加噪后的视频潜在表示 $L_{video}$ 在<strong>通道维度 (channel dimension)</strong> 上拼接 (concatenate) 起来，共同作为去噪网络的输入。
*   **训练目标:** 训练时，通常使用视频的第一帧作为条件图像。模型的任务是预测所添加的噪声，并用预测噪声与真实噪声之间的均方误差 (MSE) 作为损失函数进行优化。

### 4.2.2. 第一阶段：训练 RefAdapter
**目标:** 解决空间结构自适应和外观一致性保持的问题。

**挑战:** 直接使用 `CogVideoX-I2V` 有两个问题：
1.  **强约束:** 它默认将条件图像视为视频的第一帧，这种强约束不利于动作的灵活迁移，特别是当参考动作的起始姿态与目标图像的姿态不同时。
2.  **一致性降级:** 后续的动作提取过程可能会干扰模型原有的保持外观一致性的能力。

**`RefAdapter` 的解决方案:**
`RefAdapter` 是注入到 `CogVideoX-I2V` 的 `MMDiT` 层中的一组 `LoRA` 参数。它的训练过程与基础模型有两点关键不同，以打破上述强约束：

1.  **随机帧作为条件:** 在训练时，不再固定使用视频的第一帧作为条件图像，而是从原始视频中**随机抽取一帧**。这样做可以人为地在条件图像和视频的起始状态之间制造一个“鸿沟”，迫使模型学会即使在条件图像的姿态与视频第一帧的姿态不一致的情况下，也能生成连贯且内容正确的视频。这极大地增强了模型对**空间结构差异**的适应能力。
2.  **替换第一帧嵌入:** 标准 I2V 模型将条件图像作为视频的“起点”。为了让条件图像变成一个更灵活的“参考”而非“起点”，`RefAdapter` 在训练时，将视频潜在表示 $L_{video}$ 在时间维度上的第一个切片（即第一帧的潜在表示）**替换**为图像的潜在表示 $L_{image}$。这样，模型就能学会将这个位置的嵌入用作一个全局的外观参考，来指导整个视频的生成，而不是被它束缚住视频的起始状态。

    通过这种方式训练后，`RefAdapter` 使得基础模型具备了在给定任意参考图像（即使其姿态、布局与参考动作不匹配）的情况下，依然能生成外观一致的视频的能力。由于只训练 `LoRA` 参数，这个过程非常高效。

### 4.2.3. 第二阶段：训练 FAE (频率感知动作提取)
**目标:** 从参考视频中精准地提取动作信息。

**观察与直觉:** 作者发现，在扩散模型的去噪过程中，不同时间步 (timestep) $t$ 对信息的关注点是不同的。
*   <strong>早期去噪阶段 (t 很大，如 800):</strong> 此时输入还是大片噪声，模型主要关注构建视频的**全局结构和低频信息**，这其中就包括了主体的**运动轨迹和姿态变化**。
*   <strong>后期去噪阶段 (t 很小，如 200):</strong> 此时视频轮廓已基本清晰，模型转而关注生成**高频细节**，如纹理、背景、光影等**外观信息**。

    下图（原文 Figure 2）可视化了这一观察。在 $t=800$ 时，注意力集中在主体的运动部位；而在 $t=200$ 时，注意力分散到整个图像，关注背景等细节。

    ![该图像是示意图，展示了在不同时间步（800、600、200步）下，视频中体态转移到各类目标图像的过程。图像上部为原始视频，下方则为不同步长的频率特征图，展示了动作转移过程中在空间适应和细节保持方面的表现。](images/2.jpg)

    **`FAE` 的解决方案:**
`FAE` 利用了上述观察，设计了一种在推理时动态调整注意力权重的方法来提取动作。

1.  <strong>频率感知嵌入 (Frequency-aware Embedding) 的训练:</strong>
    *   对于每一个给定的参考视频，论文训练一组专属的、可学习的嵌入向量，称之为<strong>频率感知嵌入 (Frequency-aware Embedding)</strong>。
    *   在训练时，这些嵌入向量被拼接到 `MMDiT` 每一层输入的末尾。模型的目标是学习将参考视频的所有信息（包括动作和外观）都编码到这些嵌入中。
    *   为了防止模型“偷懒”只记住参考视频的布局，训练时会对输入视频进行**随机裁剪**。
    *   **重要:** 在这个训练阶段，**不加载** `RefAdapter`，以防止 `FAE` 的训练过程污染 `RefAdapter` 保持外观一致性的能力。

2.  **推理时的动作提取:**
    *   训练完成后，频率感知嵌入中混合了参考视频的动作和外观信息。
    *   在推理（生成）阶段，`FAE` 通过一个<strong>重加权策略 (reweighting strategy)</strong> 来引导模型只关注嵌入中的**动作信息**。
    *   具体来说，在<strong>早期去噪阶段 (t 很大)</strong>，`FAE` 会**增大**视频潜在表示对频率感知嵌入的注意力权重。这相当于告诉模型：“在构建视频的宏观运动时，请重点参考这些嵌入里的信息。”
    *   随着 $t$ 变小，这个增大的权重会平滑地衰减至零。到了后期去噪阶段，模型就不再额外关注这些嵌入，从而避免从嵌入中学习到参考视频的外观细节（如衣服、背景）。

        这个权重偏置 $W_{bias}$ 的计算公式如下，它被加到原始的注意力权重 $W_{ori}$ 上：

    $$
    W_{bias} = \begin{cases} \alpha, & t_l \le t \le T \\ \displaystyle \frac{\alpha}{2} \left[ \cos\left( \displaystyle \frac{\pi}{t_h - t_l} (t - t_l) \right) + 1 \right], & t_h \le t < t_l \\ 0, & 0 \le t < t_h \end{cases}
    $$

    **符号解释:**
    *   $W_{bias}$: 应用于原始注意力权重上的偏置值。
    *   $t$: 当前的去噪时间步，T 是总步数（例如 1000）。
    *   $\alpha$: 控制偏置强度的超参数，决定了对动作信息的关注强度。论文中设为 1。
    *   $t_l$: 低频时间步阈值（论文中设为 800）。当 $t \ge t_l$ 时，施加最大偏置 $\alpha$。
    *   $t_h$: 高频时间步阈值（论文中设为 700）。当 $t < t_h$ 时，不施加偏置。
    *   $\cos(\cdot)$ 函数: 在 $t_h$ 和 $t_l$ 之间创建一个平滑的过渡，使得权重的变化不是突兀的，而是逐渐减小的。这对于平衡动作和外观至关重要。

### 4.2.4. FlexiAct 的完整流程
**训练:**
1.  **阶段一:** 在一个大规模视频数据集上训练 `RefAdapter`，使其学会空间适应和外观保持。
2.  **阶段二:** 针对**每一个**想要迁移的参考视频，单独训练一组对应的**频率感知嵌入**。

**推理/生成:**
1.  加载预训练的 `CogVideoX-I2V` 模型。
2.  加载训练好的 `RefAdapter` 权重。
3.  加载针对特定参考视频训练好的**频率感知嵌入**。
4.  输入任意的目标图像。
5.  在扩散模型的去噪过程中，应用 `FAE` 的动态重加权策略。
6.  最终生成一段视频，其中目标图像的主体执行了参考视频中的动作。

# 5. 实验设置

## 5.1. 数据集
*   **训练数据集:**
    *   `RefAdapter` 的训练使用了 **Miradata** 数据集中的 42,000 个视频。Miradata 是一个包含长时程和结构化字幕的大规模视频数据集，其多样性有助于模型学习通用的空间适应能力。
*   **评估数据集:**
    *   作者构建了一个包含 **250 个视频-图像对**的评估集。
    *   该数据集包含 **25 种不同的动作类别**，涵盖人类（如瑜伽、健身）和动物（如跳跃、奔跑）的动作。
    *   每种动作都对应 **10 个不同的目标图像**，这些图像包括真实人类、动物、动画角色和游戏角色。
    *   这种多样化的设计确保了能够全面评估方法在各种异构场景下的泛化能力。

## 5.2. 评估指标
论文使用了四项自动评估指标和一项人工评估。

### 5.2.1. Text Similarity (文本相似度)
*   **概念定义:** 该指标用于衡量生成的视频内容与给定的文本提示 (prompt) 在语义上的一致性。分数越高，表示视频内容越符合文本描述。
*   **数学公式:** 通常使用预训练的视觉-语言模型（如 CLIP）来计算。对于视频中的每一帧 $f_i$ 和文本提示 $p$，计算它们的特征向量的余弦相似度，然后取平均值。
    $$
    \text{Text Similarity} = \frac{1}{N} \sum_{i=1}^{N} \frac{\text{CLIP}_{img}(f_i) \cdot \text{CLIP}_{txt}(p)}{\|\text{CLIP}_{img}(f_i)\| \cdot \|\text{CLIP}_{txt}(p)\|}
    $$
*   **符号解释:**
    *   $N$: 视频的总帧数。
    *   $f_i$: 视频的第 $i$ 帧。
    *   $p$: 输入的文本提示。
    *   $\text{CLIP}_{img}(\cdot)$: CLIP 的图像编码器，用于提取图像特征。
    *   $\text{CLIP}_{txt}(\cdot)$: CLIP 的文本编码器，用于提取文本特征。
    *   $\cdot$: 向量点积。
    *   $\|\cdot\|$: 向量的L2范数。

### 5.2.2. Motion Fidelity (运动保真度)
*   **概念定义:** 该指标用于量化生成视频中的运动与参考视频中的运动的相似程度。它尤其适用于评估**未对齐**视频之间的运动一致性，这正是本论文场景的核心。分数越高，表示动作模仿得越像。
*   **计算方法:** 论文遵循 [Yatim et al. 2023] 的方法，使用一个名为 `CoTracker` 的跟踪模型来计算视频中物体的运动轨迹 (tracklets)，然后比较生成视频和参考视频中对应轨迹的相似性。由于其计算复杂，通常不表示为单一封闭公式，而是依赖于 `CoTracker` 模型的输出。

### 5.2.3. Temporal Consistency (时间一致性)
*   **概念定义:** 该指标衡量视频序列的平滑度和连贯性，即相邻帧之间或整个视频帧之间的内容变化是否自然，有无闪烁或突变。分数越高，视频看起来越流畅。
*   **数学公式:** 计算视频中所有帧对之间的 CLIP 图像特征相似度的平均值。
    $$
    \text{Temporal Consistency} = \frac{2}{N(N-1)} \sum_{i=1}^{N} \sum_{j=i+1}^{N} \frac{\text{CLIP}_{img}(f_i) \cdot \text{CLIP}_{img}(f_j)}{\|\text{CLIP}_{img}(f_i)\| \cdot \|\text{CLIP}_{img}(f_j)\|}
    $$
*   **符号解释:**
    *   $N$: 视频的总帧数。
    *   $f_i, f_j$: 视频的第 $i$ 帧和第 $j$ 帧。
    *   $\text{CLIP}_{img}(\cdot)$: CLIP 的图像编码器。

### 5.2.4. Appearance Consistency (外观一致性)
*   **概念定义:** 该指标衡量生成的视频内容与作为条件的**目标图像**在外观上的相似度。分数越高，表示生成的主体越好地保持了原始图像中的身份和外观特征。
*   **数学公式:** 计算生成视频的第一帧与所有后续帧之间的平均 CLIP 图像特征相似度。（*注：原文描述为“第一帧和其余帧”，但更标准的做法是计算条件图像与所有生成帧的相似度，此处遵循原文描述的计算方式*）。
    $$
    \text{Appearance Consistency} = \frac{1}{N-1} \sum_{i=2}^{N} \frac{\text{CLIP}_{img}(f_1) \cdot \text{CLIP}_{img}(f_i)}{\|\text{CLIP}_{img}(f_1)\| \cdot \|\text{CLIP}_{img}(f_i)\|}
    $$
*   **符号解释:**
    *   $N$: 视频的总帧数。
    *   $f_1$: 视频的第一帧。
    *   $f_i$: 视频的第 $i$ 帧 ($i > 1$)。

### 5.2.5. Human Evaluation (人工评估)
邀请 5 位评估者对生成视频的**运动一致性**（与参考视频相比）和**外观一致性**（与目标图像相比）进行两两比较打分。

## 5.3. 对比基线
*   **MotionDirector (MD-I2V):** `MotionDirector` 是一个强大的全局运动迁移方法。为了公平比较，作者在与 `FlexiAct` 相同的 `CogVideoX-I2V` 主干网络上重新实现了它，称为 `MD-I2V`。
*   <strong>BaseModel (基础模型):</strong> 这是一个简化版的模型，它直接通过训练可学习的嵌入来学习动作，但<strong>不使用 <code>RefAdapter</code> 和 <code>FAE</code></strong>。这个基线用于验证 `FlexiAct` 两个核心组件的有效性。
*   <strong>Ablation Models (消融模型):</strong>
    *   `w/o FAE`: 移除了 `FAE` 模块的 `FlexiAct`。
    *   `w/o RefAdapter`: 移除了 `RefAdapter` 模块的 `FlexiAct`。

        这些基线的选择具有代表性，能够从不同角度验证 `FlexiAct` 相对于现有技术和其内部组件的优越性。

# 6. 实验结果与分析

## 6.1. 核心结果分析
论文的核心实验结果展示在 Table 1 中，包括了自动评估和人工评估的结果。

以下是原文 Table 1 的结果：

<table>
<thead>
<tr>
<th rowspan="2">Method</th>
<th colspan="4">Automatic Evaluations</th>
<th colspan="3">Human Evaluations</th>
</tr>
<tr>
<th>Text Similarity ↑</th>
<th>Motion Fidelity ↑</th>
<th>Temporal Consistency ↑</th>
<th>Appearance Consistency ↑</th>
<th></th>
<th>Motion Consistency ↑</th>
<th>Appearance Consistency ↑</th>
</tr>
</thead>
<tbody>
<tr>
<td>MD-I2V [Zhao et al. 2023]</td>
<td>0.2446</td>
<td>0.3496</td>
<td>0.9276</td>
<td>0.8963</td>
<td>v.s. Base Model</td>
<td>47.2 v.s. 52.8</td>
<td>53.1 v.s. 46.9</td>
</tr>
<tr>
<td>Base Model</td>
<td>0.2541</td>
<td>0.3562</td>
<td>0.9283</td>
<td>0.8951</td>
<td>v.s. Base Model</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>w/o FAE</td>
<td>0.2675</td>
<td>0.3614</td>
<td>0.9255</td>
<td>0.9134</td>
<td>v.s. Base Model</td>
<td>59.7 v.s. 40.3</td>
<td>76.4 v.s. 23.6</td>
</tr>
<tr>
<td>w/o RefAdapter</td>
<td>0.2640</td>
<td>0.3856</td>
<td>0.9217</td>
<td>0.9021</td>
<td>v.s. Base Model</td>
<td>68.6 v.s. 31.4</td>
<td>52.2 v.s. 47.8</td>
</tr>
<tr>
<td>Ours</td>
<td><strong>0.2732</strong></td>
<td><strong>0.4103</strong></td>
<td><strong>0.9342</strong></td>
<td><strong>0.9162</strong></td>
<td>v.s. Base Model</td>
<td><strong>79.5 v.s. 20.5</strong></td>
<td><strong>78.3 v.s. 21.7</strong></td>
</tr>
</tbody>
</table>

**分析:**
1.  <strong>`FlexiAct` (Ours) 的全面领先:</strong> 完整的 `FlexiAct` 方法在所有四项自动评估指标上均取得了最佳成绩。尤其是在最关键的两个指标上：
    *   <strong>Motion Fidelity (运动保真度):</strong> `FlexiAct` 的得分为 0.4103，显著高于所有其他方法，证明了其在精准复现参考动作方面的卓越能力。
    *   <strong>Appearance Consistency (外观一致性):</strong> `FlexiAct` 的得分为 0.9162，同样是最高分，表明其在执行复杂动作迁移的同时，能极好地保持目标主体的身份特征。
2.  **与 `MD-I2V` 和 `BaseModel` 的对比:** `FlexiAct` 在运动保真度和外观一致性上远超 `MD-I2V` 和 `BaseModel`。这说明传统的全局运动方法或简单的嵌入学习方法，难以应对异构场景下的挑战。
3.  **人工评估结果:** 人工评估的结果与自动评估高度一致。在与 `BaseModel` 的对比中，`FlexiAct` (Ours) 在**运动一致性**和**外观一致性**方面分别获得了 79.5% 和 78.3% 的偏好度，呈现出压倒性优势。这直观地表明，人类观察者认为 `FlexiAct` 生成的视频在动作和外观两方面都质量更高。

**定性结果分析:**
下图（原文 Figure 5）展示了定性对比结果，更加直观地体现了 `FlexiAct` 的优势。

![该图像是一个示意图，展示了动作定制的方法对比。第一行是参考视频，第二行是MD-12V模型结果，第三行是基础模型结果，第四行则是我们的方法效果。该图像展示了不同模型在动作转移任务中的表现，突出我们方法的优越性。](images/5.jpg)
*该图像是一个示意图，展示了动作定制的方法对比。第一行是参考视频，第二行是MD-12V模型结果，第三行是基础模型结果，第四行则是我们的方法效果。该图像展示了不同模型在动作转移任务中的表现，突出我们方法的优越性。*

*   **`MD-I2V` 的问题:** 生成的动作不准确（例如，下蹲后没有站起），且出现伪影（眼睛闭合）。
*   **`BaseModel` 的问题:** 动作不准确（腿部抬升过高），并且出现了外观漂移（身上出现类似衣服的褶皱，改变了原始图像）。
*   **`FlexiAct` 的表现:** 动作精准，外观稳定。生成的角色既准确地模仿了参考视频的动作，又完美地保持了原始图像中的人物形象。

## 6.2. 消融实验/参数分析
消融实验（Ablation Study）旨在验证模型中每个组件的必要性。Table 1 中的 `w/o FAE` 和 `w/o RefAdapter` 即为消融实验的结果。

<strong>1. FAE 的作用 (对比 Ours 和 w/o FAE):</strong>
*   **定量分析:** 移除 `FAE` 后，`Motion Fidelity` 从 0.4103 大幅下降到 0.3614，这是所有模型中最显著的性能下降之一。这强有力地证明了 **`FAE` 是精准提取和控制动作的关键**。
*   **定性分析:** 下图（原文 Figure 6）的消融实验部分显示，没有 `FAE` 时，模型无法准确复现动作细节。例如，在伸展动作中，角色只是简单地举手，而没有做出正确的弯曲和伸展；在另一个例子中，动作也与参考视频不符。

    ![该图像是示意图，展示了使用不同方法进行动作转移的效果对比。第一行是参考视频，接下来的行分别展示了使用和不使用FAE及RefAda的效果，可以观察到动作传递的变化和相应的适应性。](images/6.jpg)

    <strong>2. RefAdapter 的作用 (对比 Ours 和 w/o RefAdapter):</strong>
*   **定量分析:** 移除 `RefAdapter` 后，`Appearance Consistency` 从 0.9162 下降到 0.9021，同时 `Motion Fidelity` 也从 0.4103 下降到 0.3856。这说明 `RefAdapter` 不仅对保持外观至关重要，也间接影响了动作迁移的质量，因为它为动作提供了必要的**空间自适应能力**。没有它，模型难以将动作适配到不同空间结构的目标上，从而导致动作和外观双双降级。
*   **定性分析:** Figure 6 显示，没有 `RefAdapter` 时，生成视频中角色的面部和服装细节与目标图像存在明显差异。在另一个例子中，手臂没有完全伸展。这表明 `RefAdapter` 在**确保外观细节一致性**和**辅助动作正确适配**方面都扮演了重要角色。

<strong>3. 偏置过渡函数的作用 (Ablation of bias transition):</strong>
下图（原文 Figure 11）展示了 `FAE` 中权重偏置过渡函数的重要性。

![Fig. 11. Ablation of bias transition.](images/11.jpg)
*该图像是图11，展示了偏差转移的消融实验。左侧为目标图像和参考视频，中间分别为没有转移和使用我们的方法的结果，展示了在不同条件下的效果对比。*

*   <strong>无过渡函数 (硬切换):</strong> 如果在 $t=700$ 时突然撤销偏置，模型会错误地从参考视频学习外观信息（如图中人物穿上了参考视频中的衣服）。如果在 $t=800$ 时才撤销，则会导致动作不准确。
*   <strong>有过渡函数 (平滑切换):</strong> 使用 `cos` 函数在 $t=800$ 到 $t=700$ 之间平滑过渡，可以完美地平衡动作学习和外观保持，生成既动作准确又外观正确的视频。这证明了 `FAE` 中频率感知设计的精妙之处。

# 7. 总结与思考

## 7.1. 结论总结
`FlexiAct` 是一项在视频动作迁移领域的突破性工作。它成功地解决了现有方法在处理**异构场景**时的核心痛点。
*   **主要贡献:** 论文提出了一个新颖的 `FlexiAct` 框架，能够将参考视频中的动作灵活地迁移到具有不同空间结构（布局、骨骼、视角）的任意目标主体上，同时保持出色的动作准确性和外观一致性。
*   **核心创新:**
    *   `RefAdapter` 模块通过高效的参数微调，巧妙地解决了空间自适应和外观保持的难题。
    *   `FAE` 方法基于对扩散模型去噪过程的深刻洞察，提出了一种全新的、在采样过程中直接提取动作信息的机制，既高效又精准。
*   **实验验证:** 全面的定量和定性实验，包括与最先进方法的对比和详尽的消融研究，都有力地证明了 `FlexiAct` 的优越性和其核心组件的有效性。

## 7.2. 局限性与未来工作
论文作者也坦诚地指出了当前工作的局限性，并展望了未来的研究方向：
*   **局限性:** `FlexiAct` 和其他类似方法（如 `MotionDirector`）一样，需要为**每一个参考视频进行单独的优化**（即训练专属的频率感知嵌入）。这限制了它的实时性和即时可用性。你无法拿来一个新视频就立刻进行动作迁移。
*   **未来工作:** 一个关键的未来研究方向是开发<strong>前馈式 (feed-forward)</strong> 的异构场景动作迁移方法。这样的方法将能够直接从任意参考视频中提取动作并应用于目标图像，而无需针对每个视频进行耗时的单独训练，这将极大地提升该技术的实用价值。

## 7.3. 个人启发与批判
这篇论文带来了几点深刻的启发，同时也引人思考其更深层次的潜力与挑战。

*   **对扩散模型内部机制的洞察:** `FAE` 的成功是本文最令人兴奋的亮点之一。它没有设计复杂的外部网络，而是通过深入观察和利用扩散模型去噪过程本身的特性（不同时间步关注不同频率信息）来解决问题。这为未来的研究开辟了一个新思路：与其不断堆叠模块，不如更深入地理解和利用现有强大模型的内在机理。

*   <strong>“解耦”</strong>思想的又一次胜利: 从 `ControlNet` 的内容与控制解耦，到 `MotionDirector` 的外观与运动解耦，再到 `FlexiAct` 的动作与空间结构解耦，我们可以看到“解耦”是解决复杂生成任务的一个核心且有效的设计哲学。`FlexiAct` 将这一思想在更具挑战性的异构场景中推进了一步。

*   **潜在的应用价值:** `FlexiAct` 极大地拓宽了动作迁移的应用范围。它不仅能用于同类角色动画（人到人），还能轻松实现跨物种（人到动物，如图10所示）、跨领域（真人到卡通角色）的动画制作。这在电影特效、游戏开发、虚拟化身驱动、创意短视频制作等领域拥有巨大的应用潜力，有望大幅降低专业动画制作的门槛。

*   **批判性思考与待探索的问题:**
    *   **动作复杂度的天花板:** 论文展示的动作多为周期性或单次性的全身运动。对于包含复杂手势、精细面部表情或长序列交互的动作，`FAE` 所捕捉的“低频信息”是否足够？模型能否准确迁移这种高频、精细的动作细节，仍有待进一步验证。
    *   <strong>“动作”</strong>的可解释性: 频率感知嵌入虽然有效，但它仍然是一个“黑箱”。我们无法明确地知道哪些向量对应了“抬手”，哪些对应了“转身”。未来能否设计出更具可解释性的动作表示，甚至可以对提取出的动作进行编辑（如“让动作再快一点”或“只迁移上半身动作”），将是一个非常有趣的方向。
    *   **对失败案例的分析:** 论文展示了大量的成功案例，但对于何种情况下 `FlexiAct` 可能会失败（例如，极端视角差异、目标主体与动作的物理可能性完全不符等），缺乏深入的分析。理解方法的边界和失效模式，对于其实际应用同样重要。