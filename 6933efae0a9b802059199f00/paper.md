# Mastering Diverse Domains through World Models

Danijar Hafner,2 Jurgis Pasukonis, Jimmy Ba2 Timothy Lillicrap1

# Abstract

Developing a general algorithm that learns to solve tasks across a wide range of applications has been a fundamental challenge in artificial intelligence. Although current reinforcement learning algorithms can be readily applied to tasks similar to what they have been developed for, configuring them for new application domains requires significant human expertise and experimentation. We present DreamerV3, a general algorithm that outperforms specialized methods across over 150 diverse tasks, with a single configuration. Dreamer learns a model of the environment and improves its behavior by imagining future scenarios. Robustness techniques based on normalization, balancing, and transformations enable stable learning across domains. Applied out of the box, Dreamer is the first algorithm to collect diamonds in Minecraft from scratch without human data or curricula. This achievement has been posed as a significant challenge in artificial intelligence that requires exploring farsighted strategies from pixels and sparse rewards in an open world. Our work allows solving challenging control problems without extensive experimentation, making reinforcement learning broadly applicable.

![](images/1.jpg)  

Figure 1: Benchmark summary. a, Using fixed hyperparameters across all domains, Dreamer outperforms tuned expert algorithms across a wide range of benchmarks and data budgets. Dreamer also substantially outperforms a high-quality implementation of the widely applicable PPO algorithm. b, Applied out of the box, Dreamer learns to obtain diamonds in the popular video game Minecraft from scratch given sparse rewards, a long-standing challenge in artificial intelligence for which previous approaches required human data or domain-specific heuristics.

![](images/2.jpg)  

Figure 2: Diverse visual domains used in the experiments. Dreamer succeeds across these domains, ranging from robot locomotion and manipulation tasks over Atari games, procedurally generated ProcGen levels, and DMLab tasks, that require spatial and temporal reasoning, to the complex and infinite world of Minecraft. We also evaluate Dreamer on non-visual domains.

# Introduction

Reinforcement learning has enabled computers to solve tasks through interaction, such as surpassing humans in the games of Go and Dota1.2. It is also a key component for improving large language models beyond what is demonstrated in their pretraining data3.4. While $\mathrm { P P O } ^ { 5 }$ has become a standard algorithm in the field of reinforcement learning, more specialized algorithms are often employed to achieve higher performance. These specialized algorithms target the unique challenges posed by different applicatin domains, such as contiuus control, discrte actons parse ewars, image inputs10, spatial environments11, and board games12. However, applying reinforcement learning algorithms to sufficiently new tasks—such as moving from video games to robotics tasks— requires substantial effort, expertise, and computational resources for tweaking the hyperparameters of the algorithm 13. This brittleness poses a bottleneck in applying reinforcement learning to new problems and also limits the applicability of reinforcement learning to computationally expensive models or tasks where tuning is prohibitive. Creating a general algorithm that learns to master new domains without having to be reconfigured has been a central challenge in artificial intelligence and would open up reinforcement learning to a wide range of practical applications. We present Dreamer, a general algorithm that outperforms specialized expert algorithms across a wide range of domains while using fixed hyperparameters, making reinforcement learning readily applicable to new problems. The algorithm is based on the idea of learning a world model that equips the outcomes of potential actions, a critic neural network judges the value of each outcome, and an actor neural network chooses actions to reach the best outcomes. Although intuitively appealing, robustly learning and leveraging world models to achieve strong task performance has been an open problem 17. Dreamer overcomes this challenge through a range of robustness techniques based on normalization, balancing, and transformations. We observe robust learning not only across over 150 tasks from the domains summarized in Figure 2, but also across model sizes and training budgets, offering a predictable way to increase performance. Notably, larger model sizes not only achieve higher scores but also require less interaction to solve a task. To push the boundaries of reinforcement learning, we consider the popular video game Minecraft that has become a focal point of research in recent years 18,19,20, with international competitions held for developing algorithms that autonomously learn to collect diamonds in Minecraft\*. Solving this problem without human data has been widely recognized as a substantial challenge for artificial intelligence because of the sparse rewards, exploration difficulty, long time horizons, and the procedural diversity of this open world game18. Due to these obstacles, previous approaches resorted to using human expert data and domain-specific curricula19,20. Applied out of the box, Dreamer is the first algorithm to collect diamonds in Minecraft from scratch.

![](images/3.jpg)  

Figure 3: Training process of Dreamer. The world model encodes sensory inputs into discrete representations $z _ { t }$ that are predicted by a sequence model with recurrent state $h _ { t }$ given actions $a _ { t }$ . The inputs are reconstructed to shape the representations. The actor and critic predict actions $a _ { t }$ and values $v _ { t }$ and learn from trajectories of abstract representations predicted by the world model.

# Learning algorithm

We present the third generation of the Dreamer algorithm 21,22. The algorithm consists of three neural networks: the world model predicts the outcomes of potential actions, the critic judges the value of each outcome, and the actor chooses actions to reach the most valuable outcomes. The components are trained concurrently from replayed experience while the agent interacts with the environment. To succeed across domains, all three components need to accommodate different signal magnitudes and robustly balance terms in their objectives. This is challenging as we are not only targeting similar tasks within the same domain but aim to learn across diverse domains with fixed hyperparameters. This section introduces the world model, critic, and actor along with their robust loss functions, as well as tools for robustly predicting quantities of unknown orders of magnitude.

# World model learning

The world model learns compact representations of sensory inputs through autoencoding23 and enables planning by predicting future representations and rewards for potential actions. We implement the world model as a Recurrent State-Space Model $( \mathrm { R S S M } ) ^ { 2 4 }$ , shown in Figure 3. First, an encoder maps sensory inputs $x _ { t }$ to stochastic representations $z _ { t }$ . Then, a sequence model with recurrent state $h _ { t }$ predicts the sequence of these representations given past actions $a _ { t - 1 }$ . The concatenation of $h _ { t }$ and $z _ { t }$ forms the model state from which we predict rewards $r _ { t }$ and episode continuation flags $c _ { t } \in \{ 0 , 1 \}$ and reconstruct the inputs to ensure informative representations:

![](images/4.jpg)  

Figure 4: Multi-step video predictions of a DMLab maze (top) and a quadrupedal robot (bottom). Given 5 context images and the full action sequence, the model predicts 45 frames into the future without access to intermediate images. The world model learns an understanding of the underlying structure of each environment.

Sequence model: $\begin{array}{c} { \scriptstyle h _ { t } = f _ { \phi } ( h _ { t - 1 } , z _ { t - 1 } , a _ { t - 1 } ) } \\ { { \scriptstyle z _ { t } \sim q _ { \phi } ( z _ { t } \mid h _ { t } , x _ { t } ) } } \\ { { \scriptstyle \hat { z } _ { t } \sim p _ { \phi } ( \hat { z } _ { t } \mid h _ { t } ) } } \\ { { \scriptstyle \hat { r } _ { t } \sim p _ { \phi } ( \hat { r } _ { t } \mid h _ { t } , z _ { t } ) } } \\ { { \scriptstyle \hat { c } _ { t } \sim p _ { \phi } ( \hat { c } _ { t } \mid h _ { t } , z _ { t } ) } } \\ { { \scriptstyle \hat { x } _ { t } \sim p _ { \phi } ( \hat { x } _ { t } \mid h _ { t } , z _ { t } ) } } \end{array} $ RSSM Encoder: Dynamics predictor: Reward predictor: Continue predictor: Decoder:

Figure 4 visualizes long-term video predictions of the world world. The encoder and decoder use convolutional neural networks (CNN) for image inputs and multi-layer perceptrons (MLPs) for vector inputs. The dynamics, reward, and continue predictors are also MLPs. The representations are sampled from a vector of softmax distributions and we take straight-through gradients through the sampling step25,22. Given a sequence batch of inputs $x _ { 1 : T }$ ,actions $a _ { 1 : T }$ , rewards $r _ { 1 : T }$ ,and continuation flags $c _ { 1 : T }$ , the world model parameters $\phi$ are optimized end-to-end to minimize the prediction loss $\mathcal { L } _ { \mathrm { p r e d } }$ , the dynamics loss $\mathcal { L } _ { \mathrm { d y n } }$ , and the representation loss $\mathcal { L } _ { \mathrm { r e p } }$ with corresponding loss weights $\beta _ { \mathrm { p r e d } } = 1$ , $\beta _ { \mathrm { d y n } } = 1$ , and $\beta _ { \mathrm { r e p } } = 0 . 1$ :

$$
\begin{array} { r } { \mathcal { L } ( \phi ) \doteq \mathrm { E } _ { q _ { \phi } } \Big [ \sum _ { t = 1 } ^ { T } ( \beta _ { \mathrm { p r e d } } \mathcal { L } _ { \mathrm { p r e d } } ( \phi ) + \beta _ { \mathrm { d y n } } \mathcal { L } _ { \mathrm { d y n } } ( \phi ) + \beta _ { \mathrm { r e p } } \mathcal { L } _ { \mathrm { r e p } } ( \phi ) ) \Big ] . } \end{array}
$$

The prediction loss trains the decoder and reward predictor via the symlog squared loss described later, and the continue predictor via logistic regression. The dynamics loss trains the sequence model to predict the next representation by minimizing the KL divergence between the predictor $p _ { \phi } ( z _ { t } \mid h _ { t } )$ and the next stochastic representation $q _ { \phi } ( z _ { t } \mid h _ { t } , x _ { t } )$ . The representation loss, in turn, trains the representations to become more predictable allowing us to use a factorized dynamics predictor for fast sampling during imagination training. The two losses differ in the stop-gradient operator $\operatorname { s g } ( \cdot )$ and their loss scale. To avoid a degenerate solution where the dynamics are trivial to predict but fail to contain enough information about the inputs, we employ free bits 26 by clipping the dynamics and representation losses below the value of 1 nat $\approx 1 . 4 4$ bits. This disables them while they are already minimized well to focus learning on the prediction loss:

$$
\begin{array} { r l } & { \mathcal { L } _ { \mathrm { p r e d } } ( \phi ) \doteq - \ln p _ { \phi } ( x _ { t } \mid z _ { t } , h _ { t } ) - \ln p _ { \phi } ( r _ { t } \mid z _ { t } , h _ { t } ) - \ln p _ { \phi } ( c _ { t } \mid z _ { t } , h _ { t } ) } \\ & { \mathcal { L } _ { \mathrm { d y n } } ( \phi ) \doteq \operatorname* { m a x } \bigl ( 1 , \mathrm { K L } \bigl [ \operatorname* { s g } ( q _ { \phi } ( z _ { t } \mid h _ { t } , x _ { t } ) ) \bigr \| \bigr . \qquad \left. p _ { \phi } ( z _ { t } \mid h _ { t } ) \right] \bigr ) } \\ & { \mathcal { L } _ { \mathrm { r e p } } ( \phi ) \doteq \operatorname* { m a x } \bigl ( 1 , \mathrm { K L } \bigl [ \mathstrut \quad \left. q _ { \phi } ( z _ { t } \mid h _ { t } , x _ { t } ) \bigr \rvert \bigr ] \mathstrut \right. \qquad \left. \operatorname { s g } ( p _ { \phi } ( z _ { t } \mid h _ { t } ) ) \right] \bigr ) } \end{array}
$$

Previous world models require scaling the representation loss differently based on the visual complexity of the environment21. Complex 3D environments contain details unnecessary for control and thus prompt a stronger regularizer to simplify the representations and make them more predictable. In games with static backgrounds and where individual pixels may matter for the task, a weak regularizer is required to extract fine details. We find that combining free bits with a small representation loss resolves this dilemma, allowing for fixed hyperparameters across domains. Moreover, we transform vector observations using the symlog function described later, to prevent large inputs and large reconstruction gradients, further stabilizing the trade-off with the representation loss. We occasionally observed spikes the in KL losses in earlier experiments, consistent with reports for deep variational autoencoders27. To prevent this, we parameterize the categorical distributions of the encoder and dynamics predictor as mixtures of $1 \%$ uniform and $9 9 \%$ neural network output, making it impossible for them to become deterministic and thus ensuring well-behaved KL losses. Further model details and hyperparameters are included in the supplementary material.

# Critic learning

The actor and critic neural networks learn behaviors purely from abstract trajectories of representations predicted by the world model 14. For environment interaction, we select actions by sampling from the actor network without lookahead planning. The actor and critic operate on model states $s _ { t } \doteq \{ h _ { t } , z _ { t } \}$ and thus benefit from the Markovian representations learned by the recurrent world el $\begin{array} { r } { R _ { t } \doteq \sum _ { \tau = 0 } ^ { \infty } \gamma ^ { \tau } r _ { t + \tau } } \end{array}$ with a discount factor $\gamma = 0 . 9 9 7$ for each model state. To consider rewards beyond the prediction horizon $T = 1 6$ , the critic learns to approximate the distribution of returns 28 for each state under the current actor behavior:

$$
{ \mathrm { A c t o r : } } \qquad a _ { t } \sim \pi _ { \theta } ( a _ { t } \mid s _ { t } ) \qquad { \mathrm { C r i t i c : } } \qquad v _ { \psi } ( R _ { t } \mid s _ { t } )
$$

Starting from representations of replayed inputs, the world model and actor generate a trajectory of imagined model states $s _ { 1 : T }$ , actions $a _ { 1 : T }$ , rewards $r _ { 1 : T }$ , and continuation flags $c _ { 1 : T }$ .Because the critic predicts a distribution, we read out its predicted values $v _ { t } \doteq \mathrm { E } \bigl [ v _ { \psi } ( \cdot \mid s _ { t } ) \bigr ]$ as the expectation of the distribution. To estimate returns that consider rewards beyond the prediction horizon, we compute bootstrapped $\lambda$ -returns 29 that integrate the predicted rewards and the values. The critic learns to predict the distribution of the return estimates $R _ { t } ^ { \lambda }$ using the maximum likelihood loss:

$$
\begin{array} { r } { \mathcal { L } ( \psi ) \dot { = } - \sum _ { t = 1 } ^ { T } \ln p _ { \psi } ( R _ { t } ^ { \lambda } \mid s _ { t } ) \qquad R _ { t } ^ { \lambda } \doteq r _ { t } + \gamma c _ { t } \Big ( ( 1 - \lambda ) v _ { t } + \lambda R _ { t + 1 } ^ { \lambda } \Big ) \qquad R _ { T } ^ { \lambda } \doteq ( 0 , T ) \lVert \dot { \phi } \rVert _ { \infty } , } \end{array}
$$

While a simple choice would be to parameterize the critic as a Normal distribution, the return distribution can have multiple modes and vary by orders of magnitude across environments. To stabilize and accelerate learning under these conditions, we parameterize the critic as categorical distribution with exponentially spaced bins, decoupling the scale of gradients from the prediction targets as described later. To improve value prediction in environments where rewards are challenging to predict, we apply the critic loss both to imagined trajectories with loss scale $\beta _ { \mathrm { v a l } } = 1$ and to trajectories sampled from the replay buffer with loss scale $\beta _ { \mathrm { r e p v a l } } = 0 . 3$ The critic replay loss uses the imagination returns $R _ { t } ^ { \lambda }$ at the start states of the imagination rollouts as on-policy value annotations for the replay trajectory to then compute $\lambda$ -returns over the replay rewards. Because the critic regresses targets that depend on its own predictions, we stabilize learning by regularizing the critic towards predicting the outputs of an exponentially moving average of its own parameters. This is similar to target networks used previously in reinforcement learning7 but allows us to compute returns using the current critic network. We further noticed that the randomly initialized reward predictor and critic networks at the start of training can result in large predicted rewards that can delay the onset of learning. We thus initialize the output weight matrix of the reward predictor and critic to zeros, which alleviates the problem and accelerates early learning.

# Actor learning

The actor learns to choose actions that maximize return while exploring through an entropy regularizer30. However, the correct scale for this regularizer depends both on the scale and frequency of rewards in the environment. Ideally, we would like the agent to explore more if rewards are sparse and exploit more if rewards are dense or nearby. At the same time, the exploration amount should not be influenced by arbitrary scaling of rewards in the environment. This requires normalizing the return scale while preserving information about reward frequency. To use a fixed entropy scale of $\eta = 3 \times 1 0 ^ { - 4 }$ across domains, we normalize returns to be approximately contained in the interval $[ 0 , 1 ]$ . In practice, substracting an offset from the returns does not change the actor gradient and thus dividing by the range $S$ is sufficient. Moreover, to avoid amplifying noise from function approximation under sparse rewards, we only scale down large return magnitudes but leave small returns below the threshold of $L = 1$ untouched. We use the Reinforce estimator31 for both discrete and continuous actions, resulting in the surrogate loss function:

$$
\begin{array} { r } { \mathcal { L } ( \boldsymbol { \theta } ) \dot { = } - \sum _ { t = 1 } ^ { T } \mathrm { s g } \Big ( \big ( R _ { t } ^ { \lambda } - v _ { \psi } ( s _ { t } ) \big ) / \operatorname* { m a x } ( 1 , S ) \Big ) \log \pi _ { \theta } ( a _ { t } \mid s _ { t } ) + \eta \mathrm { H } \left[ \pi _ { \theta } ( a _ { t } \mid s _ { t } ) \right] , } \end{array}
$$

The return distribution can be multi-modal and include outliers, especially for randomized environments where some episodes have higher achievable returns than others. Normalizing by the smallest and largest observed returns would then scale returns down too much and may cause suboptimal convergence. To be robust to these outliers, we compute the range from the $5 ^ { \mathrm { t h } }$ to the $9 5 ^ { \mathrm { t h } }$ return percentile over the return batch and smooth out the estimate using an exponential moving average:

$$
S \doteq \mathrm { E M A } \big ( \mathrm { P e r } ( R _ { t } ^ { \lambda } , 9 5 ) - \mathrm { P e r } ( R _ { t } ^ { \lambda } , 5 ) , 0 . 9 9 \big )
$$

Previous work typically normalizes advantages5 rather than returns, which puts a fixed amount of emphasis on maximizing returns over entropy regardless of whether rewards are within reach. Scaling up advantages when rewards are sparse can amplify noise that outweighs the entropy regularizer and stagnates exploration. Normalizing rewards or returns by standard deviation can fail under sparse rewards where their standard deviation is near zero, drastically amplifying rewards regardless of their size. Constrained optimization targets a fixed entropy on average across states 32,33 regardless of achievable returns, which is robust but explores slowly under sparse rewards and converges lower under dense rewards. We did not find stable hyperparameters across domains for these approaches. Return normalization with a denominator limit overcomes these challenges, exploring rapidly under sparse rewards and converging to high performance across diverse domains.

# Robust predictions

Reconstructing inputs and predicting rewards and returns can be challenging because the scale of these quantities can vary across domains. Predicting large targets using a squared loss can lead to divergence whereas absolute and Huber losses7 stagnate learning. On the other hand, normalizing targets based on running statistics5 introduces non-stationarity into the optimization. We suggest the symlog squared error as a simple solution to this dilemma. For this, a neural network $f ( x , \theta )$ with inputs $x$ and parameters $\theta$ learns to predict a transformed version of its targets $y$ To read out predictions $\hat { y }$ of the network, we apply the inverse transformation:

$$
{ \mathcal { L } } ( \theta ) \doteq { \frac { 1 } { 2 } } { \bigl ( } f ( x , \theta ) - \operatorname { s y m l o g } ( y ) { \bigr ) } ^ { 2 } \qquad { \hat { y } } \doteq \operatorname { s y m e x p } \bigl ( f ( x , \theta ) { \bigr ) }
$$

Using the logarithm as transformation would not allow us to predict targets that take on negative values. Therefore, we choose a function from the bi-symmetric logarithmic family34 that we name symlog as the transformation with the symexp function as its inverse:

$$
\operatorname { s y m l o g } ( x ) \doteq \operatorname { s i g n } ( x ) \ln \left( \left| x \right| + 1 \right) \qquad \operatorname { s y m e x p } ( x ) \doteq \operatorname { s i g n } ( x ) { \bigl ( } \exp ( \left| x \right| ) - 1 { \bigr ) }
$$

The symlog function compresses the magnitudes of both large positive and negative values. Unlike the logarithm, it is symmetric around the origin while preserving the input sign. This allows the optimization process to quickly move the network predictions to large values when needed. The symlog function approximates the identity around the origin so that it does not affect learning of targets that are already small enough. For potentially stochastic targets, such as rewards or returns, we introduce the symexp twohot loss. Here, the network outputs the logits for a softmax distribution over exponentially spaced bins $b _ { i } \in B$ . Predictions are read out as the weighted average of the bin positions weighted by their predicted probabilities. Importantly, the network can output any continuous value in the interval because the weighted average can fall between the buckets:

$$
\begin{array} { r } { \hat { y } \doteq \mathrm { s o f t m a x } ( f ( x ) ) ^ { T } B \qquad B \doteq \mathrm { s y m e x p } ( \bigl [ - 2 0 \quad \ldots \quad + 2 0 \bigr ] ) } \end{array}
$$

The network is trainedon twohot encoded targets8,, a generalizationof onehot encoding to continuous values. The twohot encoding of a scalar is a vector with $| B |$ entries that are all 0 except at the indices $k$ and $k + 1$ of the two bins closest to the encoded scalar. The two entries sum up to 1, with linearly higher weight given to the bin that is closer to the encoded continuous number. The network is then trained to minimize the categorical cross entropy loss for classification with soft targets. Note that the loss only depends on the probabilities assigned to the bins but not on the continuous values associated with the bin locations, decoupling the size of the gradients from the size of the targets:

$$
\mathcal { L } ( \boldsymbol { \theta } ) \doteq - \operatorname { t w o h o t } ( \boldsymbol { y } ) ^ { T } \log \operatorname { s o f t m a x } ( \boldsymbol { f } ( \boldsymbol { x } , \boldsymbol { \theta } ) )
$$

Applying these principles, Dreamer transforms vector observations using the symlog functions, both for the encoder inputs and the decoder targets and employs the synexp twohot loss for the reward predictor and critic. We find that these techniques enable robust and fast learning across many diverse domains. For critic learning, an alternative asymmetric transformation has previously been proposed35, which we found less effective on average across domains. Unlike alternatives, symlog transformations avoid truncating large targets 7, introducing non-stationary from normalization5, or adjusting network weights when new extreme values are detected36.

![](images/5.jpg)  

Figure 5: Fraction of trained agents that discover each of the three latest items in the Minecraft Diamond task. Although previous algorithms progress up to the iron pickaxe, Dreamer is the only compared algorithm that manages to discover a diamond, and does so reliably.

# Results

We evaluate the generality of Dreamer across 8 domains—with over 150 tasks—under fixed hyperparameters. We designed the experiments to compare Dreamer to the best methods in the literature, which are often specifically designed and tuned for the benchmark at hand. We further compare to a high-quality implementation of $\mathrm { P P O } ^ { 5 }$ , a standard reinforcement learning algorithm that is known for its robustness. We run PPO with fixed hyperparameters chosen to maximize performance across domains and that reproduce strong published results of PPO on ProcGen 37. To push the boundaries of reinforcement learning, we apply Dreamer to the challenging video game Minecraft, comparing it to strong previous algorithms. Finally, we analyze the importance of individual components of Dreamer and its robustness to different model sizes and computational budgets. All Dreamer agents are trained on a single Nvidia A100 GPU each, making it reproducible for many research labs. A public implementation of Dreamer that reproduces all results is available on the project website. Benchmarks We perform an extensive empirical study across 8 domains that include continuous and discrete actions, visual and low-dimensional inputs, dense and sparse rewards, different reward scales, 2D and 3D worlds, and procedural generation. Figure 1 summarizes the benchmark results, showing that Dreamer outperforms a wide range of previous expert algorithms across diverse domains. Crucially, Dreamer substantially outperforms PPO across all domains.

•Atari This established benchmark contains 57 Atari 2600 games with a budget of 200M frames, posing a diverse range of challenges 38. We use the sticky action simulator setting39. Dreamer outperforms the powerful MuZero algorithm ° while using only a fraction of the computational resources. Dreamer also outperforms the widely-used expert algorithms Rainbow 40 and $\mathrm { I Q N ^ { 4 1 } }$ . •ProcGen This benchmark of 16 games features randomized levels and visual distractions to test the robustness and generalization of agents42. Within the budget of 50M frames, Dreamer matches the tuned expert algorithm $\mathrm { P P G } ^ { 3 7 }$ and outperforms Rainbow42,40. Our PPO agent with fixed hyperparameters matches the published score of the highly tuned official PPO implementation 37. DMLab This suite of 30 tasks features 3D environments that test spatial and temporal reasoning 43. . In 100M frames, Dreamer exceeds the performance of the scalable IMPALA and $\mathrm { R } 2 \mathrm { D } 2 +$ agents 35 at 1B environment steps, amounting to a data-efficiency gain of over $100 \%$ .We note that these baselines were not designed for data-efficiency but serve as a valuable comparison point for the performance previously achievable at scale.

![](images/6.jpg)  

Figure 6: Ablations and robust scaling of Dreamer. a, All individual robustness techniques contribute to the performance of Dreamer on average, although each individual technique may only affect some tasks. Training curves of individual tasks are included in the supplementary material. b, The performance of Dreamer predominantly rests on the unsupervised reconstruction loss of its world model, unlike most prior algorithms that rely predominantly on reward and value prediction gradients 7,5,8. . c, The performance of Dreamer increases monotonically with larger model sizes, ranging from 12M to 400M parameters. Notably, larger models not only increase task performance but also require less environment interaction. d, Higher replay ratios predictably increase the performance of Dreamer. Together with model size, this allows practitioners to improve task performance and data-efficiency by employing more computational resources.

•Atari100k This data-efficiency benchmark comntains 26 Atari games and a budget of only 400K frames, amounting to 2 hours of game time17. EfficientZero44 holds the state-of-the-art by combining online tree search, prioritized replay, and hyperparameter scheduling, but also resets levels early to increase data diversity, making a comparison difficult. Without this complexity, Dreamer outperforms the best remaining methods, including the transformer-based IRIS and TWM agents, the model-free SPR, and SimPLe45. •Proprio Control This benchmark contains 18 control tasks with continuous actions, proprioceptive vector inputs, and a budget of 500K environment steps 46. The tasks range from classical control over locomotion to robot manipulation tasks, featuring dense and sparse rewards. Dreamer sets a new state-of-the-art on this benchmark, outperforming D4PG, DMPO, and MPO33 •Visual Control This benchmark consists of 20 continuous control tasks where the agent receives only high-dimensional images as input and has a budget of 1M environment steps46. Dreamer establishes a new state-of-the-art on this benchmark, outperforming $\mathrm { D r Q - v } 2$ and CURL 47, which are specialized to visual environments and leverage data augmentation. •BSuite This benchmark includes 23 environments with a total of 468 configurations that are specifically designed to test credit assignment, robustness to reward scale and stochasticity, memory, generalization, and exploration 48. Dreamer establishes a new state-of-the-art on this benchmark, outperforming Boot DQN and other methods 49. Dreamer improves over previous algorithms especially in the scale robustness category. Minecraft Collecting diamonds in the popular game Minecraft has been a long-standing challenge and infinite 3D world. Episodes last until the player dies or up to 36000 steps equaling 30 minutes, during which the player needs to discover a sequence of 12 items from sparse rewards by foraging for resources and crafting tools. It takes about 20 minutes for experienced human players to obtain diamonds20. We follow the block breaking setting of prior work19 because the provided action space would make it challenging for stochastic policies to keep a key pressed for a prolonged time. Because of the training time in this complex domain, extensive tuning would be difficult for Minecraft. Instead, we apply Dreamer out of the box with its default hyperparameters. As shown in Figures 1 and 5, Dreamer is the first algorithm to collect diamonds in Minecraft from scratch without using human data as was required by $\mathrm { V P T } ^ { 2 0 }$ or adaptive curricula19. All the Dreamer agents we trained on Minecraft discover diamonds in 100M environment steps. While several strong baselines progress to advanced items such as the iron pickaxe, none of them discovers a diamond. Ablations In Figure 6, we ablate the robustness techniques and learning signals on a diverse set of 14 tasks to understand their importance. The training curves of individual tasks are included in the supplementary material. We observe that all robustness techniques contribute to performance, most notably the KL objective of the world model, followed by return normalization and symexp twohot regression for reward and value prediction. In general, we find that each individual technique is critical on a subset of tasks but may not affect performance on other tasks. To investigate the effect of the world model, we ablate the learning signals of Dreamer by stopping either the task-specific reward and value prediction gradients or the task-agnostic reconstruction gradients from shaping its representations. Unlike previous reinforcement learning algorithms that often rely only on task-specific learning signals7.8, Dreamer rests predominantly on the unsupervised objective of its world model. This finding could allow for future algorithm variants that leverage pretraining on unsupervised data. Scaling properties To investigate whether Dreamer can scale robustly, we train 6 model sizes ranging from 12M to 400M parameters, as well as different replay ratios on Crafter50 and a DMLab task43. The replay ratio affects the number of gradient updates performed by the agent. Figure 6 shows robust learning with fixed hyperparameters across the compared model sizes and replay ratios. Moreover, increasing the model size directly translates to both higher task performance and a lower data requirement. Increasing the number of gradient steps further reduces the interactions needed to learn successful behaviors. The results show that Dreamer learns robustly across model sizes and replay ratios and that its performance and provides a predictable way for increasing performance given computational resources.

# Previous work

Developing general-purpose algorithms has long been a goal of reinforcement learning research. $\mathrm { P P O } ^ { 5 }$ is one of the most widely used algorithms and is relatively robust but requires large amounts of experience and often yields lower performance than specialized alternatives. $\mathrm { S A C } ^ { 3 2 }$ is a popular choice for continuous control and leverages experience replay for data-efficiency, but in practice requires tuning, especially for its entropy scale, and struggles under high-dimensional inputs51. MuZero8 plans using a value prediction model and has been applied to board games and Atari, but the authors did not release an implementation and the algorithm contains several complex components, making it challenging to reproduce. Gato52 fits one large model to expert demonstrations of multiple tasks, but is only applicable when expert data is available. In comparison, Dreamer masters a diverse range of environments with fixed hyperparameters, does not require expert data, and its implementation is open source.

Minecraft has been a focus of recent research. With MALMO53, Microsoft released a free version of the successful game for research purposes. MineRL 18 offers several competition environments, which we rely on as the basis for our experiments. The MineRL competition supports agents in exploring and learning meaningful skills through a diverse human dataset18. Voyager obtains items at a similar depth in the technology tree as Dreamer using API calls to a language model but operates on top of the MineFlayer bot scripting layer that was specifically engineered to the game and exposes high-level actions5. $\mathrm { V P T } ^ { 2 0 }$ trained an agent to play Minecraft through behavioral cloning based on expert data of keyboard and mouse actions collected by contractors and finetuning using reinforcement learning to obtain diamonds using 720 GPUs for 9 days. In comparison, Dreamer uses the MineRL competition action space to autonomously learn to collect diamonds from sparse rewards using 1 GPU for 9 days, without human data.

# Conclusion

We present the third generation of the Dreamer algorithm, a general reinforcement learning algorithm that masters a wide range of domains with fixed hyperparameters. Dreamer excels not only across over 150 tasks but also learns robustly across varying data and compute budgets, moving reinforcement learning toward a wide range of practical applications. Applied out of the box, Dreamer is the first algorithm to collect diamonds in Minecraft from scratch, achieving a significant milestone in the field of artificial intelligence. As a high-performing algorithm that is based on a learned world model, Dreamer paves the way for future research directions, including teaching agents world knowledge from internet videos and learning a single world model across domains to allow artificial agents to build up increasingly general knowledge and competency. Acknowledgements We thank Mohammad Norouzi, Jessy Lin, Abbas Abdolmaleki, John Schulman, Adam Kosiorek, and Oleh Rybkin for insightful discussions. We thank Bobak Shahriari, Denis Yarats, Karl Cobbe, and Hubert Soyer for sharing training curves of baseline algorithms. We thank Daniel Furrer, Andrew Chen, and Dakshesh Garambha for help with Google Cloud infrastructure.

# References

1. David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):   
484, 2016. 2. OpenAI. OpenAI Five. https://blog.openai.com/openai-five/, 2018. 3. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems,   
35:2773027744, 2022. 4. Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven Chu Hong Hoi. Coderl: Mastering code generation through pretrained models and deep reinforcement learning. Advances in Neural Information Processing Systems, 35:2131421328, 2022. 5. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.   
6Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015. 7. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015. 8. Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, ae atari, go, chess and shogi by planning with a learned model. arXiv preprint arXiv:1911.08265,   
2019. 9. Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z Leibo, David Silver, and Koray Kavukcuoglu. Reinforcement learning with unsupervised auxiliary tasks. arXiv preprint arXiv:1611.05397, 2016. 10. Ankesh Anand, Evan Racah, Sherjil Ozair, Yoshua Bengio, Marc-Alexandre Côté, and R Devon Hjelm. Unsupervised state representation learning in atari. Advances in neural information processing systems, 32, 2019.   
11 Danny Driess, Ingmar Schubert, Pete Florence, Yunzhu Li, and Marc Toussaint. Reinforcement learning with neural radiance fields. arXiv preprint arXiv:2206.01634, 2022. 12. David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. Nature, 550(7676):354, 2017. 13. Marcin Andrychowicz, Anton Raichuk, Piotr Stanczyk, Manu Orsini, Sertan Girgin, Raphael Marinier, Léonard Hussenot, Matthieu Geist, Olivier Pietquin, Marcin Michalski, et al. What matters in on-policy reinforcement learning? a large-scale empirical study. arXiv preprint arXiv:2006.05990, 2020. 14. Richard S Sutton. Dyna, an integrated architecture for learning, planning, and reacting. ACM SIGART Bulletin, 2(4):160163, 1991. 15. Chelsea Finn and Sergey Levine. Deep visual foresight for planning robot motion. In 2017 IEEE International Conference on Robotics and Automation (ICRA), pages 27862793. IEEE, 2017. 16. David Ha and Jürgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018. 17. Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H Campbell, Konrad Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, et al. Modelbased reinforcement learning for atari. arXiv preprint arXiv:1903.00374, 2019. 18. William H Guss, Cayden Codel, Katja Hofmann, Brandon Houghton, Noboru Kuno, Stephanie Milani, Sharada Mohanty, Diego Perez Liebana, Ruslan Salakhutdinov, Nicholay Topin, et al. The minerl competition on sample efficient reinforcement learning using human priors. arXiv e-prints, pages arXiv1904, 2019. 19. Ingmar Kanitscheider, Joost Huizinga, David Farhi, William Hebgen Guss, Brandon Houghton, Raul Sampedro, Peter Zhokhov, Bowen Baker, Adrien Ecoffet, Jie Tang, et al. Multi-task curriculum learning in a complex, visual, hard-exploration domain: Minecraft. arXiv preprint arXiv:2106.14876, 2021. 20. Bowen Baker, Ilge Akkaya, Peter Zhokhov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon unlabeled online videos. arXiv preprint arXiv:2206.11795, 2022. 21. Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination. arXiv preprint arXiv:1912.01603, 2019. 22. Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with discrete world models. arXiv preprint arXiv:2010.02193, 2020. 23. Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 24. Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson. Learning latent dynamics for planning from pixels. arXiv preprint arXiv:1811.04551, 2018. 2. Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013. 26. Durk P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Improved variational inference with inverse autoregressive flow. Advances in neural information processing systems, 29, 2016. 27. Rewon Child. Very deep vaes generalize autoregressive models and can outperform them on images. arXiv preprint arXiv:2011.10650, 2020. 28. Marc G Bellemare, Will Dabney, and Rémi Munos. A distributional perspective on reinforcement learning. In International Conference on Machine Learning, pages 449458. PMLR, 2017.   
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018. 30. Ronald J Williams and Jing Peng. Function optimization using connectionist reinforcement learning algorithms. Connection Science, 3(3):241268, 1991. 31. Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229256, 1992. 32. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor. arXiv preprint arXiv:1801.01290, 2018. 33. Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, Remi Munos, Nicolas Heess, and Martin Riedmiller. Maximum a posteriori policy optimisation. arXiv preprint arXiv:1806.06920, 2018. 34. J Beau W Webber. A bi-symmetric log transformation for wide-range data. Measurement Science and Technology, 24(2):027001, 2012. 35. Steven Kapturowski, Georg Ostrovski, John Quan, Remi Munos, and Will Dabney. Recurrent experience replay in distributed reinforcement learning. In International conference on learning representations, 2018. 36. Matteo Hessel, Hubert Soyer, Lasse Espeholt, Wojciech Czarnecki, Simon Schmitt, and Hado van Hasselt. Multi-task deep reinforcement learning with popart. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 37963803, 2019. 37. Karl W Cobbe, Jacob Hilton, Oleg Klimov, and John Schulman. Phasic policy gradient. In International Conference on Machine Learning, pages 20202027. PMLR, 2021.   
Marc GBellemare, Yavar Naddaf, Joel Veness, and Michael Bowling.The arcade lar environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253279, 2013. 39. Marlos C Machado, Marc G Bellemare, Erik Talvitie, Joel Veness, Matthew Hausknecht, and Michael Bowling. Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents. Journal of Artificial Intelligence Research, 61:523562, 2018. 40. Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in deep reinforcement learning. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018. 41. Will Dabney, Georg Ostrovski, David Silver, and Rémi Munos. Implicit quantile networks for distributional reinforcement learning. In International conference on machine learning, pages 10961105. PMLR, 2018. 42. Karl Cobbe, Chris Hesse, Jacob Hilton, and John Schulman. Leveraging procedural generation arIn Ittial cer 20482056. PMLR, 2020. 43. Charles Beattie, Joel Z Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich Küttler, Andrew Lefrancq, Simon Green, Víctor Valdés, Amir Sadik, et al. Deepmind lab. arXiv preprint arXiv:1612.03801, 2016. 44. Weirui Ye, Shaohuai Liu, Thanard Kurutach, Pieter Abbeel, and Yang Gao. Mastering atari games with limited data. Advances in Neural Information Processing Systems, 34:2547625488, 2021. 4. Vincent Micheli, Eloi Alonso, and François Fleuret. Transformers are sample efficient world models. arXiv preprint arXiv:2209.00588, 2022. 46. Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite. arXiv preprint arXiv:1801.00690, 2018. 47. Denis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto. Mastering visual continuous control: Improved data-augmented reinforcement learning. arXiv preprint arXiv:2107.09645, 2021. 48. Ian Osband, Yotam Doron, Matteo Hessel, John Aslanides, Eren Sezener, Andre Saraiva, Katrina McKinney, Tor Lattimore, Csaba Szepesvari, Satinder Singh, et al. Behaviour suite for reinforcement learning. arXiv preprint arXiv:1908.03568, 2019. 49. Olivia Dizon-Paradis, Stephen Wormald, Daniel Capecci, Avanti Bhandarkar, and Damon Woodard. Investigating the practicality of existing reinforcement learning algorithms: A performance comparison. Authorea Preprints, 2023. 50. Danijar Hafner. Benchmarking the spectrum of agent capabilities. arXiv preprint arXiv:2109.06780, 2021. 51. Denis Yarats, Amy Zhang, Ilya Kostrikov, Brandon Amos, Joelle Pineau, and Rob Fergus. Improving sample efficiency in model-free reinforcement learning from images. arXiv preprint arXiv:1910.01741, 2019. 52. Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al. A generalist agent. arXiv preprint arXiv:2205.06175, 2022. 53. Matthew Johnson, Katja Hofmann, Tim Hutton, and David Bignell. The malmo platform for artificial intelligence experimentation. In IJCAI, pages 42464247. Citeseer, 2016. 54. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023. 55. Shengyi Huang, Rousslan Fernand Julien Dossa, Antonin Raffin, Anssi Kanervisto, and Weixun Wang. The 37 implementation details of proximal policy optimization. The ICLR Blog Track 2023, 2022. 56. Matt Hoffman, Bobak Shahriari, John Aslanides, Gabriel Barth-Maron, Feryal Behbahani, Tamara Norman, Abbas Abdolmaleki, Albin Cassirer, Fan Yang, Kate Baumli, et al. Acme: A research framework for distributed reinforcement learning. arXiv preprint arXiv:2006.00979, 2020. 57. Simon Schmitt, Matteo Hessel, and Karen Simonyan. Off-policy actor-critic with shared experience replay. In International Conference on Machine Learning, pages 85458554. PMLR, 2020. 58. Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. arXiv preprint arXiv:1511.05952, 2015. 59. Andy Brock, Soham De, Samuel L Smith, and Karen Simonyan. High-performance large-scale image recognition without normalization. In International Conference on Machine Learning, pages 10591071. PMLR, 2021.   
6 Liu Ziyin, Zhikang T Wang, and Masahito Ueda. Laprop: Separating momentum and adaptivity in adam. arXiv preprint arXiv:2002.04839, 2020. 61. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 62. Audrunas Gruslys, Will Dabney, Mohammad Gheshlaghi Azar, Bilal Piot, Marc Bellemare, and Remi Munos. The reactor: A fast and sample-efficient actor-critic agent for reinforcement learning. arXiv preprint arXiv:1704.04651, 2017. 63. Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoderdecoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014. 64. Matthijs Van Keirsbilck, Alexander Keller, and Xiaodong Yang. Rethinking full connectivity in recurrent neural networks. arXiv preprint arXiv:1905.12340, 2019. 65. Marlos C Machado, Marc G Bellemare, Erik Talvitie, Joel Veness, Matthew Hausknecht, and Michael Bowling. Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents. Journal of Artificial Intelligence Research, 61:523562, 2018. 66. Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, et al. Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures. arXiv preprint arXiv:1802.01561, 2018.

# Methods

# Baselines

We employ the Proximal Policy Optimization (PPO) algorithm5, which has become a standard choice in the field, to compare Dreamer under fixed hyperparameters across all benchmarks. There are a large number of PPO implementations available publicly and they are known to substantially vary in task performance55. To ensure a comparison that is representative of the highest performance PPO can achieve under fixed hyperparameters across domains, we choose the high-quality PPO implementation available in the Acme framework56 and select its hyperparameters in Table 1 folowin ecomendations,and additionally tune t eoch batc ize o be large enou for complex environments 42, its learning rate, and its entropy scale. We match the discount factor to Dreamer because it works well across domains and is a common choice in the literature 35,8 We choose the IMPALA network architecture that we have found performed better than alternatives 42 and set the minibatch size to the largest possible for one A100 GPU. We verify the performance of our PPO implementation and hyperparameters on the ProcGen benchmark, where a highly tuned PPO implementation has been reported by the PPO authors37. We find that our implementation matches or slightly outperforms this performance reference.

Table 1: PPO hyperparameters used across all benchmarks.   

<table><tr><td>Parameter</td><td>Value</td></tr><tr><td>Observation normalization Reward normalization Reward clipping (stddev.) Epoch batch Number of epochs Minibatch size Minibatch length Policy trust region Value trust region Advantage normalization Entropy penalty scale Discount factor GAE lambda Learning rate Gradient clipping Adam epsilon</td><td>Yes Yes 10 64 × 256 3 8 256 0.2 No Yes 0.01 0.997 0.95 3 × 10−4 0.5 10-5</td></tr></table>

For Minecraft, we additionally tune and run the IMPALA and Rainbow algorithms because not successful end-to-end learning from scratch has been reported in the literature18. We use the Acme implementations 56 of these algorithms, use the same IMPALA network we used for PPO, and tuned the learning rate and entropy regularizers. For all other benchmarks, we compare to tuned expert algorithms reported in the literature as referenced in the results section.

# Implementation

Experience replay We implement Dreamer using a uniform replay buffer with an online queue 57. Specifically, each minibatch is formed first from non-overlapping online trajectories and then filled up with uniformly sampled trajectories from the replay buffer. We store latent states into the replay buffer during data collection to initialize the world model on replayed trajectories, and write the fresh latent states of the training rollout back into the buffer. While prioritized replay58 is used by some of the expert algorithms we compare to and we found it to also improve the performance of Dreamer, we opt for uniform replay in our experiments for ease of implementation. We parameterize the amount of training via the replay ratio. This is the fraction of time steps trained on per time step collected from the environment, without action repeat. Dividing the replay ratio by the time steps in a minibatch and by action repeat yields the ratio of gradient steps to env steps. For example, a replay ratio of 32 on Atari with action repeat of 4 and batch shape $1 6 \times 6 4$ corresponds to 1 gradient step every 128 env steps, or $1 . 5 \mathbf { M }$ gradient steps over 200M env steps.

Optimizer We employ Adaptive Gradient Clipping $( \mathrm { A G C } ) ^ { 5 9 }$ , which clips per-tensor gradients if they exceed $3 0 \%$ of the L2 norm of the weight matrix they correspond to, with its default $\epsilon = 1 0 ^ { - 3 }$ AGC decouples the clipping threshold from the loss scales, allowing to change loss functions or loss scales without adjusting the clipping threshold. We apply the clipped gradients using the LaProp optimizer60 with $\epsilon = 1 0 ^ { - 2 0 }$ and its default parameters $\beta _ { 1 } = 0 . 9$ and $\beta _ { 2 } = 0 . 9 9$ LaProp normalizes gradients by RMSProp and then smoothes them by momentum, instead of computing both momentum and normalizer on raw gradients as Adam does 61. This simple change allows for a smaller epsilon and avoids occasional instabilities that we observed under Adam. Distributions The encoder, dynamics predictor, and actor distributions are mixtures of $9 9 \%$ the predicted softmax output and $1 \%$ of a uniform distribution 62 to prevent zero probabilities and infinite log probabilities. The rewards and critic neural networks output a softmax distribution over exponentially spaced bins $b \in B$ and are trained towards twohot encoded targets:

$$
\operatorname { t w o h o t } ( x ) _ { i } \doteq \left\{ { \begin{array} { l l } { \displaystyle | b _ { k + 1 } - x | / \left| b _ { k + 1 } - b _ { k } \right| } & { { \mathrm { i f ~ } } i = k } \\ { \displaystyle | b _ { k } } & { - x | / \left| b _ { k + 1 } - b _ { k } \right| } & { { \mathrm { i f ~ } } i = k + 1 } \\ { 0 } & { \mathrm { e l s e } } \end{array} } \right. \qquad k \doteq \sum _ { j = 1 } ^ { | B | } \delta ( b _ { j } < x )
$$

The output weights of twohot distributions are initialized to zero to ensure that the agent does not hallucinate rewards and values at initialization. For computing the expected prediction of the softmax distribution under bins that span many orders of magnitude, the summation order matters and positive and negative bins should be summed up separately, from small to large bins, and then added. Refer to the source code for an implementation. Networks Images are encoded using stride 2 convolutions to resolution $6 \times 6$ or $4 \times 4$ and then flattened and are decoded using transposed stride 2 convolutions, with sigmoid activation on the output. Vector inputs are symlog transformed and then encoded and decoded using 3-layer MLPs. The actor and critic neural networks are also 3-layer MLPs and the reward and continue predictors are 1-layer MLPs. The sequence model is a $\mathbf { G R U } ^ { 6 3 }$ with block-diagonal recurrent weights 64 of 8 blocks to allow for a large number of memory units without quadratic increase in parameters and FLOPs. The input to the GRU at each time step is a linear embedding of the sampled latent $z _ { t }$ , of the action $a _ { t }$ , and of the recurrent state to allow mixing between blocks.

# Benchmarks

Protocols Summarized in Table 2, we follow the standard evaluation protocols for the benchmarks where established. Atari38 uses 57 tasks with sticky actions 65. The random and human reference scores used to normalize scores vary across the literature and we chose the most common reference values, replicated in Table 6. DMLab43 uses 30 tasks 66 and we use the fixed action space 36,35. We evaluate at 100M steps because running for 10B as in some prior work was infeasible. Because existing published baselines perform poorly at 100M steps, we compare to their performance at 1B steps instead, giving them a $1 0 \times$ data advantage. ProcGen uses the hard difficulty setting and the unlimited level set4. Prior work compares at different step budgets42,37 and we compare at 50M steps due to computational cost, as there is no action repeat. For Minecraft Diamond purely from sparse rewards, we establish the evaluation protocol to report the episode return measured at 100M env steps, corresponding to about 100 days of in-game time. Atari $1 0 0 \mathrm { k } ^ { 1 7 }$ includes 26 tasks with a budget of 400K env steps, 100K after action repeat. Prior work has used various environment settings, summarized in Table 10, and we chose the environments as originally introduced. Visual Control46,21 spans 20 tasks with an action repeat of 2. Proprioceptive Control follows the same protocol but we exclude the two quadruped tasks because of baseline availability in prior work47.

Table 2: Benchmark overview. All agents were trained on a single Nvidia A100 GPU each.   

<table><tr><td>Benchmark</td><td>Tasks</td><td>Env Steps</td><td>Action Repeat</td><td>Env Instances</td><td>Replay Ratio</td><td>GPU Days</td><td>Model Size</td></tr><tr><td>Minecraft</td><td>1</td><td>100M</td><td>1</td><td>64</td><td>32</td><td>8.9</td><td>200M</td></tr><tr><td>DMLab</td><td>30</td><td>100M</td><td>4</td><td>16</td><td>32</td><td>2.9</td><td>200M</td></tr><tr><td>ProcGen</td><td>16</td><td>50M</td><td>1</td><td>16</td><td>64</td><td>16.1</td><td>200M</td></tr><tr><td>Atari</td><td>57</td><td>200M</td><td>4</td><td>16</td><td>32</td><td>7.7</td><td>200M</td></tr><tr><td>Atari100K</td><td>26</td><td>400K</td><td>4</td><td>1</td><td>128</td><td>0.1</td><td>200M</td></tr><tr><td>BSuite</td><td>23</td><td></td><td>1</td><td>1</td><td>1024</td><td>0.5</td><td>200M</td></tr><tr><td>Proprio Control</td><td>18</td><td>500K</td><td>2</td><td>16</td><td>512</td><td>0.3</td><td>12M</td></tr><tr><td>Visual Control</td><td>20</td><td>1M</td><td>2</td><td>16</td><td>512</td><td>0.1</td><td>12M</td></tr></table>

Environment instances In earlier experiments, we observed that the performance of both Dreamer and PPO is robust to the number of environment instances. Based on the CPU resources available on our training machines, we use 16 environment instance by default. For BSuite, the benchmark requires using a single environment instance. We also use a single environment instance for Atari100K because the benchmark has a budget of 400K env steps whereas the maximum episode length in Atari is in principle 432K env steps. For Minecraft, we use 64 environments using remote CPU workers to speed up experiments because the environment is slower to step. Seeds and error bars We run 5 seeds for each Dreamer and PPO per benchmark, with the exception of 1 seed for ProcGen due to computational constraints, 10 seeds for BSuite as required by the benchmark, and 10 seeds for Minecraft to reliably report the fraction of runs that achieve diamonds. All curves show the mean over seeds with one standard deviation shaded. Computational choices All Dreamer and PPO agents in this paper were trained on a single Nvidia A100 GPU each. Dreamer uses the 200M model size by default. On the two control suitse, Dreamer the same performance using the substantially faster 12M model, making it more accessible to researchers. The replay ratio control the trade-off between computational cost and data efficiency as analyzed in Figure 6 and is chosen to fit the step budget of each benchmark.

# Model sizes

To accommodate different computational budgets and analyze robustness to different model sizes, we define a range of models ranging from 12M to 400M parameters shown in Table 3. The sizes are parameterized by the model dimension, which approximately increases in multiples of 1.5, alternating between powers of two and power of two scaled by 1.5. This yields tensor shapes that are multiples of 8 as required for hardware efficiency. Sizes of different network components derive from the model dimension. The MLPs have the model dimension as the number of hidden units. The sequence model has 8 times the number of recurrent units, split into 8 blocks of the same size as the MLPs. The convolutional encoder and decoder layers closest to the data use $1 6 \times$ fewer channels than the model dimension. Each latent also uses $1 6 \times$ fewer codes than the model dimension. The number of hidden layers and number of latents is fixed across model sizes. All hyperparamters, including the learning rate and batch size, are fixed across model sizes.

<table><tr><td>Parameters</td><td>12M</td><td>25M</td><td>50M</td><td>100M</td><td>200M</td><td>400M</td></tr><tr><td>Hidden size (d)</td><td>256</td><td>384</td><td>512</td><td>768</td><td>1024</td><td>1536</td></tr><tr><td>Recurrent units (8d)</td><td>1024</td><td>3072</td><td>4096</td><td>6144</td><td>8192</td><td>12288</td></tr><tr><td>Base CNN channels (d/16)</td><td>16</td><td>24</td><td>32</td><td>48</td><td>64</td><td>96</td></tr><tr><td>Codes per latent (d/16)</td><td>16</td><td>24</td><td>32</td><td>48</td><td>64</td><td>96</td></tr></table>

Table 3: Dreamer model sizes. The number of MLP hidden units defines the model dimension, from which recurrent units, convolutional channels, and number of codes per latent are derived. The number of layers and latents is constant across model sizes.

# Previous Dreamer generations

We present the third generation of the Dreamer line of work. Where the distinction is useful, we refer to this algorithm as DreamerV3. The DreamerV1 algorithm21 was limited to continuous control, the DreamerV2 algorithm22 surpassed human performance on Atari, and the DreamerV3 algorithm enables out-of-the-box learning across diverse benchmarks. We summarize the changes that DreamerV3 introduces as follows: Robustness techniques: Observation symlog, KL balance and free bits, $1 \%$ unimix for all categor. icals, percentile return normalization, symexp twohot loss for the reward head and critic. Network architecture: Block GRU, RMSNorm normalization, SiLu activation. Optimizer: Adaptive gradient clipping (AGC), LaProp (RMSProp followed by momentum).   
Rep bufLar paciy qu stor n dtateta.

# Hyperparameters

<table><tr><td rowspan=1 colspan=2>Name</td><td rowspan=1 colspan=4>Symbol</td><td rowspan=1 colspan=1>Value</td></tr><tr><td rowspan=1 colspan=7>General</td></tr><tr><td rowspan=1 colspan=2>Replay capacityBatch sizeBatch lengthActivationLearning rateGradient clippingOptimizer</td><td rowspan=1 colspan=4>BT−</td><td rowspan=1 colspan=1>5 × 1061664RMSNorm + SiLU4 × 10-5AGC(0.3)LaProp( = 10−20)</td></tr><tr><td rowspan=1 colspan=7>World Model</td></tr><tr><td rowspan=1 colspan=2>Reconstruction loss scaleDynamics loss scaleRepresentation loss scaleLatent unimixFree nats</td><td rowspan=1 colspan=4>βpredβdynBrp</td><td rowspan=1 colspan=1>110.11%1</td></tr><tr><td rowspan=1 colspan=7>Actor Critic</td></tr><tr><td rowspan=12 colspan=2>Imagination horizonDiscount horizonReturn lambdaCritic loss scaleCritic replay loss scaleCritic EMA regularizerCritic EMA decayActor loss scaleActor entropy regularizerActor unimixActor RetNorm scaleActor RetNorm limitActor RetNorm decay</td><td rowspan=1 colspan=4>H</td><td rowspan=1 colspan=1>15</td></tr><tr><td rowspan=1 colspan=4>1/(1− γ)</td><td rowspan=2 colspan=1>3330.95</td></tr><tr><td rowspan=1 colspan=4>λ</td></tr><tr><td rowspan=1 colspan=4>βval</td><td rowspan=9 colspan=1>10.310.9813 × 10-41%Per(R, 95) − Per(R, 5)10.99</td></tr><tr><td rowspan=1 colspan=4>βrepval</td></tr><tr><td rowspan=2 colspan=4>—</td></tr><tr><td rowspan=1 colspan=3></td></tr><tr><td rowspan=1 colspan=4>βpol</td><td rowspan=1 colspan=1></td></tr><tr><td rowspan=2 colspan=4>η—</td></tr><tr><td rowspan=1 colspan=1></td><td rowspan=1 colspan=2></td><td rowspan=1 colspan=1></td></tr><tr><td rowspan=1 colspan=4>S</td></tr><tr><td rowspan=1 colspan=4>L</td></tr></table>

Table 4: Dreamer hyperparameters. The same values are used across all benchmarks, including proprioceptive and visual inputs, continuous and discrete actions, and 2D and 3D domains. We do not use any hyperparameter annealing, prioritized replay, weight decay, or dropout.

# Minecraft

Game description With 100M monthly active users, Minecraft is one of the most popular video games worldwide. Minecraft features a procedurally generated 3D world of different biomes, including plains, forests, jungles, mountains, deserts, taiga, snowy tundra, ice spikes, swamps, savannahs, badlands, beaches, stone shores, rivers, and oceans. The world consists of 1 meter sized blocks that the player and break and place. There are about 30 different creatures that the player can interact with or fight. From gathered resources, the player can use over 350 recipes to craft new items and progress through the technology tree, all while ensuring safety and food supply to survive. There are many conceivable tasks in Minecraft and as a first step, the research community has focused on the salient task of obtaining a diamonds, a rare item found deep underground and requires progressing through the technology tree.

Learning environment We built the Minecraft Diamond environment on top of MineRL to define a flat categorical action space and fix issues we discovered with the original environments via human play testing. For example, when breaking diamond ore, the item sometimes jumps into the inventory and sometimes needs to be collected from the ground. The original environment terminates episodes when breaking diamond ore so that many successful episodes end before collecting the item and thus without the reward. We remove this early termination condition and end episodes when the player dies or after 36000 steps, corresponding to 30 minutes at the control frequency of $2 0 \mathrm { H z }$ Another issue is that the jump action has to be held for longer than one control step to trigger a jump, which we solve by keeping the key pressed in the background for $2 0 0 \mathrm { m s }$ We built the environment on top of MineRL $\mathrm { v 0 . 4 . 4 ^ { 1 8 } }$ , which offers abstract crafting actions. The Minecraft version is 1.11.2.

Observations and rewards The agent observes a $6 4 \times 6 4 \times 3$ first-person image, an inventory count vector for the over 400 items, a vector of maximum inventory counts since episode begin to tell the agent which milestones it has achieved, a one-hot vector indicating the equipped item, and scalar inputs for the health, hunger, and breath levels. We follow the sparse reward structure of the MineRL competition environment18 that rewards 12 milestones leading up to the diamond, for obtaining the items log, plank, stick, crafting table, wooden pickaxe, cobblestone, stone pickaxe, iron ore, furnace, iron ingot, iron pickaxe, and diamond. The reward for each item is only given once per episode, and the agent has to learn to collect certain items multiple times to achieve the next milestone. To make the return easy to interpret, we give a reward of $+ 1$ for each milestone instead of scaling rewards based on how valuable each item is. Additionally, we give $- 0 . 0 1$ for each lost heart and $+ 0 . 0 1$ for each restored heart, but did not investigate whether this is helpful.

# Supplementary material

# Minecraft video predictions

![](images/7.jpg)  

Figure 7: Multi-step predictions on Minecraft. The world model receives the first 5 frames as context input and the predicts 45 steps into the future given the action sequence and without access to intermediate images.

# Minecraft additional results

Table 5: Minecraft Diamond scores at 100M environment steps.   

<table><tr><td>Method</td><td>Return</td></tr><tr><td>Dreamer</td><td>9.1</td></tr><tr><td>IMPALA</td><td>7.1</td></tr><tr><td>Rainbow</td><td>6.3</td></tr><tr><td>PPO</td><td>5.1</td></tr></table>

![](images/8.jpg)  

Figure 8: Minecraft learning curves.

![](images/9.jpg)  

Figure 9: Item success rates as a percentage of episodes. Dreamer obtains items at substantially higher rates than the baselines and continues to improve until the 100M step budget. At the budget, Dreamer obtains diamonds in $0 . 4 \%$ of episodes, leaving a challenge for future research. This metric differs from Figure 5, which shows that over the course of training, $100 \%$ of Dreamer agents obtain one or more diamonds regardless of episode boundaries, compared to $0 \%$ of the baseline agents.

# Atari learning curves

![](images/10.jpg)  

Figure 10: Atari learning curves.

# Atari scores

Table 6: Atari scores.   

<table><tr><td>Task</td><td>Random</td><td>Gamer</td><td>Record</td><td>PPO</td><td>MuZero</td><td>Dreamer</td></tr><tr><td>Environment steps</td><td></td><td></td><td></td><td>200M</td><td>200M</td><td>200M</td></tr><tr><td></td><td></td><td>7128</td><td>251916</td><td>5476</td><td>56835</td><td>10977</td></tr><tr><td>Alien Amidar</td><td>228 </td><td>72</td><td> 104159</td><td>7</td><td>1517</td><td> 6122</td></tr><tr><td>Asssault</td><td></td><td>8503</td><td>8647</td><td>673</td><td></td><td> 60100</td></tr><tr><td></td><td></td><td></td><td> 1000</td><td></td><td>42742</td><td></td></tr><tr><td> Asterix</td><td></td><td></td><td></td><td> 190</td><td>79375</td><td> 6</td></tr><tr><td>ASsteroids</td><td></td><td> 7389</td><td> 10506650</td><td>22479</td><td>74146</td><td>8684</td></tr><tr><td>Atlantis</td><td>128</td><td> 9028</td><td>1004840</td><td> 39721</td><td> 1353617</td><td> 15322</td></tr><tr><td> Bank Heist</td><td>14</td><td> 753</td><td>2058</td><td>946</td><td> 1077</td><td> 1083</td></tr><tr><td>a Bat Zone</td><td>236</td><td>3788</td><td>0100</td><td>816</td><td>167412</td><td>9653</td></tr><tr><td>Bea Rider</td><td>364</td><td> 1926</td><td>999</td><td>7</td><td> 1154</td><td>7073</td></tr><tr><td>D Berzerk</td><td>124</td><td>630</td><td> 1057940</td><td> 186</td><td>1698</td><td> 557</td></tr><tr><td>T Bowling</td><td></td><td>161</td><td>300</td><td> 118</td><td>3</td><td>5</td></tr><tr><td> Boxing</td><td></td><td>12</td><td> 00</td><td>98</td><td> 100</td><td>10 00</td></tr><tr><td> Breakout</td><td></td><td>30</td><td>86</td><td>99</td><td> 9</td><td>38</td></tr><tr><td> Centipede</td><td></td><td>12017</td><td>13019</td><td>1833</td><td>774421</td><td>455</td></tr><tr><td>Chopper Command</td><td>811</td><td>738</td><td>99999</td><td> 1267</td><td>8945</td><td> 02698</td></tr><tr><td> CLlimber</td><td>1078</td><td>35829</td><td>219900</td><td>3176</td><td>18439</td><td>1993204</td></tr><tr><td>T Defender</td><td>8</td><td> 18689</td><td> 010500</td><td>270</td><td> 492</td><td>7987 5</td></tr><tr><td>Memon Aattaca</td><td>152</td><td>1971</td><td> 156345</td><td>8229</td><td> 142509</td><td> 12109</td></tr><tr><td>oubule unk</td><td>-19</td><td>-16</td><td>22</td><td>16</td><td>2233</td><td>24</td></tr><tr><td>Enduro</td><td></td><td>860</td><td>9500</td><td>1887</td><td>236</td><td>2166</td></tr><tr><td>ih Derby</td><td></td><td>-3</td><td>71</td><td>43</td><td>58</td><td>8</td></tr><tr><td> Freeway</td><td></td><td>30</td><td>38</td><td>33</td><td>0</td><td>34</td></tr><tr><td> Frostbite</td><td></td><td>4335</td><td>454830</td><td>1123</td><td>17087</td><td>41888</td></tr><tr><td>D ophere</td><td></td><td>2412</td><td>5040</td><td>24792</td><td>12025</td><td></td></tr><tr><td> Gravitar</td><td></td><td>351</td><td>162850</td><td>3436</td><td>10301</td><td>0</td></tr><tr><td>Hero</td><td>1173 1027</td><td>0826</td><td> 1000</td><td> 1967</td><td></td><td> 12570</td></tr><tr><td>Ice Hockey</td><td>-11</td><td>1</td><td>36</td><td>12</td><td>6063 26</td><td> 0677</td></tr><tr><td>Jamesbond</td><td></td><td>303</td><td>455</td><td>1 019</td><td>14872</td><td>57</td></tr><tr><td>Kangaroo</td><td>39</td><td> 35</td><td>1424600</td><td> 769</td><td> 14380</td><td>24010</td></tr><tr><td>Krull</td><td></td><td> 266</td><td>1 104100</td><td></td><td></td><td> 2229</td></tr><tr><td>Kung Fu Master</td><td>198</td><td> 2736</td><td> 1000</td><td>9193</td><td>11476</td><td>985</td></tr><tr><td></td><td>258</td><td>4753</td><td></td><td>235</td><td> 1448936</td><td> 154893</td></tr><tr><td>Montezuma Revenge</td><td>0</td><td></td><td>1 29200</td><td>68</td><td>0</td><td>1852</td></tr><tr><td>Ms Pacman Name This Game</td><td>307</td><td>5</td><td>10090</td><td>7041</td><td> 310</td><td> 079</td></tr><tr><td></td><td>2292</td><td>049</td><td>5220</td><td> 11941</td><td> 531</td><td> 7809</td></tr><tr><td>D Phoenix</td><td>7611</td><td> 243</td><td> 0140</td><td>31412</td><td> 10593</td><td>31606</td></tr><tr><td> Pitfall</td><td>229</td><td> 4</td><td>14000</td><td>-2</td><td></td><td>0</td></tr><tr><td>T Pongt</td><td>-21</td><td>15</td><td>21</td><td>19</td><td>21</td><td>20</td></tr><tr><td> Privte Eye</td><td>25</td><td>69571</td><td>101800</td><td>73</td><td> 00</td><td>26432</td></tr><tr><td>Qbert</td><td>1164</td><td>155</td><td>4000</td><td>14554</td><td>102129</td><td>201084</td></tr><tr><td> Riverraid</td><td>138</td><td>11</td><td> 1000</td><td> 4860</td><td>1 198</td><td>48080</td></tr><tr><td>Road Runner</td><td></td><td>784</td><td>C238100</td><td>3995</td><td> 4083</td><td> 150402</td></tr><tr><td> Robotank</td><td></td><td>12</td><td>76</td><td>63</td><td>70</td><td>1</td></tr><tr><td> Seaquest</td><td>$10</td><td>42055</td><td>999999</td><td>1927</td><td>39976</td><td>356584</td></tr><tr><td>T Skiin</td><td>17098</td><td>437</td><td>&quot;-3272</td><td>2926</td><td>-3000</td><td> 9965</td></tr><tr><td> Solari</td><td>236</td><td>2327</td><td>111420</td><td>36</td><td> 860</td><td> 851</td></tr><tr><td>Space Invaders</td><td>148</td><td>1669</td><td>1535</td><td>49</td><td> 3639</td><td> 1005</td></tr><tr><td> Sar Gunner</td><td>664</td><td> 10250</td><td> 7400</td><td> 53439</td><td>27417</td><td> 48961</td></tr><tr><td> Surround</td><td>-10</td><td>8</td><td>6</td><td>6</td><td>9</td><td></td></tr><tr><td> Tennis</td><td>-24</td><td></td><td>21</td><td>−1</td><td></td><td>-3</td></tr><tr><td> Time Pilot</td><td>5 </td><td>5229</td><td>65300</td><td>17250</td><td>427209</td><td>314947</td></tr><tr><td> Tuankham</td><td>11</td><td>168</td><td> 5384</td><td> 25</td><td>235 522962</td><td>395 614065</td></tr><tr><td>Up N Down Venture</td><td>533 0</td><td>193 18</td><td>2840 900</td><td>83743 953</td></table>

# ProcGen learning curves

![](images/11.jpg)  

Figure 11: ProcGen learning curves.

# ProcGen scores

Table 7: ProcGen scores. The PPO implementation we use throughout our paper under fixed hyperparameters performs on par or better than the original PPO, which its authors describe as highly tuned with near optimal hyperparameters 37.   

<table><tr><td>Task</td><td>Original PPO</td><td>PPO</td><td>PPG</td><td>Dreamer</td></tr><tr><td>Environment steps</td><td>50M</td><td>50M</td><td>50M</td><td>50M</td></tr><tr><td>Bigfish</td><td>10.92</td><td>12.72</td><td>31.26</td><td>8.62</td></tr><tr><td>Bossfight</td><td>10.47</td><td>9.36</td><td>11.46</td><td>11.61</td></tr><tr><td>Caveflyer</td><td>6.03</td><td>6.71</td><td>10.02</td><td>9.42</td></tr><tr><td>Chaser</td><td>4.48</td><td>3.54</td><td>8.57</td><td>5.49</td></tr><tr><td>Climber</td><td>7.59</td><td>9.04</td><td>10.24</td><td>11.43</td></tr><tr><td>Coinrun</td><td>7.93</td><td>6.71</td><td>8.98</td><td>9.86</td></tr><tr><td>Dodgeball</td><td>4.80</td><td>3.44</td><td>10.31</td><td>10.93</td></tr><tr><td>Fruitbot</td><td>20.28</td><td>21.69</td><td>24.32</td><td>11.04</td></tr><tr><td>Heist</td><td>2.25</td><td>6.87</td><td>3.77</td><td>8.51</td></tr><tr><td>Jumper</td><td>5.09</td><td>6.13</td><td>5.84</td><td>9.17</td></tr><tr><td>Leaper</td><td>5.90</td><td>4.07</td><td>8.76</td><td>7.05</td></tr><tr><td>Maze</td><td>4.97</td><td>7.86</td><td>7.06</td><td>6.85</td></tr><tr><td>Miner</td><td>7.56</td><td>12.97</td><td>9.08</td><td>5.71</td></tr><tr><td>Ninja</td><td>6.16</td><td>3.62</td><td>9.38</td><td>9.82</td></tr><tr><td>Plunder</td><td>11.16</td><td>3.99</td><td>13.44</td><td>23.81</td></tr><tr><td>Starpilot</td><td>17.00</td><td>10.13</td><td>21.57</td><td>28.00</td></tr><tr><td>Normalized mean</td><td>41.16</td><td>42.80</td><td>64.89</td><td>66.01</td></tr></table>

![](images/12.jpg)  
DMLab learning curves   

Figure 12: DMLab learning curves.

# DMLab scores

Table 8: DMLab scores at 100M environment steps and larger budgets. The IMPALA agent corresponds to "IMPALA (deep)" presented by Kapturowski et al.35 who made the learning curves available.   

<table><tr><td>Task</td><td>R2D2+</td><td>IMPALA</td><td>IMPALA</td><td>IMPALA</td><td>PPO</td><td>Dreamer</td></tr><tr><td>Environment steps</td><td>10B</td><td>10B</td><td>1B</td><td>100M</td><td>100M</td><td>100M</td></tr><tr><td>Explore Goal Locations Large</td><td>174.7</td><td>316.0</td><td>137.8</td><td>64.2</td><td>21.2</td><td>116.7</td></tr><tr><td>Explore Goal Locations Small</td><td>460.7</td><td>482.0</td><td>302.8</td><td>196.1</td><td>115.1</td><td>372.8</td></tr><tr><td>Explore Object Locations Large</td><td>60.6</td><td>91.0</td><td>55.1</td><td>34.3</td><td>22.5</td><td>63.9</td></tr><tr><td>Explore Object Locations Small</td><td>83.7</td><td>100.4</td><td>75.9</td><td>50.6</td><td>38.9</td><td>93.5</td></tr><tr><td>Explore Object Rewards Few</td><td>80.7</td><td>92.6</td><td>46.9</td><td>34.6</td><td>19.0</td><td>40.0</td></tr><tr><td>Explore Object Rewards Many</td><td>75.8</td><td>89.4</td><td>68.5</td><td>53.3</td><td>25.5</td><td>58.1</td></tr><tr><td>Explore Obstructed Goals Large</td><td>95.5</td><td>102.0</td><td>57.9</td><td>23.7</td><td>12.9</td><td>52.8</td></tr><tr><td>Explore Obstructed Goals Small</td><td>311.9</td><td>372.0</td><td>214.9</td><td>118.0</td><td>70.4</td><td>224.6</td></tr><tr><td>Language Answer Quantitative Question</td><td>344.4</td><td>362.0</td><td>304.7</td><td>1.0</td><td>0.3</td><td>266.0</td></tr><tr><td>Language Execute Random Task</td><td>497.4</td><td>465.4</td><td>140.8</td><td>44.4</td><td>-2.5</td><td>223.7</td></tr><tr><td>Language Select Described Object</td><td>617.6</td><td>664.0</td><td>618.4</td><td>0.2</td><td>526.9</td><td>665.5</td></tr><tr><td>Language Select Located Object</td><td>772.8</td><td>731.4</td><td>413.0</td><td>38.2</td><td>397.5</td><td>679.5</td></tr><tr><td>Lasertag One Opponent Large</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.1</td><td>0.0</td><td>-0.1</td></tr><tr><td>Lasertag One Opponent Small</td><td>31.8</td><td>0.0</td><td>0.0</td><td>-0.1</td><td>0.0</td><td>0.0</td></tr><tr><td>Lasertag Three Opponents Large</td><td>28.6</td><td>32.2</td><td>10.4</td><td>-0.1</td><td>0.0</td><td>9.0</td></tr><tr><td>Lasertag Three Opponents Small</td><td>49.0</td><td>57.2</td><td>37.1</td><td>12.1</td><td>0.4</td><td>18.8</td></tr><tr><td>Natlab Fixed Large Map</td><td>60.6</td><td>63.4</td><td>53.8</td><td>13.0</td><td>15.8</td><td>50.5</td></tr><tr><td>Natlab Varying Map Randomized</td><td>42.4</td><td>47.0</td><td>40.5</td><td>35.3</td><td>29.6</td><td>31.2</td></tr><tr><td>Natlab Varying Map Regrowth Psychlab Arbitrary Visuomotor Mapping</td><td>24.6</td><td>34.0 38.4</td><td>25.5</td><td>15.3</td><td>16.3</td><td>16.7</td></tr><tr><td>Psychlab Continuous Recognition</td><td>33.1</td><td></td><td>16.5</td><td>11.9</td><td>16.0</td><td>30.7</td></tr><tr><td></td><td>30.0</td><td>28.6</td><td>30.0</td><td>30.1</td><td>30.5</td><td>33.8</td></tr><tr><td>Psychlab Sequential Comparison</td><td>30.0</td><td>29.6</td><td>0.0</td><td>0.0</td><td>30.0</td><td>44.3</td></tr><tr><td>Psychlab Visual Search Rooms Collect Good Objects Test</td><td>79.9</td><td>80.0</td><td>0.0</td><td>0.0</td><td>76.6</td><td>40.1</td></tr><tr><td>Rooms Exploit Deferred Effects Test</td><td>9.9</td><td>10.0</td><td>9.9</td><td>9.3</td><td>9.7</td><td>9.9</td></tr><tr><td></td><td>38.1</td><td>62.2</td><td>37.6</td><td>34.5</td><td>39.0</td><td>40.4</td></tr><tr><td>Rooms Keys Doors Puzzle</td><td>46.2</td><td>54.6</td><td>36.9</td><td>24.2</td><td>26.0</td><td>42.4</td></tr><tr><td>Rooms Select Nonmatching Object</td><td>63.6</td><td>39.0</td><td>63.2</td><td>4.0</td><td>2.7</td><td>73.1</td></tr><tr><td>Rooms Watermaze</td><td>49.0</td><td>47.0</td><td>50.1</td><td>23.6</td><td>21.2</td><td>26.1</td></tr><tr><td>Skymaze Irreversible Path Hard Skymaze Irreversible Path Varied</td><td>76.0</td><td>80.0 100.0</td><td>46.4</td><td>7.7</td><td>0.0</td><td>80.9</td></tr><tr><td></td><td>76.0</td><td></td><td>69.8</td><td>32.7</td><td>31.3</td><td>87.7</td></tr><tr><td>Human mean capped (%)</td><td>85.4</td><td>85.1</td><td>66.3</td><td>31.0</td><td>35.9</td><td>71.4</td></tr></table>

![](images/13.jpg)  
Atari100k learning curves   

Figure 13: Atari100k learning curves.

Atari100k scores

<table><tr><td>Task</td><td>Random Human</td><td></td><td>PPO</td><td>SimPLe</td><td>SPR</td><td>TWM</td><td>IRIS</td><td>Dreamer</td></tr><tr><td>Environment steps</td><td></td><td></td><td>400K</td><td>400K</td><td>400K</td><td>400K</td><td>400K</td><td>400K</td></tr><tr><td>Alien</td><td>228</td><td>7128</td><td>276</td><td>617</td><td>842</td><td>675</td><td>420</td><td>1118</td></tr><tr><td>Amidar</td><td>6</td><td>1720</td><td>26</td><td>74</td><td>180</td><td>122</td><td>143</td><td>97</td></tr><tr><td>Assault</td><td>222</td><td>74</td><td>327</td><td>527</td><td>56</td><td>683</td><td>1524</td><td>683</td></tr><tr><td>Asterix</td><td>210</td><td>8503</td><td>292</td><td>1128</td><td>962</td><td>117</td><td>854</td><td>1062</td></tr><tr><td>Bank Heist</td><td>14</td><td>753</td><td>14</td><td>34</td><td>345</td><td>467</td><td>53</td><td>398</td></tr><tr><td>Battle Zone</td><td>2360</td><td>37188</td><td>2233</td><td>4031</td><td>14834</td><td>5068</td><td>13074</td><td>2300</td></tr><tr><td>Boxing</td><td>0</td><td>12</td><td>3</td><td>8</td><td>36</td><td>78</td><td>70</td><td>82</td></tr><tr><td>Breakout</td><td>2</td><td>30</td><td></td><td>16</td><td>20</td><td>20</td><td>84</td><td>10</td></tr><tr><td>Chopper Command</td><td>811</td><td>7388</td><td>1005</td><td>979</td><td>946</td><td>1697</td><td>1565</td><td>2222</td></tr><tr><td>Crazy Climber</td><td>10780</td><td>35829</td><td>14675</td><td>62584</td><td>36700</td><td>71820</td><td>59324</td><td>86225</td></tr><tr><td>Demon Attack</td><td>152</td><td> 1971</td><td>1600</td><td>208</td><td>518</td><td>350</td><td>2034</td><td>577</td></tr><tr><td>Freeway</td><td>0</td><td>30</td><td>2</td><td>17</td><td>19</td><td>24</td><td>31</td><td>0</td></tr><tr><td> Frostbite</td><td>65</td><td>4335</td><td>127</td><td>237</td><td>1171</td><td>1476</td><td>259</td><td>3377</td></tr><tr><td>GGopher</td><td>258</td><td>2412</td><td>368</td><td>597</td><td>661</td><td>1675</td><td>2236</td><td>2160</td></tr><tr><td>Hero</td><td>1027</td><td>30826</td><td>2596</td><td>2657</td><td> 5859</td><td>7254</td><td>7037</td><td>11354</td></tr><tr><td>Jamesbond</td><td>29</td><td>303</td><td>41</td><td>100</td><td>366</td><td>362</td><td>463</td><td>540</td></tr><tr><td>Kangaroo</td><td>52</td><td>035</td><td>55</td><td>51</td><td>3617</td><td>1240</td><td>838</td><td>2643</td></tr><tr><td>Krull</td><td>11598</td><td>266</td><td>3222</td><td>2205</td><td>3682</td><td>6349</td><td>616</td><td>8171</td></tr><tr><td>Kung Fu Master</td><td>258</td><td>22736</td><td>2090</td><td>14862</td><td>4783</td><td>24555</td><td>21760</td><td>2500</td></tr><tr><td>Ms Pacman</td><td>307</td><td>6952</td><td>366</td><td>1480</td><td>1318</td><td>158</td><td>999</td><td>1521</td></tr><tr><td>Pong</td><td>-21</td><td>15</td><td>-20</td><td>13</td><td>-5</td><td>119</td><td>15</td><td>-4</td></tr><tr><td>Private Eye</td><td>25</td><td>69571</td><td>100</td><td>35</td><td>86</td><td>87</td><td>100</td><td>3238</td></tr><tr><td>bert</td><td>164</td><td>13455</td><td>317</td><td>1289</td><td>866</td><td>3331</td><td>746</td><td>2921</td></tr><tr><td>Road Runner</td><td>12</td><td>7845</td><td>602</td><td>5641</td><td>1213</td><td>9109</td><td>9615</td><td>19230</td></tr><tr><td>Seaquest</td><td>68</td><td>42055</td><td>305</td><td>683</td><td>58</td><td>744</td><td>661</td><td>962</td></tr><tr><td>Up N Down</td><td>533</td><td>1693</td><td>1502</td><td>3350</td><td>10859</td><td>15982</td><td>3546</td><td>46910</td></tr><tr><td>Gamer mean (%)</td><td>0</td><td>100</td><td>11</td><td>33</td><td>62</td><td>96</td><td>105</td><td>125</td></tr><tr><td>Gamer median (%)</td><td>0</td><td> 100</td><td>2</td><td>13</td><td>40</td><td>51</td><td>29</td><td>49</td></tr></table>

Table 9: Atari $1 0 0 \mathrm { k }$ scores at 400K environment steps, corresponding to $1 0 0 \mathrm { k }$ agent steps.   

<table><tr><td>Setting</td><td>SimPLe</td><td>EffMuZero</td><td>SPR</td><td>IRIS</td><td>TWM</td><td>Dreamer</td></tr><tr><td rowspan="3">Gamer score (%) Gamer median (%) GPU days</td><td>33</td><td>190</td><td>62</td><td>105</td><td>96</td><td>125</td></tr><tr><td>13</td><td>109</td><td>40</td><td>29</td><td>51</td><td>49</td></tr><tr><td>5.0</td><td>0.6</td><td>0.1</td><td>3.5</td><td>0.4</td><td>0.1</td></tr><tr><td>Online planning</td><td></td><td>X</td><td></td><td></td><td></td><td></td></tr><tr><td>Data augmentation Non-uniform replay</td><td></td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td></td><td></td><td>X</td><td></td><td></td><td>X</td><td></td></tr><tr><td>Separate hparams</td><td></td><td></td><td></td><td>X</td><td></td><td></td></tr><tr><td>Increased resolution Uses life information</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td>X</td><td></td></tr><tr><td>Uses early resets</td><td></td><td>2</td><td></td><td>x</td><td></td><td></td></tr><tr><td>Separate eval episodes</td><td>X</td><td></td><td></td><td>X</td><td>X</td><td></td></tr></table>

Table 10: Evaluation protocols for the Atari $1 0 0 \mathrm { k }$ benchmark. Computational resources are converted to A100 GPU days. EffcientMuZero44 achieves the highest scores but changed the environment configuration from the standard17. IRIS uses a separate hyperparameter for its exploration strength on Freeway.

![](images/14.jpg)  
Proprioceptive control learning curves   

Figure 14: DeepMind Control Suite learning curves under proprioceptive inputs.

# Proprioceptive control scores

Table 11: DeepMind Control Suite scores under proprioceptive inputs.   

<table><tr><td>Task</td><td>PPO</td><td>DDPG</td><td>DMPO</td><td>D4PG</td><td>Dreamer</td></tr><tr><td>Environment steps</td><td>500K</td><td>500K</td><td>500K</td><td>500K</td><td>500K</td></tr><tr><td>Acrobot Swingup</td><td>6</td><td>100</td><td>103</td><td>124</td><td>134</td></tr><tr><td>Ball In Cup Catch</td><td>632</td><td>917</td><td>968</td><td>968</td><td>962</td></tr><tr><td>Cartpole Balance</td><td>523</td><td>997</td><td>999</td><td>999</td><td>990</td></tr><tr><td>Cartpole Balance Sparse</td><td>930</td><td>992</td><td>999</td><td>974</td><td>990</td></tr><tr><td>Cartpole Swingup</td><td>240</td><td>864</td><td>860</td><td>875</td><td>852</td></tr><tr><td>Cartpole Swingup Sparse</td><td>7</td><td>703</td><td>438</td><td>752</td><td>491</td></tr><tr><td>Cheetah Run</td><td>82</td><td>596</td><td>650</td><td>624</td><td>614</td></tr><tr><td>Finger Spin</td><td>18</td><td>775</td><td>769</td><td>823</td><td>931</td></tr><tr><td>Finger Turn Easy</td><td>281</td><td>499</td><td>620</td><td>612</td><td>793</td></tr><tr><td>Finger Turn Hard</td><td>106</td><td>313</td><td>495</td><td>421</td><td>889</td></tr><tr><td>Hopper Hop</td><td>0</td><td>36</td><td>68</td><td>80</td><td>113</td></tr><tr><td>Hopper Stand</td><td>3</td><td>484</td><td>549</td><td>762</td><td>576</td></tr><tr><td>Pendulum Swingup</td><td>1</td><td>767</td><td>834</td><td>759</td><td>788</td></tr><tr><td>Reacher Easy</td><td>494</td><td>934</td><td>961</td><td>960</td><td>954</td></tr><tr><td>Reacher Hard</td><td>288</td><td>949</td><td>968</td><td>937</td><td>938</td></tr><tr><td>Walker Run</td><td>31</td><td>561</td><td>493</td><td>616</td><td>649</td></tr><tr><td>Walker Stand</td><td>159</td><td>965</td><td>975</td><td>947</td><td>964</td></tr><tr><td>Walker Walk</td><td>64</td><td>952</td><td>942</td><td>969</td><td>936</td></tr><tr><td>Task mean</td><td>94</td><td>771</td><td>801</td><td>792</td><td>871</td></tr><tr><td>Task median</td><td>215</td><td>689</td><td>705</td><td>733</td><td>754</td></tr></table>

# Visual control learning curves

![](images/15.jpg)  

Figure 15: DeepMind Control Suite learning curves under visual inputs.

# Visual control scores

Table 12: DeepMind Control Suite scores under visual inputs.   

<table><tr><td>Task</td><td>PPO</td><td>SAC</td><td>CURL</td><td>DrQ-v2</td><td>Dreamer</td></tr><tr><td>Environment steps</td><td>1M</td><td>1M</td><td>1M</td><td>1M</td><td>1M</td></tr><tr><td>Acrobot Swingup</td><td>3</td><td>4</td><td>4</td><td>166</td><td>229</td></tr><tr><td>Ball In Cup Catch</td><td>829</td><td>176</td><td>970</td><td>928</td><td>972</td></tr><tr><td>Cartpole Balance</td><td>516</td><td>937</td><td>980</td><td>992</td><td>993</td></tr><tr><td>Cartpole Balance Sparse</td><td>881</td><td>956</td><td>999</td><td>987</td><td>964</td></tr><tr><td>Cartpole Swingup</td><td>290</td><td>706</td><td>771</td><td>863</td><td>861</td></tr><tr><td>Cartpole Swingup Sparse</td><td>1</td><td>149</td><td>373</td><td>773</td><td>759</td></tr><tr><td>Cheetah Run</td><td>95</td><td>20</td><td>502</td><td>716</td><td>836</td></tr><tr><td>Finger Spin</td><td>118</td><td>291</td><td>880</td><td>862</td><td>589</td></tr><tr><td>Finger Turn Easy</td><td>253</td><td>200</td><td>340</td><td>525</td><td>878</td></tr><tr><td>Finger Turn Hard</td><td>79</td><td>94</td><td>231</td><td>247</td><td>904</td></tr><tr><td>Hopper Hop</td><td>0</td><td>0</td><td>164</td><td>221</td><td>227</td></tr><tr><td>Hopper Stand</td><td>4</td><td>5</td><td>777</td><td>903</td><td>903</td></tr><tr><td>Pendulum Swingup</td><td>1</td><td>592</td><td>413</td><td>843</td><td>744</td></tr><tr><td>Quadruped Run</td><td>88</td><td>54</td><td>149</td><td>450</td><td>617</td></tr><tr><td>Quadruped Walk</td><td>112</td><td>49</td><td>121</td><td>726</td><td>811</td></tr><tr><td>Reacher Easy</td><td>487</td><td>67</td><td>689</td><td>944</td><td>951</td></tr><tr><td>Reacher Hard</td><td>94</td><td>7</td><td>472</td><td>670</td><td>862</td></tr><tr><td>Walker Run</td><td>30</td><td>27</td><td>360</td><td>539</td><td>684</td></tr><tr><td>Walker Stand</td><td>161</td><td>143</td><td>486</td><td>978</td><td>976</td></tr><tr><td>Walker Walk</td><td>87</td><td>40</td><td>822</td><td>768</td><td>961</td></tr><tr><td rowspan="2">Task mean Task median</td><td>94</td><td>81</td><td>479</td><td>770</td><td>861</td></tr><tr><td>206</td><td>226</td><td>525</td><td>705</td><td>786</td></tr></table>

# BSuite performance spectrum

![](images/16.jpg)  

Figure 16: BSuite scores visualized by category48. Dreamer exceeds previous methods in the categories scale and memory. The scale category measure robustness to reward scales.

BSuite scores

Table 13: BSuite scores for each task averaged over environment configurations, as well as aggregated performance by category and over all tasks.   

<table><tr><td>Task</td><td>Random</td><td>PPO</td><td>AC-RNN</td><td>DQN</td><td>Boot DQN</td><td>Dreamer</td></tr><tr><td>Bandit</td><td>0.00</td><td>0.38</td><td>1.00</td><td>0.93</td><td>0.98</td><td>0.96</td></tr><tr><td>Bandit Noise</td><td>0.00</td><td>0.61</td><td>0.63</td><td>0.71</td><td>0.80</td><td>0.75</td></tr><tr><td>Bandit Scale</td><td>0.00</td><td>0.39</td><td>0.60</td><td>0.74</td><td>0.83</td><td>0.78</td></tr><tr><td>Cartpole</td><td>0.04</td><td>0.84</td><td>0.40</td><td>0.85</td><td>0.69</td><td>0.93</td></tr><tr><td>Cartpole Noise</td><td>0.04</td><td>0.77</td><td>0.20</td><td>0.82</td><td>0.69</td><td>0.93</td></tr><tr><td>Cartpole Scale</td><td>0.04</td><td>0.83</td><td>0.12</td><td>0.72</td><td>0.65</td><td>0.92</td></tr><tr><td>Cartpole Swingup</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.15</td><td>0.03</td></tr><tr><td>Catch</td><td>0.00</td><td>0.91</td><td>0.87</td><td>0.92</td><td>0.99</td><td>0.96</td></tr><tr><td>Catch Noise</td><td>0.00</td><td>0.54</td><td>0.27</td><td>0.58</td><td>0.68</td><td>0.53</td></tr><tr><td>Catch Scale</td><td>0.00</td><td>0.90</td><td>0.17</td><td>0.85</td><td>0.65</td><td>0.94</td></tr><tr><td>Deep Sea</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.00</td><td>1.00</td><td>0.00</td></tr><tr><td>Deep Sea Stochastic</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.90</td><td>0.00</td></tr><tr><td>Discounting Chain</td><td>0.20</td><td>0.24</td><td>0.39</td><td>0.25</td><td>0.22</td><td>0.40</td></tr><tr><td>Memory Len</td><td>0.00</td><td>0.17</td><td>0.70</td><td>0.04</td><td>0.04</td><td>0.65</td></tr><tr><td>Memory Size</td><td>0.00</td><td>0.47</td><td>0.29</td><td>0.00</td><td>0.00</td><td>0.59</td></tr><tr><td>Mnist</td><td>0.05</td><td>0.77</td><td>0.56</td><td>0.85</td><td>0.85</td><td>0.61</td></tr><tr><td>Mnist Noise</td><td>0.05</td><td>0.41</td><td>0.22</td><td>0.38</td><td>0.34</td><td>0.34</td></tr><tr><td>Mnist Scale</td><td>0.05</td><td>0.76</td><td>0.09</td><td>0.49</td><td>0.31</td><td>0.55</td></tr><tr><td>Mountain Car</td><td>0.10</td><td>0.10</td><td>0.10</td><td>0.93</td><td>0.93</td><td>0.92</td></tr><tr><td>Mountain Car Noise</td><td>0.10</td><td>0.10</td><td>0.10</td><td>0.89</td><td>0.82</td><td>0.87</td></tr><tr><td>Mountain Car Scale</td><td>0.10</td><td>0.10</td><td>0.10</td><td>0.85</td><td>0.56</td><td>0.90</td></tr><tr><td>Umbrella Distract</td><td>0.00</td><td>1.00</td><td>0.09</td><td>0.30</td><td>0.26</td><td>0.74</td></tr><tr><td>Umbrella Length</td><td>0.00</td><td>0.87</td><td>0.43</td><td>0.39</td><td>0.39</td><td>0.78</td></tr><tr><td>Basic</td><td>0.04</td><td>0.60</td><td>0.58</td><td>0.90</td><td>0.89</td><td>0.88</td></tr><tr><td>Credit assignment</td><td>0.03</td><td>0.76</td><td>0.37</td><td>0.59</td><td>0.56</td><td>0.75</td></tr><tr><td>Exploration</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.68</td><td>0.01</td></tr><tr><td>Generalization</td><td>0.06</td><td>0.47</td><td>0.19</td><td>0.68</td><td>0.60</td><td>0.70</td></tr><tr><td>Memory</td><td>0.00</td><td>0.32</td><td>0.49</td><td>0.02</td><td>0.02</td><td>0.62</td></tr><tr><td>Noise</td><td>0.02</td><td>0.54</td><td>0.24</td><td>0.51</td><td>0.61</td><td>0.62</td></tr><tr><td>Scale</td><td>0.04</td><td>0.60</td><td>0.22</td><td>0.73</td><td>0.60</td><td>0.82</td></tr><tr><td>Task mean (%)</td><td>3</td><td>49</td><td>32</td><td>54</td><td>60</td><td>66</td></tr><tr><td>Category mean (%)</td><td>3</td><td>47</td><td>30</td><td>49</td><td>57</td><td>63</td></tr></table>

# Robustness ablations

![](images/17.jpg)  

Figure 17: Individual learning curves for the robustness ablation experiment. All robustness techniques contribute to the overall performance of Dreamer, although each individual technique may only improve the performance on a subset of the tasks.

# Learning signal ablations

![](images/18.jpg)  

Figure 18: Individual learning curves for the learning signal ablation experiment. Dreamer relies predominantly on the undersupervised reconstruction objective of its world model and additional reward and value gradients further improve performance on a subset of tasks.