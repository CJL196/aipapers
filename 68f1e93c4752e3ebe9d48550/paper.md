# Classroom Simulacra: Building Contextual Student Generative Agents in Online Education for Learning Behavioral Simulation

Songlin Xu   
Department of Electrical and   
Computer Engineering   
University of California San Diego   
San Diego, California, USA   
soxu@ucsd.edu   
Hao-Ning Wen   
Department of Electrical and   
Computer Engineering   
University of California San Diego   
San Diego, California, USA   
hwen@ucsd.edu   
Hongyi Pan   
Department of Electrical and   
Computer Engineering   
University of California San Diego   
San Diego, California, USA   
h2pan@ucsd.edu   
Dallas Dominguez   
Department of Electrical and   
Computer Engineering   
University of California San Diego   
San Diego, California, USA   
dadoming@ucsd.edu

Dongyin Hu University of Pennsylvania Philadelphia, Pennsylvania, USA dyhu@seas.upenn.edu

Xinyu Zhang   
Department of Electrical and   
Computer Engineering   
University of California San Diego   
San Diego, California, USA   
xyzhang@ucsd.edu

![](images/1.jpg)  
F l simulation.

# Abstract

Student simulation supports educators to improve teaching by interacting with virtual students. However, most existing approaches ignore the modulation effects of course materials because of two challenges: the lack of datasets with granularly annotated course materials, and the limitation of existing simulation models in processing extremely long textual data. To solve the challenges, we first run a 6-week education workshop from $\mathrm { N } = 6 0$ students to collect fine-grained data using a custom built online education system, which logs students' learning behaviors as they interact with lecture materials over time. Second, we propose a transferable iterative reflection (TIR) module that augments both prompting-based and finetuning-based large language models (LLMs) for simulating learning behaviors. Our comprehensive experiments show that TIR enables the LLMs to perform more accurate student simulation than classical deep learning models, even with limited demonstration data. Our TIR approach better captures the granular dynamism of learning performance and inter-student correlations in classrooms, paving the way towards a "digital twin" for online education.

# CCS Concepts

Human-centered computing Human computer interaction (HCI).

# Keywords

Student Simulation, Generative Agents, Classroom Digital Twin

# ACM Reference Format:

Songlin Xu, Hao-Ning Wen, Hongyi Pan, Dallas Dominguez, Dongyin Hu, and Xinyu Zhang. 2025. Classroom Simulacra: Building Contextual Student Generative Agents in Online Education for Learning Behavioral Simulation.

In CHI Conference on Human Factors in Computing Systems (CHI '25), April 26May 01, 2025, Yokohama, Japan. ACM, New York, NY, USA, 26 pages. https://doi.org/10.1145/3706598.3713773

# 1 Introduction

Accurate simulation of students' learning behaviors in online education settings can help building a "digital twin" classroom, which can serve as a high-fidelity sandbox for the instructors to explore diverse pedagogies. This can in turn help improve students' learning performance. With the rapid advancement of generative AI, using large language models (LLMs) for student simulation is becoming a promising approach. For example, GPTeach [53] used GPT-based virtual students for interactive TA training and MATHVC [92] used LLMs-based virtual classroom for mathematics education. However, these approaches did not systematically evaluate the realism of the virtual students. On the other hand, LLMs-based knowledge tracing models [15, 31, 36, 37, 42] demonstrated high accuracy, but they focus on students' performance prediction rather than multifaceted behavioral simulation. We argue that an accurate digital twin should encompass contextual simulation of students' behaviors, capturing the dynamic modulation effect of course materials on both individual students' learning performance and correlations among students. Such dynamism can be reflected in various contextual factors, such as lecture content, individual background, questions, skills, and so on.

However, two main challenges hinder the integration of course materials' effects into student simulation. The first lies in the lack of fine-grained datasets that annotate course materials along with students' real-time performance. Most existing datasets (such as Ednet [7], Junyi [61] and Assistments 2009-2010 benchmark 1) only contain the test question without course materials. The EduAgent dataset [86] indeed contains the course content, but the lecture length is too short (5 minutes) to reveal the student learning process across a whole lecture. Moreover, students may get tired easily during the course and the resulting data quality may not be guaranteed [76], which unfortunately is not considered by most existing data collection efforts [7, 61].

A second challenge is that existing language models can only deal with limited contextual data when learning from example demonstrations. LLM-based simulation typically adopts either finetuningbased or prompting-based approaches. The former [36] enable pretrained models to learn from new training data directly through model finetuning. But they require a significant amount of computational resources. Therefore, researchers usually resort to smaller language models such as BERT for student simulation [36, 37, 42]. However, such smaller language models have very low token limits (e.g., 512 tokens), which can only deal with short textual input but fail to capture complex contextual information such as course materials. Advanced LLMs such as GPT4 could support longer contextual text input, but their performance also drops under a long context [41] and they need significantly more computational resources for fine-tuning. On the other hand, prompting-based methods [38] do not need model training since they directly learn from contextual prompts and example demonstrations. However, the model's incontext learning ability drops significantly in the presence of long demonstrations [41].

To tackle the first challenge, we run a new user study $\mathrm { \Delta N } = 6 0$ students and ${ \Nu } = 8$ instructors for real-time teaching) in the form of a 6-week online education workshop including 12 lectures (1 hour per lecture). We collected student learning performance and fine-grained annotations of course materials and mapped them to specific post-test questions. To guarantee the data quality and improve students' engagement during learning [76], we developed a new online education system that integrated multi-modality sensing techniques to monitor students' cognitive states and prompt instructors to take recommended actions to increase students' learning engagement in real-time.

Furthermore, to deal with the second challenge, inspired by the self-reflection ability of LLMs [52] to distill knowledge [20, 87], we propose a new LLM-based student simulation framework by introducing a transferable iterative reflection (TIR) module, which guides the LLMs to perform reflections on specific course materials and compress the learned knowledge to augment the LLM simulation. Different from a straightforward self-reflection [52], our TIR architecture incorporates iterations between a novice agent and reflective agent to ensure that the reflections could be generalized into new domains to enable transferable reflections (Section 3). As a result, this TIR module can augment and solve the bottlenecks of both finetuning-based models and prompting-based models. For finetuning-based models, TIR overcomes the token limit issue by focusing its reflections on specific course materials so as to compress the learned knowledge. For prompting-based models, TIR provides an efficient way to enable the LLMs to learn from example demonstrations more effectively, where the learned knowledge can be transferred to a new simulation without example demonstrations. This ensures that LLMs learn general knowledge instead of locally optimal knowledge from example demonstrations.

We have evaluated the student simulation performance in both the EduAgent public dataset [86] and our newly collected dataset. The results show that our TIR modules enhance the LLM-based student simulation, making it even more powerful than deep learning methods that are trained and fit to given datasets. Specifically, the evaluation examines whether our model can better capture the dynamics of student behaviors. Existing research generally defines human behavior as a collection of observable actions and reactions in response to internal (genetic factors) and external (environmental factors) stimuli [48]. Therefore, to demonstrate that our simulator replicates student learning behaviors, we model student responses to these stimuli, represented by post-test accuracy after engaging with course content. Course knowledge, delivered via lecture slides, constitutes external stimuli, while internal stimuli arise from individual differences such as prior knowledge. Accordingly, we assess the model's ability to capture variations in post-test accuracy at multiple levels: individual (per student), lecture (per session), question (per post-test item), and skill (per knowledge concept). Additionally, we evaluate inter-student correlations to determine whether the simulation model accurately reflects the response patterns between student pairs. Overall, the results show that our approach better captures the granular dynamism of learning performance and inter-student correlations in classroom, pointing towards a potential "digital twin" for online classrooms.

To summarize, the main contributions in this paper include:

• We run a new 6-week online education experiment with $\mathbf { N } =$ 60 students and $\Nu = 8$ instructors to collect student learning performance with fine-grained annotations of course materials. This is powered by our newly developed online education system that integrates sensing techniques and feedback recommendations to increase student learning engagement during real-time online instruction. The online education system implementation is available at: https://github.com/cogteachadmin/CogTeach, and the data/model implementation is available at: https://github.com/songlinxu/ClassroomSimulacra We propose a transferable iterative reflection module that can augment the student simulation performance in both finetuning-based models and prompting-based models. • We systematically evaluate the student simulation performance and show how it can capture the dynamic modulation effect of course materials by revealing lecture-level, individual-level, question-level, skill-level differences and inter-student correlation in classroom.

# 2 Related Work

Our work draws inspirations from and advances the knowledge in the following three categories of research.

# 2.1 Generative Agents in HCI

The rapid advancement of LLMs has inspired a wide range of HCI applications, including social behaviors (Generative Agents [58]), virtual reality [72] with tour guidance (VirtuWander [79]), Human-AI collaboration (AI Chains [83], [77]), creative tasks (CharacterMeet [62], Luminate [69], C2Ideas [24], ABScribe [64], AngleKindling [59], PopBlends [74]), healthcare (MindfulDiary [33], ChaCha [67], Narrating Fitness [68], [63]) with health intervention (MindShift [82], [5, 29, 30, 51]), web interaction [9] with UI design (ReactGenie [89], [12], [73], coding (CollabCoder [16], [44]), behavioral change (CatAlyst [3],[4]) with human augmentation (Memoro [98], [26]), business (Marco [14]), and so on.

In educational context, LLMs-powered agents have been utilized to serve as teachable agents (Mathemyths [93], [28], [43]) to provide instructions [71], recommend learning concepts [40], and give feedback [54]. For example, DevCoach [75] supports students' learning in software development at scale with LLMs-powered generative agents. ReadingQizMaker [49] proposes a Human-NLP collaborative system which supports instructors to design high-quality reading quiz. In programming education, CodeTailor [23] uses LLMpowered personalized parsons puzzles to support engagement in programming. PaTAT [17] presents a human-AI collaborative qualitative coding system using LLMs for explainable interactive rule synthesis.

Such existing work either uses generative agents as the instructors to directly teach students [93] or serves as student agents [53] to augment intelligent tutoring systems. Our work focuses on the second aspect. In what follows, we specifically discuss related work in student simulation using either machine learning or generative agents.

# 2.2 Student Simulation

Student simulation aims to predict student learning behaviors in education, thus providing insights for supporting intelligent tutoring systems [19]. A majority of existing research formulates student simulation as a knowledge tracing problem, i.e. predicting students' future learning performance based on their past records [1]. This learning performance is usually represented by the question answering accuracy in the course to measure students' skill levels for specific course concepts. Early work in this domain employed classical Bayesian models [91]. In recent years, deep learning models [60] have been the predominant approach for knowledge tracing, combining graph models [55], cognitive theories [97], memoryaugmented components [94], and so on.

# 2.3 LLM-based Student Simulation

Recent work has explored the feasibility of using LLMs directly for predicting students' learning performance [85, 95] or for knowledge tracing [90] in open-ended questions [45]. These methods have better explainability than deep learning models, owing to LLMs' capability to reveal the reasoning process[38]. They can also augment deep learning-based knowledge tracing [15, 31]. In HCI, researchers have developed multi-agent collaboration environment to simulate the whole classroom interaction [6, 96] and used LLMsimulated student profiles to support question item evaluation [50]. These approaches can enable adaptive and personalized exercise generation to augment student learning performance [8]. For example, Sarshartehrani et al. [65] further leveraged embodied AI tutors for personalized educational adaptation. GPTeach [53] also demonstrated the feasibility of using GPT-based virtual students for interactive TA training. In addition, MATHVC [92] explored the effectiveness of LLM-simulated multi-character virtual classroom for mathematics education. Moreover, LLMs can help students engage in post-lesson self-reflection [34] and also support language learning and growth mindset cultivation [32].

However, there is also evidence [56] showing the limitation of LLMs in student performance prediction compared with deep learning (DL) models. We argue that this is mainly because of the lack of contextual course materials. Specifically, existing LLM-based approaches [15, 31, 36, 37, 42] simply treat student simulation as a sequence prediction problem, which predicts future test performance based on past records, ignoring the modulation effect of course materials. In this case, DL models are very likely to work better than LLMs owing to their capacity to learn from historical data [35, 70]. In contrast, LLMs are better at few-shot contextual learning based on their large pretrained knowledge base [11]. Therefore, incorporating contextual course materials could better unleash the power of LLMs to capture the potential effects of course materials on learning performance even with limited data, thus enabling more accurate student simulation.

Existing work in this respect is quite limited, due to both the model limitations and the lack of dataset containing course materials (Section 1). EduAgent [86] incorporated course materials to simulate students' cognitive states and post-test performance, but the lectures' duration was too short (5 minutes) to represent the learning process across a typical lecture. By contrast, our work conducts longer-term experiments (6-week, 12 lectures, 1 hour per lecture) to collect high quality learning behavioral data. Our study employs a self-developed online education system integrating sensing techniques and action recommendations to help instructors increase students' learning engagement. Moreover, our proposed transferable iterative reflection module further augments the student simulation performance for both finetuning-based and prompting-based models, which departs from existing approaches in language models [36, 37, 42] and deep learning models [60, 94].

![](images/2.jpg)  
Training/Testing Scheme for Prompting-Based (Standard/CoT) Models   

Figure 2: Training (left) and testing (right) schemes for prompting-based models.

# 3 Simulation Methodology

Our classroom simulacra framework aims to build LLM-based generative student agents that could mimic real students' learning behaviors based on their learning histories. The agents can then simulate the students' future learning performance, which is represented by the question answering correctness in the post-lecture tests.

# 3.1 Problem Formulation

In our online education scenario, students first listen to the lecture and then finish a post-course test, which evaluates their learning performance based on the accuracy of their answers. As depicted in Fig. 3, the input of our simulation includes past learning history $( l _ { f a s t } )$ and future learning information $( l _ { f u t u r e } )$ Here lpast includes past questions' contents $( q _ { P } \mathrm { { } } a s t )$ , past answers' correctness (i.e. labels in past questions, denoted as $y _ { P a s t . }$ ), and course materials related to specific questions $( c _ { p a s t } )$ in the learning history. The future learning information $l _ { f u t u r e }$ includes future question contents $( q _ { f u t u r e } )$ and corresponding course materials $( c _ { f u t u r e } )$ The output of the model is the sequence of future answers' correctness $( \hat { y } _ { f u t u r e } )$ ,wit the corresponding ground truth denoted as ue

Note that the course material inputs are represented by text only to match the LLM's requirement. Although there might be images in the actual lecture slides, we have converted the images into textual descriptions during our dataset annotation process. The course materials include the titles/bullet points on slides, or human-annotated descriptions of images on slides. The lecturers' didactics typically align with the slides but are not delivered as a word-for-word readout. Therefore, we do not use the lecturer's transcripts as course materials nor model input. Examples of model input can be found in Fig. 4.

# 3.2 Model Training and Evaluation

We totally have three kinds of simulation models: prompting-based LLM simulation, finetuning-based LLM simulation, and deep learning based simulation (baseline). In order to evaluate the models in a comparable manner, all kinds of models use the same training set to train the model and the same testing set to evaluate the simulation performance. However, some prompting-based models only need part of the training set, which will be specified later. For each dataset, we first split it into training and testing set by following an individual-wise manner with a specific ratio $R \%$ (detailed ratio is depicted in each experiment). Specifically, all test performance of $R \%$ students was used as the training set and all test performance of another $( 1 \% { R \% } )$ students was testing set. The training set was further divided into model training and model validation set following the same individual-wise manner with the same $R \%$ ratio. The reason why we use the individual-wise dataset splitting is that our classroom simulacra instantiates each digital student based on the corresponding real student's past learning history and simulates that student's future learning process. We set the first five questions as past questions $( q _ { P } \mathrm { { } } a s t )$ of the student history and other questions as future questions $( q _ { f u t u r e } )$ for prediction.

![](images/3.jpg)  
Training/Testing Scheme for Finetuning-Based (BertKT) Models   

Figure 3: Training (left) and testing (right) scheme for finetuning-based models.

# 3.3 Transferable Iterative Reflection (TIR)

The objective of transferable iterative reflection (TIR) is to improve the LLM-based student simulation by learning from the students' past learning history more effectively. The main difference between the TIR and the existing (multi-round) reflection-based methods [25, 27, 34, 39, 52, 78, 88] lies in the transferable feature in the model design. Traditional reflection methods simply ask LLMs to reflect based on the difference between their predictions and labels. In contrast, the TIR module iteratively prompts LLMs to reflect on its previous simulations by comparing with labels in the example demonstrations (i.e. past learning history) so that LLMs could generate general reflections results that could be easily transferred to novice LLMs which do not have the example demonstrations. This ensures the generalization of such reflected results to be applied into new future learning information. As a result, the reflected results can not only directly improve the simple prompting-based simulation by increasing the data utilization efficiency, but also compress the information while avoiding missing important information to improve the finetuning-based simulation. At a higher level, the TIR module consists of four phases: initial prediction, reflection, testing, and iteration (Fig. 2).

(Initial prediction: Ask the LLM (reflective agent) to predict future question correctness (i.e., $\hat { y } _ { f u t u r e } )$ based on $l _ { f p a s t }$ and $l _ { f u t u r e }$ , and obtain the initial prediction accuracy $( a c c _ { 0 } )$ by comparing $\hat { y } _ { f u t u r e }$ with the ground truth $( y _ { f u t u r e } )$ , as depicted in Section 3.1 and Fig. 3.

Reflection: Provide the reflective agent with the ground truth of future question correctness (i.e. label: $y _ { f u t u r e } )$ , and ask it to reflect on why it fails to predict some future answers' correctness. The reflection at iteration $k$ is denoted as $r _ { k }$ .

Testing: Use $r _ { k }$ together with $l _ { f r a s t }$ and $l _ { f u t u r e }$ to ask another novice agent which has not experienced the ground truth to make a new prediction. Denote the predictions from the novice agent in this iteration $k$ as $\hat { y } _ { n o v i c e , k }$ .

Iteration: Obtain the prediction accuracy $( a c c _ { k } )$ by comparing $\hat { y } _ { n o v i c e , k }$ with the ground truth $( y _ { f u t u r e } )$ If $a c c _ { k }$ is lower than the initial prediction accuracy $( a c c _ { 0 } )$ , we ask the reflective agent to reflect in a different direction in the next iteration. Otherwise, we inform the reflective agent that the accuracy has indeed improved and it could reflect towards the similar direction. The iteration ends when the novice agent achieves $1 0 0 \%$ accuracy with the help of $r _ { k }$ or we reach the maximum number of iterations.

Finally, we select the reflection that yields the highest prediction accuracy by the novice agent as the best reflection $( r _ { b e s t } =$ $r _ { a r g m a x ( a c c _ { k } ) } )$ and log it into the reflection database. This database will be used to augment existing prompting-based LLMs by giving example demonstrations or augment finetuning-based models to provide reflections.

The iterative reflection in TIR improves the simulation performance by adjusting the reasoning process of LLMs during reflection. Due to the diversity of individuals and course contents, responses to course stimuli varies a lot across students. With a simple reflection, LLMs' results can be biased due to its pre-training corpus [22], and cannot be well-adapted to the specific student and course stimuli. The iterative reflection helps LLMs to overcome such bias from its pre-training corpus and iteratively adjust the reasoning process during reflection regarding the causality of how course stimuli modulate students' behaviors while also respecting the students' past history. One example is depicted in Fig. 4. When the LLM had a straightforward reflection in $r _ { 1 }$ , it overestimated the student's comprehension and thought the student could answer Question 12 correctly. However, After iterative reflection in $r _ { 3 }$ , the LLM found a potential misunderstanding or oversight that led to the wrong prediction. Such an experience was stored in the successful reflection database. Once retrieved, it could give inspiration for the future new reflection agent to consider such oversight in the testing set.

For prompt-based models, TIR can directly improve the LLMs' performance without fine-tuning. For finetuning-based models, TIR can effectively compress the overflowed information in $l _ { f p a s t }$ and $l _ { f u t u r e }$ to deal with the token limit problem. More details are described in the following sections.

# 3.4 TIR Augments Standard Prompts

Here we describe how to apply the TIR module to augment the standard prompting-based simulation models.

A standard simulation model simply uses LLMs to take all information as prompt input $( l _ { f a s t }$ and $l _ { f u t u r e } )$ and predict future question correctness sequence $( \hat { y } _ { f u t u r e } )$ , as depicted in Fig. 18. Directly inputting all of students' data from the training set as example demonstrations poses an obvious challenge. The data including course materials often exceed the token limits of LLMs, hampering their capability to extract useful information.

To this end, we apply the TIR module to enable the LLM to effectively learn from the training data set. Specifically, in the training stage, we first run the TIR module for each student in the training set, following the procedure in Section 3.3. The output reflections are stored into a successful reflection database. In the testing stage, as depicted in Fig. 2, we do not run the TIR module. Instead, we use a new reflective agent powered by LLMs to retrieve reflections from the successful reflection database. For each simulated student in the testing set, we retrieve the reflections of $M$ students from the reflection database. The $M$ students from the training set are randomly selected but we make sure they are in the same course as the simulated student in the testing set. This is the only criteria of retrieving reflections, which ensures contextual consistency during reflection. Random selection ensures that the example demonstrations are not manually biased. However, we use a random seed to also ensure that we can replicate such random selection to enhance the replicability of our results. Our pilot experiment shows that $M = 4$ is enough to achieve reasonable simulation performance. The retrieved reflection from $M$ students serves as the example demonstrations in the same course so that the new reflection agent in the testing set can leverage the experience from the retrieved reflections to perform a transferable reflection. Based on the retrieved reflections and past learning history $( l _ { p a s t } )$ and future learning information $( l _ { f u t u r e } )$ , the new reflective agent conducts simulation for the specific student in the testing set. To prevent label leakage, this new reflective agent does not experience any other training data. So it is different from the reflective agent in the training set.

It is worth noting that the iterative reflection process only happens in the training set. Moreover, we do not limit the LLMs' reflection to be either content-specific or metacognitive, in order to give LLMs free enough space to do reflection. But we make it generalizable by evaluating whether the reflections can be transferred to a novice agent for prediction. In addition, the reflection has to be both content-specific and metacognitive, because the course modulation effect is usually different across different lectures. So contentspecific reflection is necessary to adapt to specific course context. However, LLMs also have metacognitive reflections because the example demonstration students in the training set are different from the simulated students in the testing set. In our current setting, we only need to run the iterative reflection once per lecture to generate the successful reflection database for that speicific lecture. There is no need to run it for each question/knowledge concept/student. Running reflection offline for each lecture is reasonable, because the lecture materials are usually prepared in advance and available well before the class in real-world teaching scenarios. We have not tested if TIR can generalize across different lectures by using one single lecture's reflection database, but this can be one promising future exploration. The different reflection direction means that LLMs are instructed to reflect in another reasoning about why a wrong prediction is made. But we do not limit the specific direction content to give LLMs enough space to explore. Examples about such different directions are in Fig. 4 and Appendix Fig. 18.

# 3.5 TIR Augments CoT-based Models

Existing work has shown that using the chain of thought (CoT) prompting strategy can improve the capability of LLMs [80]. The idea of CoT is to use prompts to guide the LLMs to reason step by step like a human, instead of solving the problems all at once. Our TIR module can be integrated into CoT to further improve the simulation accuracy of prompting-based models. The integration works similarly to that in standard prompting-based model in Section 3.4. One example workflow is depicted in Appendix Fig. 19. The only difference is that we provide step-by-step guidance to the LLMs whenever predicting future questions' correctness, as depicted below.

(1) Analyze the student's past performance: Identify course concepts in past questions the student has performed well in and those they have struggled with. Consider the complexity of the questions and the related course materials.

(2) Review the course concepts and related lecture materials of the future questions: •Determine the difficulty level of the future questions based on the related course concepts and course materials. Identify if the future questions are related to the concepts of past questions that the student has previously struggled with or excelled in.

(3) Predict the student's performance in future questions: Based on the analysis from steps (1) and (2), predict whether the student will answer each future question correctly or not.

# 3.6 TIR Augments Finetuning-based Models

In addition to prompting-based methods, TIR can also improve finetuning-based language models by compressing the input tokens to avoid token overflow. We fine-tune BERT (Bidirectional Encoder Representations from Transformers), a language representation model that has pre-trained weights [10]. The input of

<table><tr><td rowspan="2">Metric</td><td colspan="3">GPT4o-Mini (Without/With TIR)</td><td colspan="5">Deep Learning Models</td></tr><tr><td>Standard</td><td>CoT</td><td>BertKT</td><td>DKT</td><td>AKT</td><td>ATKT</td><td>DKVMN</td><td>SimpleKT</td></tr><tr><td>Accuracy</td><td>0.6025+0.0469</td><td>0.6222-0.0049</td><td>0.6074+0.0938</td><td>0.6351</td><td>0.6171</td><td>0.6396</td><td>0.6171</td><td>0.6772</td></tr><tr><td>F1 score</td><td>0.5128+0.1346</td><td>0.5610+0.0341</td><td>0.6110+0.0770</td><td>0.6352</td><td>0.6051</td><td>0.6390</td><td>0.6051</td><td>0.6698</td></tr></table>

![](images/4.jpg)  

Figure 4: Prompt examples in the Transferable Iterative Reflection process.

Table 1: Simulation results on EduAgent dataset   

BERT is a sentence and the output could be anything from question answering to semantic classification. However, BERT has very low token limits (512 tokens), which apparently can not directly handle all past/future question inputs or related course materials. The TIR module solves this problem by distilling useful reflections from such data so that there is no need to input the extremely long course materials into BERT. As depicted in Fig. 3, the input of the TIR augmented BERT is composed of three parts: future question contents $q _ { f u t u r e }$ , initial LLMs-based future question correctness prediction results, and reflections from the TIR module. The model output is a binary value to decide whether one student answers one future question correctly or not. This is achieved by finetuning the BERT-based classifier from HuggingFace 2. In the training stage, we have the labels for the training set, so the TIR module directly runs on the training set to generate successful reflections, as depicted in Fig. 3(a) and Section 3.3. However, in the testing stage, the labels can not be used for TIR to avoid label leakage. Therefore, we instead use a new reflective agent to generate new reflections based on the retrieved reflections as example demonstrations from the successful reflection database in the training set.

To show the effectiveness of our TIR module in augmenting the BERT model, we prepare another baseline BERT model without TIR, which directly takes all information as input (past questions $q _ { P } a s t$ with related course materials $c _ { P a s t }$ and real past question correctness labels $y _ { P a s t }$ , future questions $q _ { f u t u r e }$ with related course materials $c _ { f u t u r e } )$ and predict the correctness of future questions. We denote the BERT model without TIR as BertKT, in contrast to that with TIR $( B e r t K T + T I R )$ . One example of the workflow is depicted in Appendix Fig. 20.

For a fair comparison, the fine-tuned data is the same as the training set of deep learning models. Therefore, we only fine-tune the BertKT once in our data. However, for future potential applications to extend the fine-tuned models in external datasets, it is necessary to fine-tune models again in such new datasets, which is similar to deep learning models that use training data to update model weights.

# 3.7 Deep Learning Models

We have also implemented five deep learning models with pyKT [47] for student simulation as baseline models, which come from recent state-of-the-art knowledge tracing models. These five models are from four categories: attention-based models (AKT [18], SimpleKT [46]), adversarial-based models (ATKT [21]), deep sequential models (DKT [60]), and memory-augmented models (DKVMN [94]). These models are widely used knowledge tracing models in the computational education domain to model student learning [18, 21, 46, 60, 94]. For example, DKT is the first architecture that applies deep learning to model student learning behaviors [60], which has become the standard baseline model for benchmarking in computational education domain. AKT [18] is the first model that applies the monotonic attention mechanism into student modeling. The Appendix Section A.1 includes more details about each model.

The training and testing scheme in deep learning models are the same as other models for fair comparison, as depicted in Section 3.2. The model input is the same as BertKT, i.e. past questions $q _ { P a s t }$ with related course materials $c _ { P a s t }$ and real past question correctness labels $y _ { P a s t }$ , future questions $q _ { f u t u r e }$ with related course materials $c _ { f u t u r e }$ The model output is the prediction of future question correctness $\hat { y } _ { f u t u r e }$ The difference is that deep learning models can not directly take textual data as input. Therefore, we use BERT again to extract embeddings from the textual data (such as question contents and course materials) as deep learning model input, which is a common practice for knowledge tracing models to predict student performance [46]. Each model is initialized using the default configurations in pyKT. The models are trained with Binary Cross Entropy Loss and the AdamW optimizer with a learning rate of 1e5. Our pilot experiments show that the model validation accuracy stablizes after about 15 epochs. Therefore, we train each model for 30 epochs and select the best model in validation for testing.

# 4 Simulation Study

We first explored the feasibility of our framework compared with baseline models in the public dataset named EduAgent[86].

# 4.1 The EduAgent Dataset

The EduAgent dataset was collected from $\mathrm { N } = 3 0 1$ students, who were asked to watch 5-min online course videos. After that, students were prompted to finish a post test which comprises 10-12 questions. The dataset contained students' correctness on each post test question, as well as corresponding question contents and course materials which were specifically related to each question. More details about this dataset could be obtained from [86].

# 4.2 Experiment Settings

We split the dataset into training and testing set by following a individual-wise manner with 0.8 ratio. Specifically, all post test performance of $8 0 \%$ students were used as the training set and all post test performance of another $2 0 \%$ students were testing set. The training set was further divided into model training and model validation set following the same individual-wise manner with 0.8 ratio as well. We set the first five questions as past questions of the student history and other questions as future questions for prediction. As depicted in Fig. 2, the simulation model input included the correctness of past questions of real students, as well as corresponding past questions contents and course materials, which were specifically related to each corresponding past question. Moreover, the model input also included future question contents and course materials which are specifically related to each future question. The model output was the correctness of each future question for predictions.

As depicted in Section 3, our TIR module could augment both prompting-based simulation (standard prompt, CoT prompt) and finetuning-based (BertKT) simulation performance. Therefore, in the experiment, we show results of both simulation types with or without the integration of our TIR module. All LLMs-based models used GPT4o-mini. We also compared with five state-of-theart knowledge tracing models based on deep learning, as depicted in Section. 3.

# 4.3 Results and Analysis

Results were depicted in Table. 1. We found that the integration of the TIR module improved the simulation performance so that both the simulation accuracy and f1 score were better than all deep learning baseline models. Specifically, the best deep learning model was SimpleKT with 0.6772 accuracy and 0.6698 F1 score. Without the TIR module, the best LLMs-based model was CoTbased prompting with 0.6222 accuracy and $0 . 5 6 1 0 \mathrm { F } 1$ score. However, after integrating the TIR module, the best LLMs-based model was finetuning-based BertKT model with 0.7012 accuracy and $0 . 6 8 8 0 \mathrm { F } 1$ score, which was superior than the best deep learning model.

Moreover, we found that the integration of the TIR module could improve all LLMs-based models including standard prompting, CoT prompting, and BertKT, as supported by Table. 1. Although the accuracy in CoT slightly decreased, its F1 score was however obviously improved.

These results demonstrate the feasibility and effectiveness of our TIR module to enhance existing LLMs-based approaches for more realistic student simulation, which were even better than deep learning models.

# 5 Online Education Workshop and Dataset

Although our simulation experiment on the EduAgent dataset demonstrated the effectiveness of our framework compared with baseline models, the EduAgent dataset itself only contains 5-min lectures. Such short duration may not capture the fine-grained effect of course stimuli on student learning performance. Therefore, it is necessary to examine the student simulation in lectures with longer duration to reveal further insights.

# 5.1 Workshop Design

To this end, we conducted a 6-week online education workshop to deliver 12 lectures, where each lecture lasted 1 hour. The longduration lectures could not only verify the simulation results, but also reveal new insights about how the simulation models can capture students' learning performance variations across the whole lecture (depicted in Section 6.8). The workshop syllabus is depicted in Appendix Table. 2.

5.1.1 Participants. We recruited 30 elementary school students,   
30 high school students, and 8 instructors from high schools and

universities in the local area using emails and social media. We removed the demographic information (such as age and gender) for privacy concerns. Our data collection was approved by the Institutional Review Board (IRB). All participated students and instructors were informed of the experiment form and then signed consent forms. For participants under 18 years old, we obtained the written consent form from both participants and their parents.

![](images/5.jpg)  

Figure 5: (a). Illustration of our CogEdu system. (b). Our action prompt strategy for instructors based on attention ratio and knowledge ratio. $^ { ( \mathbf { c } , \mathbf { d } ) }$ . The UI of the user end (c) and server end (d) of CogEdu.

![](images/6.jpg)  

Figure 6: A real online education example of our live CogEdu system shown in Fig. 5

5.1.2 Task and Procedure. We first prompted the students and instructors to watch an introduction video about how to use our online education system to facilitate learning and teaching, as well as the detailed procedures of our data collection (Fig. 7). After that, students were required to first go through a gaze calibration process (depicted in Section. 5.2.2) for accurate gaze collection. Then students were prompted to perform facial expressions (including confused and neutral expressions) in order to train a model for cognitive information detection (more details in Section 5.2.2). After that, both students and instructors were in the same online video conference system (Section 5.2) and instructors presented the course materials to the students. The lecture materials were slides prepared by our research team. During the lectures, our online education system provided visual feedback to the instructors about the students' learning status, and the instructors could adapt their teaching strategies accordingly (depicted in Section. 5.2.3). After the lecture, students were required to finish a post-test composed of 10-12 questions related to each specific lecture to measure their learning outcome.

![](images/7.jpg)  

Figure 7: The procedure to use our online learning system. (a)(b)(c). Gaze calibration process for gaze tracking. (d)(e). Facial expression model training data collection process. (f)(g). Students and teachers join in the online video calling from their own clients in class.

5.1.3 Experiment Design. Our six-week workshop was composed of a series of 12 lectures about the basics of artificial intelligence, covering different topics such as basic concepts in machine learning, computer vision, natural language processing, reinforcement learning, etc. Each lecture lasted one hour. The difficulty of the lectures was tailored to match the knowledge level of elementary and high school students, respectively. For each week, the instructors delivered two lectures. Students were encouraged to select the same time slot for real-time and synchronous teaching among all students together (Fig. 5(a)). If students had time conflicts with the instructors, we made new time arrangements for these students for additional data collection. Each student was encouraged to attend as many lectures as they could. Overall, each student attended 3 lectures on average.

5.1.4 Measurement. As depicted above, for students, we mainly collected their gaze, facial expressions, and post-test answers. The gaze and facial expressions were mainly used to generate student status feedback to instructors so that the instructors could take specific actions to increase the students' engagement and improve the quality of collected data. The post-test answers were used to measure the student learning outcomes.

![](images/8.jpg)  
value (solid lines). Vertical axis on the right is the metrics (accuracy and F1 score) value (dotted lines).

![](images/9.jpg)  
(b). BertKT training curve with TIR.

# 5.2 Online Education System

Although existing video conference software such as $\mathrm { Z o o m } ^ { 3 }$ provided a stable solution for online education, the subtle student behaviors may not be captured to provide insights to teachers. Moreover, research showed that students' learning performance might become worse compared with in-person instructions[57]. As a result, the quality of our collected data could be severely compromised. Therefore, to solve this problem and facilitate subtle communication between students and instructors, we developed an online education system named CogEdu that could support synchronized teaching between students and teachers in a client on the computer, while also providing real-time student status feedback to teachers to enhance the education process. Based on the ubiquitous webcams on laptops, we collected the gaze information and facial expressions of students. By analyzing the collected data, we provided real-time feedback to the instructors about the understanding of current contents, the attention status, and a finegrained visualization of contents that students were concerned about. Understanding about contents (or confusion) and attention were referred to as cognitive information. To further assist instructors, the system also provided teaching strategy suggestions based on the collected data.

As a result, this system could augment student learning engagement and teaching effectiveness to enable high-quality data collection. More details were depicted below.

5.2.1 System Implementation. We implemented the CogEdu system on a cloud server. Users (students and instructors) could access the system using their browsers (Fig. 5). Considering that most users were more familiar with Zoom, a video teleconferencing software program, we implemented the video conference function based on Zoom APIs4. All the feedback was overlaid over the embedded Zoom interface. To support the large flow of facial expression data before and during the lecture, and to enhance the robustness of the system, we adopted Kubernetes on the google cloud5 to manage the deployment, scaling, and management. Instances scaled up when the load was growing to reduce latency and achieve satisfying real-time performance.

5.2.2 Student Client. On the student's side, students were required to first go through a gaze calibration process and then collected facial expressions for cognitive information detection. To collect gaze information from the students, we used the service from GazeRecorder6. Around 28 gaze positions were provided from the service per second, which were then labeled as fixations or saccades using a velocity-based method [13]. Meanwhile, the system sent facial expressions to the server for cognitive information detection every second. The students' side uploaded all gaze information and cognitive information every five seconds.

The algorithm we used to transform raw gaze into fixations was from [13]. The basic idea was to calculate a velocity threshold, and gaze points with velocity below were labeled as fixations. The confusion information of students was detected using a support vector machine (SVM). Before the lecture started, students were asked to make confused expressions and neutral expressions. Collected data were cropped to focus on the eyebrow-eye region and then fed to principal component analysis (PCA) to extract features. An SVM was trained based on the features to classify either confused or neutral expressions.

Attention detection was facilitated by gaze detection, confusion detection, and browser built-in properties. When the user switched to another tab or application, document.visibilityState in the browser became hidden. This property was checked together with confusion detection. When the confusion detection algorithm on the server failed to detect a face, which meant the student's face was out of the camera, the system then asserted the student to be not attentive. Thirdly, when the gaze of the student fell out of the screen, the student was labeled as not attentive as well.

5.2.3 Teacher Client. On the instructor's side, the system fetched all information that students uploaded to the server every five seconds and then processed the information to provide the instructor with feedback. The feedback provided to instructors consisted of three modules: area of interest (AoI) feedback, general cognitive feedback, and action prompt.

Area of Interest Feedback: This module took as input the gaze and cognitive information and visualized feedback as bounding boxes on the ongoing lecture. These bounding boxes (depicted in Fig. 5 and Fig. 6) were clustered from gazes that were labeled as fixations using a spectral clustering algorithm. The bounded area was where students were looking. The opacity of the bounding box represented the ratio of students looking at this area over all students, and the color represented the ratio of students that were confused about the contents in the area over students looking at this area. More details about the spectral clustering method were depicted in Appendix A.2.

![](images/10.jpg)  

Figure 9: Model accuracy and F1 score comparison on our newly collected dataset. Left $^ { ( \mathbf { a } , \mathbf { c } ) }$ shows comparison (a: accuracy, c: F1 score) among deep learning models and LLMs-based models using GPT-4o. Right $^ { ( \mathbf { b } , \mathbf { d } ) }$ shows comparison (b: accuracy, d: F1 score) of LLMs-based models using GPT-4o v.s. GPT-4o mini.

General Cognitive Feedback: This module took as input the cognitive information, and visualized feedback as a summarized bar chart (depicted in Fig. 5 and Fig. 6). We displayed the ratio of students that were not confused about the contents over all students (knowledge ratio), and the ratio of students that were paying attention over all students (attention ratio).

Action Prompt: Based on the general cognitive information, the system provided teaching suggestions to the instructor (depicted in Fig. 5 and Fig. 6). When the attention ratio fell lower, instructors were prompted to draw attention from students. When the knowledge ratio fell lower, repeating the current contents was recommended (Fig. 5(b)).

# 6 Evaluation

Based on the students' data collected from our workshop, we explored the simulation performance in not only straightforward

accuracy comparison, but also the dynamic patterns of students' learning performance at fine-grained levels.

# 6.1 Simulation Settings

The simulation settings were similar with those in the EduAgent dataset. We split the dataset into training and testing set by following a individual-wise manner with 0.7 ratio. We set the first five questions as past questions of the student history and other questions as future questions for prediction. Both of the model input and output were the same as the EduAgent simulation experiment (Fig. 2 and Fig. 3). For LLMs-based models, we obtained the results with or without our TIR module for both prompting-based models (standard and CoT prompt) and finetuning-based models (BertKT), which were compared with deep learning models. All LLMs-based models used both GPT-4o and GPT-4o mini.

# 6.2 TIR Makes LLMs Superior than Deep Learning Models

We first compared the accuracy and F1-score of the simulated performance of various models by comparing with the real students' performance. As depicted in Fig. 9(a), without the integration of our TIR module, the best model is SimpleKT with 0.656 accuracy, which was better than all LLMs-based models. However, with our TIR module, all LLMs-based models increased the simulation accuracy and all had larger accuracy than the SimpleKT model. The F1 score results were similar as well in Fig. 9(c), i.e. all TIR-augmented LLMs held better F1 score than all deep learning models. These results are encouraging because the selected deep learning models are proven educational models widely accepted in the educational domain [18, 21, 46, 60, 94] to inform teaching practices and support adaptive learning strategies [66]. Therefore, the superior predictive capability of our model indicates a strong potential for realworld applicability. To further demonstrate our model's impact, we selected BertKT with TIR as an example to perform statistical analysis by comparing with the five deep learning models in individual-level, lecture-level, and question-level. The effect of different models on student simulation performance was measured by repeated-measures ANOVA with paired t-tests for pair-wise comparisons.

![](images/11.jpg)  
F Heaa hove cura  rdividl sude si el

Individual-Level: We calculated the average simulation accuracy for each individual student, as depicted in Fig. 10. We found a significant effect of the model on simulation accuracy $( F ( 5 , 8 5 ) =$ 5.16, $\mathnormal { p } = 0 . 0 0 0 4$ $\eta _ { p } ^ { 2 } = 0 . 1 8 )$ , indicating a large effect size. For pairwise comparisons, significant differences were observed between the following model pairs:

AKT vs. DKT: $t ( 1 7 ) = 2 . 2 9$ $p = 0 . 0 3 5$ $\eta _ { p } ^ { 2 } = 0 . 1 6$ .   
BertKT with TIR vs. ATKT: $t ( 1 7 ) = 3 . 4 2$ $p = 0 . 0 0 3$ , $\eta _ { p } ^ { 2 } =$ 0.24.   
BertKT with TIR vs. DKT: $t ( 1 7 ) = 3 . 2 4$ , $p = 0 . 0 0 5$ , $\eta _ { p } ^ { 2 } = 0 . 3 0$ .   
BertKT with TIR vs. DKVMN: $t ( 1 7 ) = 3 . 8 3$ , $p = 0 . 0 0 1$ , $\eta _ { \mathscr P } ^ { 2 } =$ 0.24.   
BertT with TIR vs. SimpleKT: $t ( 1 7 ) = 2 . 4 5$ $p = 0 . 0 2 5$ $\eta _ { \mathscr P } ^ { 2 } =$ 0.14.

These results indicate that BertKT (with TIR) exhibited significantly better performance compared to most deep learning models. Although we did not find significance between the BertKT (with TIR) and AKT, Fig. 10 still showed better simulation accuracy of BertKT (with TIR) than AKT for most individual students.

Lecture-Level: We then calculated the average simulation accuracy for each specific course lecture ID, as depicted in Fig. 11.

Results showed a significant effect of the model on simulation accuracy $( F ( 5 , 5 5 ) = 4 . 5 3$ , $\mathnormal { p } = 0 . 0 0 2$ $\eta _ { p } ^ { 2 } = 0 . 2 0 )$ , indicating a large effect size. Significant differences were observed between the following model pairs:

AKT vs. DKVMN: $t ( 1 1 ) = 2 . 4 9$ $\begin{array} { r } { p = 0 . 0 3 } \end{array}$ , $\eta _ { \mathscr P } ^ { 2 } = 0 . 2 1$ . BertKT with TIR vs. ATKT: $t ( 1 1 ) = 3 . 0 6$ $p = 0 . 0 1$ , $\eta _ { p } ^ { 2 } = 0 . 2 8$ BertKT with TIR vs. DKT: $t ( 1 1 ) = 2 . 9 1$ $\mathnormal { p } = 0 . 0 1 4$ $\eta _ { \mathscr P } ^ { 2 } = 0 . 2 7$ BertKT with TIR vs. DKVMN: $t ( 1 1 ) = 4 . 2 7$ $p = 0 . 0 0 1$ , $\eta _ { \mathscr P } ^ { 2 } =$ 0.35.

Similar with individual-level results, these results indicate the superiority of the BertKT (with TIR) than these deep learning models. Although we did not find significance between the BertKT (with TIR) and AKT/SimpleKT, Fig. 11 still showed better simulation accuracy of BertKT (with TIR) than AKT/SimpleKT for most lectures.

Question-Level: We then calculated the average simulation accuracy for each specific question $\mathrm { I D }$ in post-test, as depicted in Fig. 12. Results did not find a significant effect of the model on simulation accuracy $( F ( 5 , 3 0 ) = 1 . 0 9$ $p = 0 . 3 8 7$ , $\eta _ { p } ^ { 2 } = 0 . 1 4 )$ .However, significant differences were observed between the following model pairs:

BertKT with TIR vs. DKVMN: $t ( 6 ) = 3 . 6 9 _ { }$ $p = 0 . 0 1$ , $\eta _ { p } ^ { 2 } =$   
0.46. BertKT with TIR vs. simpleKT: $t ( 6 ) = 2 . 5 9$ $\mathnormal { p } = 0 . 0 4 1$ , $\eta _ { \mathscr P } ^ { 2 } =$   
0.32.

Although we did not find significance between the BertKT (with TIR) and AKT/ATKT/DKT, Fig. 12 still showed better simulation accuracy of BertKT (with TIR) than AKT/ATKT/DKT for most question IDs.

# 6.3 TIR Enhances Model Learning Efficiency

Although the training of BertKT $^ { \cdot } +$ TIR model used all training students, it is worth noting that, for other prompt-based models (Standard and CoT), we only used four example students as the contextual example demonstration instead of all students in the training set. However, after integrating our TIR module, both of prompt-based models (Standard and CoT) achieved better simulation performance than deep learning models (Fig. 9), which used all students in the training set for model training. This demonstrates that our TIR module could enhance the exploitation efficiency of prompt-based models to achieve comparable or even more realistic student simulation within more limited training data.

![](images/12.jpg)  

Figure 11: Heatmap to show the average simulation accuracy (each cell) for each specific lecture using each model.

# 6.4 TIR Empowers Smaller LLMs

We also compared the simulation performance using both GPT-4o and GPT-4o mini. We found that the integration of our TIR module increased all of the GPT-4o mini based simulation models (Standard, CoT, BertKT), which were even better than the simulation models using GPT-4o without the TIR module. Note that GPT-4o mini is a much smaller model than GPT ${ 4 0 } ^ { 7 }$ . Without TIR, GPT-4o had apparently better simulation performance than GPT-4o mini in Standard and CoT models, as depicted in Fig. 9(b). However, after integrating the TIR module, both Standard and CoT models in GPT-4o mini outperformed GPT-4o in an obvious margin (Fig. 9(b)). These results demonstrate that the TIR module could improve the smaller LLMs to learn from example students in the training set more effectively. As a result, smaller LLMs could achieve comparable or even better performance, eliminating the need of using larger size LLMs.

# 6.5 TIR Captures Individual Differences Better

We then examined whether the models could capture the individual differences and correlation among simulated and real students. Specifically, we used the BertKT with or without the TIR module (baseline) for simulation and compared with the label (real students' groundtruth). This was quantitatively measured by the Pearson correlation between the simulated students' test accuracy sequence along with student IDs and the real students'. As depicted in Fig. 13, we found that the integration of the TIR module better captured the correlation between simulated and real students regarding the learning performance sequence along with student IDs than the no TIR case and apparently improved the Pearson correlation from $r = 0 . 0 2$ (No TIR) to $r = 0 . 4 2$ (With TIR). These results demonstrate that our TIR module enabled more realistic simulation by better capturing the individual differences of student learning performance.

We further checked the statistical difference of the average simulation accuracy per student with or without the TIR module (baseline). The normality of the differences between both was assessed using the Shapiro-Wilk test, which indicated no significant deviation from normality $W = 0 . 9 2 3 5$ $\mathnormal { p } = 0 . 1 4 9 3$ . $\mathrm { d f } = 1 7$ . A paired

$$
F ( 5 , 3 0 ) \mathrm { = } 1 . 0 9 , p = 0 . 3 8 7 , \eta _ { p } ^ { 2 } = 0 . 1 4
$$

![](images/13.jpg)  

Figure 12: Heatmap to show the average simulation accuracy (each cell) for each post-test question ID using each model.

t-test was then conducted to evaluate the impact of the TIR module on prediction performance. The results showed a statistically significant improvement in accuracy with the TIR module compared to the baseline $\mathit { \check { t } } = 2 . 4 1 3 9$ $p = 0 . 0 2 7 3$ . $\operatorname { d f } = 1 7$ ). A Bland-Altman analysis revealed a mean difference (bias) of 0.0881 ( $9 5 \%$ CI: 0.0186 to 0.1577), with the limits of agreement ranging from -0.2069 $9 5 \%$ CI: -0.5100 to 0.0962) to 0.3832 ( $9 5 \%$ CI: 0.0800 to 0.6863). The BlandAltman plot (Fig. 13(d)) visualizes these findings, showing the mean difference as a dashed red line and the limits of agreement as dashed blue lines. The scatter of points around the mean difference is relatively consistent, suggesting that the agreement between the two models does not vary substantially across the range of predicted accuracy values. These results indicate a consistent positive effect of the TIR module on prediction performance, while maintaining acceptable levels of agreement with the baseline model.

# 6.6 TIR Captures Lecture Correlation Better

We then examined whether the models could capture the lecture correlation and differences. Specifically, we still used the BertKT with or without the TIR module (baseline) for simulation and compared with the label (real students' groundtruth). Since different lectures had their own difficulty, students therefore had different learning performance (post-test question accuracy) across different lectures. Therefore, by comparing the trend of simulated and real students' learning performance along with the lectures, we could see whether the simulation models could capture such variations of lecture difficulty and cross-lecture correlation. This trend was quantitatively measured by the Pearson correlation between the simulated students' test accuracy sequence along with lectures and the real students' sequence. As depicted in Fig. 14(a), we found that the integration of the TIR module better captured the correlation between simulated and real students regarding the learning performance sequence along with lectures than the no TIR case and apparently improved the Pearson correlation from $r = 0 . 4 2$ (No TIR) to $r = 0 . 5 2$ (With TIR). For more intuitive visualization in individual students, we showed the average question answering accuracy in each lecture for each specific simulated and real student, as depicted in Fig. 14(b,c,d). This visualization also revealed larger similarity between simulated students (with TIR) and real students, compared with the simulation similarity without TIR. These results demonstrate that our TIR module enables more realistic simulation by better capturing the lecture correlation in student learning performance.

![](images/14.jpg)  
vaveur (e) (error bar: $\mathbf { 9 5 \% }$ confidence interval) between two models. (f. Average simulation accuracy and F1 score for each individual student using two models.

We further checked the statistical difference of the average simulation accuracy per lecture with or without the TIR module. The normality of the accuracy differences between them was evaluated using the Shapiro-Wilk test, which indicated no significant deviation from normality $\mathrm { ' } W = 0 . 9 7 3 6$ , $\mathnormal { p } = 0 . 9 4 4 4$ : $\operatorname { d f } { = } 1 1$ ). A paired t-test was then performed to assess the impact of the TIR module on simulation performance. The analysis revealed a statistically significant improvement in accuracy with the TIR module compared to the no TIR case $\prime = 2 . 5 1 7 3$ , $p = 0 . 0 2 8 6$ . $\operatorname { d f } { = } 1 1$ ). Bland-Altman analysis (Fig. 14(g)) showed a mean difference (bias) of 0.0764 ( $9 5 \%$ CI: 0.0195 to 0.1334), with limits of agreement ranging from -0.1209

$9 5 \%$ CI: -0.3263 to 0.0845) to 0.2738 ( $9 5 \%$ CI: 0.0684 to 0.4792). These findings suggest that the TIR module consistently enhances prediction performance while demonstrating acceptable agreement with the baseline model.

# 6.7 TIR Captures Question Differences Better

Moreover, we further explored whether the models could capture the different questions' correlation using the BertKT with or without the TIR module. Different questions corresponded to specific knowledge concepts of course materials. Therefore, by comparing the trend of simulated and real students' test accuracy along with different questions, we could see whether the simulation models could capture students' learning performance across fine-grained and varying knowledge concepts. This trend was also quantitatively measured by the Pearson correlation between the simulated student question accuracy sequence along with question ID and the real students'. As depicted in Fig. 15(a), we found that the integration of the TIR module better captured the correlation with question ID compared with real cases (label) than the no TIR case and apparently improved the Pearson correlation from $r = - 0 . 5 0$ (No TIR) to $r = 0 . 3 7$ (With TIR). For more intuitive visualization in individual students, we showed the average question answering accuracy in each question for each specific simulated and real student, as depicted in Fig. 15(b,c,d). This visualization also revealed larger similarity between simulated students (with TIR) and real students, compared with the simulation similarity without TIR. These results demonstrate that our TIR module enables more realistic simulation by better capturing the question correlation (i.e. knowledge concept correlation) in student learning performance.

![](images/15.jpg)  
FvB t question answer accuracy. $( \mathbf { e } , \mathbf { f } , \mathbf { g } , \mathbf { h } )$ . The distribution of simulation accuracy differences (e), boxplot (f), Bland-Altman plot to show the mean differences $\mathbf { \mu } ( \mathbf { g } )$ , and barplot (h) (error bar: $9 5 \%$ confidence interval) between two models. (i). Average simulation accuracy and F1 score for each lecture ID using two models.

We then checked the statistical difference of the average simulation accuracy per question with or without the TIR module (baseline). The normality of the differences between both was assessed using the Shapiro-Wilk test, which indicated a significant deviation from normality $W = 0 . 7 4 5 1$ , $\mathbf { \mathit { p } } = 0 . 0 1 1 3$ $\mathrm { d f } = 6$ . Therefore, a Wilcoxon signed-rank test (instead of a paired t-test) was performed to evaluate the impact of the TIR module on prediction performance. The test revealed a statistically significant improvement in accuracy with the TIR module compared to the baseline $\mathbf { \mathit { p } } = 0 . 0 2 7 7$ . $\mathrm { d f } = 6$ ). A Bland-Altman analysis (Fig. 15(g)) showed a mean difference (bias) of 0.1437 ( $9 5 \%$ CI: 0.0306 to 0.2568), with the limits of agreement ranging from -0.1555 $9 5 \%$ CI: -0.4754 to 0.1644) to 0.4429 ( $9 5 \%$ CI: 0.1230 to 0.7628). These results demonstrate a significant positive effect of the TIR module on prediction performance, with an acceptable level of agreement between the two models.

# 6.8 Dynamism of Skill Levels in Learning Path

Furthermore, we explored whether the simulation captured the dynamism of students' skill levels in the learning path. Here the learning path referred to the chronological learning process from the first slide to the last slide in the lecture. In our online education system, students' skill levels were represented by the average question answering accuracy where the questions were corresponding to a specific slide. This enabled us to measure to what extent the students mastered the knowledge concepts per slide. We still used the BertKT with or without the TIR module for simulation and compared with the label. Then we compared the trend of simulated and real students' skill levels along with the slide ID. As depicted in Fig. 16(a), we found that the integration of the TIR module better captured the dynamism of skill levels in the whole learning path from the first slide to the last slide compared with real cases (label) than the no TIR case. For more intuitive visualization in individual students, we also showed the average question answering accuracy in each slide for each specific simulated and real student, as depicted in Fig. $^ { 1 6 ( \mathrm { b } , \mathrm { c } , \mathrm { d } ) }$ . This visualization also revealed larger similarity between simulated students (with TIR) and real students, compared with the simulation similarity without TIR. This trend was also quantitatively measured by the Pearson correlation between the simulated skill level sequence along with slide ID and the real student sequence along with slide ID. However, since different lectures had different slides, we analyzed the Pearson correlation in each lecture. As depicted in Fig. 16(e), the integration of our TIR module captured better correlation in most lectures. These results demonstrate that our TIR module enables more realistic simulation by better capturing the dynamism of students' skill levels across the learning path.

![](images/16.jpg)  
: . Catw  s ur anar. (HaerBr r represents higher question answer accuracy. $( \mathbf { e } , \mathbf { f } , \mathbf { g } , \mathbf { h } )$ . The distribution of simulation accuracy differences (e), boxplot (f), Bland-Altman plot to show the mean differences $\mathbf { \rho } ( \mathbf { g } )$ , and barplot (h) (error bar: $\mathbf { 9 5 \% }$ confidence interval) between two models. (i). Average simulation accuracy and F1 score for each post-test question ID using two models.

# 6.9 Fine-Grained Inter-Student Correlation

One important aspect of contextual simulation was to not only capture the individual differences in learning but also the individual correlations in the same course. Fig. 17 showed the inter-student correlation for both simulated students (BertKT with or without TIR) and real students. Each node represented one student and two nodes were connected if both students took the same lecture. The color depth of each node represented the average question answering correctness of all lectures that the individual student attended. The weight of the edge connected by two nodes represented the inter-student correlation, which was calculated by the Pearson correlation between the question correctness sequences of two students corresponding to the questions from the overlapped lectures between two students. Note that each student attended multiple lectures. But two students might not attend the same lectures. Therefore, the edge weight only considered the overlapped lectures between two students. However, the color depth of each node considered all lectures that one student had attended. That was why the edge weight might be 1 but the two nodes had different color depth. This meant that the students had the same correctness sequence for the overlapped lectures but their overall accuracy for all lectures attended by each student was different. As depicted in Fig. 17, we found that the integration of the TIR module better captured both individual student learning performance (average question answering correctness represented by the color depth of each node) and inter-student correlation (Pearson correlation of question correctness sequences between two students, represented by the edge weight between two nodes), which were more similar with real students (label), compared with the model without the TIR module. These results demonstrate that our TIR module enables more realistic and finer-grained simulation by better capturing the inter-student correlation in student learning performance.

![](images/17.jpg)  
r y real student sequence along with slide ID in each lecture on our newly collected dataset.

# 7 Discussion

In this work, we run a 6-week online education workshop to collect fine-grained annotations of both course materials and student learning performance. This enabled a contextual student simulation to consider the effect of course materials' modulation on student learning. We further improved the student simulation by proposing a transferable iterative reflection module that augmented both prompting-based LLMs' simulation and finetuning-based LLMs' simulation, which achieved even better performance than deep learning models. This was also verified in another public dataset.

# 7.1 Application Scenarios

With the increasing importance of AI-assisted education and intelligent tutoring systems, our work could serve as the important groundwork to support a list of applications in educational context.

7.1.1 Student: Enhancing Self-Learning. The classroom simulacra could create digital twins about a specific student based on the past learning histories. This digital twin further emulates the student's learning performance in the future course materials and tests. This could support the self-assessment and reflections of students to set personalized goals based on their learning pace. Specifically, students could track their learning trends and progress and see how their skills have developed. As a result, it could help students reflect on their strengths and weakness to be improved during learning. Students can also set more appropriate learning goals, milestones, and study priorities based on the simulated results so that they can be motivated to achieve clearer learning targets. Such decision making and reflection in learning are also related to the metacognitive skills of students to develop learning strategies and study habits.

![](images/18.jpg)  
b between two students.

7.1.2 Instructor: Adapting Teaching Strategy. Teachers utilizing this system will be able to analyze predictions across an entire class or cohort, allowing them to test pedagogical approaches or curriculum structures before they even deploy them on a class of real human students. Having access to such a representative digital class can allow teachers to use simulation results to tailor the learning experience, presenting students with course contents that truly match their skill levels. With the classroom simulacra, the instructors could optimize the curriculum design by identifying the common areas of difficulty, which is achieved by analyzing simulation results across the whole class. For instance, students who turn out to learn fast can accept accelerated course pace while students who may struggle with upcoming tests or course contents can be delivered more related course resources to build personalized learning path. This could also improve the teaching resource allocation by enabling instructors to allocate teaching resources more effectively by identifying skills that may require more time for students to master. Last but not least, this system could provide insights for adaptive interventions of instructors so that students could receive personalized interventions (like verbal reminder or one-on-one tutoring) when they are identified to be at risk of falling behind.

7.1.3 Parents: Home Support. In the home environment, the classroom simulacra can simulate specific children's learning performance based on their past learning history. As a result, parents can have insightful information about how well their children will do in certain curriculum (strengths and weakness), and they could make better decision-making for tasks like choosing the right school, extracurricular activities, choosing advanced courses, and wisely investing in a tutoring service that can target specific areas where their children may struggle. Parents can also support learning outside of school through informed insights about necessary home activities and resources which align with their children's predicted needs and create a conducive study environment tailored to their children's learning style according to the performance simulation insights.

# 7.2 Limitations and Future Work

We also acknowledge the potential limitations in this work.

7.2.1 Population Diversity. The first limitation is the population diversity in our online education workshop. We decide to recruit students from elementary schools and high schools because these students usually do not have prior knowledge about our course materials related to Artificial Intelligence. As a result, we could better capture the learning performance and learning outcomes of students when they learn new knowledge. However, we also acknowledge that the data collection with more diverse populations (such as different age groups) could better support and further extend the findings of our experiments.

7.2.2 Simulated Behavioral Types. In our work, we use the students' question answering correctness to represent their learning performance, which can be mapped to students' skill levels on related course concepts. By further mapping them into specific slide IDs in the course (such as Section. 6.8), this simulation can reflect the students' learning behaviors during the course. However, we acknowledge that student behaviors are not limited to question correctness. We clarify that the cognitive states information was only used for teachers to obtain students' learning states during data collection, and was not used for behavioral simulation. There are also more diverse learning behaviors in the real world scenarios such as reasoning processes, learning reflections, personal preferences, learning styles, etc. For example, students' cognitive states (such as attention, confusion) during the course could directly reflect their learning styles and personal preferences in the course. The simulation for such additional learning behaviors could provide further insights and evidence support. We also believe that LLMs have great potential in simulating such diverse behaviors due to the strong in-context learning ability [81] and large knowledge base [2]. Therefore, such simulation on more diverse learning behavioral types could be the future directions for exploration.

7.2.3 Generalization and Cost Differences. We clarify that our goal is to show that TIR can augment LLMs to achieve better performance than themselves without TIR and deep learning models. We have a fair comparison within each LLMs-based model with or without TIR using the same training/testing data. But we do not intend to directly compare prompting-based LLMs with finetuning-based LLMs. Because both have their own pros and cons. For example, prompting-based LLMs need less training data but finetuning-based LLMs have better simulation performance. However, for future potential applications to extend the fine-tuned models in external datasets, it is also necessary to fine-tune models again in such new datasets, which is similar to deep learning models that use training data to update model weights. As such, it is not comparable/applicable to directly compare both regarding the generality or training time/computational resources. When comparing with deep learning, we mainly use finetuning-based LLMs for a fair comparison because they use the same training data. However, using much less training data, the TIR-augmented prompting-based LLMs can also achieve comparable or even better performance than deep learning. This also demonstrates the effectiveness of our TIR module.

7.2.4 Insights for Educators. Our current classroom simulacra serves as a student simulation model. Integrating it into an end-to-end intelligent teaching system entails non-trivial efforts. Nevertheless, the classroom simulacra is grounded in a real-world studenteducator interaction dataset. Its predictive capability aligns with proven educational models that have been utilized to successfully inform teaching practices and support adaptive learning strategies [66]. The foundational accuracy of our model indicates a strong potential for real-world applicability, as seen in similar simulations that have influenced educational strategies even before empirical testing [84]. The simulator can support educators by delivering actionable insights that enhance personalized interventions, curriculum design, and evidence-based teaching practices. It can identify specific knowledge gaps for individual students, enabling targeted interventions, and allows educators to explore hypothetical scenarios to optimize teaching strategies for diverse learner profiles. Additionally, the simulator aids in curriculum optimization by simulating student responses to different teaching methodologies, helping to refine pacing and content sequencing. Therefore, the simulator provides a research-backed tool for testing the impact of instructional methods and predicting long-term outcomes. Beyond learning analytics, it integrates behavioral insights to detect learning issues and offers a safe experimental environment for innovative teaching approaches. Case studies in our work illustrate its practical utility, such as identifying impactful topics for exam preparation based on students' different learning performance on different questions (question-level) and guiding classroom time allocation based on students' learning performance across different slides (slide-level). In conclusion, while real-world educator experiences would strengthen our findings, the current study offers a solid foundation that demonstrates the simulator's predictive power and practical potential. Future work that includes educator feedback will further bolster our understanding and validation of its effectiveness in real classroom settings.

# 8 Conclusion

We present Classroom Simulacra, a contextual student generative agent environment powered by large language models for learning behavioral simulation in online education. Such student agent mimics real students' past learning histories and takes actions in new course materials and test questions. This student agent environment is powered by our new fine-grained datasets and a powerful transferable iterative reflection module to augment the simulation performance. A series of experiments demonstrate the feasibility and effectiveness of our student simulation. With the increasing importance of AI-based education and intelligent tutoring systems, we believe that our work can serve as the important groundwork to support diverse applications in educational context.

# Acknowledgments

This work is supported by National Science Foundation CNS-2403124. CNS-2312715, CNS-2128588 and the University of California San Diego Center for Wireless Communications.