# Tuna: Taming $\mathbf { U }$ nified Visual Representations for Native Unified Multimodal Models

Zhiheng Liu1Weiming Ren1,Haozhe Liu1 Ziian Zhou1Shoufa Chen1 $\mathsf { \pmb { Q } i \mathsf { \pmb { u } } ^ { 1 } }$ , Xiaoke Huang1, Zhaochong $\mathsf { \pmb { A } } \mathsf { \pmb { n } } ^ { 1 }$ ,Fanny Yang1, Aditya Patel, Viktar Atliha1,Tony ${ \mathsf { N } } { \mathsf { g } } ^ { 1 }$ , Xiao Han1, Chuyan ${ \tt z h u ^ { 1 } }$ , Chenyang Zhang1, Ding Liu1, Juan-Manuel Perez-Rua1, Sen $\mathsf { H } \mathsf { e } ^ { 1 }$ , Jürgen Schmidhuber4, Wenhu Chen3, Ping Luo2, Wei Liu1, Tao Xiang1, Jonas Schult $^ { 1 , * }$ , Yuren Cong1,\* 1Meta BizAI, $^ 2$ HKU, $^ 3$ University of Waterloo, $^ 4$ KAUST †Joint first authors, listedalphabetically by last name, $^ { \ddag }$ Core contributors, \*Joint project lead

Unified multimodal models (UMMs) aim to jointly perform multimodal understanding and generation within a single framework. We present TuNA, a native UMM that builds a unified continuous visual representation by cascading a VAE encoder with a representation encoder. This unified representation space allows end-to-end processing of images and videos for both understanding and generation tasks. Compared to prior UMMs with decoupled representations, TuNA's unified visual space avoids representation format mismatches introduced by separate encoders, outperforming decoupled alternatives in both understanding and generation. Moreover, we observe that stronger pretrained representation encoders consistently yield better performance across all multimodal tasks, highlighting theimportance of the representation encoder. Finaly, in this unifed setting, jointly training on both understanding andgeneration data allows the two tasks to benefit from each other rather than interfere. Our extensive experiments on multimodal understanding and generation benchmarks show that TuNA achieves state-of-the-art results in image and video understanding, image and video generation, and image editing, demonstrating the effectiveness and scalability of its unified representation design. Correspondence: zhihengl0528@connect.hku.hk, w2ren@uwaterloo.ca, schult@meta.com, yuren@meta.com Project page: https://tuna-ai.org

# 1 Introduction

A long-term aspiration of multimodal AI is natively $\bot$ unified multimodal generation, in which a single model can seamlessly understand and generate diverse modalities such as text, images, and videos. Recent advances in unifed multimodal models (Team, 2024; Deng et al., 2025a; Xie et al., 2025a) have shown promising results towards this vision, suggesting that truly integrated multimodal intelligence is increasingly within reach. A central challenge in developing native UMMs lies in how visual inputs are encoded into representations. Current UMMs adopt one of two approaches: (1) decoupled visual representations for understanding and generation tasks,or ( a unifedvisual representation shared across both tasksIntuitively, learning aid visual representation for both tasks offers compelling advantages for UMMs.

First, UMMs with decoupled representations, such as BAGEL (Deng et al., 2025a) and Mogao (Liao et al., 2025), often adopt MoE-style architectures to handle different visual encoders, introducing additional parameters that increase training and inference costs. In contrast, a unifed representation alows the model to operate withina single representation space, simplifying training and improving efficiency.Second, different vision encoders typically produce representations with incompatible formats. For the same input, features from a representation encoder (e.g. SigLIP (Zhai et al, 2023)) and a causal VAE encoder (e.g. Wan 2.1 VAE (Wan et al., 2025)) differ in (1) spatial compression ( $1 6 \times$ vs. $8 \times$ ), (2) temporal compression (none vs. $4 \times$ ), and (3) channel dimension (1152 vs. 16). These discrepancies can cause representation conficts in decoupled models, whereas unified representations inherently avoid such inconsistencies. Finally, unified visual representations provide a clear pathway for achieving mutual enhancement between understanding and generation. While recent studies like Ross (Wang et al., 2024a) and REPA (Yu et al., 2024) show task-specific improvements in understanding-only and generation-only models, this synergy remains underexplored in existing UMMs.

![](images/1.jpg)  
Figure 1 We present TuNA, a native unified multimodal model built on a unifed visual representation, enabling diverultimoalnderstandinnd eneratio pabil u as magendeunderstandi agei generation, and image editing.

Nevertheless, current UMMs with unified visual representations often underperform their decoupled counterparts. Most existing approaches adopt a single type of vision encoder for both understanding and generation. For example, Chameleon (Team, 2024) and Transfusion (Zhou et al., 2024) use VQ-VAE (Esser et al., 2021), while Harmon (Wu et al., 2025d) utilizes the MAR encoder (Li et al., 2024c). This unified design tends to favor one task at the expense of the other. Show-o2 (Xie et al., 2025a) attempts to mitigate this issue by u SgLIPZhai al 03and VAEWan  025tu rou alateus ratey.Howv, our analysis in Section 3.4 reveals that its learned representation remains biased toward semantic features, resulting in limited generation quality. To systematically address these limitations, we propose TuNA, a native UMM that employs unified visual representations across understanding and generation. Our design is simple yet highly effective: by directly connecting a VAE encoder to a representation encoder, weobtain representations that are suffciently expressive for diverse multimodal tasks. These unified visual features are fused with text tokens and processed by an LLM decoder, which subsequently generates new text tokens and denoised images through autoregressive next-token prediction and fow matching. As illustrated in Figure 1, our unified visual representation enables TUNA to handle image and video understanding, image and video generation, and image editing within a single framework. By conducting a three-stage training, our model achieves state-of-the-art performance on multimodal understanding and generation benchmarks (e.g. 61.2% on MMStar (Chen et al., 2024b) and 0.90 on GenEval (Ghosh et al., 2023)). Our main contributions can be summarized as follows: 1. We propose TuNA, a native unified multimodal model with unified visual representations, enabling image/video understanding, image/video generation, and image editing within a single framework. 2.Our extensive experiments show that TunA's unified visual representation is highly efective, achieving state-of-the-art performance across multiple multimodal understanding and generation tasks. 3. We further perform a comprehensive ablation study, demonstrating the superiority of our unified visual representation design over existing methods such as Show-o2 and other models employing decoupled representations.

# 2 Our Method: Tuna

In this section, we introduce TunA, a native unified multimodal model that employs unifed visual representations across all multimodal understanding and generation tasks. We begin by outlining the key motivations behind our model design in Section2.1, followed by a detailed desciption f TuA' architecture and training pipeline in Sections 2.2 and 2.3, respectively. An overview of our overall framework is shown in Figure 2.

# 2.1 Motivation and Design Principles

Wediscuss hefollowingservations, which motivate he desiTunA and its nife visual repreentations: Autoregressive vs. diffusion. Both text and image/video generation can be achieved using autoregressiveGrattafori et al, 024; Yang et al, 2025; Sun et al 2024)or diffusin models (Nieet al 2025; Bati et al 2025; Wan et al, 2025). In practice, leading understanding-ony models (Bai et al., 2025; Wangetal, 20 adopt autoregressive models for text generation.On the other hand state-of-the-art image and video generators (Esser et al., 2024; Wan et al., 2025) employ (latent2) diffusion models with fow matching. Continuous vs. discrete visual representations. We observe that image and video generation models operating in a continuous (e.g., KL-regularized) VAE latent space (Esser et al., 2024; Wan et al., 2025) outperform those using discrete representations (Sun et al., 2024), as discretization causes information loss and reduces fidelity. Similarly, multimodal understanding models (Bai et al., 2025; Wang et al., 2025c) typically rely on continuous semantic features (e.g., CLIP (Radfor et al., 2021) features), suggesting that continuous visua representations are inherently more effective for both understanding and generation tasks.

![](images/2.jpg)  
Figure 2 Overview of the TuNA architecture. Our model employs a VAE encoder and a representation encoder to construc unid visual represntatins, which are then combine with text tkens and processed by an LLM decer. The decer peortrgressivexneatio ndertandingasksanoatchngbsil fskunkeb

Semantic representations benefit visual generation. Recent studies suggest that semantic featuresehance visual generation. For instance, REPA (Yu et al., 2024) demonstrates that diffusion transformers benefit from aligning intermediate features with pretrained representation encoders like DINOv2 (Oquab et al., 2023). Concurrent to our work, RAE (Zheng et al., 2025) employs a frozen representation encoder to encode images into latent representations, showing that pretrained semantic features alone can reconstruct input images effectively. VAE latents can oupprtnderstandintasks.Weserv hat bo iscand continuus VAE lants, orginally designed for visualreconstruction, can also support semanticunderstanding tasksRecent approaches such as UniTok (Ma et al., 2025a) and TokLIP (Lin et al., 2025b) enhance VQ-VAE latents with semantic understanding capability through contrastive learning. Other works explore diffusion models with continuous VAE latents for semantic understanding and dense prediction tasks, including semantic segmentation (Zhu et al., 2024), object recognition (Li et al., 2023a), and image retrieval (Zuo et al., 2024). Building on these observations, we design TuNA with the following key characteristics: TunA integrates autoregressive text generation with flow matching for image and video generation. TUNA builds its unified visual representation on continuous VAE latents, as these latents efectively support both understanding and generation tasks. To further enhance performance, TunA employs representation encoders to extract higher-level features from the VAE latents, improving the quality of both understanding and generation.

# 2.2 Model Architecture

Unifidvisualrepresentations.As illustrated in Figure, TunA constructs its unifed visual representation using a VAE encoder and a representation encoder. Given an input image or video $\mathbf { X }$ , we apply the 3D causal VAE encoder from Wan 2.2 (team, 2025), which downsamples the input by $1 6 \times$ spatially and $4 \times$ temporally, producing the latent $\mathbf { x } _ { 1 }$ . We then generate a noisy latent ${ \bf x } _ { t } = t { \bf x } _ { 1 } + ( 1 - t ) { \bf x } _ { 0 }$ , where $t \in [ 0 , 1 ]$ is a sampled timestep and $\mathbf { x } _ { 0 } \sim \mathcal { N } ( 0 , 1 )$ . Next, we use the SigLIP 2 vision encoder $\Phi$ (patch size 16, pretrained resolution 512) to extract semantic features from the VAE latents. Since the VAE encoder has $1 6 \times$ downsampling, we replace SigLIP 2's original 16 $\times$ 16 patch embedding layer with a randomly initialized $1 \times 1$ patch embedding layer, forming a modified encoder $\Phi ^ { \prime }$ . This ensures that the token sequence lengths of $\Phi ( \mathbf { X } )$ and $\Phi ^ { \prime } ( \mathbf { x } _ { t } )$ are consistent. Finally, we apply a two-layer MLP connector to obtain the unified visual representations $\mathbf { z } = \mathbb { M L P } ( \Phi ^ { \prime } ( \mathbf { x } _ { t } ) )$ . During training, we randomly sample $t$ between $[ 0 , 1 ]$ for visual generation and fix $t = 1$ for multimodal understanding such that $\mathbf { x } _ { t }$ always corresponds to the clean latent.

![](images/3.jpg)  
Fiur3Attention masks in the  decoder or understanding and generation tasks. indicates that thevisua tokens are noised.

For video inputs, where $\mathbf { x } _ { t } \in \mathbb { R } ^ { b \times c \times f \times h \times w }$ (with $b$ as batch size, $f$ as the number of latent frames, and $c$ , $h$ , $w$ as channel, height, and width), we aim to prevent the representation encoder $\Phi ^ { \prime }$ from processing excessively long sequences. Instead of fattening all latent frames into a single sequence, we apply a window-based attention mechanism by reshaping the frame dimension into the batch dimension in $\Phi ^ { \prime }$ . In einops notation, the unified visual representation $\mathbf { z } _ { v }$ can be expressed as:

$$
\begin{array} { r l } & { \bar { \mathbf { x } } _ { t } = \mathrm { r e a r r a n g e } ( \mathbf { x } _ { t } , \mathrm { b ~ c ~ f ~ h ~ w \to ~ ( b ~ f ~ ) ~ c ~ h ~ w } ) , } \\ & { \bar { \mathbf { z } } _ { v } = \mathtt { M L P } ( \Phi ^ { \prime } ( \bar { \mathbf { x } } _ { t } ) ) \in \mathbb { R } ^ { ( b \times f ) \times d } , } \\ & { \mathbf { z } _ { v } = \mathrm { r e a r r a n g e } ( \bar { \mathbf { z } } _ { v } , ( \mathrm { b ~ f ~ ) ~ d \to ~ b ~ ( f ~ d ) ) } , } \end{array}
$$

vhere $d$ is the hidden dimension of the video tokens. This operation effectively allows $\Phi ^ { \prime }$ to operate ndependently on each 4-frame window, significantly improving efficiency when processing video tokens.

LLM decoder and flow matching head. After obtaining the unified visual representation $\mathbf { z }$ , we prepend a timestep token representing the sampled timestep $t$ to $\mathbf { z }$ , concatenate this visual token sequence with language tokens and feed the combined sequence into an LLM decoder (Qwen-2.5 (Bai et al., 2025)) for joint multimodal processing. Following standard UMM practices (Xie et al., 2024b; Deng et al., 2025a), we apply a causal attention mask on language tokens and a bidirectional attention mask on visual tokens within the LLM decoder layers, as illustrated in Figure 3. For multimodal understanding tasks, the LLM decoder output is passed through a language modeling head to generate text token predictions. For visual generation and image editing, we feed the full token sequence to a randomly initialized flow matching head to predict the velocity for fow matching. This head shares the LLM decoder architecture and adds timestep conditioning via AdaLN-Zero, following Show-02 (Xie et al., 2025a) and DiT (Peebles and Xie, 2023). For generation and editing tasks, we adopt multimodal 3D-RoPE (Seawead et al., 2025; Su et al., 2024) over the concatenated text-visual sequence to handle interleaved instructions and visual content.

# 2.3 Training Pipeline

Toeffctively train ur unif model, we adopt a three-stagetraining strategy that progressively adaptsc model component to both understanding and generation tasks. adapt the semantic representation encoder to generate unified visual representations and to establish a robust initialization for the fow matching head. To this end, we train the representation encoder and fow matching head while freezing the LLM decoder, using two objectives: image captioning and text-to-image generation. Table1 Comparisons between TuNA and baseline models on multimodal understanding benchmarks. Results with mizerea tharrayBobe su ac scnernst. the results based on our evaluation scripts.   

<table><tr><td rowspan="2">Models</td><td rowspan="2">Size</td><td>MME</td><td>GQA</td><td>RealWorldQA</td><td>SEED</td><td>MMMU</td><td>MMStar</td><td>AI2D</td><td>ChartQA</td><td>OCRBench</td></tr><tr><td>perception</td><td>test-dev</td><td>test</td><td>image</td><td>val</td><td>avg</td><td>test</td><td>test</td><td>test</td></tr><tr><td colspan="10">Understanding-only Models (LMMs)</td></tr><tr><td>LLaVA-1.5 (Liu et al., 2023a)</td><td>7B</td><td>1510.7</td><td>62.00</td><td>54.8</td><td>65.8</td><td>35.7</td><td>33.1</td><td>55.5</td><td>17.8</td><td>31.8</td></tr><tr><td>Qwen-VL-Chat (Bai et al., 2023)</td><td>7B</td><td>1487.6</td><td>57.5</td><td>49.3</td><td>64.8</td><td>37.0</td><td>34.5</td><td>57.7</td><td>49.8</td><td>48.8</td></tr><tr><td>LLaVA-OV (Li et al., 2024a)</td><td>7B</td><td>1580.0</td><td>-</td><td>69.9</td><td>76.7</td><td>48.8</td><td>57.5</td><td>81.4</td><td>80.9</td><td>62.2</td></tr><tr><td colspan="10">Composite UMMs</td></tr><tr><td>TokenFlow-XL (Qu et al., 2025)</td><td>14B</td><td>1551.1</td><td>62.5</td><td>56.6</td><td>72.6</td><td>43.2</td><td></td><td></td><td></td><td></td></tr><tr><td>BLIP3-o (Chen et al., 2025a)</td><td>4B</td><td>1527.7</td><td>-</td><td>60.4</td><td>73.8</td><td>46.6</td><td>-</td><td>-</td><td></td><td></td></tr><tr><td>Tar (Han et al., 2025)</td><td>7B</td><td>1571.0</td><td>61.3</td><td>-</td><td>73.0</td><td>39.0</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>X-Omni (Geng et al., 2025)</td><td>7B</td><td>-</td><td>62.8</td><td>62.6</td><td>74.3</td><td>47.2</td><td>-</td><td>76.8</td><td>81.5</td><td>70.4</td></tr><tr><td colspan="10">1.5B-scale Native UMMs</td></tr><tr><td>Show-o (Xie et al., 2024b)</td><td>1.3B</td><td>1097.2</td><td>58.0</td><td>-</td><td>51.5</td><td>27.4</td><td></td><td>-</td><td>-</td><td>-</td></tr><tr><td>Harmon (Wu et al., 2025d)</td><td>1.5B</td><td>1155.0</td><td>58.9</td><td>49.8*</td><td>67.1</td><td>38.9</td><td>35.3*</td><td>57.0*</td><td>29.8*</td><td>11.2*</td></tr><tr><td>JanusFlow (Ma et al., 2025c)</td><td>1.3B</td><td>1333.1</td><td>60.3</td><td>41.2*</td><td>70.5</td><td>29.3</td><td>40.6*</td><td>54.2</td><td>42.4*</td><td>53.2*</td></tr><tr><td>SynerGen-VL (Li et al., 2025b)</td><td>2.4B</td><td>1381.0</td><td>-</td><td>-</td><td>-</td><td>34.2</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Janus-Pro (Chen et al., 2025b)</td><td>1.5B</td><td>1444.0</td><td>59.3</td><td>52.6*</td><td>68.3</td><td>36.3</td><td>43.1*</td><td>64.5*</td><td>23.4</td><td>48.7</td></tr><tr><td>Show-o2 (Xie et al., 2025a)</td><td>1.5B</td><td>1450.9</td><td>60.0</td><td>56.5*</td><td>65.6</td><td>37.1</td><td>43.4</td><td>69.0</td><td>40.*</td><td>24.5*</td></tr><tr><td>Tuna</td><td>1.5B</td><td>1461.5</td><td>61.4</td><td>62.5</td><td>69.3</td><td>39.1</td><td>54.6</td><td>71.4</td><td>82.1</td><td>71.9</td></tr><tr><td colspan="10">7B-scale Native UMMs</td></tr><tr><td>BAGEL (Deng et al., 2025a)</td><td>14B</td><td>1687.0</td><td>−</td><td>72.8</td><td>78.5</td><td>55.3</td><td>−</td><td>89.2</td><td>78.5</td><td>73.3</td></tr><tr><td>Emu3 (Wang et al., 2024c)</td><td>8B</td><td>-</td><td>60.3</td><td>57.4</td><td>68.2</td><td>31.6</td><td>-</td><td>70.0</td><td>-</td><td>68.7</td></tr><tr><td>VILA-U (Wu et al., 2024b)</td><td>7B</td><td>1401.8</td><td>60.8</td><td>-</td><td>59.0</td><td>-</td><td>-</td><td></td><td>-</td><td>-</td></tr><tr><td>MUSE-VL (Xie et al., 2024c)</td><td>7B</td><td>-</td><td>-</td><td>-</td><td>69.1</td><td>39.7</td><td>49.6</td><td>69.8</td><td>-</td><td>-</td></tr><tr><td>Janus-Pro (Chen et al., 2025b)</td><td>7B</td><td>1567.1</td><td>62.0</td><td>58.0*</td><td>72.1</td><td>41.0</td><td>48.3*</td><td>71.3*</td><td>25.8</td><td>59.0</td></tr><tr><td>Mogao (Liao et al., 2025)</td><td>7B</td><td>1592.0</td><td>60.9</td><td>-</td><td>74.6</td><td>44.2</td><td></td><td></td><td>-</td><td>-</td></tr><tr><td>Show-o2 (Xie et al., 2025a)</td><td>7B</td><td>1620.5</td><td>63.1</td><td>64.7*</td><td>69.8</td><td>48.9</td><td>56.6</td><td>78.6</td><td>52.3*</td><td>32.4*</td></tr><tr><td>Tuna</td><td>7B</td><td>1641.5</td><td>63.9</td><td>66.1</td><td>74.7</td><td>49.8</td><td>61.2</td><td>79.3</td><td>85.8</td><td>74.3</td></tr></table>

Our image captioning objective aligns with the pretraining objectives of strong semantic encoders, such as SigLIP 2 (Tschannen et al., 2025) and the Qwen2.5-VL (Bai et al., 2025) vision encoder. Image captioning has also been shown to provide semantic richness comparable to contrastive learning (Tschannen et al., 2023), thereby enhancing our unified representation's visual understanding capability. Meanwhile, the text-toimage generation objective trains the fow matching head to generate images from text conditions, laying the groundwork for later image editing and text-to-video generation tasks. Additionally, this objective allows generation gradients to fow back into the representation encoder, further aligning our unified visual representation with both understanding and generation tasks. Stage 2: full model continue pretraining. In the second training stage, we unfreeze the LLM decoder and pretrain the entire model using the same image captioning and text-to-image generation objectives from Stage 1.During later training steps of Stage 2, we further introduce image instruction-following, image editing, and video-captioning datasets to extend the model's capabilities. This stage enables TunA to perform more complex multimodal reasoning and generation tasks, bridging the gap between basic visual-text alignment and higher-level instruction-driven multimodal understanding and generation. S : peifieg(). ialy, i e hi e cnu upe e- T a combination of image editing, image/video instruction-following, and high-quality image/video generation datasets, trained with a reduced learning rate. This stagefurther refines TunA ' capabilities, improving its performance and generalization across diverse multimodal understanding and generation tasks.

# 3 Experiments

# 3.1 Experiment Setup

Implementation details.We verify TuNA with two LLM models at different scales, i.e., Qwen2.5-1.5B-Instruct and Qwen2.5-7B-Instruct (Bai et al, 2025). In the pretraining stage, we optimize the representation encoder, projection layers, and diffusion head with AdamW (Loshchilov and Hutter, 2017) using a learning rate of $1 \times 1 0 ^ { - 4 }$ . We train on images with a base resolution of 512 $\times$ 512, as well as alternative aspect ratios that yield a similar number f visual tokens. In the second stage, we enable end-to-end training after a linear warm-up of 2K steps and continue optimization with the same learning rate. At this point, we extend the training data to include video-caption pairs and editing data. In the final stage, we perform supervised fine-tuning for instruction following on our curated SFT corpus with a smaller learning rate of $2 \times 1 0 ^ { - 5 }$ . Due to the substantial computational cost of video training, the 7B variant is trained without video data.

<table><tr><td>Models</td><td>Size</td><td>Single Obj. Two Obj. Counting Colors Position Color Attr.</td><td></td><td></td><td></td><td></td><td></td><td>Overall</td></tr><tr><td colspan="9">Generation-only Models</td></tr><tr><td>SD3-Medium (Esser et al., 2024)</td><td>2B</td><td>0.99</td><td>0.94</td><td>0.72</td><td>0.89</td><td>0.33</td><td>0.60</td><td>0.74</td></tr><tr><td>FLUX.1 [Dev]† (Batifol et al., 2025)</td><td>12B</td><td>0.98</td><td>0.93</td><td>0.75</td><td>0.93</td><td>0.68</td><td>0.65</td><td>0.82</td></tr><tr><td colspan="9">Composite UMMs</td></tr><tr><td>MetaQuery-XL†(Pan et al., 2025) Tar (Han et al., 2025)</td><td>7B 7B</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>0.80</td></tr><tr><td></td><td>8B</td><td>0.99</td><td>0.92</td><td>0.83</td><td>0.85</td><td>0.80</td><td>0.65</td><td>0.84</td></tr><tr><td>BLIP3-o (Chen et al., 2025a)</td><td>12B</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>0.84</td></tr><tr><td>UniWorld-V1† (Lin et al., 2025a)</td><td></td><td>0.98</td><td>0.93</td><td>0.81</td><td>0.89</td><td>0.74</td><td>0.71</td><td>0.84</td></tr><tr><td>OmniGen2† (Wu et al., 2025c)</td><td>7B</td><td>0.99</td><td>0.96</td><td>0.74</td><td>0.98</td><td>0.71</td><td>0.75</td><td>0.86</td></tr><tr><td colspan="9">1.5B-scale Native UMMs</td></tr><tr><td>D-DiT (Li et al., 2025c)</td><td>2B 1.5B</td><td>0.97</td><td>0.80</td><td>0.54</td><td>0.76</td><td>0.32</td><td>0.50</td><td>0.65</td></tr><tr><td>Show-o (Xie et al., 2024b)</td><td></td><td>0.98</td><td>0.80</td><td>0.66</td><td>0.84</td><td>0.31</td><td>0.50</td><td>0.68</td></tr><tr><td>Janus-Pro (Chen et al., 2025b)</td><td>1.5B</td><td>0.98</td><td>0.82</td><td>0.51</td><td>0.89</td><td>0.65</td><td>0.56</td><td>0.73</td></tr><tr><td>Show-o2 (Xie et al., 2025a)</td><td>1.5B</td><td>0.99</td><td>0.86</td><td>0.55</td><td>0.86</td><td>0.46</td><td>0.63</td><td>0.73</td></tr><tr><td>Harmon (Wu et al., 2025d)</td><td>1.5B</td><td>0.99</td><td>0.86</td><td>0.66</td><td>0.85</td><td>0.74</td><td>0.48</td><td>0.76</td></tr><tr><td>Tuna</td><td>1.5B</td><td>1.00</td><td>0.94</td><td>0.83</td><td>0.91</td><td>0.81</td><td>0.79</td><td>0.88</td></tr><tr><td colspan="9">7B-scale Native UMMs</td></tr><tr><td>MUSE-VL (Xie et al., 2025b)</td><td>7B 7B</td><td>-</td><td>-</td><td></td><td>-</td><td>-</td><td>-</td><td>0.57</td></tr><tr><td>Transfusion (Zhou et al., 2024)</td><td></td><td>-</td><td>-</td><td></td><td>-</td><td>-</td><td></td><td>0.63</td></tr><tr><td>Emu3 (Wang et al., 2024c)</td><td>8B</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>0.66</td></tr><tr><td>Show-o2 (Xie et al., 2025a)</td><td>7B</td><td>1.00</td><td>0.87</td><td>0.58</td><td>0.92</td><td>0.52</td><td>0.62</td><td>0.76</td></tr><tr><td>Janus-Pro (Chen et al., 2025b)</td><td>7B</td><td>0.99</td><td>0.89</td><td>0.59</td><td>0.90</td><td>0.79</td><td>0.66</td><td>0.80</td></tr><tr><td>BAGEL† (Deng et al., 2025a)</td><td>14B</td><td>0.98</td><td>0.95</td><td>0.84</td><td>0.95</td><td>0.78</td><td>0.77</td><td>0.88</td></tr><tr><td>Mogao (Liao et al., 2025)</td><td>7B</td><td>1.00</td><td>0.97</td><td>0.83</td><td>0.93</td><td>0.84</td><td>0.80</td><td>0.89</td></tr><tr><td>TUna</td><td>7B</td><td>1.00</td><td>0.97</td><td>0.81</td><td>00.91</td><td>0.88</td><td>0.83</td><td>0.90</td></tr></table>

Tamresult GenElre eh usi LLrwersol su section. Underline: second-best.

# 3.2 Main Results

Image understanding. We evaluate TunA's multimodal understanding capabilities on nine benchmarks, including general VQA benchmarks such as MME (Fu et al., 2025a), GQA (Hudson and Manning, 2019), RealWorldQA (xAI) and SEED-Bench (Li et al., 2023b); knowledge-intensive benchmarks such as MMMU (Yue et al, 2024), MMStar (Chen et al., 2024b), and AI2D (Kembhavi et al, 2016); and text-centric benchmarks including ChartQA (Masry et al., 2022) and OCRBench (Liu et al., 2024b). As shown in Table 1, both 1.5B and 7B TuNA achieve state-of-the-art results across nearly all benchmarks, demonstrating strong and consistent performance. Notably, TunA delivers competitive image understanding results compared to understanding-only models and outperforms many composite UMMs and UMMs with larger model sizes, highlighting the effectiveness of its unified representations. Image generation. We evaluate TuNA's image generation performance on three benchmarks: GenEval (Ghosh et al., 2023), DPG-Bench (Hu et al., 2024) and OneIG-Bench (Chang et al., 2025). Results are presented in Table 2 and Table 3. Across all three benchmarks, TuNA consistently outperforms contemporary approaches such as Janus-Pro, BAGEL and Mogao, achieving state-of-the-art results for both the 1.5B and 7B variants. Notably, TuNA shows a substantial advantage in text rendering quality in OneIG-Bench, indicating its strong semantic understanding capability when generating images from complex instructions containing visual text-related information. Our results show that TunA consistently outperforms models with decoupled visual representations on image generation tasks, underscoring the strength and robustness of its unifed representation design. TableImage generation results on DPG-Bench. Bold:best results among each section.Underline:second-best.   

<table><tr><td rowspan="3">Models</td><td rowspan="3">Size</td><td colspan="5">DPG-Bench</td><td colspan="6">OnelG-Bench</td></tr><tr><td colspan="9">lolnyu elath l Alyty </td></tr><tr><td></td><td>Generation-only Models</td></tr><tr><td>FLUX.1 [Dev] (Batifol et al., 2025)</td><td>12B 82.10 89.50</td><td>88.70</td><td>91.10</td><td></td><td>89.40</td><td>84.00</td><td>0.79</td><td>0.52</td><td>0.25</td><td></td><td></td><td>0.43</td></tr><tr><td>Qwen-Image (Wu et al., 2025a)</td><td> 20B</td><td>91.32 91.56</td><td>92.02</td><td>94.31</td><td>92.73</td><td>88.32</td><td>0.88</td><td>0.89</td><td>0.31</td><td>0.37 0.42</td><td>0.24 0.20</td><td>0.54</td></tr><tr><td colspan="10">1.5B-scale Native UMMs</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Show-o (Xie et al., 2024b)</td><td>| 1.3B |</td><td>-</td><td></td><td></td><td>-</td><td>-</td><td>0.70 0.80</td><td>0.00</td><td>0.21</td><td>0.36</td><td>0.24</td><td>0.25</td></tr><tr><td>Show-o2 (Xie et al., 2025a) Tuna</td><td>1.5B 1.5B</td><td>87.5390.38</td><td>91.34</td><td>90.30</td><td>91.21</td><td>85.02</td><td></td><td>0.13</td><td>0.27</td><td>0.35</td><td>0.19</td><td>0.35</td></tr><tr><td></td><td>88.87 90.32</td><td></td><td>91.71</td><td>91.79</td><td>90.14</td><td>86.03</td><td>0.82</td><td>0.77</td><td>0.25</td><td>0.36</td><td>0.20</td><td>0.48</td></tr><tr><td colspan="10">7B-scale Native UMMs</td></tr><tr><td>Emu3-DPO (Wang et al., 2024c)</td><td>-</td><td></td><td>-</td><td>-</td><td>-</td><td>81.60</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Janus-Pro (Chen et al., 2025b)</td><td>8B 7B</td><td>86.90 88.90</td><td>89.40</td><td>89.32</td><td>89.48</td><td>84.19</td><td>0.55</td><td>0.00</td><td>0.14</td><td>0.28</td><td>0.37</td><td>0.27</td></tr><tr><td>Mogao (Liao et al., 2025)</td><td>7B</td><td>82.3790.03</td><td>88.26</td><td>93.18</td><td>85.40</td><td>84.33</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>BAGEL (Deng et al., 2025a)</td><td>14B</td><td>88.94 90.37</td><td></td><td>91.29</td><td>90.82 88.67</td><td>885.07</td><td>0.77</td><td>0.24</td><td>0.17</td><td>0.37</td><td>0.25</td><td>0.36</td></tr><tr><td>Show-o2 (Xie et al., 2025a)</td><td>7B</td><td>89.00</td><td>91.78</td><td>89.96</td><td>91.81 91.64</td><td>86.14</td><td>0.82</td><td>0.00</td><td>0.23</td><td>0.32</td><td>0.18</td><td>0.31</td></tr><tr><td>Tuna</td><td>7B</td><td>90.42</td><td>91.68</td><td>90.94</td><td>91.87 90.73</td><td>86.76</td><td></td><td>0.84 0.82</td><td>0.27</td><td>0.40</td><td>0.19</td><td>0.50</td></tr></table>

<table><tr><td rowspan="3">Models</td><td rowspan="3">Size</td><td colspan="10">ImgEdit-Bench</td><td colspan="3">GEdit-Bench</td></tr><tr><td colspan="10">Add Adj. Ext.</td><td></td><td>|Overal| | G-SC G-PQ | G-Overall</td></tr><tr><td>Generation-only Models</td><td></td><td></td><td>Rep.</td><td>Rm.</td><td>Bg.</td><td>Sty.</td><td></td><td>Hyb. Act.</td><td></td><td></td><td></td><td></td></tr><tr><td>FLUX.1 Kontext [Pro] (Batifol et al., 2025) 12B</td><td colspan="10">4.25 4.15 2.35 4.56 3.57 4.26 4.57</td><td colspan="3">7.02 7.60</td><td>6.56</td></tr><tr><td>Qwen-Image (Wu et al., 2025a)</td><td>|20B</td><td></td><td>4.38 4.16 3.43 4.66 4.14 4.38 4.81</td><td></td><td></td><td></td><td></td><td></td><td>3.68 4.63 3.82 4.69</td><td>4.00 4.27</td><td>8.00</td><td>7.86</td><td>7.56</td></tr><tr><td colspan="10">Native or Composite UMMs</td><td colspan="3"></td></tr><tr><td>OmniGen (Xiao et al., 2025)</td><td>3.8B</td><td>3.47</td><td>3.04 1.71</td><td>2.94</td><td></td><td>2.43 3.21</td><td>4.19</td><td>2.24</td><td>3.38</td><td>2.96</td><td>5.96</td><td>5.89</td><td>5.06</td></tr><tr><td>BAGEL (Deng et al., 2025a)</td><td>14B</td><td>3.56</td><td>3.31 1.70</td><td>3.30</td><td>2.62</td><td>3.24</td><td>4.49</td><td>2.38</td><td>4.17</td><td>3.20</td><td>7.36</td><td>6.83</td><td>6.52</td></tr><tr><td>UniWorld-V1 (Lin et al., 2025a)</td><td>12B</td><td>3.82</td><td>3.64 2.27</td><td>3.47</td><td>3.24</td><td>2.99</td><td>4.21</td><td>2.96</td><td>2.74</td><td>3.26</td><td>4.93</td><td>7.43</td><td>4.85</td></tr><tr><td>OmniGen2 (Wu et al., 2025c)</td><td>4B</td><td>3.57</td><td>3.06 1.77</td><td>3.74</td><td>3.20</td><td>3.57</td><td>4.81</td><td>2.52</td><td>4.68</td><td>3.44</td><td>7.16</td><td>6.77</td><td>6.41</td></tr><tr><td>Tuna</td><td>7B</td><td></td><td>4.46 4.52 2.47</td><td></td><td>4.68 4.58</td><td>4.56</td><td>4.73</td><td>4.07</td><td>4.69</td><td>4.31</td><td>7.79</td><td>7.48</td><td>7.29</td></tr></table>

Table Image editing results on ImgEdit-Bench and GEdit-Bench. For ImgEdit-Bench, we test editing performance cs varu iensions, including 'Ad Adjust 'Extract Replace Remove Bacground Style 'Hyri and 'Action' For GEdit-Bench, "G-SC" and "G-PQ" denote "G-Semantic Consistency" and "G-Perceptual Quality", respectively. Bold: best results among each section. Underline: second-best.

Image editing. We employ ImgEdit-Bench (Ye et al., 2025) and GEdit-Bench as our evaluation suite for image editing. As shown in Table 4, TuNA achieves an overall score of 4.31 on ImgEdit-Bench, ranking highest among all UMMs. TuNA's performance is also comparable to generation-only models such as FLUX.1 Kontext (Batifol et al., 2025) and Qwen-Image (Wu et al., 2025a). For GEdit-Bench, although TuNA performs slightly below the best generation-only model (Qwen-Image (Wu et al, 2025a), it again achieves the highest overall score among all unified models. TunA's consistently strong results on both ImgEdit-Bench and GEdit-Bench demonstrate its robust image editing capability and highlight the effectiveness of our unified visual representation when handling visual generation tasks that require precise semantic understanding and accurate prompt following. Video understanding. We employ four video understanding benchmarks to evaluate TuNA: MVBench (Li et al 2024b), Video-MME (Fu et al., 2025b), LongVideoBench (Wu et al., 2024a) and LVBench (Wang et al, 2025b). As shown in Table 5, TUNA outperforms Show-02 on MVBench and Video-MME, while achieving competitive results on LongVideoBench and LVBench. Notably, despite being only a 1.5B-parameter model, TUNA performs on par with larger understanding-only models on MVBench and LVBench, demonstrating the efficiency and effectiveness of our unified representation for video understanding tasks. Video generation. We evaluate TuNA on VBench (Huang et al., 2024) for text-to-video generation, comparing it against other UMMs and generation-only models. As shown in Table 6, TUNA achieves state-of-the-art performance, surpassing all existing UMMs capable of video generation, while using only a 1.5B-parameter LLM decoder. This demonstrates the eficiency and scalability of our unified architecture for high-quality video generation. Fable 5 Experimental results on video understanding benchmarks. $\#$ Frames denotes the number of frames used durin inference. Bold: best results. Underline: second-best.   

<table><tr><td rowspan="2">Models</td><td rowspan="2"></td><td rowspan="2">Size #Frames</td><td></td><td></td><td>MVBench Video-MME LongVideoBench LVBench</td><td></td></tr><tr><td>test</td><td>w/o sub</td><td>val</td><td>test</td></tr><tr><td colspan="8">Understanding-only Models (LMMs)</td></tr><tr><td>GPT-4o (OpenAI, 2024)</td><td>-</td><td>-</td><td>-</td><td>71.9</td><td>66.7</td><td>48.9</td></tr><tr><td>Gemini-1.5-Pro (Team et al., 2024)</td><td>-</td><td>-</td><td>54.2</td><td>75.0</td><td>64.0</td><td>33.1</td></tr><tr><td>LongVA (Zhang et al., 2024a)</td><td>7B</td><td>64</td><td>49.2</td><td>52.6</td><td>51.8</td><td>-</td></tr><tr><td>VideoLLaMA2 (Cheng et al., 2024)</td><td>7B</td><td>16</td><td>54.6</td><td>47.9</td><td>-</td><td>-</td></tr><tr><td>LLaVA-OV (Li et al., 2024a)</td><td>7B</td><td>32</td><td>56.7</td><td>58.2</td><td>56.5</td><td>26.9</td></tr><tr><td colspan="8">1.5B-scale Native UMMs</td></tr><tr><td>Show-o2 (Xie et al., 2025a)</td><td>1.5B</td><td>32</td><td>49.8</td><td>48.0</td><td>49.2</td><td></td></tr><tr><td>TUna</td><td>1.5B</td><td>49</td><td>54.4</td><td>49.1</td><td>49.7</td><td>27.4</td></tr></table>

<table><tr><td>Models</td><td>Size</td><td>QS</td><td>SS SC</td><td>BC</td><td>TF MS</td><td></td><td></td><td>DD AQ IQ OC</td><td></td><td></td><td></td><td>MO HA</td><td></td><td>SR</td><td>s</td><td>AS TS OC&#x27; | Total</td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>Generation-only Models</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>CogVideoX | 5B |</td><td></td><td></td><td>|8.75 77.04 96.23 96.52 98.66 96.92 70.97 61.98 62.90 85.23 62.11 99.40 82.81 66.35 53.20 24.91 25.38 27.59|81.61</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>Native or Composite UMMs</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>VILA-U</td><td>7B</td><td>|76.26 65.04</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>| 74.01</td></tr><tr><td>HaploOmni</td><td>7B</td><td></td><td></td><td>96.40 97.60</td><td></td><td></td><td>96.80 65.30</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>34.60</td><td></td><td></td><td></td><td>78.10</td></tr><tr><td>Emu3 Show-o2</td><td>8B</td><td></td><td></td><td>95.32 97.69</td><td>-</td><td></td><td>98.93 79.27 59.64</td><td></td><td></td><td></td><td></td><td>86.17 44.64 77.71</td><td></td><td></td><td></td><td>68.73 37.11 20.92</td><td></td><td></td><td>80.96</td></tr><tr><td>Tuna</td><td>1.5B</td><td></td><td>82.10 78.31 97.28 96.78 97.68 98.25 40.83 65.15 67.06 94.81 76.01 95.20 80.89 62.61 57.67 23.29 25.27 27.00</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>81.34</td></tr><tr><td></td><td>1.5B</td><td></td><td>84.32 83.04 95.99 96.72 98.02 98.33 69.39 65.88 66.83 95.41 92.31 97.50 87.67 78.12 58.59 23.18 24.68 27.71</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>84.06</td></tr></table>

Tabl Video generation results n VBench. ull column names: S: Quality Score, SS: Semantic Score, SC:Subject Consistency, BC: Background Consistency, TF: Temporal Flickering, MS: Motion Smoothness, DD: Dynamic Degree, AAestheic Quality, I: Imagin Quality, OC:Object Class MO: Multiple Objects, HA: Human Action, C:Color, SRSpatialRelationship, S: Scene, AS:Appearance style TS: Temporal Style, OC Overal Consistency. Bold: best results among each section. Underline: second-best.

# 3.3 Ablation: Visual Representation Design

In this section, we conduct a series of ablation experiments to systematically assess the effectiveness  our model architecture and training pipeline. For all experiments, we use a lightweight variant of TunA built on the Qwen2.5-1.5B LLM with a smaller fow matching head. Using this setup, we evaluate three visual representation designs: 1. Decoupled representations, using SigLIP 2 features for understanding and Wan 2.2 VAE latents for generation (denoted as "Decoupled"). Show-o2-style unifed representations, using a dual-path late fusion strategy to obtain the final representations (denoted as "Show-o2"). A detailed explanation of this design can be found in Section 3.4. 3.TunA's unifed representation, initialized from three different pretrained representation encoders: SigLIP (Zhai et al., 2023), SigLIP 2 (Tschannen et al., 2025), and DINOv3 $^ { 3 }$ (Siméoni et al., 2025). All models are trained on a subset of our training data using a two-stagetraining pipeline (corresponding to Stage 1 and Stage 3 in Section 2.3), with an equal number of training steps per stage. Our ablation study results are shown in Table 7. Unified vs. decoupled visual representation. Comparing Model 8 and Model 12 in Table 7, we observe that our unifed representation consistently outperforms the decoupled setting across all understanding and generation benchmarks. Comparing Models 2 and 5 with Model 8, we find that training a unified model using decoupled visual representations results in substantial degradation on understanding tasks, compared to only training the model on understanding data. In contrast, Model 12 surpasses Model 3 on most understanding benchmarks and outperforms Model 6 across all generation benchmarks. These results indicate that our unified representation suffers far less from representation conflicts than decoupled designs, enabling stronger performance in both understanding and generation.

<table><tr><td rowspan="2">Models</td><td rowspan="2">ID</td><td rowspan="2">Data</td><td colspan="4">Understanding</td><td colspan="2">Generation</td></tr><tr><td>MME-p</td><td>MMMU</td><td>SEED</td><td>GQA</td><td>GenEval</td><td>DPG</td></tr><tr><td>Show-o2 (Wan 2.1 VAE + SigLIP)</td><td>1</td><td>Und.</td><td>1351</td><td>36.1</td><td>62.1</td><td>56.8</td><td></td><td>-</td></tr><tr><td>Decoupled (SigLIP 2 only)</td><td>2</td><td>Only</td><td>1392</td><td>38.2</td><td>62.9</td><td>58.1</td><td>-</td><td></td></tr><tr><td>Tuna (Wan 2.2 VAE + SigLIP 2)</td><td>3</td><td></td><td>1386</td><td>37.6</td><td>62.9</td><td>57.4</td><td>-</td><td>-</td></tr><tr><td>Show-o2 (Wan 2.1 VAE + SigLIP)</td><td>4</td><td>Gen.</td><td>-</td><td>-</td><td>-</td><td>-</td><td>76.2</td><td>82.56</td></tr><tr><td>Decoupled (Wan 2.2 VAE only)</td><td>5</td><td>Only</td><td></td><td>-</td><td>-</td><td></td><td>77.3</td><td>82.87</td></tr><tr><td>Tuna (Wan 2.2 VAE + SigLIP 2)</td><td>6</td><td></td><td>-</td><td>-</td><td>-</td><td>-</td><td>77.8</td><td>83.33</td></tr><tr><td>Show-o2 (Wan 2.1 VAE + SigLIP)</td><td>7</td><td></td><td>1339</td><td>35.4</td><td>61.7</td><td>55.9</td><td>75.9</td><td>82.32</td></tr><tr><td>Decoupled (Wan 2.2 VAE + SigLIP 2)</td><td>8</td><td></td><td>1346</td><td>37.2</td><td>61.4</td><td>56.5</td><td>78.3</td><td>83.50</td></tr><tr><td>TUna (Wan 2.1 VAE + SigLIP)</td><td>9</td><td>Und. &amp;</td><td>1358</td><td>35.9</td><td>64.2</td><td>57.2</td><td>77.2</td><td>83.29</td></tr><tr><td>Tuna (Wan 2.2 VAE + SigLIP)</td><td>10</td><td>Gen.</td><td>1349</td><td>36.3</td><td>64.6</td><td>57.4</td><td>76.9</td><td>83.10</td></tr><tr><td>TUNA (Wan 2.1 VAE + SigLIP 2)</td><td>11</td><td></td><td>1379</td><td>37.7</td><td>65.9</td><td>58.4</td><td>79.1</td><td>83.98</td></tr><tr><td>Tuna (Wan 2.2 VAE + SigLIP 2)</td><td>12</td><td></td><td>1361</td><td>38.1</td><td>66.5</td><td>58.2</td><td>79.4</td><td>84.20</td></tr><tr><td>Tuna (Wan 2.2 VAE + DINOv3)</td><td>13</td><td></td><td>1396</td><td>37.3</td><td>65.6</td><td>58.6</td><td>78.9</td><td>84.08</td></tr></table>

Table 7 Ablation study results. Und. Only", "Gen. Only" and "Und. & Gen." refer to models trained with understanding data only, generation data only, and both data, respectively. Selection of representation encoders. We find that TunA's unified representation generally benefits from stronger representation encoders. As shown in Table 7, comparing Models 10, 12, and 13, both SigLIP 2 (400M parameters) and DINOv3 (800M) outperform SigLIP (400M) on all benchmarks. Furthermore, comparing Model 9 to Model 11 and Model 10 to Model 12, we observe that regardless of which VAE encoder is used in the model, replacing SigLIP with SigLIP 2 in the representation encoder consistently improves performance across all understanding and generation benchmarks. We ultimately adopt SigLIP 2 for TuNA as it delivers comparable understanding performance, superior generation quality relative to DINOv3, and maintains a significantly smaller model size.

Understanding-generation synergy. Our experimental results on training models exclusively on either understanding (Models 1, 2 and 3) or generation data (Models 4, 5 and 6) demonstrate that TuNA benefits from joint training on both data types. Specifically, we observe that Model 12 surpasses Model 3 on understanding benchmarks and Model 6 on generation benchmarks. Although the comparison between Model 2 and Model 3 shows that TunA's VAE $^ +$ representation encoder architecture incurs a slight performance drop relative to using only a representation encoder (the standard setup for understanding-only models), our joint understanding $^ +$ generation training pipeline largely compensates for this degradation. Specifically, Model 12 recovers its understanding performance and becomes comparable to or even better than Model 2 on several understanding benchmarks. Moreover, Model 12 substantially outperforms both Model 5 and Model 6 on all generation benchmarks. These results demonstrate the mutual enhancement between understanding and generation made possible by our unified visual representation design. Comparisonwith Show-.A closely related work to ours is Show-2 (Xiee al., 2025a), which uses a dual-path late-fusion mechanism, merging features from separate VAE and semantic encoders via a fusion layer to build unified representations. In contrast, TUNA extracts unified representations directly from VAE latents using a semantic encoder, achieving deep feature fusion across all layers in the semantic encoder. Comparing Model 7 with Models 9, 10, 11 and 12 in Table 7, our unified representation consistently outperforms Show-02 on all benchmarks, regardles of the choice of the VAE encoder and the representation encoder. Model 1 vs. 3 and Model 4 vs. 6 further show that Show-o2 underperforms even when trained on a single task. We atribute this to its late-fusion strategy, which introduces representation conficts and degrades overal performance.

# 3.4 Discussion: Unified Representation Analysis

As discussed in Section 3.3, both TuNA and Show-02 (Xie et al, 2025a) employ unifed visual representations for understanding and generation, but they construct these representations in fundamentally different ways. In this section, we first describe Show-o2's unifed visual representation design in detail, and then provide an in-depth analysis of why TuNA's unified representation achieves superior performance than Show-02.

![](images/4.jpg)  
Figure 4 Comparison between TunA and Show-o2 on how unified visual representations are produced.

As illustrated in Figure 4, Show- constructs unifed visual representations usinga dual-path featurefusion mechanism. The input image or video is first encoded by a VAE encoder, after which the latent is processed throug two parallel branches. The semantic projection branch feeds the VAE latents into a set of semantic layers to extract features for understanding tasks. The VAE projection branch applies 2D patch embedding layers to produce features tailored for generation tasks. Importantly, the semantic layers are pre-distilled usig a rozen representation encoder: given the same image, theiroutputs are frst aligned with a pretrained SigLIP model before conducting end-to-end training of the Show-o2 model. This pre-distillation stage is proposed to preserve semantic understanding capability. Finally, Show-o2 merges the outputs of both paths using a feature fusion layer to obtain its unified visual representation. To better understand why our unified representation yields superior performance, we perform a representation aligment analysis using CKNNA scores (Huh et al., 2024) with respect to two reference models: (1) a strong semantic encoder SigLIP 2 (Tschannen et al., 2025), and (2) a strong generation model SD3-Medium (Esser et al., 2024). Concretely, we extract unified visual representations from TuNA and Show-02 based on 1,024 images from the Wikipedia Captions dataset (Srinivasan et al., 2021) and compute their CKNNA scores relative to features from all intermediate layers of the two reference models. The results are presentedin Figure 5a and Figure 5b. As shown in the figures, both TuNA and Show-02 exhibit strong alignment with the SigLIP 2 intermediate features, with CKNNA scores exceeding 0.5. This high similarity reflects their strong semantic understanding capability, consistent with their solid performance on multimodal understanding tasks. On the other hand, TUNA's unified representation shows consistently higher alignment with the SD3-Medium intermediate features compared to Show-o2, indicating that TunA learns a more balanced unified representation suited for both understanding and generation. In contrast, Show-o2 remains biased toward semantic features, which limits its generation quality. The above findings prompt us tofurtherinvestigate why Show-o's dual-path fusion mechanism produces biased features toward semantic understanding. To analyze this, we compute CKNNA scores between Show-o2's final fusdfeatures and the intermediate features from its understanding (semantic projection) and generation (VAE projection) branches before fusion. We find that Show-o2's unified representation exhibits a strong correlation with its understanding branch (CKNNA=0.45) but a very weak correlation with the generation branch (CKNNA=0.07). This demonstrates that the late-fusion strategy merges features in an imbalanced manner, causing the representation to remain dominated by semantic information. In contrast, TunA's end-to-endtraining of the unified representation on both objectives enables early fusion of understanding and generation signals at every layer of the representation encoder. This layer-wise interaction captures richer cross-task dependencies and is inherently more robust than the late-fusion strategy adopted in Show-o2.

![](images/5.jpg)  
(a) Alignment to SigLIP 2

![](images/6.jpg)  
(b) Alignment to SD3-Medium   
Figure5 Representation alignment analysis with SigLIP 2 and SD3-Medium. For both TuNA and Show-02, we extract visual representations at the input layer of the LLM decoder.

# 3.5 Qualitative Results

Image generation. We compare TunA with state-of-the-art generation-only and unified models across diverse image generation instructions in Figure 6. In the first two examples, TuNA exhibits strong text rendering ability, accurately reproducing all visual text in the prompts without errors. In the whiteboard example, TUNA is the only model that correctly places an underline beneath "with everyone", demonstrating precise prompt-following capability. Moreover, TunA accurately generates two black shelves, one containing books and markers on top and the other containing black cloth and hand sanitizer at the bottom, each in the correct position. Other models either fail to produce the correct number of shelves or place the wrong items on them. These results show that TunA excels at compositional image generation, enabled by its unified visual representation with strong semantic understanding capabilities. In the "tuna"example, both TunA and Flu (Batifol et al, 2025) successfully render the Hawaiian shirt, while other models either fail to depict the shirt or generate an incorrect tuna body. Finally, in the "red t-shirt"example, TunA accurately refects the classic 1960s Walt Disney animation style" and correctly includes all required elements from the prompt, maintaining a coherent and well-structured composition. Image editing. We compare TUNA with BAGEL (Deng et al., 2025a), Qwen-Image (Wu et al., 2025a), and Flux.1 Kontext (Batifol et al., 2025) on image editing tasks in Figure 7. As shown, TuNA not only correctly performs explicit editing operations, such as style transfer (photorealistic hand-sculpted claymation in the "dog" example), environment change (daylight nighttime in the "red car" example), and object replacement (boat puppy with a swim ring in the "boat" example), but also handles more implicit and nuanced istins, scs applyinghtig from the  side the oexampleThese result frthe TuNA's strong semantic understanding and high-fidelity image generation capabilities. Video generation. We present TunA's video generation results in Figure 8. The model produces high-fidelity videos across a wide range of instructions, demonstrating the strength of its unified visual representation space for jointly modeling both images and videos.

# 4 Related Work

# 4.1 Large Multimodal Models

Large multimodal models (LMMs) aim to generate text responses from multimodal inputs spanning images, videos, and text. Early LMMs such as Flamingo (Alayrac et al., 2022) and Idefics (Laurençon et al., 2023) introduced cross-attention layers to enable interaction between visual and linguistic features. Modern LMMs generally follow the LLaVA paradigm (Liu et al., 2023a), where visual inputs are encoded by a vision encoder (e.g., CLIP (Radford et al., 2021)) and then concatenated with text tokens for joint processing by a language model decoder. Recent research advances focus on improving instruction-following through higher-quality traii data (Liuet al 2024a; Liet al 2024a; Chen et al 2024a; Li et al 202b; Ren et al 2024;Zhag

![](images/7.jpg)  
T drtailor

![](images/8.jpg)  
T th wrd

![](images/9.jpg)  
T plu Filt coral silhouettes surround the tuna, rendered with a soft, painterly touch.

![](images/10.jpg)  
An sn damans beth carton-cothinnhealit

Fiur Qualitative cparison between TuA nd baseine models nmage eeration tasks. The istrucios tha are correctly reflected in our results but failed in some of the baseline models are bolded.

e a 2024b; Wiedmann et al., 2025; An et al, 025), developing stronger vision encoders capable f handlin higher-resolution images (Liu et al., 2024a; Laurençon et al., 2024; Wang et al., 2024b; Bai et al., 2025), extending LMMs to interleaved image (Laureço et al., 2024; Li etal. 2024a; Jiang t al., 024 andvideo understanding (Maaz et al., 2023; Lin et al, 2023; Zhang et al., 2024a; Li et al, 2024b,d; Ren et al, 2025), and incorporating reinforcement learning with thinking modes (Deng et al., 2025b; Huang et al, 2025; Feng et al., 2025) or pixel-space reasoning (Su et al., 2025a,b; Liu et al., 2025a).

# 4.2 Diffusion Generative Models

Diffusion generative models have become the de facto backbone of high-fidelity image (Esser et al., 2024; Batiol et al 025; Li et al 2024e; Wu e al 2025a; Liuetal203b,c, 02) and vido (Ko et al2; Seawead et al., 2025; Wan et al., 2025; Liu et al., 2025c) synthesis.Modern large-scale visual generation models typically apply difsion ina continuus latent space defined by alarned VAE, following the Latent Diffusion Model (LDM) paradigm (Rombach et al., 2022), which offers superior perceptual quality and sampling efficiency compared to autoregressiv decodingof long sequences of discrete tokens based on VQ-VAE (Van Den Oord et al., 2017; Esser et al., 2021). Within diffusion itself, latent-space models (Rombach etal, 20; Podell et al., 023;Esser e al 2024)are generally preferred over pixel-space appraches (Dhariwal and Nichol, 2021; Saharia et al, 2022) because they reduce computational cost, ease scaling to higher resolutions, and allow the denoising network to focus on semantically meaningful structure rather than low-level pixel noise. Architecturally, diffusion backbones have evolved from convolutional U-Net designs (Ronneberger et al, 2015; Hoet al. 2020) to diffusion transormers (DT) (Peebles and Xie, 2023;Ma et al 2024); In parallel, the learning objective has been generalized from Gaussian noise prediction and score matching (Ho et al., 2020; Sont al., 2020) to more expressive formulations such as rectified fows (Liu e al, 2022)and fow matching objectives (Lipman et al., 2022; Albergo et al., 2023).

![](images/11.jpg)  
shados ndhlightsor proessinal photraphiookwithoutlterepressin,utm.

![](images/12.jpg)

![](images/13.jpg)  
Replace the red car with a white horse with flaming wings, and replace the sky with a starry sky.

![](images/14.jpg)  

Figure 7 Qualitative comparison between TuNA and baseline models on image editing tasks.

![](images/15.jpg)

A bird building a nest from twigs and leaves.

![](images/16.jpg)

A .

![](images/17.jpg)

![](images/18.jpg)

enclosure. The lion's expression is calm and regal, exuding a sense of power and serenity.

![](images/19.jpg)  
T   
Figure 8 Qualitative results for TunA on the task of text-to-video generation.

# 4.3 Unified Multimodal Models

Unified multimodal models (UMMs) have gained growing attention for their ability to flexibly generate text and visual content from diverse multimodal inputs. Recent approaches (Luo et al., 2025) such as MetaQuery (Pan et al., 2025), BLIP-3o (Chen et al., 2025a), and UniWorld-V1 (Lin et al., 2025a) achieve this by connecting understanding-only and generation-only models through learnable adapters. While achieving proising results, their capabilities largely rely on pretrained task-specific models, limiting the potential synergy between understanding and generation. In contrast, native UMMs are pretrained from scratch to perform both tasks within a single unifed architecture. Among these works, models such as the Janus series (Wu al 025; Ma etal 2025;Chenet al 02b) and UniFluid (Fan al 2025) adopt decupledvisual representations for understanding and generation. BAGEL (Deng et al., 2025a), Mogao (Liao et al., 2025), and OneCAT (Li et al., 2025a) further use MoE-style architectures to route inputs separately, mitigating conflicts between distinct representations from the decoupled vision encoders. Alternatively, models like Chameleon (Team, 2024), Transfusion (Zhou et al., 2024), Harmon (Wu et al., 2025d), and the Show-0 series (Xie et al., 2024b, 2025a) employ unifed visual representations for both tasks. While being more efficient, these models often exhibit weaker or imbalanced performance, excelling in one task but underperforming in theother.TunA overcomes these limitations by learning balanced, unifed visual representations and achieves strong performance in both understanding and generation.

# 4.4 Representation in Multimodal Models

Recent studies have explored learning better representations to enhance multimodal understanding and generation models. From the perspective of improving understanding models, methods such as Ross (Wang e al 2024a), GenHancer (Ma et al., 2025b) and ASVR (Wang et al, 2025a) enhance multimodal understanding by introducing generation or reconstruction objectives, encouraging the model to capture fine-grained visual details. Conversely, to improve generative models, approaches such as REPA (Yu et al., 2024) and VA-VAE (Yao et al., 2025) align diffusion transformers or VAE representations with semantic vision encoders, thereby achieving stronger generative performance. Similarly, Dispersive Loss (Wang and He, 2025) introduces an auxiliary contrastive-like objective to further enhance generation quality.

In the domain of unified multimodal models, recent research has primarily focused on developing unified visual tokenizers that support both understanding and generation tasks. For instance, TokenFlow (Qu al 2025) and MUSE-VL (Xie et al 2024c) adopt late-usion strategies to merge features from separate understanding and generation encoders into quantized codebooks. DualToken (Song et al., 2025), UniTok (Ma e al 2025a)and TokLIP (Lin al 2025)trai  singe encoder to produce vector-quantize repreentations for both tasks. However, these methods rely on discrete representations, limiting their ability to perform hi-fidelity visual generation.UniFlow (Yue et al, 2025)and UniLIP Tang et al, 025) adapt representatin encoders into continuous unifed visual tokenizers, but both rely on relatively complex alignment (e.g. selfdistillation or reconstruction schemes). In contrast, TUNA learns a unified representation end-to-end under joint understanding and generation objectives, and is validated at a larger scale across more tasks.Moreover, UniLIPadopts a composite design where the nified features only serve as conditions for a separate pretrained generative model (SANA (Xie et al., 2024a). On the other hand, TUNA trains a native unified model that jointly performs understanding and generation within a single framework.

# 5 Conclusion

We introduced TuNA, a native unified multimodal model that constructs a unified visual representation space by cascading a VAE encoder with a representation encoder. We train an LLM decoder and a flow matching head on this unifed representation, achieving strong performance across image and video understanding, image and video generation, and image editing. TuNA not only surpasses prior UMM baselines but also performs competitively with leading understanding-only and generation-only models. Our ablation studies further show that (1) TunA's unified representation space outperforms both Show-o2-style unified representations and decoupled representation designs, (2) stronger pretrained representation encoders consistently yield better performance within our framework, and (3) our unified visual representation design enables mutual enhancement between understanding and generation.

# 6 Acknowledgment

We would like to thank Yukang Yang (Princeton University), Ji Xie (UC Berkeley), and Jinheng Xie (NUS for their constructive feedback on this project.

References   
Jea-BaptiAlayrac, J onaue, ul Luc Ant Mich, ai Bar YnHaon Kare Lenc Arthur Mnsh, Katherie Milan, MalcolReynolds  l Flamigovisalanguge modelor ehot arniAdva neural information processing systems, 35:2371623736, 2022.   
Michael Albergo, Nicholas  Bof and EricVanden-Eijnde.Stochasticinterpolants:A unifying framework for flows and diffusions. arXiv preprint arXiv:2303.08797, 2023.   
Xiang An, Yin Xie, Kaicheng Yang, Wenkang Zhang, Xiuwei Zhao, Zheng Cheng, Yirui Wang, Songcen Xu, Changrui Chn, Chusheng Wu, e al. Llavaonevision-5:Fully penrameworkordemctizmultimoal trainXiv preprint arXiv:2509.23661, 2025.   
Jinz Bai, Suai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiadong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023.   
Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shije Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025.   
Stephen Batifol, Andreas Blattman, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, ZioEnglish, Patrick Esser, Sumith Kulal, eal.Flux. kontext: Floatchingorn-ontext mageg and editing in latent space. arXiv e-prints, pages arXiv2506, 2025.   
Jining Chang, Yixiao Fang, Peng Xing, Shuhan Wu, Wei Cheng, Rui Wang, Xianfang Zeng, Gang Yu, and Hai-Bao Chen. Oneig-bench: Omni-dimensional nuanced evaluation for image generation. arXiv preprint arXiv:2506.07977, 2025.   
Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Svl-u. arXiv preprint arXiv:2505.09568, 2025a.   
Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegptv: Improving large multi-modal models with better captions. In European Conference on Computer Vision, pages 370387. Springer, 2024a.   
Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? Advances in Neural Information Processing Systems, 37:2705627087, 2024b.   
Xhen, Zhiy W, Xin Lu, Zizhean,Wen u, Zhen ie, Xiniu,and ChoRua JUnified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025b.   
Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, De Zhao, al. Videollama:Advancig spatial-temporal modeling and audiounderstanding invideo-lms.arXiv preprint arXiv:2406.07476, 2024.   
Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Sng, et al. Emerging properties in unified multimodal pretraining arXiv preprint arXiv:2505.14683, 2025a.   
Yihe Deng, Hritik Bansal, Fan Yin, Nanyun Peng, Wei Wang, and Kai-Wei Chang. Openvlthinker: Complex visangueeaingiiterative-cycs.nTheThirt-nintaloencnNeural no Processing Systems, 2025b.   
Praullaralicofoe beayh processing systems, 34:87808794, 2021.   
Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021.   
Patric Esser, Sumit Kulal, Andreas Blattman, Rahim Entezari, Jonas Müer, Harry Saini Yam Levi, Do LorxelSu eBe Scalcnoe orheolu nths.In Forty-first international conference on machine learning, 2024.   
L an, Lu TangSyang in, Tiaoi Xuan Yang Syun iao, ndr Seir, Chen un, Yuane Li, TaoZhu  al. Unifd autoregresive visual generation and understandig with continuous tokens.arXiv rei arXiv:2503.13436, 2025.   
Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Junfei Wu, Xiaoying Zhang, Benyou Wang, and Xiangyu Yue. Video-r: Reinforcing video reasoning in mllms.arXiv preprint arXiv:2503.2177, 2025.   
Chaoyu Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, Rongrong Ji, Caifeng Shan, and Ran He. Mme: A comprehensive evaluation benchmark for multimodal large language models, 2025a. https://arxiv.org/abs/2306.13394.   
Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Cheny Zhou, Yunhang Shen, Mengdan Zhang et al. Video-mme: The frst-ever comprehensive evaluation benchmark o multi-modallms in video analysis. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2410824118, 2025b.   
Zigang Geng, Yibing Wang, Yeyao Ma, Chen Li, Yongming Rao, Shuyang Gu, Zhao Zhong, Qinglin Lu, Han Hu, Xon Zhag al X-oReiremt learg make discreeutoregrev ma enerativ odes re again. arXiv preprint arXiv:2507.22058, 2025.   
Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36:5213252152, 2023.   
Aaron Grattafori Abhimany Dubey, Abhiav Jauhri Abhinav Pandey, Abhishek Kadian, Ahmad A-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.   
Jiaming Han, Hao Chen, Yang Zhao, Hanyu Wang, Qi Zhao, Ziyan Yang, Hao He, Xiangyu Yue, and Lu Jiang. Visin as a dialect: Unifying visual understanding and generation viatext-ligned representations.arXipre arXiv:2506.18898, 2025.   
Jathan Ho,Ajy Jain nd AbbDeiif probabilodes.Adviai processing systems, 33:68406851, 2020.   
Xiwei Hu, Rui Wang, Yixio Fang, Bin Fu, Pei Cheng, and Gang Yu. Ella: Equip diffusion models with lm for enhanced semantic alignment. arXiv preprint arXiv:2403.05135, 2024.   
Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. VisInctivii reasg capabil mutial largelanguge models.arXiv reprirXiv:2503.7 2025.   
Zqi Huang, Ynan He, Jiashu u, Fan Zhang, Chenyangi, Yumig Jiang, Yuanan Zhang, Tianxig Wu, Qing Jin NattapolChanpais,  lbenCmrehensivebencmar suite r vide enraivemodels. In ro of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024.   
Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 67006709, 2019.   
Minyoung Huh, Brian Cheung, Tongzhou Wang, and Phillip Isola. The platonic representation hypothesis.arXiv preprint arXiv:2405.07987, 2024.

Do Jang, Xuan He, Huaye Zeng, Cong Wei, Max Ku, Qian Lu, and Wenu Chen Manti: Interleavdmulti-age instruction tuning. arXiv preprint arXiv:2405.01483, 2024.   
Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannane Hajishirzi, and Ali Farhadi. A diagram is worth a dozen images. In European conference on computer vision, pages 235251. Springer, 2016.   
WeijKong, i Tin,Zij Zhang, Rox Min,Zuozoi, Jin Zhou, Jia Xiong, Xin L, Bo Wu, J Zha, et al Hunyuanvideo:A systematicramework for large video generative models.arXiv preprint arXiv:241.0360, 2024.   
Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siar ndeRushDoKiOee w-caleatav image-text documents. Advances in Neural Information Processing Systems, 36:7168371702, 2023.   
Hugo Laurençon, Léo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language models? arXiv preprint arXiv:2405.02246, 2024.   
Alexander C Li, Mihir Prabhudesai, Shivam Duggal, Ellis Brown, and Deepak Pathak. Your diffusion model is scrl a zero-shot assir. In rocedingsf theE/CVInteratinal ConferencnmpuerVisin, pas 22062217, 2023a.   
Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024a.   
BoaoLi, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. See-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023b.   
Han Li, Xinyu Peng, Yaoming Wang, Zelin Peng, Xin Chen, Rongxiang Weng, Jingang Wang, Xunliang Cai, Wenrui Dai, and Hongkai Xiong. Onecat: Decoder-only auto-regressive model for unified understanding and generation. arXiv preprint arXiv:2509.03498, 2025a.   
Hao Li, Changyao Tian, Jie Shao, Xizhou Zhu, Zhaokai Wang, Jinguo Zhu, Wenhan Dou, Xiaogang Wang, Hongsheng L, Lewe Lu,t al. Synergen-vTowards synergisticmage understanding and generation wit visionexperts and token folding. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2976729779, 2025b.   
Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: A comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2219522206, 2024b.   
Tianong Li, Yonglong Tin, He Li, Mingyang Deng, and Kaiming He.Autoregressivemageeneration without veor quantization. Advances in Neural Information Processing Systems, 37:5642456445, 2024c.   
Xinhao Li, Yi Wang, Jiashuo Yu, Xiangyu Zeng, Yuhan Zhu, Haian Huang, Jianfei Gao, Kunchang Li, Yinan He, Chenting Wang, et al. Videochat-fash: Hierarchical compresion for long-context video modeling.arXiv preprint arXiv:2501.00574, 2024d.

Zhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao Liu, MinHuan Zedon Xiao alHuyuan-di: powu muiresolutiffusiransormer wihnerai chinese understanding. arXiv preprint arXiv:2405.08748, 2024e.   
Zie Li, Henry Li, Yichun Shi, Amir Barati Farimani, Yuval Kluger, Linjie Yang, and Peng Wang Dual diffusion for unified image generation and understanding. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 27792790, 2025c.   
Chao Liao, Liyang Liu, Xun Wang, Zhengxiong Luo, Xinyu Zhang, Wenliang Zhao, Jie Wu, Liang Li, Zhi Tian, and Weilin Huang. Mogao: An omni foundation model or interleaved multi-modal generation. arXiv preprint arXiv:2505.05472, 2025.   
Bin Lin, Yang Ye, Bin Zhu, Jaxi ui, Mu Ning,Peng Jin, and Li Yua.Videoav:Learntd sal representation by alignment before projection. arXiv preprint arXiv:2311.10122, 2023.   
Bin Lin, Zongjan i, Xinhua Cheng, Yuwei Niu, Yang Ye, Xianyi He, Shengai Yuan, Wangbo Yu, Shaodong Wang, Yunyang Ge, et al.Uniword: High-resolution semanticencoders or unified visual understanding and generation. arXiv preprint arXiv:2506.03147, 2025a.   
Haokun Lin, Teng Wang, Yixiao Ge, Yuying Ge, Zhichao Lu, Ying Wei, Qingu Zhang, Zhenan Sun, and Ying Shan. ToklipMarry visual tokens to clip for multimodal comprehension and generation, 2025b.https://arxiv.org/abs/ 2505.05422.   
Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022.   
Ho  Wu,anYoJ Llr processing systems, 36:3489234916, 2023a.   
Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next.https: //llava-vl.github.io/blog/2024-01-30-1lava-next/, 2024a. Accessed: 2025-02-14. Xinao LiuChengyue Gong and Qiang LiuFlow traight nd as: Learni  enerateandtransr at wih rectified flow. arXiv preprint arXiv:2209.03003, 2022.   
Yuliag Liu, Zhang Li, Mini HuangBioYang, Wenen Yu,Chuyuan Li Xu-Cheng in,Cheng-Lin Liu, Liaw Jin, and Xiang BaiOcbenc:on the hiddemyster c in larg multidal modes.ScicChiInorain Sciences, 67(12):220102, 2024b. YLu Ty u, Zhish ZhoBoh engShu Li, BeiYu, and Jiay JaVisiUnivua perception and reasoning via reinforcement learning. arXiv preprint arXiv:2505.12081, 2025a. Zhiheng Liu, Ruili Feng, Kai Zhu, Yifei Zhang, Kecheng Zheng, Yu Liu, Deli Zhao, Jingren Zhou, and Yang Cao. Cones: Concept neurons in diffusion models for customized generation. arXiv preprint arXiv:2303.05125, 2023b. Zhieg Liu, Yifei Zhang, Yujun Shen, Kecheg Zheng,Kai Zhu, Ruili Feng, Yu Liu, Deli Zhao, Jingren Zhou, and Yan CaoCustomizableimage syntheis with multipe ubjects.Advaninneuralnoratiprocesi stms 36:5750057519, 2023c. Zhieg Liu, Ka Leong Cheng, Xi Chen, Jie Xiao, Hao Ouyang, Kai Zhu, Yu Liu, Yujun Shen, Qfeng Chen, and Ping LuManganinj:Line art colorization with precie rerence followin. In Procings  the Computer Visnan Pattern Recognition Conference, pages 56665677, 2025b. Zhiheng Liu, Xueqing Deng, Shoufa Chen, Angtian Wang, Qiushan Guo, Mingfei Han, Zeyue Xue, Mengzhao Chen, Pi Luo, and Linje Yang. Worldweaver: Generating long-horizn video worlds via rich perception.arXiv preprint arXiv:2508.15720, 2025c. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:171.05101, 2017. Run Luo, Xiaobo Xia, Lu Wang, Longze Chen, Renke Shan, Jing Luo, Min Yang, and Tat-Seng Chua. Next-mni: Towards any-to-any omnimodal foundation models with discrete fow matching. arXiv preprint arXiv:2510.13721, 2025. Chuan Ma, Yi Jiang, Junfeg Wu, Jhan Yang, Xin Yu, Zehuan Yuan, Bingyue Peng, and Xiaojuan i.Unitok:A unified tokenizer for visual generation and understanding. arXiv preprint arXiv:2502.20321, 2025a. NanyeMa Mark Goldstein, MichaelAlbergo, Nicholas Bo Erianden-Eijnden, and Sainig XieSi:Exporin fow and diffusion-based generative models with scalable interpolant transformers. In European Conference on Computer Vision, pages 2340. Springer, 2024. Shije Ma, Yuying Ge, Teng Wang, Yuxin Guo, Yixiao Ge, and Ying Shan.Genhancer: Imperfect generative models are secretly strong vision-centric enhancers. arXiv preprint arXiv:2503.19480, 2025b. Yiyang Ma, Xingchao Liu, Xiakang Chen, Wen Liu, Chengyue Wu, Zhiyu Wu, Zizheng an, Zhenda Xie, Haowei Zhan, Xikai Yu, eal. Janusfow Harmoizing autoregreson and rectife fow or unif multimoal understndi and generation. In Proceedings of the Coputer Vision and Pattern Recognition Conference, pages 77397751, 2025c. Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424, 2023. Ahmed Masry, Xuan Long Do, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: A benchmark for question ao harilndnhetl ACL 2022, pages 22632279, 2022. Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, and Chongxuan Li. Large language diffusion models. arXiv preprint arXiv:2502.09992, 2025. OpenAI. Gpt-4o. https://openai.com/index/hello-gpt-4o/, 2024. Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. Xichen Pan, Satya Narayan Shukla, Aashu Singh, Zhuokai Zhao, Shlok Kumar Mishra, Jialiang Wang, Zhiyang Xu, Juhai Chen, Kunpeng Li, Felix Juefei-Xu, et al. Transfer between modalities with metaqueries. arXiv preprint arXiv:2504.06256, 2025. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. Dustn Podel ZionEnglish, Kyle LaceyAndres Blattan, TimDockorn, JonasMüller, Joe ena, and RobRombac.Sdx Improvig latent diffus modes or high-resolu mage yntheisarXiv prerint arXiv:2307.095, 2023. Liao Qu, Huichao Zhang, Yiheng Liu, Xu Wang, Yi Jiang, Yiming Gao, Hu Ye, Daniel K Du, Zehuan Yuan, and Xinglong Wu. Tokenfow: Unifed image tokenizer for multimodal understanding and generation. In Proceedings the Computer Vision and Pattern Recognition Conference, pages 25452555, 2025.

AlecRadford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, SandhiniAgarwal, Girish Sastry, Amanda Ak ameMishkn, JacClark Learanable isloels om atual ngu upeisn. In International conference on machine learning, pages 87488763. PmLR, 2021.   
Weiing Ren, Huan Yang, Jie Min, Cong Wei, and Wenhu Chen.Vista:Enhancng long-duration and high-resolution video understanding by video spatiotemporal augmentation. arXiv preprint arXiv:2412.00927, 2024.   
Weiming Ren, Wentao Ma, Huan Yang, Cong Wei, Ge Zhang, and Wenhu Chen. Vamba: Understanding hour-long videos with hybrid mamba-transformers. arXiv preprint arXiv:2503.11579, 2025.   
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image syntheis with latent diffsion models. In Proceedings of the IEEE/CV conference on computervision and pattern recognition, pages 1068410695, 2022.   
ORerehilise Thoa rox.-ne:oa eorkor l In Interatinal Conferencn Medicalmage cmputingan coputerassisteintervention, page234241Spre, 2015.   
Chitwan Saharia, Willam Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022.   
Team Seawead, Ceyuan Yang, Zhijie Lin, Yang Zhao, Shanchuan Lin, Zhibei Ma, Haoyuan Guo, Hao Chen, Lu Qi, Sen Wang, et al. Seaweed-7b: Cost-effective training of video generation foundation model. arXiv preprint arXiv:2504.08685, 2025.   
OrianeSiméoni, Huy V Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab,Cijo Jose,Vasil Khaliov, Marc Szafraniec, Seungeun Yi, Michaël Ramamonjisoa, et al. Dinov3. arXiv preprint arXiv:2508.10104, 2025.   
Wei Song, Yuran Wang, Zijia Song, Yadong Li, Haoze Sun, Weipeng Chen, Zenan Zhou, Jianhua Xu, Jiaqi Wang, and Kaicheng Yu Dualtoken:Towards unifyng visualunderstanding andgeneration with dual visual vocabularies. arXiv preprint arXiv:2503.14324, 2025.   
Yan Song, Jscha SohDickstein, Diederik PKingma, Ahishek Kumar, StefanoErmon, and Be PooleScorased generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020.   
Krishna Srinivasan, Karthik Raman, Jecao Chen, Michael Bendersky, and Marc Najork. Wit: Wikipedia-based image text dataset for multimodal multilingual machine learning. In Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval, pages 24432449, 2021.   
Alex Su, Haozhe Wang, Weiming Ren, Fangzhen Lin, and Wenhu Chen. Pixel reasoner: Incentivizing pixel-space reasoning with curiosity-driven reinforcement learning. arXiv preprint arXiv:2505.15966, 2025a.   
Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024.

Zhen u, Peng Xia, Hanu Guo, Zhenu Lu, Yan Ma, Xiaye u, Jaqi Liu, Yanshu Li, Kaide Zeng, Zhegn Yang, et al Thinking with images fr multimodal reasoning: Foundations, methods, andfuture frontiers.arXiv preprint arXiv:2506.23918, 2025b.   
un i JiangShouhenShil Zhang, Bngu g g Luo,andZehuuanutovel beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024.   
Hao Tang, Chenwei Xie, Xiaoyi Bao, Tingyu Weng, Pandeng Li, Yun Zheng, and Liwei Wang.Unilip:Adaptingclip for unified multimodal understanding, generation and editing. arXiv preprint arXiv:2507.23278, 2025.   
Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024.   
Gemi Team, Petko Georgiv, Ving In Lei, RyanBurel Libin Bai,Anol Gulati, Garett Tanzer, DamieVnct, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5 Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024.   
Wan-Video team. Wan:Open and advanced large-scale vid generativemodels (wan2.), 025.https://github.com/ Wan-Video/Wan2.2. GitHub repository, Apache-2.0 License.   
Michael Tschannen, Manoj Kumar, Andreas Steiner, Xiaohua Zhai, Neil Houlsby, and Lucas Beyer. Image captioners are scalable vision learners to0. Advances in Neural Information Processing Systems, 36:4683046855, 2023.   
Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, et al.Siglip Multilingual vision-language encders wit improved semantic understandin, localization, and dense features.arXiv preprint arXiv:2502.1786, 2025.   
Aaron Van Den Oord Oriol Vinyals,e al Neural dscrete representation learning.Advance inneuralinformatin processing systems, 30, 2017.   
Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaoje Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haimig Zhao, Jianxio Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025.   
Dianyi Wang, Wei Song, Yikun Wang, Siyuan Wang, Kaicheng Yu, Zhongyu Wei, and Jiaqi Wang. Autoregressive semantic visual reconstruction helps vlms understand better. arXiv preprint arXiv:2506.09040, 2025a.   
Haochen Wang, Anlin Zheng, Yucheng Zhao, Tiancai Wang, Zheng Ge, Xiangyu Zhang, and Zhaoxiang Zhang. Reconstructive visual instruction tuning. arXiv preprint arXiv:2410.09575, 2024a.   
Wn ShuBiSiTan Shij Wag Zhian JizBai Keqhen, Xueu Jial Wan Ge, et al. Qwen2-vEnhancing vision-anguage model's perception of the world at any resolution.arXiv prerint arXiv:2409.12191, 2024b.   
Runqian Wang and Kaiming He. Diffuse and disperse: Image generation with representation regularization. arXiv preprint arXiv:2506.09027, 2025.   
Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Ming Ding, Xiaotao Gu, Shiyu Huang, Bin Xu, et al. Lvbench: An extreme long video understanding benchmark. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2295822967, 2025b.   
WeWag Zhae o, Lixi Gu, Henguu, Longui, Xing Wi, Zhaag Lu, Lingl Jing, Sheglong Ye Ji hInteAvsulle. arXiv preprint arXiv:2508.18265, 2025c.   
Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024c.   
Luis Wiedmann, Orr Zohar, Amir Mahla, Xiaohan Wang, Rui Li, Thibaud Frere, Leandro von Werra, Aritra Roy Gosthipaty, and Andés Marafioti. Finevision: Open data is all you need. arXiv preprint arXiv:2510.17269, 2025.   
Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025a.   
Chengu Wu, Xiakang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1296612977, 2025b.   
Chenyuan Wu, Pengei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junje Zhou, et al. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025c.   
Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: A benchmark for long-context interleaved video-language understanding. Advances in Neural Information Processing Systems, 37:2882828857, 2024a.   
Size Wu, Wenwei Zhang, Lumin Xu, Sheng Jin, Zhonghua Wu, Qingyi Tao, Wentao Liu, Wei Li, and Chen Change Loy. Harmonizing visual representations for unified multimodal understanding and generation. arXiv preprint arXiv:2503.21979, 2025d.   
Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Y LYVlaa mo ntatsalunanXi arXiv:2409.04429, 2024b.   
xAI. Grok-1.5 vision preview. https://x.ai/news/grok-1.5v. Company news post.   
Shi XioYuee Wang, Jun Zhou Huy Yuan Xingr Xing, Ruiran Yan,Caoan Li Shuti Wang Tieju Huang, and Zheng Liu. Omnigen: Unified image generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1329413304, 2025.   
Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang Li, Ligeng Zhu, Yo Lu, et al. Sana: Effiient high-resolution image syntheis with lneardiffusin tranformers.arXiv prerint arXiv:2410.10629, 2024a.   
Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024b.   
Jinheng Xie, Zhenheng Yang, and Mike Zheng Shou. Show-o2: Improved native unified multimodal models. arXiv preprint arXiv:2506.15564, 2025a.   
Rongchang Xie, Chen Du, Ping Song, and Chang Liu. Muse-vl: Modeling unifed vlm through semantic discrete encoding. arXiv preprint arXiv:2411.17762, 2024c.   
Rongchang Xie, Chen Du, Ping Song, and Chang Liu. Muse-vl: Modeling unified vlm through semantic discrete encoding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2413524146, 2025b.   
An Yang, Anfeng Li, Baosong Yag, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025.   
Jinng Yao, Bin Yangand Xingang Wang.Reconstructions.eneratio:Tami optimizationdilem inlatent diffusion models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1570315712, 2025.

Yae, Xia He, ZoBin in, SheaiYuan Zhi Yan Bo Hou and  YuaIe: image editing dataset and benchmark. arXiv preprint arXiv:2505.20275, 2025.   
Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation algnment for generation:Training diffsion transformers is easier than you think.arXiv preprint arXiv:2410.06940, 2024.   
Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu:A massive multi-discipline multimodal understanding and reasonin bencmak for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024.   
Zhengrong Yue, Haiyu Zhang, Xiangyu Zeng, Boyu Chen, Chenting Wang, Shaobin Zhuang, Lu Dong, KunPeng Du, Yi Wang, Limi Wang, al Unifow:A unid pixel fow tokenizer or visual undertandin an generatin.arXiv preprint arXiv:2510.10575, 2025.   
Xi Zhai, Basi Mstaa Alexanr Kolesniov, and Lucs Beyer.gmoid oss or langemage pre-rai. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1197511986, 2023.   
Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chuuan Li and Ziwei Liu. Long cntext transer om anguage o vision.arXiv prerit arXiv:2406.16852, 024a.   
Yuanan Zhang, Jinmig Wu, Wei Li, BoLi,Zejun a, Ziwei Liu, and Chuyuan Liido instructiontuni wih synthetic data. arXiv preprint arXiv:2410.02713, 2024b.   
Boyang Zheng Nanye Ma, Shengbang Tong, and Sainng Xie. Diffusin transormers with representation autoencodes. arXiv preprint arXiv:2510.11690, 2025.   
Chuntig Zhou, Lii Yu, Aru Babu, Kushal Tirumala, Michihiro Yasunaga, LeonidShamis, Jacob Kahn, Xuee Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024.   
Muzhi Zhu, Yang Liu, Zekai Luo, Chenchen Jing, Hao Chen, Guangkai Xu, Xinlong Wang, and Chunhua Shen. Unleashinthe potential the diffusion model infew-shot semanticgmentatio.Advanc inNeural Inorati Processing Systems, 37:4267242695, 2024.   
Ran Zuo, Haoxiang Hu, Xiaoming Deng, Cangjun Gao, Zhengming Zhang, Yukun Lai, Cuixia Ma, Yong-Jin Liu, and Hongan Wang.SceneifGenerative scene-leve image retrieval with text and sketch usin diffusion models. 2024.