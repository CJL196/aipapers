# Self-Chained Image-Language Model for Video Localization and Question Answering

Shoubin Yu Jaemin Cho Prateek Yadav Mohit Bansal UNC Chapel Hill {shoubin, jmincho, praty, mbansal}@cs.unc.edu

# Abstract

Recent studies have shown promising results on utilizing large pre-trained imagelanguage models for video question answering. While these image-language models can efficiently bootstrap the representation learning of video-language models, they typically concatenate uniformly sampled video frames as visual inputs without explicit language-aware, temporal modeling. When only a portion of a video input is relevant to the language query, such uniform frame sampling can often lead to missing important visual cues. Although humans often find a video moment to focus on and rewind the moment to answer questions, training a query-aware video moment localizer often requires expensive annotations and high computational costs. To address this issue, we propose Self-Chained Video Localization-Answering (SEVILA), a novel framework that leverages a single image-language model (BLIP2) to tackle both temporal keyframe localization and question answering on videos. SEVILA framework consists of two modules: LocALIzER and AnswERER, where both are parameter-efficiently fine-tuned from BLIP-2. We propose two ways of chaining these modules for cascaded inference and self-refinement. First, in the forward chain, the LocALizeR finds multiple language-aware keyframes in a video, which the AnswEREr uses to predict the answer. Second, in the reverse chain, the AnswErER generates keyframe pseudo-labels to refine the LocALIzER, alleviating the need for expensive video moment localization annotations. Our SeViLA framework outperforms several strong baselines/previous works on five challenging video question answering and event prediction benchmarks, and achieves the stateof-the-art in both fine-tuning (NExT-QA and STAR) and zero-shot (NExT-QA, STAR, How2QA, and VLEP) settings. We show a comprehensive analysis of our framework, including the impact of LocALIzER, comparisons of LocALIZER with other temporal localization models, pre-training/self-refinement of LocALIzER, and varying the number of keyframes.1

# 1 Introduction

The recent success of large-scale pre-trained language models [2, 7, 65] has led to a burst of multimodal vision-and-language models that can jointly understand visual (image/video) and language data [64, 6, 60]. However, due to higher computational and annotation costs, video-language models (video-LMs) are more challenging to scale in terms of model and data size than image-language models (image-LMs). Hence, recent studies have explored efficient training of video-LMs by leveraging pre-trained image-LMs [44, 14, 23, 85, 70, 31, 19, 82]. While such a warm-start strategy facilitates visual representation learning of video-LMs, they typically concatenate uniformly/randomly sampled video frames as visual inputs without explicit language-aware, temporal modeling. However, such a simple uniform/random sampling of frames can lead to losing important visual cues, resulting in the video-LMs focusing on frames that are unimportant/irrelevant to language queries [42].

![](images/1.jpg)  

Figure 1: Self-Chained Video Localization-Answering $( \mathbf { S E V I L A } )$ consists of a LocALIzER and an AnswEreR. Left: Forward chain for language-aware temporal keyframe localization and question answering. Right: Reverse chain for LocALizER self-refinement with keyframe pseudo-labels.

To address this, we introduce Self-Chained Video Localization-Answering (SEViLA), a novel video-language framework where we adopt a single image-LM to handle both temporal localization and question answering on videos, while avoiding expensive language-aware, temporal grounding annotations (plus self-refinement [48] between the two modules). Our SEViLA framework obtains two modules, a LocALIzer and an AnswERer through parameter-efficient finetuning [62] of BLIP-2 [35], a recent state-of-the-art image-LM. SEVLA tackles video-language tasks by chaining the output of LocALizeR to the input of AnswERer (forward chain, Fig. 1 left), while the AnswERer gives feedback to refine the LocALizeR (backward chain, Fig. 1 right). In the forward chain, LocALizeR leverages the original image-language understanding of the BLIP-2 backbone and chooses the important language-aware video keyframes via the localization prompt "Does the information within the frame provide the necessary details to accurately answer the given question?" for each video frame. Then AnswEREr takes the concatenation of selected keyframes as visual input to predict video-level answers. In the backward chain, we generate keyframe pseudo-labels [26] to refine the LocALizEr, where we label a video frame as a keyframe if Answerer can output the correct answer using that frame. This self-refinement improves the language-aware temporal localization accuracy and alleviates the need for expensive keyframe annotations. We demonstrate the effectiveness of SeViLA framework on five challenging video question answering and event prediction benchmarks (NExT-QA, STAR, How2QA, TVQA, and VLEP) [75, 77, 36, 27, 28], where $\mathrm { S E V I L A }$ outperforms several strong baselines/previous works, and achieves the state-of-the-art in both fine-tuning (NExT-QA and STAR) and zero-shot (NExT-QA, STAR, How2QA, and VLEP) settings. We also show that our LocALizer can be used as a strong stand-alone moment retrieval model. We present a comprehensive analysis to elaborate the design choices of the proposed framework, including the impact of temporal localization, the impact of the self-refinement process (backward chain), and varying the number of keyframes. Our contributions are summarized as follows: •A novel video-language framework SEVILA, where the LocALIzER and AnswERER are initialized from a single image-language model to handle temporal localization and question answering on videos, respectively. •A new self-refinement method for language-aware temporal keyframe localization, where the AnswEReR generates keyframe pseudo-labels to refine the LocALIzER, without expensive temporal grounding annotation.   
Strong empirical performance with state-of-the-art on multiple video-language benchmarks.   
Comprehensive analysis elaborating the design choices of the proposed framework.

# 2 Related Work

Image-Language Pre-trained Models. As the demand for cross-modal applications continues to grow, image-language pre-training studies have received tremendous attention and success. Imagelanguage pre-trained models [55, 12, 69, 5, 36, 64, 31, 34, 86] have advanced more rapidly than video-language pre-trained models [71, 90, 45, 17, 80, 67, 81, 83, 87, 37, 88], both in terms of model [61, 38, 35, 1, 70, 43] and pre-training data scale [61, 1, 89, 21] (more detailed model size and pre-training data scale comparisons are in Appendix). This can be attributed to the increased accessibility of image data and the comparatively simpler data structures, which makes scaling up image-language learning easier [58]. In our paper, SEViLA is built on the recent state-of-the-art image-LM BLIP-2 [35] and extends it to adopt video input for video-language tasks. We also compare our SEVILA framework with the current state-of-the-art video-LM, InternVideo [71], to demonstrate the superiority of a large image-LM that incorporates keyframe localization.

Image-to-Video Transfer Learning. The gap between image- and video-language models has inspired numerous useful methods focusing on image-to-video transfer learning, which leverage a limited number of video frames to enhance learning efficiency [80, 32, 23, 46, 15, 44, 14, 31, 4, 82, 68]. Luo et al. [44] adapt pre-trained CLIP [55] backbone to solve video clip retrieval. Yang et al. [85] extend frozen bidirectional language models [66] to incorporate multiple images and apply additional video-level pre-training to facilitate model adaptation [62]. Wang et al. [72] convert multiple images into hierarchical captions, arranged with a temporal order prompt to help language models comprehend video-level events. However, these works employ a uniform sampling strategy that is not language-aware. This can lead to the loss of key visual cues for temporal modeling and even burden the models with irrelevant information [31, 76]. In this paper, we propose a LocALizeR to provide language-aware visual information to video-language tasks.

Language-aware Keyframe Localization. Many methods [42, 3, 18, 54, 24, 42, 41, 73, 9] have been proposed to address the challenge of language-aware keyframe localization. Buch et al. [3] optimized an end-to-end pipeline using answer labels to select a single keyframe for downstream tasks. Lu et al. [42] selects frames with separate image and language models, and answers questions by a QA model with multiple training objectives. Qian et al. [54] designed a video clip proposal model with predefined ranges, iteratively training it with a QA model. Kim et al. [24] utilized a semi-parametric retriever to obtain keyframes based on frame and language feature similarity. We adopt a large image-LM as our LocALIzER and chain it with an AnswERER. Our LocALizER can help to fine-tune Answerer in the forward chain and be refined with pseudo-labels in the reverse chain.

# 3 Method: SEVILA

In this section, we introduce the method details of our Self-Chained Video Localization-Answering (SEViLA) framework. First, we provide BLIP-2 preliminaries, which serve as the foundation for our framework. Then we elaborate our design of the BLIP-2 LocALIzER and BLIP-2 AnswERER for temporal localization and question answering on videos. Finally, we present the SeViLA framework's training and inference processes in the forward and reverse chain.

# 3.1 Preliminaries: BLIP-2

We adopt BLIP-2 [35] as the backbone of our SEVILA framework. BLIP-2 is a recent state-of-the-art pre-trained image-language model (image-LM) comprising of: (1) a frozen image encoder [11, 16]; (2) a frozen large language model (LLM) [7, 91]; and (3) a Q-former, which is a trainable transformer [66] module that bridges the image encoder and LLM, similar to acting as an adapter [62, 20]. It takes as input visual features $h$ from the image encoder and learnable query embeddings $q$ ,and outputs fixed-length visual features $v$ . The BLIP-2 Q-Former undergoes a two-stage pre-training. First, it connects to the image encoder to perform image-to-text pre-training. This stage enables the Q-Former to extract the most informative visual information for the text and remove any irrelevant details in $v$ Subsequently, the Q-Former is connected to the LLM to leverage its generative language capabilities. This is achieved using a fully-connected layer to project query embeddings into the LLM's dimension with image-to-text pre-training. As a result, these query features serve as soft visual prompts [22] for the LLM. With the two-stage pre-trained Q-former and LLM, BLIP-2 shows advanced performance on various image-language tasks. In our SEVILA framework, we adopt BLIP-2 as the basic building block for both video temporal localization and question answering modules. We retain the visual encoder and the LLM from BLIP-2 by keeping them frozen during training. In this case, only these two Q-formers are updated during the LocALIzER and AnswERER training (see Sec. 3.3).

![](images/2.jpg)  

Figure 2: In SEVILA framework, LocALIzer (top) selects top-K video frames, which guides AnswErer (bottom) to focus on important language-aware video moments and predict answers. Both LocALIzER and AnswERER are initialized from a single pre-trained BLIP-2 model, where only Q-formers and a linear layer $2 . 5 \%$ of total parameters) are tuned for each module. We omit the linear layer after the Q-former for simplicity.

# 3.2 Self-Chained Video Localization-Answering

Adapting BLIP-2 to Temporal Localization and Question Answering on Videos. As illustrated in Fig. 2, our SEViLA framework adopts BLIP-2 to tackle both video temporal localization and question-answering. We assign BLIP-2 two roles of being a LocALizer and an AnswERer by using different Q-formers. We first elaborate our LocALIzER and AnswEReR in detail as follows:

LocALIzER. We first extract frame features via the frozen image-encoder ViT [16], referred to $E _ { v }$ . Given the video, we uniformly sample $n$ frames $\{ f _ { 1 } , . . . , \bar { f _ { n } } \}$ .We then obtain $i _ { t h }$ frame feature $h _ { i }$ as $h _ { i } = E _ { v } ( f _ { i } )$ . Finally, we represent the video as a set of frame features $V =$ $\{ h _ { 1 } , . . . , h _ { n } \}$ . These features are extracted once and then saved to be subsequently reused by the LoCALIzER and the AnswERER. The primary objective of the LocALIzER is to select $k$ language-aware keyframe features from $V$ ,where $k$ is typically much smaller than $n$ . As illustrated in Fig. 2 (top), we then independently extract visual query features $v _ { i }$ from original frame features in $V$ via a Q-Former $Q _ { l o c }$ . Next, visual query features $v _ { i }$ and language contexts $L$ are concatenated and fed into the LLM (Flan-T5 [7]), where we create $L$ by combining question, options, and localization prompt "Does the information within the frame provide the necessary details to accurately answer the given question?". The LocALizer outputs the score for each frame $s _ { i }$ , whic is the probability of generating a word 'yes', given the visual features $v _ { i }$ and language context $L$ : $s _ { i } = L L M ( c o n c a t ( v _ { i } , L ) )$ . We can localize language-aware keyframes $K = \{ v _ { 1 } ^ { k } , . . . , v _ { K } ^ { k } \}$ , bas on the top-frame cores. Our LocALize can be formulat as:

$$
K = \operatorname { L o c a L I Z E R } ( V , L ) , \quad | K | = k \ll n
$$

AnswereR. With the keyframe set $K$ obtained from the LocALIzER, as illustrated in Fig. 2 (bottom), can procd o gnera nswers usig he As.We rst obtain keyrame quey u $v _ { i }$ by processing them through $Q _ { a n s }$ , following the same procedure used in the LocALIzER. Next, we feed the LLM with all query features and language contexts by concatenating them and obtain the video-level answer $\boldsymbol { a } = \dot { L } L \dot { \boldsymbol { M } } ( c o n c a t ( v _ { 1 } ^ { k } , . . . , \mathbf { \bar { \boldsymbol { v } } } _ { K } ^ { k } , \mathbf { \bar { \boldsymbol { L } } } ) ) ^ { 2 }$ modeling with multiple frame inputs. Our AnswERER can be formulated as:

$$
a = \operatorname { A N S W E R E R } ( K , L )
$$

![](images/3.jpg)  

Figure 3: Top: In the forward chain, the LocALizer finds multiple language-aware keyframes, then the Answerer utilizes these keyframes to predict answers. We use the forward chain for both inference and AnswErer fine-tuning. Bottom: In the reverse chain, we generate keyframe pseudo-labels by using the AnSWERer to refine the LocALIZER.

# 3.3 Training AnswERER and LoCALIZER via Self-Chaining

Fine-tuning AnswErer in forward chain. As illustrated in Fig. 3 (top), we fine-tune the AnswERER on downstream tasks using keyframes from LocALizEr via the forward chain. AnswErer takes the keyframes generated by LocALizER. We compare the default setting to other settings (e.g., input frames are uniformly selected) in Appendix. Refining LocALizer in reverse chain. We adopt pseudo-labeling [26] in our reverse chain to address the costly frame-level localization annotations. We use binary pseudo-labels, where we label a video frame as a keyframe if Answerer can output the correct answer using that frame. As shown in Fig. 3 (bottom), The frozen AnsweRer is first prompted by a QA task prompt and generates a frame-level answer, then we obtain pseudo labels by comparing this prediction with the ground-truth answer. The LocALIzER is trained to locate language-aware pseudo-label keyframes. Pre-training LocALIzER with moment retrieval label. To enhance our LocALIZER, we conduct transfer learning from a video moment retrieval/grounding task [56, 30] via pre-training. We use videos, queries, and video-level temporal span labels from QVHighlights [30], and assign a binary localization label to each frame by comparing its timestamp to the span annotations. We provide more details of pre-training in Appendix.

# 4 Experiments

In this section, we first outline our experimental setup (Sec. 4.1). Then, we demonstrate the superiority of SeVILA framework on 5 challenging long-form video question answering and event prediction benchmarks in both fine-tuning (Sec. 4.2) and zero-shot (Sec. 4.3) settings. we also conduct ablation studies on $\mathrm { S E V I L A }$ framework to show the effectiveness of each of its components on downstream tasks (Sec. 4.4). Next, We report the performance of our LocALizeR on video moment retrieval (Sec. 4.5). Lastly, we perform in-depth quantitative, and qualitative analyses on our LocALizeR to show the effect of our design for temporal keyframe localization (Sec. 4.6 and Appendix). More results on single v.s. multi-frame LocALizER, pre-training strategies, iterative self-refinement, computational cost, and extension to another Image-LM model are in Appendix.

# 4.1 Experimental setup

Benchmarks. We evaluate our SEViLA framework on 3 video-language tasks, including multi-choice Video Question Answering (NExT-QA [77], STAR [75], How2QA [36], TVQA [27], Video Event Prediction (VLEP [28]), and Moment Retrieval (QVHighlights [30]). See Appendix for details.

Table 1: Fine-tuning results on video question answering (NExT-QA, STAR, How2QA, TVQA) and video event prediction (VLEP). We gray out the methods take extra speech input or use dense frames. We bold the best numbers, and underlined the second-best numbers. dense/1fps: the model takes dense (1fps) video frames instead of a fixed number of frames. $3 2  4$ : our LocALizer selects 4 keyframes from 32 frames. \* represents the results tested by ourselves. $\mathbf { S } \mathbf { E } \mathbf { V } \mathbf { I } \mathbf { L } \mathbf { A } ^ { \dagger }$ uses the zero-shot LocALizer without refining on pseudo-labels via the reverse chain.   

<table><tr><td rowspan="2">Model (# Frames)</td><td colspan="4">NExT-QA</td><td colspan="4">STAR</td><td rowspan="2" colspan="4">How2QA TVQA VLEP</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td>Tem. Cau. Des. Avg. Int. Seq. Pre. Fea. Avg.</td><td></td></tr><tr><td colspan="9">(w/ speech input or use dense frames)</td><td></td><td></td><td></td></tr><tr><td>HERO (dense/1fps) [36]</td><td>-</td><td>-</td><td>-</td><td></td><td></td><td></td><td></td><td>-</td><td>73.8</td><td>73.6</td><td>-</td></tr><tr><td>JustAsk (20) [84]</td><td>51.4</td><td>49.6</td><td>63.1</td><td>52.3</td><td></td><td></td><td>-</td><td></td><td>84.4</td><td>-</td><td>-</td></tr><tr><td>FrozenBiLM (10) [85]</td><td>-</td><td></td><td></td><td></td><td></td><td></td><td>-</td><td>-</td><td>86.7</td><td>82.0</td><td>-</td></tr><tr><td>VidIL 4-shot (12) [72]</td><td>-</td><td></td><td></td><td></td><td></td><td></td><td>-</td><td>-</td><td>-</td><td>-</td><td>72.0</td></tr><tr><td>T+T (dense/1fps) [40]</td><td></td><td></td><td></td><td></td><td></td><td></td><td>-</td><td>-</td><td>92.4</td><td>-</td><td>-</td></tr><tr><td>T+T (+ASR, dense/1fps) [40]</td><td></td><td></td><td></td><td>-</td><td></td><td></td><td>-</td><td>-</td><td>93.2</td><td></td><td>-</td></tr><tr><td>Flamingo-80B 32-shot (30) [1]</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>FrozenBiLM (10) [85]</td><td>-</td><td></td><td>-</td><td></td><td></td><td></td><td></td><td>42.2 -</td><td>- 81.5</td><td>57.5</td><td></td></tr><tr><td>All-in-One (32) [67]</td><td>48.6</td><td></td><td></td><td></td><td></td><td></td><td></td><td>48.0 63.2 50.6 47.5 50.8 47.7 44.0 47.5</td><td>-</td><td>-</td><td></td></tr><tr><td>Temp[ATP] (32) [3]</td><td>49.3</td><td>48.6</td><td></td><td></td><td></td><td></td><td></td><td>65.0 51.5 50.6 52.8 49.3 40.6 48.3</td><td>-</td><td>-</td><td></td></tr><tr><td>VGT (32) [78]</td><td>55.0</td><td>52.2</td><td>64.0</td><td>55.0</td><td></td><td></td><td></td><td>44.2</td><td></td><td></td><td></td></tr><tr><td>MIST (32) [18]</td><td>56.6</td><td>54.6</td><td>66.9</td><td></td><td></td><td></td><td></td><td>57.1 55.5 54.2 54.2 44.4 51.1</td><td>-</td><td></td><td></td></tr><tr><td>VFC (32) [50]</td><td>53.3</td><td>57.6</td><td>72.8</td><td>58.6</td><td>-</td><td>-</td><td></td><td>-</td><td>-</td><td></td><td></td></tr><tr><td>CoVGT (32) [79]</td><td>57.4</td><td>58.8</td><td>69.3</td><td>60.0</td><td></td><td>-</td><td></td><td>45.9</td><td>-</td><td></td><td></td></tr><tr><td>SeViTFiD (10) [24]</td><td>-</td><td></td><td></td><td>60.6</td><td></td><td></td><td></td><td>-</td><td></td><td>-</td><td></td></tr><tr><td>HiTeA (16) [87]</td><td>58.3</td><td>62.4</td><td>75.6</td><td>63.1</td><td>-</td><td>-</td><td></td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>InternVideo* (8) [71]</td><td>58.5</td><td>62.5</td><td>75.8</td><td>63.2</td><td></td><td></td><td></td><td>62.7 65.6 54.9 51.9 58.7</td><td>79.0</td><td>57.2</td><td>63.9</td></tr><tr><td>BLIP-2voting (4)</td><td>65.2</td><td>70.1</td><td>80.1</td><td>70.1</td><td></td><td></td><td></td><td>52.3 54.8 49.0 51.2 51.8</td><td>79.6</td><td>54.5</td><td>67.0</td></tr><tr><td>BLIP-2concat (ANSWERER) (4)</td><td>68.1</td><td>72.9</td><td>81.2</td><td>72.6</td><td></td><td></td><td></td><td>65.4 69.0 59.7 54.2 62.0</td><td>82.2</td><td>59.8</td><td>68.6</td></tr><tr><td>SEVILA† (32 → 4)</td><td>68.8</td><td>73.4</td><td>83.5</td><td>73.4</td><td></td><td>63.2 66.6 61.3 60.0 62.7</td><td></td><td></td><td>83.7</td><td>59.7</td><td>69.0</td></tr><tr><td>SeViLA (32 → 4)</td><td>69.4</td><td>74.2</td><td>81.3</td><td>73.8</td><td></td><td>63.7 70.4 63.1 62.4 64.9</td><td></td><td></td><td>83.6</td><td>61.6</td><td>68.9</td></tr></table>

Baselines. We compare our $\mathrm { S E V I L A }$ framework with the state-of-the-art video-language pre-trained model, InternVideo [71] as well as our backbone BLIP-2 [35]. We extend BLIP-2 to adapt videos in two settings: (1) BLIP $2 ^ { \mathrm { v o t i n g } }$ , which processes each uniformly sampled frame independently and obtains the final answer by majority voting on all frame-level answers, and (2) BLIP $2 ^ { \mathrm { c o n c a t } }$ , where Q-former processes each frame and Flan-T5 takes the concatenation of visual features as a prefix. BLIP $2 ^ { \mathrm { c o n c a t } }$ i Implementation Details. SEVILA framework adopts BLIP-2 [35], an image-language model with 4.1B parameters and pre-trained on 129M images in total, including COCO [39], Visual Genome [25], CC12M [59], SBU [52], and 115M images from LAION400M [57]. See Appendix for details.

# 4.2 Fine-tuning Comparison to SOTA on Video QA and Event Prediction

We compare our SEViLA framework to recent state-of-the-art models on 4 video QA benchmarks anc 1 video event prediction dataset. We show results in Table 1, and summarize our findings as follows (a) Temporal modeling matters. BLIP. $2 ^ { \mathrm { v o t i n g } }$ underperforms our BLIP. $2 ^ { \mathrm { c o n c a t } }$ (ANSwEReR) and other video-LM models on STAR, How2QA, TVQA, and VLEP. Especially on STAR-Sequence, a task demanding heavy temporal understanding, our BLIP. $2 ^ { \mathrm { c o n c a t } }$ (AnswERER) outperforms BLIP-2voting significantly by $1 3 . 1 \%$ $( 6 9 . 0 \%$ vs. $5 4 . 8 \%$ . As BLIP $2 ^ { \mathrm { v o t i n g } }$ processes frames independently and lacks temporal modeling among frames, the result indicates that temporal modeling is essential to tackle video-language tasks and the effectiveness of our temporal modeling design.

(b) Keyframe selection helps. Our $\mathbf { S } \mathbf { E } \mathbf { V } \mathbf { I } \mathbf { L } \mathbf { A } ^ { \dagger }$ framework, featuring a zero-shot LocALIzER, leads on all tasks with an average advantage of $5 . 3 \%$ over the top video-LM (InternVideo). It also surpasses BLP $2 ^ { \mathrm { c o n c a t } }$ (AnswereR) that uses uniform frame sampling on NeXT-QA $( + 1 . 2 \% )$ , STAR $( + 0 . 7 \% )$ , How2QA $( + 1 . 5 \% )$ , and VLEP $( + 0 . 4 \% )$ . This highlights the importance of keyframe selection in video-language tasks, even when using a zero-shot LocALIzER.

Table 2: Zero-shot results on video question answering and video event prediction.   

<table><tr><td rowspan="2">Model (# Frames)</td><td colspan="4">NExT-QA</td><td colspan="4">STAR</td><td rowspan="2" colspan="4">How2QA TVQA VLEP</td></tr><tr><td></td><td>Tem. Cau. Des. Avg. Int. Seq. Pre. Fea. Avg.</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>(w/ speech input or use dense frames)</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>JustAsk (20) [84]</td><td>-</td><td>-</td><td></td><td></td><td></td><td>-</td><td></td><td></td><td>-</td><td>51.1</td><td>-</td><td>-</td></tr><tr><td>FrozenBiLM (10) [85]</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>-</td><td>58.4</td><td>59.2</td><td>-</td></tr><tr><td>ViperGPT (dense/1fps) [63]</td><td></td><td>-</td><td>-</td><td>60.0</td><td>-</td><td></td><td></td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Flamingo-80B (30) [1]</td><td></td><td></td><td>-</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>FrozenBiLM (10) [85]</td><td>-</td><td>- -</td><td>-</td><td>- -</td><td></td><td></td><td></td><td>-</td><td>39.7</td><td>- 41.9</td><td>- 29.7</td><td>-</td></tr><tr><td>VFC (32) [50]</td><td>- 45.4</td><td></td><td>51.6 64.1 51.5</td><td></td><td></td><td>-</td><td>-</td><td></td><td>- -</td><td>-</td><td>-</td><td>- -</td></tr><tr><td>InternVideo* (8) [71]</td><td>43.4</td><td>48.0</td><td>65.1</td><td>49.1</td><td></td><td></td><td>43.8 43.2 42.3 37.4 41.6</td><td></td><td></td><td>62.2</td><td>35.9</td><td>58.7</td></tr><tr><td>BLIP-2voting (4)</td><td>59.1</td><td>61.3</td><td>74.9</td><td>62.7</td><td>41.8</td><td></td><td>39.7 40.2 39.5 40.3</td><td></td><td></td><td>69.8</td><td>35.7</td><td>63.8</td></tr><tr><td>BLIP-2concat (AnswereR) (4)</td><td>59.7</td><td>60.8</td><td>73.8</td><td>62.4</td><td></td><td></td><td>45.5 41.8 41.8 40.0 42.2</td><td></td><td></td><td>70.8</td><td>36.6</td><td>64.0</td></tr><tr><td>SEVILA† (32 → 4)</td><td>61.3</td><td></td><td>61.5 75.6 63.6 48.3 45.0 44.4 40.8 44.6</td><td></td><td></td><td></td><td></td><td></td><td></td><td>72.3</td><td>38.2</td><td>64.4</td></tr></table>

(c) Self-refinement improves temporal localization. For $\mathrm { S E V I L A }$ , we refine the LocALizeR with pseudo-labels (see Sec. 3.3). Compared with $\mathbf { S } \mathbf { E } \mathbf { V } \mathbf { I } \mathbf { L } \mathbf { A } ^ { \dagger }$ , SEVILA further increases performance on NExT-QA $( 0 . 4 \% )$ , STAR $( + 2 . 2 \% )$ , and TVQA $( + 1 . 9 \% )$ . SEVILA framework achieves new state-of-the-art fine-tuning performance on NExT-QA, STAR, TVQA, and VLEP, using only visual and language modalities. This illustrates the significance of temporal localization and the efficacy of our self-refinement method for keyframe localization.

# 4.3 Zero-shot Comparison to SOTA on Video QA and Event Prediction

We further compare our SEViLA framework to recent state-of-the-art models in the zero-shot setting.   
We show the zero-shot results in Table 2, then discuss the findings in the following.

(a) Image-LM outperforms Video-LM, without video pretraining. Surprisingly, BLIP $2 ^ { \mathrm { v o t i n g } }$ , without inter-frame temporal modeling, outperforms the previous state-of-the-art video-LM, InternVideo on several datasets that require temporal reasoning. BLIP. $2 ^ { \mathrm { v o t i n g } }$ outperforms InternVideo on NExT-QA $( + 1 3 . 6 \% )$ , How2QA $( + 7 . 6 \% )$ , and VLEP $( + 5 . 1 \% )$ . On How2QA, BLIP- $2 ^ { \mathrm { v o t i n g } }$ even surpasses FrozenBiLM which performs extra speech and video pre-training by $1 1 . 4 \%$ It highlights the potential of image-LM for video-language tasks due to its model size and sufficient pre-training.

(b) Keyframe selection is more effective than uniform sampling. Our $\mathbf { S } \mathbf { E } \mathbf { V } \mathbf { I } \mathbf { L } \mathbf { A } \dagger$ framework, combining the zero-shot LocALIzeR and the zero-shot AnswEReR, outperforms the BLIP-2concat (AnswERER) with uniformly sampled frames on NExT-QA $( + 1 . 2 \% )$ , STAR $( + 2 . 4 \% )$ , How2QA $( + 1 . 5 \% )$ , TVQA $( + 1 . 6 \% )$ , and VLEP $( + 0 . 4 \% )$ , achieving new state-of-the-art zero-shot performance on NExT-QA, STAR, How2QA, and VLEP, and new state-of-the-art on TVQA with only visual and language modalities. On STAR, our $\mathrm { S E V I L A ^ { \dagger } }$ framework even outperforms zero-shot Flamingo [1] with 80B parameters by $4 . 9 \%$ . The result demonstrates the effectiveness of our SEViLA framework to adapt video-language tasks and the importance of language-aware keyframe selection.

# 4.4 Ablation Studies on SEVILA Framework

We conduct ablation studies on our $\mathrm { S E V I L A }$ framework about the effectiveness of LocALIzER and AnswERER. We show the results in Table 3. We summarize our findings as follows: (a) Sparse frames outperform dense frames: We observe performance declines when the AnswERER adopts more frames (A v.s. B), confirming that sparse frames work better due to the original Image-LM model's limited temporal modeling ability, while too dense frames may distract the model.

(b) Keyframes outperform uniformly sampled frames: We compare AnswERER with LocALIZER (SEViLA framework) and AnswERER that takes uniformly sampled frames. We observed significant performance gains in the zero-shot AnswErer setting when utilizing the zero-shot LocALIZER $( \mathbf { B } \ \nu . s .$ C), with improvements on NExT-QA $( + 1 . 0 \% )$ , STAR $( + 2 . 4 \% )$ , How2QA $( + 1 . 5 \% )$ , TVQA $( + 1 . 6 \% )$ , and VLEP $( + 0 . 4 \% )$ . And pseudo-label refinement for LocALizER further increased performance by an average of $2 . 1 \%$ across all tasks (B v.s. D). In the fine-tuned AnswEREr setting, the benefits of the LocALizER remained evident. Our SEVILA framework, which utilized keyframes from the

Table 3: Ablation studies on SEViLA framework. 'uniform' refers to the uniform sampling of video frames. LocALIzER† refers to the zero-shot LocALIzER without refining on pseudo-labels.   

<table><tr><td rowspan="2"></td><td colspan="2">AnSwERER</td><td rowspan="2">Keyframe</td><td>NExT-QA</td><td>STAR</td><td rowspan="2">How2QA TVQA VLEP</td><td rowspan="2"></td><td rowspan="2"></td></tr><tr><td colspan="2"># frame finetuned?</td><td></td><td>Tem. Cau. Des. Avg. Int. Seq. Pre. Fea. Avg.</td></tr><tr><td></td><td>32</td><td>X</td><td>uniform</td><td></td><td>54.7 56.7 67.8 57.7 46.2 43.6 40.7 41.042.8</td><td>67.0</td><td>33.2</td><td>54.0</td></tr><tr><td>B.</td><td>4</td><td>×</td><td>uniform</td><td></td><td>59.7 60.8 73.8 62.4 45.5 41.8 41.8 40.0 42.2</td><td>70.8</td><td>36.6</td><td>64.0</td></tr><tr><td>C.</td><td>4</td><td>×</td><td>LocaLizeR†</td><td></td><td>61.3 61.5 75.6 63.6 48.3 45.0 44.4 40.8 44.6</td><td>72.3</td><td>38.2</td><td>64.4</td></tr><tr><td>D.</td><td>4</td><td>X</td><td>LocaLizer</td><td></td><td>62.3 63.1 74.9 64.6 49.0 46.4 45.2 41.6 45.5</td><td>72.9</td><td>39.1</td><td>64.6</td></tr><tr><td>E.</td><td>4</td><td>√</td><td>uniform</td><td></td><td>68.1 72.9 81.2 72.6 65.4 69.0 59.7 54.2 62.0</td><td>82.2</td><td>59.8</td><td>68.6</td></tr><tr><td>F.</td><td>4</td><td>L</td><td>LOCAlizeR†</td><td></td><td>68.8 73.4 83.5 73.4 63.2 66.6 61.3 60.062.7</td><td>83.7</td><td>59.7</td><td>69.0</td></tr><tr><td>G.</td><td>4</td><td>4</td><td>LOcaLIzER</td><td></td><td>69.4 74.2 81.3 73.8 63.7 70.4 63.1 62.4 64.9</td><td>83.6</td><td>61.6</td><td>68.9</td></tr></table>

Table 4: Comparison on QVHighlights test split. We aggregate frame-level results of our LocALIZER for video-level evaluation (see Appendix).   

<table><tr><td>Model</td><td>R1@0.5</td><td>R1@0.7</td><td>mAP</td></tr><tr><td>CAL [13]</td><td>25.4</td><td>11.5</td><td>9.8</td></tr><tr><td>XML [29]</td><td>41.8</td><td>30.3</td><td>32.1</td></tr><tr><td>Moment-DETR [30]</td><td>52.8</td><td>33.0</td><td>30.7</td></tr><tr><td>QD-DETR [51]</td><td>62.4</td><td>44.9</td><td>39.8</td></tr><tr><td>LocALizeR (Ours)</td><td>54.5</td><td>36.5</td><td>32.3</td></tr></table>

Table 5: The impact of QVHighlights PreTraining (PT) and Self-Refinement (SR) for our LoCALIzER in Sec. 3.3.   

<table><tr><td rowspan="2">PT SR</td><td colspan="3">NExT-QA</td><td rowspan="2">How2QA</td></tr><tr><td>Tem. Cau.</td><td></td><td>Des. Avg.</td></tr><tr><td>-</td><td>-</td><td>60.4 61.0</td><td>74.6 62.9</td><td>70.7</td></tr><tr><td>✓</td><td>-</td><td>61.3 61.5</td><td>75.6 63.6</td><td>72.3</td></tr><tr><td>-</td><td>✓</td><td>62.1 62.6</td><td>75.1 64.3</td><td>72.8</td></tr><tr><td>✓</td><td>V</td><td>62.3 63.1</td><td>74.9 64.6</td><td>72.9</td></tr></table>

LocALIzER, outperformed the 4-frame AnswERER by an average of $0 . 7 \%$ across tasks $( \mathrm { E } \ \nu . s . \ \mathrm { F } )$ . Pseudo-label refinement continues to be effective in this setting, providing an average boost of $1 . 5 \%$ across all tasks (E v.s. G). These results demonstrate that keyframe selection contributes to non-trivial improvements in video-language tasks for zero-shot and fine-tuning settings.

# 4.5 Comparison to State-of-the-Art on Video Moment Retrieval

In this section, we evaluate our LocALizer on the video moment retrieval task. We pre-train the LocALizER on the QVHighlights [30] dataset, as discussed in Sec. 3.3, and then assess its performance on the same dataset. To test on QVHighlights, we first extract video frames at 0.5 fps, following Moment-DETR [30] and pass them through our LocALIzER to obtain binary frame-level predictions that indicate whether a frame matches the query sentence. Next, we combine these predictions into video-level temporal span predictions. We aggregate frame-level predictions into video-level spans by merging adjacent positive predictions with intervals not exceeding a threshold. These merged results are then consolidated into a single video-level span. More information on the aggregation process can be found in Appendix. Interestingly, as shown in Table 4, our LocALizER, which has no temporal modeling/training and operates on frame-level, outperforms many previous methods [13, 30, 29] with complex temporal modeling and video data training. It demonstrates our LocALizEr can further work as a standalone model for certain tasks. It also suggests large image-LM with temporal designs may be a promising study for video moment retrieval. This is evidenced by our LocALizer's superior performance compared to Moment-DETR, despite the absence of temporal modeling.

# 4.6 Detailed Analysis on the Localizer

In this section, we first analyze the impact of pre-training and self-refinement on our LocALIZER. Then, we compare our LocALizer with other keyframe selection methods in both zero-shot and fine-tuning settings. Next, we experiment with different keyframe selection ranges and quantities in our LocALizeR to assess the impact of temporal localization on the overall performance. We further show the impact of LocALIzeR in AnswEReR fine-tuning. We also present upper-bound analysis based on BLIP-2 and oracle keyframe localization. Lastly, we provide visualization results of our LoCALIzER. Additional experiments are in Appendix. Ablation on LocALizER pre-training and self-refinement. We performed these ablation studies with the zero-shot 4-frame AnswERER. As shown in Table 5, the untrained BLIP-2 LocALIZER results in only a minor improvement to the AnswEReR. Moreover, both QVHighlights pre-training and self-refinement via the reverse chain independently provide significant performance boosts. The optimal results are achieved when both pre-training and self-refinement are applied. It further demonstrates our method is label-efficient for keyframe temporal localization.

Comparison with other keyframe selection methods. In Table 6, we compare our LocALIzER with different keyframe localization methods, including CLIP [55], Moment-DETR [30] which are zero-shot, and ATP [3], Differentiable Top-K [8] which are fine-tuned with answer-label. We combine those keyframe localization methods with our zero-shot AnswERER. Those methods select 4 keyframes from 32 uniformly sampled frames. We find that keyframes from neither CLIP nor Moment-DETR can not help AnswERER. It may be due to their CLIP pre-training on images and short declarative sentences, which fail to produce question-aware visual features, and distract AnswERER with irrelevant features. In contrast, our zero-shot LocALIzER† improves on NExT-QA by averaging $1 . 2 \%$ .Furthermore, our LocALIzER refined on pseudo-labels outperforms fine-tuned ATP and Differentiable top $\mathbf { \nabla } \cdot \mathbf { K }$ , with a $2 . 2 \%$ average improvement across all question types. Overall, our LocALizer shows superior effectiveness in both settings.

Table 6: Comparison of our LocALIzeR with other keyframe localization methods.   

<table><tr><td>Method</td><td>NExT-QA</td></tr><tr><td>AnswERER</td><td>Tem. Cau. Des. Avg. 59.7 60.8 73.7 62.4</td></tr><tr><td>(zero-shot)</td><td>60.0 72.5 61.8</td></tr><tr><td>+ CLIP [55] + Moment-DETR [30] + Localizer</td><td>59.2 59.5 60.6 72.1 62.0 61.3 61.5 75.6 63.6</td></tr><tr><td>(fine-tuning)</td><td></td></tr><tr><td>+ ATP [3]</td><td>60.4 61.3 73.4 62.8</td></tr><tr><td>+ Differentiable Top-K [8] 59.5 59.7 72.7 61.6</td><td></td></tr><tr><td>+ LocaliZeR</td><td>62.3 63.1 74.9 64.6</td></tr></table>

Impact of keyframe selection ranges and quantities. In Table 7, we evaluate temporal keyframe localization in a zero-shot setting using various keyframe selection ranges and quantities. Even with one keyframe selected, our LocALIZER shows significant improvements over the BLIP $2 ^ { \mathrm { v o t i n g } }$ based on majority vote on 8 frame-level answers: NExT-QACausal $( + 2 . 4 \% )$ , NExT-QA-Description $( + 3 . 6 \% )$ , and How2QA $( + 2 . 6 \% )$ . This highlights our LocALizeR's effectiveness in localizing selective keyframes. We also note that multiple keyframes benefit NExT-QA-Temporal questions, but denser frames result in worse perfor mance. It supports our finding in Sec. 4.4, that using too dense frames may distract image-LM.

Table 7: Ablation of different numbers of input frames and output keyframes.   

<table><tr><td rowspan="2">Settings</td><td colspan="3">NExT-QA</td><td rowspan="2">How2QA</td></tr><tr><td>Tem.</td><td>Cau.</td><td>Des. Avg.</td></tr><tr><td>BLIP-2voting (8)</td><td>59.9</td><td>60.2</td><td>72.4 62.0</td><td>69.8</td></tr><tr><td>8→1</td><td>59.8</td><td>61.1</td><td>76.0 62.9</td><td>72.4</td></tr><tr><td>16→1</td><td>59.2</td><td>62.6</td><td>74.9 63.4</td><td>73.2</td></tr><tr><td>16→4</td><td>60.7</td><td>61.5</td><td>75.8 63.4</td><td>72.4</td></tr><tr><td>32→4</td><td>61.3</td><td>61.5</td><td>75.6 63.6</td><td>72.3</td></tr><tr><td>32→8</td><td>59.4</td><td>60.9</td><td>74.7 62.5</td><td>71.3</td></tr><tr><td>64→8</td><td>58.9</td><td>60.9</td><td>74.0 62.2</td><td>71.8</td></tr></table>

Impact of different frame sampling during AnswERer fine-tuning. In Sec. 3.3, we discussed how the AnswERER in the SEVILA framework can be further fine-tuned in the forward chain using keyframes from the LocALizER. Table 8 compares fine-tuning in different frame sampling strategies, and indicates SEVILA framework performs optimally when utilizing LocALIzER in both AnswERER training and evaluation. This is likely due to the provision of more informative keyframes and milder domain shifts between the training and evaluation. Upper-bound performance analysis on oracle keyframes. We further explore the upper-bound performance with the assumption of a 'perfect' localizer, capable of always providing the right keyframes for the Answerer. To achieve this, we uniformly sample four frames from each video, input them into BLIP-2 individually, and generate four frame-level answers. The upper-bound performance is represented by the oracle accuracy, which considers a question correctly answered if at least one of the four frames yields the right answer. As shown in Table 9, significant gaps exist between BLIP-2 majority voting and oracle accuracy in both settings. These gaps emphasize the need for more future work in temporal localization to effectively harness image-LM for video-language tasks. Qualitative analysis on LocALIzER. In Fig. 4, we present an example from NExT-QA (more in Appendix), showcasing the QA pairs, our LocALizer results, and the ground truth task-related video clips that we manually annotated. Our LocALizeR more accurately matches human annotations than

<table><tr><td colspan="2">Frame Sampling</td><td colspan="2">NExT-QA</td></tr><tr><td>Training</td><td>Inference</td><td>Temp. Cau.</td><td>Des. Avg.</td></tr><tr><td>Random</td><td>Uniform</td><td>68.1 72.9</td><td>81.2 72.6</td></tr><tr><td>Random</td><td>LoCAlizeR†</td><td>67.6 73.4</td><td>84.0 73.1</td></tr><tr><td>LOCALiZeR</td><td>Uniform</td><td>68.2 72.7</td><td>80.0 72.3</td></tr><tr><td>LocalizeR†</td><td>LOCalizeR</td><td>68.8 73.4</td><td>83.5 73.4</td></tr></table>

<table><tr><td rowspan="2">Datasets</td><td colspan="2">BLIP-2voting (Oracle)</td></tr><tr><td>Zero-Shot</td><td>Fine-tuned</td></tr><tr><td>NExT-QA (Avg.)</td><td>62.7 (70.1)</td><td rowspan="3">70.1 (79.7) 51.8 (72.2)</td></tr><tr><td>STAR (Avg.)</td><td>40.3 (52.9)</td></tr><tr><td>How2QA</td><td>69.8 (77.8) 79.6 (86.4)</td></tr><tr><td>TVQA</td><td>35.7 (45.4)</td><td>54.5 (69.0)</td></tr><tr><td>VLEP</td><td>63.8 (70.5)</td><td>67.0 (79.1)</td></tr></table>

Table 8: Comparing different frame sampling during ANSwERER fine-tuning. The LocALIzER† is frozen during fine-tuning. We use 4 frames for AnswERER training, while the LoCALIZER $^ { \dagger }$ is the default $3 2 {  } 4$ .

Table 9: BLIP. $2 ^ { \mathrm { v o t i n g } }$ and oracle (in brackets) performance analysis across datasets. We use 4 frames for each video question. Oracle: at least 1 of 4 frames can give the right answer.

Question: why did the two ladies put their hands above their eyes while staring out?

![](images/4.jpg)  

Figure 4: Visualization of our LocALIzER. We use zero-shot AnswERER with different frame sampling (uniform v.s. LocALizeR) to answer the question. Red options are answered wrongly with uniformly sampled frames. Green options are answered correctly with our LocALizeR. Best viewed in color.

uniform selection. This accurate localization enables the Answerer to correctly answer questions, while uniform selection leads to incorrect responses. These results demonstrate that our LocALIZER can effectively locate task-related keyframes in a video, thus benefiting downstream tasks.

# 5 Conclusion and Future Work

In this paper, we introduced SEViLA, a novel video-language framework. SEVILA adapts an imagelanguage model to obtain two modules: (1) LocALIzER for language-aware temporal localization and (2) AnswEREr for question answering on keyframes. SEViLA tackles video-language tasks by chaining the output of LocALIZER to the input of AnswERER (forward chain), while AnswERER can give feedback to refine the LocALizeR (backward chain) via pseudo labeling. The proposed temporal localization allows a more focused understanding of videos and improves the accuracy of videolanguage tasks, while the pseudo-labeling process alleviates the need for expensive language-aware keyframe annotations. Compared with state-of-the-art baselines, SEVILA achieves competitive or better performance on five video questions answering and event prediction benchmarks. We also provide a comprehensive analysis elaborating on the design of the proposed two-stage self-chaining. Our research encourages future work to improve temporal localization in video understanding. Limitations & Broader Impacts. See Appendix for limitations and broader impacts discussion.

# Acknowledgement

We thank the reviewers and Archiki Prasad, Hao Tan, Yi-Lin Sung, and Jie Lei for their valuable feedback and input for the paper. This work was supported by ARO Award W911NF2110220, DARPA KAIROS Grant FA8750-19-2-1004, DARPA MCS Grant N66001-19-2-4031, and NSF-AI Engage Institute DRL211263. The views, opinions, and/or findings contained in this article are those of the authors and not of the funding agency.