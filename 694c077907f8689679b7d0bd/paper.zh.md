# FoundationPose：新物体的统一 6D 位姿估计与跟踪

博文 温 韦 杨 詹 劳茨 斯坦 伯奇菲尔德 NVIDIA

# 摘要

我们提出了FoundationPose，一个统一的基础模型，用于6D物体姿态估计和跟踪，支持基于模型和无模型的设置。只要提供物体的CAD模型或捕捉到少量参考图像，我们的方法可以在测试时即时应用于新物体，而无需微调。得益于统一框架，两个设置中下游的姿态估计模块是相同的，在没有CAD模型的情况下，使用神经隐式表示进行高效的新视角合成。通过大规模的合成训练、借助大型语言模型（LLM）、一种新颖的基于变换器的架构和对比学习公式，实现了强大的泛化能力。在多个涉及挑战性场景和物体的公共数据集上的广泛评估表明，我们的统一方法在性能上显著超过了各自任务的现有专用方法。此外，尽管假设条件减少，它的结果甚至可与实例级方法相媲美。项目页面：https://nvlabs.github.io/FoundationPose/

# 1. 引言

计算从物体到相机的刚性 6D 变换，也称为物体姿态估计，对于机器人操控 [30, 69, 70] 和混合现实 [43] 等多种应用至关重要。经典方法 [20, 21, 31, 50, 68] 被称为实例级方法，因为它们仅适用于在训练时确定的特定物体实例。这类方法通常需要纹理化的 CAD 模型来生成训练数据，并且无法在测试时应用于未见过的新物体。而类别级方法 [5, 34, 60, 64, 75] 则去除了这些假设（实例级训练和 CAD 模型），但它们仅限于在预定义类别内训练的物体。此外，获取类别级训练数据 notoriously 困难，部分原因在于必须应用额外的姿态规范化和检验步骤 [64]。为了解决这些局限性，最近的努力集中在任意新物体的瞬时姿态估计问题上 [19, 32, 40, 55, 58]。根据测试时可用的信息，考虑了两种不同的设置：基于模型的，提供物体的纹理化 3D CAD 模型，以及无模型的，提供物体的一组参考图像。尽管在这两种设置上都有很大进展，但仍然需要一种统一的方法来同时解决这两种设置，因为不同的现实应用提供了不同类型的信息。

![](images/1.jpg)  
Figure 1. Our unified framework enables both 6D pose estimation and tracking for novel objects, supporting the model-based and model-free setups. On each of these four tasks, it outperforms prior work specially designed for the task $\left( \bullet \right)$ indicates RGB-only; $\times$ indicates RGBD, like ours). The metric for each task is explained in detail in the experimental results.

与单帧物体姿态估计不同，姿态追踪方法利用时间线索实现视频序列中更高效、平滑和准确的姿态估计。这些方法在假设对象知识方面与其姿态估计对应方法存在类似问题。在本文中，我们提出了一个称为FoundationPose的统一框架，使用RGBD图像在基于模型和无模型设置中同时进行新对象的姿态估计和追踪。如图1所示，我们的方法在这四项任务中超越了现有的最先进的专门方法。我们的强泛化能力通过大规模合成训练实现，辅以大型语言模型（LLM）、新颖的基于变换器的架构和对比学习。我们通过神经隐式表示弥合了基于模型和无模型设置之间的差距，允许通过少量（约16张）参考图像有效合成新视角，呈现速度显著快于之前的渲染与比较方法。我们的贡献可总结如下：• 我们提出了一个用于新对象的姿态估计和追踪的统一框架，支持基于模型和无模型设置。针对有效新视角合成的以对象为中心的神经隐式表示弥合了这两种设置之间的差距。我们提出了一种LLM辅助的合成数据生成管道，通过多样化的纹理增强扩大了3D训练资源的多样性。我们新颖的基于变换器的网络架构设计和对比学习公式实现了在仅以合成数据进行训练时的强泛化。我们的方法在多个公共数据集上大幅超越了现有的针对各任务专门化的方法。尽管假设减少，它甚至取得了与实例级方法相当的结果。我们将在此项工作中开发的代码和数据将会发布。

# 2. 相关工作

基于CAD模型的物体姿态估计。实例级姿态估计方法 [20, 21, 31, 50] 假设对象已给定纹理化的CAD模型。训练和测试是在完全相同的实例上进行的。物体姿态通常通过直接回归 [37, 73] 来解决，或者通过构建2D-3D对应关系后应用PnP [50, 61]，或者通过3D-3D对应关系后进行最小二乘拟合 [20, 21]。为了放宽对物体知识的假设，可以将类别级方法 [5, 34, 60, 64, 75, 77] 应用于同一类别的新物体实例，但它们无法推广到超出预定义类别的任意新物体。为了解决这一限制，最近的努力 [32, 55] 旨在实现对任意新物体的即时姿态估计，只要在测试时提供CAD模型。

少样本无模型物体姿态估计。无模型方法消除了对显式纹理模型的要求。相反，提供了一些捕捉目标物体的参考图像 [19, 22, 51, 58]。RLLG [3] 和 NeRF-Pose [35] 提出了无物体CAD模型的实例级训练。特别地，[35] 构建了神经辐射场，以对物体坐标图和掩码提供半监督。与此不同的是，我们引入了基于SDF表示构建的神经物体场，以高效渲染RGB和深度，从而弥合基于模型和无模型场景之间的差距。此外，我们在本文中专注于可泛化的新物体姿态估计，这与 [3, 35] 并不相同。为处理新物体，Gen6D [40] 设计了一个检测、检索和精细化管道。然而，为了避免在分布外测试集上遇到困难，它需要微调。OnePose [58] 及其扩展 $\mathrm{O n e P o s e + +}$ [19] 利用结构光流（SfM）进行物体建模，并预训练2D-3D匹配网络以通过对应关系解决姿态问题。FS6D [22] 采用类似方案，并专注于RGBD模态。然而，在应用于无纹理物体或严重遮挡时，依赖于对应关系变得脆弱。物体姿态跟踪。6D物体姿态跟踪旨在利用时间线索，使在视频序列上进行更高效、平滑和准确的姿态预测成为可能。通过神经渲染，我们的方法可以以高效率轻松扩展到姿态跟踪任务。与单帧姿态估计类似，现有跟踪方法可以根据对象知识的假设分为其对应类别。这些包括实例级方法 [8, 11, 36, 67]、类别级方法 [39, 63]、基于模型的新物体跟踪 [29, 56, 72] 和无模型的新物体跟踪 [66, 71]。在基于模型和无模型的设置下，我们在公共数据集上创下了新的基准记录，甚至超越了需要实例级训练的最先进方法 [8, 36, 67]。

# 3. 方法

如图2所示，我们的系统整体结构展示了各个组件之间的关系，具体内容将在以下小节中进行描述。

# 3.1. 语言辅助的大规模数据生成

为了实现强大的泛化能力，需要多样化的物体和场景用于训练。在现实世界中获取这样的数据，并标注准确的真实6D姿态，既耗时又成本高昂。另一方面，合成数据通常在3D资产的规模和多样性上不足。我们开发了一种新颖的合成数据生成管道用于训练，该管道利用了近期新兴的资源和技术：大规模3D模型数据库[6, 10]、大型语言模型（LLM）和扩散模型[4, 24, 53]。与先前的工作[22, 26, 32]相比，这种方法显著提高了数据的数量和多样性。3D资产。我们从最近的大规模3D数据库中获取训练资产，包括Objaverse [6]和GSO [10]。对于Objaverse [6]，我们选择了来自ObjaverseLVIS子集的物体，该子集包含超过40K的物体，属于1156个LVIS [13]类别。该列表包含了最相关的日常生活物品，具有合理的质量以及形状和外观的多样性。它还为每个物体提供了描述其类别的标签，这有助于在后续的LLM辅助纹理增强步骤中自动生成语言提示。

![](images/2.jpg)  
y

![](images/3.jpg)  
Figure 3. Top: Random texture blending proposed in FS6D [22]. Bottom: Our LLM-aided texture augmentation yields more realistic appearance. Leftmost is the original 3D assets. Text prompts are automatically generated by ChatGPT.

LLM辅助的纹理增强。尽管大多数Objaverse物体具有高质量的形状，其纹理保真度却存在显著差异。FS6D提出通过随机粘贴来自ImageNet或MS-COCO的图像来增强物体纹理。然而，由于随机UV映射，该方法会在生成的纹理网格上产生缝合等伪影（图3顶部）；将整体场景图像应用于物体也会导致不现实的结果。相较之下，我们探索了如何利用最近大语言模型和扩散模型的进展来实现更逼真（且完全自动化的）纹理增强。具体来说，我们向TexFusion提供文本提示、物体形状和随机初始化的噪声纹理，以生成增强的纹理模型。当然，如果我们希望在不同的提示指导下增强大量风格多样的物体，手动提供这样的提示是不可扩展的。因此，我们引入了两级层次提示策略。如图2左上方所示，我们首先提示ChatGPT，让它描述物体的可能外观；这个提示是模板化的，以便我们每次只需替换与物体配对的标签，该标签由Objaverse-LVIS列表提供。ChatGPT的回答随后成为提供给扩散模型用于纹理合成的文本提示。由于这种方法实现了纹理增强的完全自动化，因此促进了规模化的多样化数据生成。图3展示了更多例子，包括同一物体的不同风格化。数据生成。我们的合成数据生成在NVIDIA Isaac Sim中实现，利用路径追踪进行高保真照片现实渲染。我们进行重力和物理模拟，以生成物理可信的场景。在每个场景中，我们随机采样包括原始版本和纹理增强版本的物体。物体的大小、材料、相机姿势和光照也会被随机化；更多细节可在附录中找到。

# 3.2. 神经物体建模

在无模型设置中，当3D CAD模型不可用时，一个关键挑战是如何表示对象，以有效渲染出具有足够质量的图像，以供后续模块使用。神经隐式表示在新视角合成方面有效，并且可以在GPU上并行处理，从而在为后续姿态估计模块渲染多个姿态假设时提供高计算效率，如图2所示。为此，我们引入了一种以对象为中心的神经场表示，用于对象建模，该方法受到之前工作的启发。

场表示。我们通过两个函数来表示物体，如图 2 所示。首先，几何函数 $\Omega : x \mapsto s$ 的输入为一个 3D 点 $x \in \mathbb { R } ^ { 3 }$，输出为符号距离值 $s \in \mathbb R$。其次，外观函数 $\Phi : ( f _ { \Omega ( x ) } , n , d ) \mapsto c$ 从几何网络中获取中间特征向量 $f _ { \Omega ( x ) }$，输入一个点法向量 $n \in \mathbb { R } ^ { 3 }$ 和一个视角方向 $d \in \mathbb { R } ^ { 3 }$，最终输出颜色 $c \in \mathbb { R } _ { + } ^ { 3 }$。在实践中，我们在将 $x$ 传递给网络之前应用多分辨率哈希编码 [45]。$n$ 和 $d$ 都由一组固定的二阶球谐系数进行嵌入。隐式物体表面通过取符号距离场 (SDF) 的零级集合获得：$S = \left\{ x \in \mathbb { R } ^ { 3 } \mid \Omega ( x ) = 0 \right\}$。与 NeRF [44] 相比，SDF 表示法 $\Omega$ 提供了更高质量的深度渲染，同时消除了手动选择密度阈值的需求。场学习。对于纹理学习，我们遵循对截断近表面区域的体积渲染 [71]：

$$
c ( r ) = \int _ { z ( r ) - \lambda } ^ { z ( r ) + 0 . 5 \lambda } w ( x _ { i } ) \Phi ( f _ { \Omega ( x _ { i } ) } , n ( x _ { i } ) , d ( x _ { i } ) ) d t ,
$$

$$
w ( x _ { i } ) = \frac { 1 } { 1 + e ^ { - \alpha \Omega ( x _ { i } ) } } \frac { 1 } { 1 + e ^ { \alpha \Omega ( x _ { i } ) } } ,
$$

其中 $w ( x _ { i } )$ 是钟形概率密度函数 [65]，该函数依赖于点到隐式物体表面的符号距离 $\Omega ( x _ { i } )$，而 $\alpha$ 调整分布的柔软度。概率在表面交点处达到峰值。在公式（1）中，$z ( r )$ 是深度图中光线的深度值，$\lambda$ 是截断距离。为了更有效的训练，我们忽略距离表面超过 $\lambda$ 的空白空间的贡献，并且仅集成到 $0.5 \lambda$ 的渗透距离以建模自遮挡 [65]。在训练过程中，我们将该量与参考 RGB 图像进行比较，以进行颜色监督：

$$
\mathcal { L } _ { c } = \frac { 1 } { \vert \mathcal { R } \vert } \sum _ { r \in \mathcal { R } } \vert \vert c ( r ) - \bar { c } ( r ) \vert \vert _ { 2 } ,
$$

其中 $\bar { c } ( r )$ 表示光线 $r$ 经过的像素处的真实颜色。对于几何学习，我们采用混合 SDF 模型 [71]，通过将空间划分为两个区域来学习 SDF，从而引入了空域损失和近表面损失。我们还对近表面 SDF 应用 eikonal 正则化 [12]：

$$
\begin{array} { l } { \displaystyle \mathcal { L } _ { e } = \frac { 1 } { \left| \mathcal { X } _ { e } \right| } \sum _ { x \in \mathcal { X } _ { e } } \left| \Omega ( x ) - \lambda \right| , } \\ { \displaystyle \mathcal { L } _ { s } = \frac { 1 } { \left| \mathcal { X } _ { s } \right| } \sum _ { x \in \mathcal { X } _ { s } } \left( \Omega ( x ) + d _ { x } - d _ { D } \right) ^ { 2 } , } \\ { \displaystyle \mathcal { L } _ { e i k } = \frac { 1 } { \left| \mathcal { X } _ { s } \right| } \sum _ { x \in \mathcal { X } _ { s } } \left( \left\| \nabla \Omega ( x ) \right\| _ { 2 } - 1 \right) ^ { 2 } , } \end{array}
$$

其中 $x$ 表示在分割空间内光线沿线采样的 3D 点；$d_{x}$ 和 $d_{D}$ 分别是从光线起源到采样点和观察深度点的距离。我们不使用不确定自由空间损失 [71]，因为模板图像是在无模型设置下离线预先捕获的。总训练损失为

$$
\mathcal { L } = w _ { c } \mathcal { L } _ { c } + w _ { e } \mathcal { L } _ { e } + w _ { s } \mathcal { L } _ { s } + w _ { e i k } \mathcal { L } _ { e i k } .
$$

学习是针对每个对象优化的，无需先验，并且可以在几秒钟内高效执行。神经场只需针对新对象训练一次。渲染。一旦训练完成，神经场可以作为常规图形管道的直接替代，用于有效渲染对象以进行后续的渲染与比较迭代。除了原始NeRF [44]中的颜色渲染外，我们还需要深度渲染以支持基于RGBD的姿态估计和跟踪。为此，我们执行marching cubes [41]从SDF的零水平集提取纹理网格，结合颜色投影。每个对象只需执行一次。在推理阶段，给定一个对象的姿态，我们按照光栅化过程渲染RGBD图像。或者，可以使用$\Omega$在线直接渲染深度图像，采用球体追踪 [14]；然而，我们发现这会导致效率降低，特别是在需要并行渲染大量姿态假设时。

# 3.3. 姿态假设生成

姿态初始化。给定RGBD图像，使用现成的方法（如Mask RCNN [18]或CNOS [47]）检测物体。我们通过检测到的二维边界框内位于中位深度的三维点来初始化平移。为了初始化旋转，我们从一个以物体为中心、相机面向中心的六面体中均匀采样$N_{s}$个视点。这些相机姿态进一步增强为$N_{i}$个离散的平面内旋转，从而生成$N_{s} \cdot N_{i}$个全局姿态初始化，作为输入发送给姿态精炼器。

姿态优化。由于前一步的粗略姿态初始化通常带有较大的噪声，因此需要一个优化模块来提升姿态质量。具体而言，我们构建了一个姿态优化网络，它以基于粗略姿态渲染的物体图像和来自摄像头的输入观测裁剪图作为输入; 网络输出一个姿态更新，以提升姿态质量。与MegaPose [32]不同，该方法需要围绕粗略姿态渲染多个视角以寻找锚点，我们观察到仅渲染与粗略姿态对应的单一视角就足够了。对于输入观测，我们并不是基于固定的二维检测进行裁剪，而是采用姿态条件裁剪策略，以便为位移更新提供反馈。具体而言，我们将物体原点投影到图像空间来确定裁剪中心。然后，我们将稍微放大的物体直径（物体表面上任何一对点之间的最大距离）投影以确定裁剪大小，从而包围物体及其姿态假设周围的相关背景。因此，该裁剪基于粗略姿态，并鼓励网络更新位移，使裁剪更好地与观测对齐。优化过程可以通过将最新更新的姿态作为输入反复进行多次推理，以迭代提高姿态质量。

细化网络架构如图2所示，详细信息见附录。我们首先通过一个共享的CNN编码器从两个RGBD输入分支提取特征图。特征图被连接后，输入具有残差连接的CNN块中，并通过位置嵌入将其分割为补丁进行标记化。最后，网络预测平移更新$\Delta \pm \mathbb { R } ^ { 3 }$和旋转更新$\Delta R \in \mathbb { S O } ( 3 )$，每个更新由一个Transformer编码器单独处理，并线性投影到输出维度。更具体地说，$\Delta t$表示在相机坐标系中物体的平移位移，$\Delta R$表示物体在相机坐标系中表示的朝向更新。在实际应用中，旋转以轴角表示法进行参数化。我们还尝试了6D表示法，该方法达到了类似的效果。输入的粗略位姿$[ R | t ] \in \mathbb { S E } ( 3 )$随后被更新，其中$\otimes$表示对$\mathbb { S O } ( 3 )$的更新。此解耦表示法不同于使用单一均质位姿更新，它在应用平移更新时消除了对更新朝向的依赖。这将平移更新和输入观察统一在相机坐标系中，从而简化了学习过程。网络训练通过$L _ { 2 }$损失进行监督：

![](images/4.jpg)  
Figure 4. Pose ranking visualization. Our proposed hierarchical comparison leverages the global context among all pose hypotheses for a better overall trend prediction that aligns both shape and texture. The true best pose is annotated with red circle.

$$
\begin{array} { c } { { t ^ { + } = t + \Delta t } } \\ { { R ^ { + } = \Delta R \otimes R , } } \end{array}
$$

$$
\mathcal { L } _ { \mathrm { r e f i n e } } = w _ { 1 } \left\| \Delta t - \Delta \bar { t } \right\| _ { 2 } + w _ { 2 } \left\| \Delta R - \Delta \bar { R } \right\| _ { 2 } ,
$$

其中 $\bar { \pmb { t } }$ 和 $\bar { \pmb R }$ 是真实标注数据；$w _ { 1 }$ 和 $w _ { 2 }$ 是平衡损失的权重，经验上设定为 1。

# 3.4. 姿势选择

给定一组经过精炼的姿态假设，我们使用分层姿态排名网络来计算它们的得分。得分最高的姿态被选为最终估计。

层次比较。该网络采用两级比较策略。首先，对于每个姿态假设，渲染图像与裁剪后的输入观测进行比较，这使用了第3.3节介绍的基于姿态的裁剪操作。此比较（图2左下角）通过姿态排序编码器执行，利用与细化网络相同的主干架构进行特征提取。提取的特征被连接、标记化并传递到多头自注意力模块，以更好地利用全球图像上下文进行比较。姿态排序编码器执行平均池化，输出特征嵌入 $\mathcal { F } \in \mathbb { R } ^ { 5 1 2 }$，描述渲染与观测之间的对齐质量（图2中下部分）。在这一点上，我们可以像通常所做的那样直接将 $\mathcal { F }$ 投影到相似度标量上 [2, 32, 46]。然而，这将忽略其他姿态假设，迫使网络输出绝对评分分配，而这可能难以学习。

为了利用所有姿态假设的全局上下文以做出更为明智的决策，我们引入第二层对所有 $K$ 姿态假设的比较。通过对串联的特征嵌入 $\textbf{F} = [\mathcal{F}_{0}, \dotsc, \mathcal{F}_{K-1}]^{\top} \in \mathbb{R}^{K \times 512}$ 进行多头自注意力操作，该特征嵌入编码了来自所有姿态的姿态对齐信息。通过将 $\mathbf{F}$ 视为一个序列，这种方法自然扩展到不同长度的 $K$ [62]。我们不对 $\mathbf{F}$ 应用位置编码，以便对排列不敏感。然后，将注意到的特征线性投影到得分 $\mathbf{S} \in \mathbb{R}^{K}$，以分配给姿态假设。图 4 中展示了这种层次比较策略的有效性。对比验证。为了训练姿态排序网络，我们提出了一种姿态条件的三元组损失：

$$
\begin{array} { r } { \begin{array} { r } { \mathcal { L } ( i ^ { + } , i ^ { - } ) = \operatorname* { m a x } ( { \mathbf { S } } ( i ^ { - } ) - { \mathbf { S } } ( i ^ { + } ) + \alpha , 0 ) , } \end{array} } \end{array}
$$

其中 $\alpha$ 表示对比边界；$i ^ { - }$ 和 $i ^ { + }$ 分别表示负样本和正样本，这些样本是通过使用真实标注数据计算 ADD 指标 [73] 确定的。注意，与标准三元组损失 [27] 不同的是，在我们的案例中，锚点样本并不是在正样本和负样本之间共享的，因为输入是根据每个姿态假设进行裁剪的，以考虑平移。虽然我们可以对列表中的每一对进行损失计算，但当两个姿态远离真实标注时，比较变得模糊。因此，我们仅保留那些正样本来自于与真实标注足够接近的视角的姿态对，以使比较有意义：

$$
\mathbb { V } ^ { + } = \{ i : D ( R _ { i } , \bar { R } ) < d \}
$$

$$
\mathbb { V } ^ { - } = \{ 0 , 1 , 2 , \ldots , K - 1 \}
$$

$$
\mathcal { L } _ { \mathrm { r a n k } } = \sum _ { i ^ { + } , i ^ { - } } \mathcal { L } ( i ^ { + } , i ^ { - } )
$$

其中求和是在 $i^{+} \in \mathbb{V}^{+}$ 和 $i^{-} \in \mathbb{V}^{-}$ 上进行，且 $i^{+} \neq i^{-}$；$R_{i}$ 和 $\bar{\pmb{R}}$ 分别为假设和真实标注的旋转；$D(\cdot)$ 表示旋转之间的测地距离；$d$ 为预定义的阈值。我们还实验了在 [46] 中使用的 InfoNCE 损失 [49]，但观察到性能较差（见第 4.5 节）。我们将此归因于 [46] 中所做的完美平移假设，而在我们的设置中并不成立。

# 4. 实验

# 4.1. 数据集与设置

我们考虑了5个数据集：LINEMOD、OccludedLINEMOD、YCB-Video、T-LESS和YCBInEOAT。这些数据集涉及各种具有挑战性的场景（密集杂乱、多实例、静态或动态场景、桌面或机器人操作），以及具有多样特性的物体（无纹理、光亮、对称、大小不同）。由于我们的框架是统一的，我们考虑两种设置（无模型和基于模型）和两种姿态预测任务（6D姿态估计和跟踪）之间的组合，共产生4个任务。对于无模型设置，从数据集的训练分割中选择若干捕捉新物体的参考图像，并配备物体姿态的真实标注，遵循文献[22]。对于基于模型的设置，提供了新物体的CAD模型。在所有评估中，除了消融实验，我们的方法在推断时始终使用相同的训练模型和配置，而无需进行微调。

Table 1. Model-free pose estimation results measured by AUC of ADD and ADD-S on YCB-Video dataset. "Finetuned" means the method was fine-tuned with group split of object instances on the testing dataset, as introduced by [22].   

<table><tr><td></td><td colspan="2">PREDATOR [28]</td><td colspan="2">LoFTR [57]</td><td colspan="2">FS6D-DPM [22]</td><td colspan="2">Ours</td></tr><tr><td>Ref. images</td><td colspan="2">16</td><td colspan="2">16</td><td colspan="2">16</td><td colspan="2">16</td></tr><tr><td>Finetune-free Metrics</td><td colspan="2"></td><td colspan="2">✓</td><td colspan="2">X</td><td colspan="2">√</td></tr><tr><td></td><td>ADD-S</td><td>ADD</td><td>ADD-S</td><td>ADD</td><td>ADD-S</td><td>ADD</td><td>ADD-S</td><td>ADD</td></tr><tr><td>002_master_chef_can</td><td>73.0</td><td>17.4</td><td>87.2</td><td>50.6</td><td>92.6</td><td>36.8</td><td>96.9</td><td>91.3</td></tr><tr><td>003_cracker_box</td><td>41.7</td><td>8.3</td><td>71.8</td><td>25.5</td><td>83.9</td><td>24.5</td><td>97.5</td><td>96.2</td></tr><tr><td>004_sugar_box</td><td>53.7</td><td>15.3</td><td>63.9</td><td>13.4</td><td>95.1</td><td>43.9</td><td>97.5</td><td>87.2</td></tr><tr><td>005_tomato_soup_can</td><td>81.2</td><td>44.4</td><td>77.1</td><td>52.9</td><td>93.0</td><td>54.2</td><td>97.6</td><td>93.3</td></tr><tr><td>006_mustard_bottle</td><td>35.5</td><td>5.0</td><td>84.5</td><td>59.0</td><td>97.0</td><td>71.1</td><td>98.4</td><td>97.3</td></tr><tr><td>007_tuna_fish_can</td><td>78.2</td><td>34.2</td><td>72.6</td><td>55.7</td><td>94.5</td><td>53.9</td><td>97.7</td><td>73.7</td></tr><tr><td>008_pudding_box</td><td>73.5</td><td>24.2</td><td>86.5</td><td>68.1</td><td>94.9</td><td>79.6</td><td>98.5</td><td>97.0</td></tr><tr><td>009_gelatin_box</td><td>81.4</td><td>37.5</td><td>71.6</td><td>45.2</td><td>98.3</td><td>32.1</td><td>98.5</td><td>97.3</td></tr><tr><td>010_potted_meat_can</td><td>62.0</td><td>20.9</td><td>67.4</td><td>45.1</td><td>87.6</td><td>54.9</td><td>96.6</td><td>82.3</td></tr><tr><td>011_banana</td><td>57.7</td><td>9.9</td><td>24.2</td><td>1.6</td><td>94.0</td><td>69.1</td><td>98.1</td><td>95.4</td></tr><tr><td>019_pitcher_base</td><td>83.7</td><td>18.1</td><td>58.7</td><td>22.3</td><td>91.1</td><td>40.4</td><td>97.9</td><td>96.6</td></tr><tr><td>021_bleach_cleanser</td><td>88.3</td><td>48.1</td><td>36.9</td><td>16.7</td><td>89.4</td><td>44.1</td><td>97.4</td><td>93.3</td></tr><tr><td>024_bowl</td><td>73.2</td><td>17.4</td><td>32.7</td><td>1.4</td><td>74.7</td><td>0.9</td><td>94.9</td><td>89.7</td></tr><tr><td>025_mug</td><td>84.8</td><td>29.5</td><td>47.3</td><td>23.6</td><td>86.5</td><td>39.2</td><td>96.2</td><td>75.8</td></tr><tr><td>035_power_drill</td><td>60.6</td><td>12.3</td><td>18.8</td><td>1.3</td><td>73.0</td><td>19.8</td><td>98.0</td><td>96.3</td></tr><tr><td>036_wood_block</td><td>70.5</td><td>10.0</td><td>49.9</td><td>1.4</td><td>94.7</td><td>27.9</td><td>97.4</td><td>94.7</td></tr><tr><td>037_scissors</td><td>75.5</td><td>25.0</td><td>32.3</td><td>14.6</td><td>74.2</td><td>27.7</td><td>97.8</td><td>95.5</td></tr><tr><td>040_large_marker</td><td>81.8</td><td>38.9</td><td>20.7</td><td>8.4</td><td>97.4</td><td>74.2</td><td>98.6</td><td>96.5</td></tr><tr><td>051_large_clamp</td><td>83.0</td><td>34.4</td><td>24.1</td><td>11.2</td><td>82.7</td><td>34.7</td><td>96.9</td><td>92.7</td></tr><tr><td>052_extra_large_clamp</td><td>72.9</td><td>24.1</td><td>15.0</td><td>1.8</td><td>65.7</td><td>10.1</td><td>97.6</td><td>94.1</td></tr><tr><td>061_foam_brick</td><td>79.2</td><td>35.5</td><td>59.4</td><td>31.4</td><td>95.7</td><td>45.8</td><td>98.1</td><td>93.4</td></tr><tr><td>MEAN</td><td>71.0</td><td>24.3</td><td>52.5</td><td>26.2</td><td>88.4</td><td>42.1</td><td>97.4</td><td>91.5</td></tr></table>

# 4.2. 指标

为了紧密遵循每个设置的基线协议，我们考虑以下指标：$\bullet$ ADD 和 ADD-S 的曲线下面积（AUC）[73]。对象直径小于 0.1 的 ADD 召回率（ADD-0.1d），如 [19, 22] 中所使用的。BOP 挑战中引入的 VSD、MSSD 和 MSPD 指标的平均召回率（AR）[26]。

# 4.3. 姿态估计比较

无模型。表 1 展示了在 YCB-Video 数据集上与最先进的 RGBD 方法 [22, 28, 57] 的比较结果。基线结果来自 [22]。根据 [22] 的方法，所有方法都使用扰动后的真实边界框作为 2D 检测，以确保公平比较。表 2 展示了在 LINEMOD 数据集上的比较结果。基线结果引用自 [19, 22]。基于 RGB 的方法 [19, 40, 58] 被给予了更多的参考图像以补偿深度的不足。在 RGBD 方法中，FS6D [22] 需要在目标数据集上进行微调。我们的方法在两个数据集上显著优于现有方法，且无需在目标数据集上进行微调或 ICP 精化。图 5 可视化了定性比较。由于 FS6D [22] 的代码未公开发布，我们无法获得其姿态预测以进行定性结果比较。严重的自遮挡和胶水上缺乏纹理对 $\mathrm { O n e P o s e + + }$ [19] 和 LatentFusion [51] 提出了很大挑战，而我们的方法成功地估计了姿态。

<table><tr><td rowspan="2">Method</td><td rowspan="2">Modality</td><td rowspan="2">Finetune- free</td><td rowspan="2">Ref. images</td><td rowspan="2">benchwise ape</td><td colspan="10">cam</td><td rowspan="2"></td><td rowspan="2"></td><td rowspan="2">phone</td><td rowspan="2">Avg.</td></tr><tr><td></td><td></td><td></td><td>can</td><td>driller</td><td>duck</td><td>Objects eggbox</td><td>glue holepuncher</td><td>iron</td><td>lamp</td></tr><tr><td>Gen6D [40]</td><td>RGB</td><td>×</td><td>200</td><td>-</td><td>77</td><td>66.1</td><td>-</td><td>60.7</td><td>67.4</td><td>40.5</td><td>95.7</td><td>87.2</td><td>-</td><td>-</td><td>-</td><td></td><td></td></tr><tr><td>Gen6D* [40]</td><td>RGB</td><td>✓</td><td>200</td><td>-</td><td>62.1</td><td>45.6</td><td>-</td><td>40.9</td><td>48.8</td><td>16.2</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>- -</td><td>:</td></tr><tr><td>OnePose [58]</td><td>RGB</td><td></td><td>200</td><td>11.8</td><td>92.6</td><td>88.1</td><td>77.2</td><td>47.9</td><td>74.5</td><td>34.2</td><td>71.3</td><td>37.5</td><td>54.9</td><td>89.2</td><td>87.6</td><td>60.6</td><td>63.6</td></tr><tr><td>OnePose++ [19]</td><td>RGB</td><td></td><td>200</td><td>31.2</td><td>97.3</td><td>88.0</td><td>89.8</td><td>70.4</td><td>92.5</td><td>42.3</td><td>99.7</td><td>48.0</td><td>69.7</td><td>97.4</td><td>97.8</td><td>76.0</td><td>76.9</td></tr><tr><td>LatentFusion [51]</td><td>RGBD</td><td>✓</td><td>16</td><td>88.0</td><td>92.4</td><td>74.4</td><td>88.8</td><td>94.5</td><td>91.7</td><td>68.1</td><td>96.3</td><td>94.9</td><td>82.1</td><td>74.6</td><td>94.7</td><td>91.5</td><td>87.1</td></tr><tr><td>FS6D [22]</td><td>RGBD</td><td>X</td><td>16</td><td>74.0</td><td>86.0</td><td>88.5</td><td>86.0</td><td>98.5</td><td>81.0</td><td>68.5</td><td>100.0</td><td>99.5</td><td>97.0</td><td>92.5</td><td>85.0</td><td>99.0</td><td>88.9</td></tr><tr><td>FS6D [22] + ICP</td><td>RGBD</td><td>X</td><td>16</td><td>78.0</td><td>88.5</td><td>91.0</td><td>89.5</td><td>97.5</td><td>92.0</td><td>75.5</td><td>99.5</td><td>99.5</td><td>96.0</td><td>87.5</td><td>97.0</td><td>97.5</td><td>91.5</td></tr><tr><td>Ours</td><td>RGBD</td><td>√</td><td>16</td><td>99.0</td><td>100.0</td><td>100.0</td><td>100.0</td><td>100.0</td><td>100.0</td><td>99.4</td><td>100.0</td><td>100.0</td><td>99.9</td><td>100.0</td><td>100.0</td><td>100.0</td><td>99.9</td></tr></table>

![](images/5.jpg)  
Figure 5. Qualitative comparison of pose estimation on LINEMOD dataset under the model-free setup. Images are cropped and zoomed-in for better visualization.

<table><tr><td>Method</td><td>Unseen objects</td><td>LM-O</td><td>Dataset T-LESS</td><td>YCB-V</td><td>Mean</td></tr><tr><td>SurfEmb [15] + ICP</td><td>×</td><td>75.8</td><td>82.8</td><td>80.6</td><td>79.7</td></tr><tr><td>OSOP [55] + ICP</td><td>✓</td><td>48.2</td><td>-</td><td>57.2</td><td>-</td></tr><tr><td>(PPF, Sift) + Zephyr [48]</td><td>✓</td><td>59.8</td><td>-</td><td>51.6</td><td>-</td></tr><tr><td>MegaPose-RGBD [32]</td><td>✓</td><td>58.3</td><td>54.3</td><td>63.3</td><td>58.6</td></tr><tr><td>OVE6D [2]</td><td></td><td>49.6</td><td>52.3</td><td>-</td><td>-</td></tr><tr><td>GCPose [76]</td><td></td><td>65.2</td><td>67.9</td><td>-</td><td>-</td></tr><tr><td>Ours</td><td>√</td><td>78.8</td><td>83.0</td><td>88.0</td><td>83.3</td></tr></table>

Table 3. Model-based pose estimation results measured by AR score on representative BOP datasets. All methods use the RGBD modality.

基于模型的方法。表3展示了在BOP的三个核心数据集上RGBD方法的比较结果：Occluded-LINEMOD [1]、YCB-Video [73] 和 TLESS [25]。所有方法都使用了Mask R-CNN [18]进行二维检测。我们的算法在处理新型物体方面，明显优于现有的基于模型的方法以及实例级方法 [15]。

# 4.4. 姿态跟踪比较

Table 4. Pose tracking results of RGBD methods measured by AUC of ADD and ADD-S on YCBInEOAT dataset. Ours‡ represents our unified pipeline that uses the pose estimation module for pose initialization.   

<table><tr><td colspan="2"></td><td>se(3)- TrackNet [67]</td><td>RGF [29]</td><td>Bundle- Track [66]</td><td>Bundle- SDF [71]</td><td>Wüthrich [72]</td><td>Ours</td><td>Ours†</td></tr><tr><td rowspan="2">Properties</td><td>Novel object Initial pose</td><td>×</td><td>✓ GT</td><td>✓</td><td>✓</td><td>✓</td><td>√</td><td>√</td></tr><tr><td></td><td>GT</td><td></td><td>GT</td><td>GT</td><td>GT</td><td>GT</td><td>Est.</td></tr><tr><td rowspan="2">cracker_box</td><td>ADD-S</td><td>94.06</td><td>55.44</td><td>89.41</td><td>90.63</td><td>88.13</td><td>95.10</td><td>94.92</td></tr><tr><td>ADD</td><td>90.76</td><td>34.78</td><td>85.07</td><td>85.37</td><td>79.00</td><td>91.32</td><td>91.54</td></tr><tr><td rowspan="2">bleach_cleanser</td><td>ADD-S</td><td>94.44</td><td>45.03</td><td>94.72</td><td>94.28</td><td>68.96</td><td>95.96</td><td>96.36</td></tr><tr><td>ADD ADD-S</td><td>89.58 94.80</td><td>29.40 16.87</td><td>89.34 90.22</td><td>87.46</td><td>61.47</td><td>91.45 96.67</td><td>92.63 96.61</td></tr><tr><td rowspan="2">sugar_box</td><td>ADD</td><td>92.43</td><td>15.82</td><td>85.56</td><td>93.81 88.62</td><td>92.75 86.78</td><td>94.14</td><td>93.96</td></tr><tr><td>ADD-S</td><td>96.95</td><td>26.44</td><td>95.13</td><td>95.24</td><td>93.17</td><td>96.58</td><td>96.54</td></tr><tr><td rowspan="2">tomato_soup_can</td><td>ADD</td><td>93.40</td><td>15.13</td><td>86.00</td><td>83.10</td><td>63.71</td><td>91.71</td><td>91.85</td></tr><tr><td>ADD-S</td><td>97.92</td><td>60.17</td><td>95.35</td><td>95.75</td><td>95.31</td><td>97.89</td><td>97.77</td></tr><tr><td rowspan="2">mustard_bottle</td><td>ADD</td><td>97.00</td><td>56.49</td><td>92.26</td><td>89.87</td><td>91.31</td><td>96.34</td><td>95.95</td></tr><tr><td></td><td></td><td>39.90</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td rowspan="2">All</td><td>ADD-S</td><td>95.53</td><td></td><td>92.53</td><td>93.77</td><td>89.18</td><td>96.42</td><td>96.40</td></tr><tr><td>ADD</td><td>92.66</td><td>29.98</td><td>87.34</td><td>86.95</td><td>78.28</td><td>93.09</td><td>93.22</td></tr></table>

除非另有说明，评估方法在跟踪丢失的情况下不进行重新初始化，以评估长期跟踪的鲁棒性。有关定性结果，请参阅我们的补充材料。对于突发的面外旋转、动态外部遮挡和解耦相机运动的挑战进行全面比较，我们在 YCBInEOAT [67] 数据集上评估姿态跟踪方法，该数据集包含动态机器人操作的视频。基于模型的设置下的结果见表 4。我们的方法取得了最佳性能，甚至超越了带有真实姿态初始化的实例级训练方法 [67]。此外，我们统一的框架还允许进行端到端的姿态估计和跟踪，无需外部姿态初始化，这是唯一具有这种能力的方法，在表中标记为 $O u r s ^ { \dagger }$。表 5 展示了在 YCB-Video [73] 数据集上姿态跟踪的比较结果。在基准方法中，DeepIM [36]、se(3)-TrackNet [67] 和 PoseRBPF [8] 需要对相同的对象实例进行训练，而 Wüthrich 等 [72]、RGF [29]、ICG [56] 以及我们的方法可在提供 CAD 模型时即时应用于新对象。

# 4.5. 分析

消融研究。表6展示了关键设计选择的消融研究。结果通过YCB-Video数据集中ADD和ADD-S指标的AUC进行评估。我们的（提出的）方案是在无模型（16个参考图像）设置下的默认版本。去掉LLM纹理增强则去除了用于合成训练的LLM辅助纹理增强。在去掉变压器的情况下，我们用卷积层和线性层替代了基于变压器的架构，同时保持参数数量相似。在去掉层次比较的情况下，仅比较使用姿态条件三元组损失（公式11）训练的渲染与裁剪输入，而不进行两级层次比较。在测试时，它独立地将每个姿态假设与输入观察进行比较，并输出得分最高的姿态。示例定性结果见图4。我们的-InfoNCE用InfoNCE损失替代了对比验证的成对损失（公式14），正如[46]中所使用的。参考图像数量的影响。我们研究了参考图像数量如何影响在YCB-Video数据集中通过ADD和ADD-S的AUC测量的结果，如图6所示。总体而言，我们的方法对参考图像数量具有鲁棒性，尤其在ADD-S指标上，两个指标在12个图像时已达到饱和。值得注意的是，即使只提供4个参考图像，我们的方法仍然比装备有16个参考图像的FS6D [22]表现更强（表1）。

<table><tr><td>Approach</td><td colspan="2">DeeplM [36]</td><td colspan="2">se(3)-TrackNet [67]</td><td colspan="2">PoseRBPF [8] Wüthrich [72] +SDF</td><td colspan="2">RGF [29]</td><td colspan="2">ICG [56]</td><td colspan="2">Ours</td><td colspan="2">Ours†</td></tr><tr><td>Initial pose</td><td colspan="2">GT</td><td colspan="2">GT</td><td colspan="2">PoseCNN</td><td colspan="2">GT</td><td colspan="2"></td><td colspan="2"></td><td colspan="2"></td></tr><tr><td>Re-initialization</td><td colspan="2">Yes (290)</td><td colspan="2">No</td><td colspan="2">Yes (2)</td><td colspan="2"></td><td colspan="2"></td><td colspan="2">GT</td><td colspan="2">GT No</td></tr><tr><td>Novel object</td><td colspan="2">X</td><td colspan="2">X</td><td colspan="2">X</td><td colspan="2"></td><td colspan="2">No</td><td colspan="2">No √</td><td colspan="2">√</td></tr><tr><td>Object setup</td><td colspan="2">Model-based</td><td colspan="2">Model-based</td><td colspan="2">Model-based</td><td colspan="2">Model-based Model-based</td><td colspan="2">Model-based</td><td colspan="2">Model-based</td><td colspan="2">Model-free</td></tr><tr><td>Metric</td><td>ADD</td><td>ADD-S ADD</td><td>ADD-S</td><td>ADD</td><td>ADD-S</td><td>ADD</td><td>ADD-S</td><td>ADD</td><td>ADD-S</td><td>ADD ADD-S</td><td>ADD</td><td>ADD-S</td><td>ADD</td><td>ADD-S</td></tr><tr><td>002_master_chef_can</td><td>89.0</td><td>93.8</td><td>93.9 96.3</td><td>89.3</td><td>96.7</td><td>55.6</td><td>90.7</td><td>46.2</td><td>90.2</td><td>66.4</td><td>89.7</td><td>93.6</td><td>97.0</td><td>91.2 96.9</td></tr><tr><td>003_cracker_box</td><td>88.5</td><td>93.0</td><td>96.5 97.2</td><td>96.0</td><td>97.1</td><td>96.4</td><td>97.2</td><td>57.0</td><td>72.3</td><td>82.4</td><td>92.1</td><td>96.9</td><td>97.8 96.2</td><td>97.5</td></tr><tr><td>004_sugar_box</td><td>94.3</td><td>96.3</td><td>97.6 98.1</td><td>94.0</td><td>96.4</td><td>97.1</td><td>97.9</td><td>50.4</td><td>72.7</td><td>96.1</td><td>98.4</td><td>96.9 98.2</td><td>94.5</td><td>97.4</td></tr><tr><td>005_tomato_soup_can</td><td>89.1</td><td>93.2</td><td>95.0 97.2</td><td>87.2</td><td>95.2</td><td>64.7</td><td>89.5</td><td>72.4</td><td>91.6</td><td>73.2</td><td>97.3</td><td>96.3 98.1</td><td>94.3</td><td>97.9</td></tr><tr><td>006_mustard_bottle</td><td>92.0</td><td>95.1</td><td>95.8 97.4</td><td>98.3</td><td>98.5</td><td>97.1</td><td>98.0</td><td>87.7</td><td>98.2</td><td>96.2</td><td>98.4</td><td>97.3 98.4</td><td>97.3</td><td>98.5</td></tr><tr><td>007_tuna_fish_can</td><td>92.0</td><td>96.4</td><td>86.5 91.1</td><td>86.8</td><td>93.6</td><td>69.1</td><td>93.3</td><td>28.7</td><td>52.9</td><td>73.2</td><td>95.8</td><td>96.9 98.5</td><td>84.0</td><td>97.8</td></tr><tr><td>008_pudding_box</td><td>80.1</td><td>88.3</td><td>97.9 98.4</td><td>60.9</td><td>87.1</td><td>96.8</td><td>97.9</td><td>12.7</td><td>18.0</td><td>73.8</td><td>88.9</td><td>97.8 98.5</td><td>96.9</td><td>98.5</td></tr><tr><td>009_gelatin_box</td><td>92.0</td><td>94.4</td><td>97.8 98.4</td><td>98.2</td><td>98.6</td><td>97.5</td><td>98.4</td><td>49.1</td><td>70.7</td><td>97.2</td><td>98.8</td><td>97.7 98.5</td><td>97.6</td><td>98.5</td></tr><tr><td>010_potted_meat_can</td><td>78.0</td><td>88.9</td><td>77.8 84.2</td><td>76.4</td><td>83.5</td><td>83.7</td><td>86.7</td><td>44.1</td><td>45.6</td><td>93.3</td><td>97.3</td><td>95.1 97.7</td><td>94.8</td><td>97.5</td></tr><tr><td>011_banana</td><td>81.0</td><td>90.5</td><td>94.9 97.2</td><td>92.8</td><td>97.7</td><td>86.3</td><td>96.1</td><td>93.3</td><td>97.7</td><td>95.6</td><td>98.4</td><td>96.4 98.4</td><td>95.6</td><td>98.1</td></tr><tr><td>019_pitcher_base</td><td>90.4</td><td>94.7</td><td>96.8 97.5</td><td>97.7</td><td>98.1</td><td>97.3</td><td>97.7</td><td>97.9</td><td>98.2 97.3</td><td>97.0</td><td>98.8</td><td>96.7 98.0</td><td>96.8</td><td>98.0</td></tr><tr><td>021_bleach_cleanser</td><td>81.7</td><td>90.5</td><td>95.9 97.2</td><td>95.9</td><td>97.0</td><td>95.2</td><td>97.2</td><td>95.9</td><td>82.4</td><td>92.6 74.4</td><td>97.5</td><td>95.5 97.8</td><td>94.7</td><td>97.5</td></tr><tr><td>024_bowl</td><td>38.8 83.2</td><td>90.6</td><td>80.9 94.5 96.9</td><td>34.0</td><td>93.0</td><td>30.4</td><td>97.2 93.3</td><td>24.2</td><td>71.2</td><td>95.6</td><td>98.4</td><td>95.2 97.6 97.9</td><td>90.5</td><td>95.3</td></tr><tr><td>025_mug</td><td>85.4</td><td>92.0</td><td>91.5 96.4 97.4</td><td>86.9</td><td>96.7</td><td>83.2</td><td>97.8</td><td>60.0</td><td>98.3</td><td></td><td>98.5 98.5</td><td>95.6</td><td>91.5</td><td>96.1</td></tr><tr><td>035_power_drill</td><td>44.3</td><td>92.3 75.4</td><td>95.2 96.7</td><td>97.8</td><td>98.2 93.6</td><td>97.1 95.5</td><td>96.9</td><td>97.9 45.7</td><td>62.5</td><td>96.7 93.5</td><td>97.2</td><td>96.9 98.2 93.2</td><td>96.3</td><td>97.9 97.0</td></tr><tr><td>036_wood_block</td><td>70.3</td><td>84.5</td><td>95.7 97s</td><td>37.8 72.7</td><td>85.5</td><td>4.2</td><td>16.2</td><td>20.9</td><td>38.6</td><td>93.5</td><td>97.3</td><td>97.0 94.8</td><td>92.9</td><td>97.8</td></tr><tr><td>037_scissors</td><td>80.4</td><td>91.2</td><td>92.2 96.0</td><td>89.2</td><td>97.3</td><td>35.6</td><td>53.0</td><td>12.2</td><td>18.9</td><td>88.5</td><td>97.8 96.9</td><td>97.5</td><td>95.5 96.6</td><td></td></tr><tr><td>040_large_marker</td><td>73.9</td><td></td><td>94.7</td><td>90.1</td><td></td><td></td><td>72.3</td><td></td><td>80.1</td><td></td><td>93.6</td><td>98.6</td><td></td><td>98.6</td></tr><tr><td>051_large_clamp</td><td>49.3</td><td>84.1 90.3</td><td>96.9 91.7 95.8</td><td>84.4</td><td>95.5 94.1</td><td>61.2 93.7</td><td>96.6</td><td>62.8 67.5</td><td>69.7</td><td>91.8 85.9</td><td>96.9 94.3 94.4</td><td>97.3 97.5</td><td>92.5 93.4</td><td>96.7 97.3</td></tr><tr><td>052_extra_large_clamp 061_foam_brick</td><td>91.6</td></table>

TC 无模型设置与参考图像。

<table><tr><td></td><td>ADD ADD-S</td></tr><tr><td>Ours (proposed)</td><td>91.52 97.40</td></tr><tr><td>W/o LLM texture augmentation</td><td>90.83 97.38</td></tr><tr><td>W/o transformer</td><td>90.77 97.33</td></tr><tr><td>W/o hierarchical comparison</td><td>89.05 96.67</td></tr><tr><td>Ours-InfoNCE</td><td>89.39 97.29</td></tr></table>

Table 6. Ablation study of critical design choices.

训练数据的规模规律。从理论上讲，可以生成无限量的合成数据用于训练。图7展示了训练数据量如何影响在YCB-Video数据集上通过AUC指标测量的ADD和ADD-S结果。增益在约1M时趋于饱和。运行时间。我们在Intel i9-10980XE CPU和NVIDIA RTX 3090 GPU的硬件上测量运行时间。一个物体的姿态估计大约需要$1.3\mathrm{~s}$，其中姿态初始化需要$4\mathrm{~ms}$，精细化需要$0.88\mathrm{~s}$，姿态选择需要$0.42\mathrm{~s}$。跟踪速度更快，约为${\sim}32\ \mathrm{Hz}$，因为只需进行姿态精细化，并且没有多个姿态假设。实际上，我们可以在初始化时进行一次姿态估计，然后切换到跟踪模式以实现实时性能。

![](images/6.jpg)  
Figure 6. Effects of number of reference images.

![](images/7.jpg)  
Figure 7. Effects of training data size.

# 5. 结论

我们提出了一种统一的基础模型，用于新物体的六维姿态估计和跟踪，支持基于模型和无模型的设置。在四种不同任务组合的广泛实验中，表明该模型不仅多功能，而且在性能上显著超越了为每个任务专门设计的现有最先进方法，且有可观的优势。它甚至达到了那些需要实例级训练方法的可比结果。在未来的工作中，探索超越单一刚性物体的状态估计将是一个值得关注的方向。

参考文献 [1] Eric Brachmann, Alexander Krull, Frank Michel, Stefan Gumhold, Jamie Shotton, 和 Carsten Rother. 使用 3D 物体坐标学习 6D 物体姿态估计. 载于第 13 届欧洲计算机视觉会议 (ECCV), 页536-551, 2014. 6, 7 [2] Dingding Cai, Janne Heikkilä, 和 Esa Rahtu. OVE6D: 基于深度的 6D 物体姿态估计的物体视角编码. 载于 IEEE/CVF 计算机视觉与模式识别会议 (CVPR) 论文集, 页6803-6813, 2022. 5, 7, 3 [3] Ming Cai 和 Ian Reid. 局部重建，全球定位：一种无模型的物体姿态估计方法. 载于 IEEE/CVF 计算机视觉与模式识别会议 (CVPR) 论文集, 页3153-3163, 2020. 2 [4] Tianshi Cao, Karsten Kreis, Sanja Fidler, Nicholas Sharp, 和 Kangxue Yin. TexFusion: 使用文本引导的图像扩散模型合成 3D 纹理. 载于 IEEE/CVF 国际计算机视觉会议 (ICCV) 论文集, 页4169-4181, 2023. 2, 3 [5] Dengsheng Chen, Jun Li, Zheng Wang, 和 Kai Xu. 学习类别级 6D 物体姿态和大小估计的标准形状空间. 载于 IEEE 计算机视觉国际会议 (CVPR) 论文集, 页11973-11982, 2020. 1, 2 [6] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, 和 Ali Farhadi. Objaverse: 一个注释的 3D 物体宇宙. 载于 IEEE/CVF 计算机视觉与模式识别会议 (CVPR) 论文集, 页13142-13153, 2023. 2 [7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, 和 Li Fei-Fei. ImageNet: 一个大规模层次化图像数据库. 载于 IEEE/CVF 计算机视觉与模式识别会议 (CVPR) 论文集, 页248-255, 2009. 3 [8] Xinke Deng, Arsalan Mousavian, Yu Xiang, Fei Xia, Timothy Bretl, 和 Dieter Fox. PoseRBPF: 一种用于 6D 物体姿态跟踪的 Rao-Blackwell 化粒子滤波器. 载于机器人学：科学与系统 (RSS), 2019. 1, 2, 7, 8 [9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. 一幅图像价值 16x16 个词：用于大规模图像识别的变换器. 载于国际表征学习会议 (ICLR), 2021. 5 [10] Laura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Hickman, Krista Reymann, Thomas B McHugh, 和 Vincent Vanhoucke. Google 扫描物体：高质量 3D 扫描家居物品的数据集. 载于国际机器人与自动化会议 (ICRA) 论文集, 页2553-2560, 2022. 2 [11] Mathieu Garon, Denis Laurendeau, 和 Jean-François Lalonde. 评估 6 自由度物体跟踪器的框架. 载于欧洲计算机视觉会议 (ECCV) 论文集, 页582-597, 2018. 2 [12] Amos Gropp, Lior Yariv, Niv Haim, Matan Atzmon, 和 Yaron Lipman. 学习形状的隐式几何正则化. 载于国际机器学习会议 (ICML) 论文集, 页3789-3799, 2020. 4 [13] Agrim Gupta, Piotr Dollar, 和 Ross Girshick. LVIS: 大词汇实例分割数据集. 载于 IEEE/CVF 计算机视觉与模式识别会议 (CVPR) 论文集, 页5356-5364, 2019. 2 [14] John C Hart. 球体追踪：一种用于隐式表面抗锯齿光线追踪的几何方法. 视觉计算机, 12(10):527-545, 1996. 4 [15] Rasmus Laurvig Haugaard 和 Anders Glent Buch. Surfemb: 用学习的表面嵌入进行物体姿态估计的稠密和连续对应分布. 载于 IEEE/CVF 计算机视觉与模式识别会议 (CVPR) 论文集, 页6749-6758, 2022. 7 [16] Poly Haven. Poly Haven: 公共 3D 资产库. https://polyhaven.com/, 2023. 2 [17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, 和 Jian Sun. 深度残差学习用于图像识别. 载于 IEEE/CVF 计算机视觉与模式识别会议 (CVPR) 论文集, 页770-778, 2016. 5, 2 [18] Kaiming He, Georgia Gkioxari, Piotr Dollár, 和 Ross Girshick. Mask R-CNN. 载于 IEEE/CVF 计算机视觉与模式识别会议 (CVPR) 论文集, 页2961-2969, 2017. 5, 7, 3 [19] Xingyi He, Jiaming Sun, Yuang Wang, Di Huang, Hujun Bao, 和 Xiaowei Zhou. OnePose $^{++}$: 无需 CAD 模型的关键点无关单次物体姿态估计. 神经信息处理系统大会 (NeurIPS), 35: 3510-35115, 2022. 1, 2, 6, 7, 3 [20] Yisheng He, Wei Sun, Haibin Huang, Jianran Liu, Haoqiang Fan, 和 Jian Sun. PVN3D: 用于 6DoF 姿态估计的深度点级 3D 关键点投票网络. 载于 IEEE/CVF 计算机视觉与模式识别会议 (CVPR) 论文集, 页11632-11641, 2020. 1, 2 [21] Yisheng He, Haibin Huang, Haoqiang Fan, Qifeng Chen, 和 Jian Sun. FFB6D: 用于 6D 姿态估计的全流双向融合网络. 载于 IEEE/CVF 计算机视觉与模式识别会议 (CVPR) 论文集, 页3003-3013, 2021. 1, 2 [22] Yisheng He, Yao Wang, Haoqiang Fan, Jian Sun, 和 Qifeng Chen. FS6D: 新物体的少样本 6D 姿态估计. 载于 IEEE/CVF 计算机视觉与模式识别会议 (CVPR) 论文集, 页6814-6824, 2022. 2, 3, 6, 7, 8 [23] Stefan Hinterstoisser, Stefan Holzer, Cedric Cagniart, Slobodan Ilic, Kurt Konolige, Nassir Navab, 和 Vincent Lepetit. 多模态模板用于在高度杂乱场景中实时检测无纹理物体. 载于国际计算机视觉会议 (ICCV) 论文集, 页858-865, 2011. 6 [24] Jonathan Ho, Ajay Jain, 和 Pieter Abbeel. 去噪扩散概率模型. 神经信息处理系统大会 (NeurIPS), 33:6840-6851, 2020. 2 [25] Tomá Hodan, Pavel Haluza, tepán Obdrálek, Jiri Matas, Manolis Lourakis, 和 Xenophon Zabulis. T-LESS: 用于无纹理物体的 6D 姿态估计的 RGB-D 数据集. 载于 IEEE 冬季计算机视觉应用会议 (WACV) 论文集, 页880-888, 2017. 6, 7 [26] Tomas Hodan, Frank Michel, Eric Brachmann, Wadim Kehl, Anders Glent Buch, Dirk Kraft, Bertram Drost, Joel Vidal, Stephan Ihrke, Xenophon Zabulis, et al. BOP: 6D 物体姿态估计基准. 载于欧洲计算机视觉会议 (ECCV) 论文集, 页193-194, 2018. 2, 6 [27] Elad Hoffer 和 Nir Ailon. 使用三元组网络的深度度量学习. 第三届国际相似性基础模式识别研讨会 (SIMBAD), 页84-92, 2015. 6 [28] Shengyu Huang, Zan Gojcic, Mikhail Usvyatsov, Andreas Wieser, 和 Konrad Schindler. PREDATOR: 低重叠 3D 点云注册. 载于 IEEE/CVF 计算机视觉与模式识别会议 (CVPR) 论文集, 页4267-4276, 2021. 6 [29] Jan Issac, Manuel Wüthrich, Cristina Garcia Cifuentes, Jeannette Bohg, Sebastian Trimpe, 和 Stefan Schaal. 使用鲁棒高斯滤波器的基于深度的物体跟踪. 载于 IEEE 国际机器人与自动化会议 (ICRA) 论文集, 页608-615, 2016. 1, 2, 7, 8 [30] Daniel Kappler, Franziska Meier, Jan Issac, Jim Mainprice, Cristina Garcia Cifuentes, Manuel Wüthrich, Vincent Berenz, Stefan Schaal, Nathan Ratliff, 和 Jeannette Bohg. 实时感知与反应运动生成的结合. IEEE 机器人与自动化快报, 3(3):1864-1871, 2018. 1 [31] Yann Labbé, Justin Carpentier, Mathieu Aubry, 和 Josef Sivic. CosyPose: 一致的多视角多物体 6D 姿态估计. 载于欧洲计算机视觉会议 (ECCV) 论文集, 页574-591, 2020. 1, 2 [32] Yann Labbé, Lucas Manuelli, Arsalan Mousavian, Stephen Tyree, Stan Birchfield, Jonathan Tremblay, Justin Carpentier, Mathieu Aubry, Dieter Fox, 和 Josef Sivic. MegaPose: 通过渲染与比较实现新物体的 6D 姿态估计. 第 6 届机器人学习年会 (CoRL), 2022. 1, 2, 5, 7, 3 [33] Samuli Laine, Janne Hellsten, Tero Karras, Yeongho Seol, Jaakko Lehtinen, 和 Timo Aila. 高性能可微渲染的模块化原语. ACM 图形学汇刊, 39(6), 2020. [34] Taeyeop Lee, Jonathan Tremblay, Valts Blukis, Bowen Wen, Byeong-Uk Lee, Inkyu Shin, Stan Birchfield, In So Kweon, 和 Kuk-Jin Yoon. TTA-COPE: 类别级物体姿态估计的测试时适应. 载于 IEEE/CVF 计算机视觉与模式识别会议 (CVPR) 论文集, 页21285-21295, 2023. 1, 2 [35] Fu Li, Shishir Reddy Vutukur, Hao Yu, Ivan Shugurov, Benjamin Busam, Shaowu Yang, 和 Slobodan Ilic. NeRFPose: 一种先重建后回归的弱监督 6D 物体姿态估计方法. 载于 IEEE/CVF 国际计算机视觉会议 (ICCV) 论文集, 页2123-2133, 2023. 2 [36] Yi Li, Gu Wang, Xiangyang Ji, Yu Xiang, 和 Dieter Fox. DeepIM: 用于 6D 姿态估计的深度迭代匹配. 载于欧洲计算机视觉会议 (ECCV) 论文集, 页683-698, 2018. 1, 2, 7, 8 [37] Zhigang Li, Gu Wang, 和 Xiangyang Ji. CDPN: 基于坐标的解耦姿态网络用于实时 RGB 基于 6 自由度的物体姿态估计. 载于 CVF 国际计算机视觉会议 (ICCV) 论文集, 页7677-7686, 2019. 2 [38] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, 和 C Lawrence Zitnick. Microsoft COCO: 上下文中的常见物体. 载于第 13 届欧洲计算机视觉会议 (ECCV) 论文集, 页740-755, 2014. 3 [39] Yunzhi Lin, Jonathan Tremblay, Stephen Tyree, Patricio A Vela, 和 Stan Birchfield. 基于关键点的类别级物体姿态跟踪，结合 RGB 序列和不确定性估计. 载于国际机器人与自动化会议 (ICRA), 2022. 1, 2 [40] Yua u, Yl Wen, Sia g, Cein, Xig, Taku Komura, 和 Wenping Wang. Gen6D: 从 RGB 图像中进行可泛化的无模型 6 自由度物体姿态估计. ECCV, 2022. 1, 2, 6, 7 [41] William E Lorensen 和 Harvey E Cline. Marching cubes: 一种高分辨率 3D 表面构造算法. 载于开创性的图形学：塑造该领域的先锋工作, 页347-353. 1998. 4 [42] Miles Macklin. Warp: 一种高性能 Python 框架，用于 GPU 仿真和图形处理. https://github.com/nvidia/warp, 2022. NVIDIA GPU 技术大会 (GTC). 1 [43] Eric Marchand, Hideaki Uchiyama, 和 Fabien Spindler. 增强现实的姿态估计：一项实践调查. IEEE 可视化与计算机图形学学报 (TVCG), 22(12):2633-2651, 2015. 1 [44] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, 和 Ren Ng. NeRF: 将场景表示为神经辐射场以实现视图合成. ACM 通讯, 65(1):99-106, 2021. 4 [45] Thomas Müller, Alex Evans, Christoph Schied, 和 Alexander Keller. 具有多分辨率哈希编码的即时神经图形原语. ACM 图形学汇刊, 41(4):102:1-102:15, 2022. 4, 2 [46] Van Nguyen Nguyen, Yinlin Hu, Yang Xiao, Mathieu Salzmann, 和 Vincent Lepetit. 3D 物体姿态估计的模板再探讨：对新物体的泛化及对遮挡的鲁棒性. 载于 IEEE/CVF 计算机视觉与模式识别会议 (CVPR) 论文集, 页6771-6780, 2022. 5, 6, 7 [47] Van Nguyen Nguyen, Thibault Groueix, Georgy Ponimatkin, Vincent Lepetit, 和 Tomas Hodan. Cnos: 一种基于 CAD 的新物体分割的强基线. 载于 IEEE/CVF 国际计算机视觉会议论文集, 页2134-2140, 2023. 5, 1, 3 [48] Brian Okorn, Qiao Gu, Martial Hebert, 和 David Held. Zephyr: 零样本姿态假设评分. 载于 IEEE 国际机器人与自动化会议 (ICRA) 论文集, 页14141-14148, 2021. 7 [49] Aaron van den Oord, Yazhe Li, 和 Oriol Vinyals. 使用对比预测编码的表征学习. arXiv 预印本 arXiv:1807.03748, 2018. 6 [50] Kiru Park, Timothy Patten, 和 Markus Vincze. Pix2Pose: 用于 6D 姿态估计的逐像素坐标回归. 载于 IEEE/CVF 国际计算机视觉会议 (ICCV) 论文集, 页7668-7677, 2019. 1, 2 [51] Keunhong Park, Arsalan Mousavian, Yu Xiang, 和 Dieter Fox. LatentFusion: 用于未知物体姿态估计的端到端可微重建与渲染. 载于 IEEE/CVF 计算机视觉与模式识别会议 (CVPR) 论文集, 页10710-10719, 2020. 2, 7 [52] Edgar Riba, Dmytro Mishkin, Daniel Ponsa, Ethan Rublee, 和 Gary Bradski. Kornia: 一个用于 PyTorch 的开源可微计算机视觉库. 载于 IEEE/CVF 冬季计算机视觉应用会议论文集, 页3674-3683, 2020. 1 [53] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, 和 Björn Ommer. 使用潜在扩散模型进行高分辨率图像合成. 载于 IEEE/CVF 计算机视觉与模式识别会议 (CVPR) 论文集, 页10684-10695, 2022. 2 [54] Johannes Lutz Schönberger 和 Jan-Michael Frahm. 运动重建再探. 载于计算机视觉与模式识别会议 (CVPR), 2016. 3 [55] Ivan Shugurov, Fu Li, Benjamin Busam, 和 Slobodan Ilic. OSOP: 一种多阶段单次物体姿态估计框架. 载于 IEEE/CVF 计算机视觉与模式识别会议 (CVPR) 论文集, 页6835-6844, 2022. 1, 2, 7 [56] Manuel Stoiber, Martin Sundermeyer, 和 Rudolph Triebel. 迭代对应几何：融合区域和深度以高效跟踪无纹理物体. 载于 IEEE/CVF 计算机视觉与模式识别会议 (CVPR) 论文集, 页6855-6865, 2022.

[57] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, 和 Xiaowei Zhou. LoFTR: 无检测器的局部特征匹配方法基于变换器. 载于 IEEE/CVF 计算机视觉与模式识别会议 (CVPR), 页码 8922-8931, 2021. [58] Jiaming Sun, Zihao Wang, Siyu Zhang, Xingyi He, Hongcheng Zhao, Guofeng Zhang, 和 Xiaowei Zhou. OnePose: 无需 CAD 模型的一次性物体姿态估计. 载于 IEEE/CVF 计算机视觉与模式识别会议 (CVPR), 页码 6825-6834, 2022. [59] Zachary Teed 和 Jia Deng. DROID-SLAM: 针对单目、立体及 RGB-D 相机的深度视觉 SLAM. 神经信息处理系统进展 (NeurIPS), 34: 16558-16569, 2021. [60] Meng Tian, Marcelo H Ang, 和 Gim Hee Lee. 类别 6D 物体姿态及大小估计的形状先验变形. 载于 欧洲计算机视觉会议 (ECCV), 页码 530-546, 2020. [61] Jonathan Tremblay, Thang To, Balakumar Sundaralingam, Yu Xiang, Dieter Fox, 和 Stan Birchfield. 针对家庭物体的语义机器人抓取的深度物体姿态估计. 载于 机器人学习会议 (CoRL), 页码 306-316, 2018. [62] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, 和 Illia Polosukhin. 注意力是你所需的一切. 神经信息处理系统进展 (NeurIPS), 30, 2017. [63] Chen Wang, Roberto Martín-Martín, Danfei Xu, Jun Lv, Cewu Lu, Li Fei-Fei, Silvio Savarese, 和 Yuke Zhu. 6-PACK: 基于锚点关键点的类别级 6D 姿态追踪器. 载于 IEEE 国际机器人与自动化会议 (ICRA), 页码 10059-10066, 2020. [64] He Wang, Srinath Sridhar, Jingwei Huang, Julien Valentin, Shuran Song, 和 Leonidas J Guibas. 分类级 6D 物体姿态和大小估计的归一化物体坐标空间. 载于 IEEE 国际计算机视觉会议 (CVPR), 页码 2642-2651, 2019. [65] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, 和 Wenping Wang. NeuS: 通过体积渲染学习神经隐式表面用于多视图重建. 神经信息处理系统进展 (NeurIPS), 2021. [66] Bowen Wen 和 Kostas Bekris. BundleTrack: 针对新物体的 6D 姿态追踪，无需实例或类别级 3D 模型. 载于 IEEE/RSJ 国际智能机器人与系统会议 (IROS), 页码 8067-8074, 2021. [67] Bowen Wen, Chaitanya Mitash, Baozhang Ren, 和 Kostas E Bekris. se(3)-TrackNet: 通过在合成领域校准图像残差进行数据驱动的 6D 姿态追踪. 载于 IEEE/RSJ 国际智能机器人与系统会议 (IROS), 页码 10367-10373, 2020. [68] Bowen Wen, Chaitanya Mitash, Sruthi Soorian, Andrew Kimmel, Avishai Sintov, 和 Kostas E Bekris. 针对自适应手抓取物体的鲁棒性遮挡感知姿态估计. 载于 2020 IEEE 国际机器人与自动化会议 (ICRA), 页码 6210-6217, 2020. [69] Bowen Wen, Wenzhao Lian, Kostas Bekris, 和 Stefan Schaal. CatGrasp: 从仿真中学习类别级任务相关抓取. 载于 国际机器人与自动化会议 (ICRA), 页码 6401-6408, 2022. [70] Bowen Wen, Wenzhao Lian, Kostas Bekris, 和 Stefan Schaal. 你只需展示一次: 从单次视觉演示进行类别级操作. RSS, 2022. [71] Bowen Wen, Jonathan Tremblay, Valts Blukis, Stephen Tyree, Thomas Müller, Alex Evans, Dieter Fox, Jan Kautz, 和 Stan Birchfield. BundleSDF: 对未知物体的神经 6 自由度跟踪和 3D 重建. 载于 IEEE/CVF 计算机视觉与模式识别会议 (CVPR), 页码 606-617, 2023. [72] Manuel Wüthrich, Peter Pastor, Mrinal Kalakrishnan, 和 Jean. 使用范围相机. 载于 IEEE/RSJ 国际智能机器人与系统会议 (IROS), 页码 3195-3202, 2013. [73] Yu Xiang, Tanner Schmidt, Venkatraman Narayanan, 和 Dieter Fox. PoseCNN: 一种用于拥挤场景中 6D 物体姿态估计的卷积神经网络. 载于 机器人: 科学与系统 (RSS), 2018. [74] Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Basri Ronen, 和 Yaron Lipman. 通过解开几何和外观进行多视图神经表面重建. 神经信息处理系统进展 (NeurIPS), 33:2492-2502, 2020. [75] Ruida Zhang, Yan Di, Fabian Manhardt, Federico Tombari, 和 Xiangyang Ji. SSP-Pose: 对称感知形状先验变形用于直接类别级物体姿态估计. 载于 IEEE/RSJ 国际智能机器人与系统会议 (IROS), 页码 7452-7459, 2022. [76] Heng Zhao, Shenxing Wei, Dahu Shi, Wenming Tan, Zheyang Li, Ye Ren, Xing Wei, Yi Yang, 和 Shiliang Pu. 学习对称感知几何对应以进行 6D 物体姿态估计. 载于 IEEE/CVF 国际计算机视觉会议 (ICCV), 页码 14045-14054, 2023. [77] Linfang Zheng, Chen Wang, Yinghan Sun, Esha Dasgupta, Hua Chen, Ale Leonardis, Wei Zhang, 和 Hyung Jin Chang. HS-Pose: 用于类别级物体姿态估计的混合范围特征提取. 载于 IEEE/CVF 计算机视觉与模式识别会议 (CVPR), 页码 17163-17173, 2023. [78] Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, 和 Hao Li. 关于神经网络中旋转表示的连续性. 载于 IEEE/CVF 计算机视觉与模式识别会议 (CVPR), 页码 5745-5753, 2019.

# FoundationPose：新物体的统一 6D 位姿估计与跟踪

补充材料

# 5.1. BOP 排行榜的性能

图 8 展示了我们在“未见物体的 6D 位置定位” BOP 挑战中的结果。在提交时，我们的 FoundationPose 在排行榜上排名第一。这对应于我们研究中考虑的四个任务之一：针对新颖物体的模型基础姿态估计。我们使用了 BOP 挑战默认提供的 CNOS [47] 的二维检测结果。

# 5.2. 实现细节

在训练过程中，对于每个三维资产，我们首先用随机数量的合成参考图像对神经物体场进行预训练。训练好的神经物体场被冻结，并提供的渲染将与基于模型的 OpenGL 渲染混合，作为姿态细化和选择网络的输入。这种组合更好地覆盖了基于模型和无模型设置的分布，使其能够作为统一框架有效地泛化。在细化和选择网络方面，我们首先分别训练它们。然后，我们进行端到端的微调，再训练 5 个周期。整个训练过程是在合成数据上进行的，使用 4 个 NVIDIA V100 GPU，耗时约一周。在测试时，模型直接应用于现实世界数据，并运行在一台 NVIDIA RTX 3090 GPU 上。在少样本设置下，渲染是从针对每个物体优化的神经物体场中获得的。在基于模型的设置下，渲染则通过传统图形管线获得。我们对深度图像进行去噪，使用 Warp 实现，包括侵蚀和双边滤波。姿态条件裁剪通过 Kornia 批量实现。神经物体场。我们将物体标准化到神经体积范围 $[ - 1 , 1 ]$。几何网络 $\Omega$ 由两层 MLP 组成，隐藏维度为 64，最后一层除外使用 ReLU 激活。中间几何特征 $f _ { \Omega ( \cdot ) }$ 的维度为 16。外观网络 $\Phi$ 由三层 MLP 组成，隐藏

# BOP：六维物体姿态估计基准

首页 挑战 数据集 排行榜 提交结果 数据集：核心数据集 LM LM-O T-LESS ITODD HB HOPE YCB-V RU-APC IC-BIN IC-MI TUD-L TYO-L

# 未见物体的六维定位 - 核心数据集

T $\mathsf { A R } _ { \mathsf { C o r e } }$ 图像处理时间在核心数据集上的平均值。

<table><tr><td></td><td>Date (UTC)</td><td>Method</td><td>Test image </td><td>ARCore </td><td>ARLM-0 </td><td>ART-LESS</td><td>ARTUD-L </td><td>ARiI-BIN </td><td>ARITODD </td><td>ARHB</td><td>ARyCB-V</td><td>Time (s)</td></tr><tr><td>1</td><td>2023-11-19</td><td>FoundationPose</td><td>RGB-D</td><td>0.726</td><td>0.733</td><td>0.617</td><td>0.906</td><td>0.528</td><td>0.609</td><td>0.809</td><td>0.882</td><td></td></tr><tr><td>2</td><td>2023-11-17</td><td>PoMZ</td><td>RGB-D</td><td>0.692</td><td>0.684</td><td>0.521</td><td>0.945</td><td>0.503</td><td>0.559</td><td>0.791</td><td>0.840</td><td></td></tr><tr><td>3</td><td>2023-11-18</td><td>SAM6D</td><td>RGB-D</td><td>0.683</td><td>0.687</td><td>0.498</td><td>0.874</td><td>0.561</td><td>0.577</td><td>0.754</td><td>0.828</td><td>1.950</td></tr><tr><td>4</td><td>2023-09-28</td><td>GenFlow-MultiHlypo16</td><td>RGB-D</td><td>0.674</td><td>0.635</td><td>0.521</td><td>0.862</td><td>0.534</td><td>0.554</td><td>0.779</td><td>0.833</td><td>34.578</td></tr><tr><td>5</td><td>2023-09-25</td><td>GenFlow-MultiHypo</td><td>RGB-D</td><td>0.662</td><td>0.622</td><td>0.509</td><td>0.849</td><td>0.524</td><td>0.544</td><td>0.770</td><td>0.818</td><td>21.457</td></tr><tr><td>6</td><td>2023-09-27</td><td>Megapose-CNOS fastSAM+Multihyp_e...</td><td>RGB-D</td><td>0.628</td><td>0.626</td><td>0.487</td><td>0.851</td><td>0.467</td><td>0.468</td><td>0.730</td><td>0.764</td><td>141.965</td></tr><tr><td>7</td><td>2023-09-27</td><td>Megapose-CNOSfastSAM+MultihyTe</td><td>RGB-D</td><td>0.623</td><td>0.620</td><td>0.485</td><td>0.846</td><td>0.462</td><td>0.460</td><td>0.725</td><td>0.764</td><td>116.564</td></tr><tr><td>8</td><td>2023-09-28</td><td>SAM6D-BOP2023-CNOSmask</td><td>RGB-D</td><td>0.616</td><td>0.648</td><td>0.483</td><td>0.794</td><td>0.504</td><td>0.351</td><td>0.727</td><td>0.804</td><td>3.872</td></tr><tr><td>9</td><td>2023-10-18</td><td>GenFlow-MultiHypo16-RGB</td><td>RGB</td><td>0.576</td><td>0.572</td><td>0.528</td><td>0.688</td><td>0.458</td><td>0.398</td><td>0.746</td><td>0.642</td><td>40.528</td></tr></table>

在 $\operatorname { A R } _ { \operatorname { C o r e } }$ 上实现了 0.03 的显著收益，设定了排行榜的新基准记录。维度为 64，激活函数为 ReLU，最后一层使用 sigmoid 激活函数，将颜色预测映射至 $[ 0 , 1 ]$。我们在 CUDA 中实现了多分辨率哈希编码 [45]，并简化为 4 个级别，特征向量的数量从 16 到 128。每个级别的特征维度设为 2。哈希表大小设为 $2 ^ { 2 2 }$。在每次迭代中，光线批量大小为 2048。截断距离 $\lambda$ 设为 $1 \ \mathrm { c m }$。训练损失中，$w _ { e } = 1，w _ { s } = 1000，w _ { c } = 100$。训练大约需要 2000 步，通常在几秒内完成。

姿态假设生成。对于全局姿态初始化，$N_{s} = 42, N_{i} = 12$。为了训练细化网络，姿态通过在XYZ轴上添加幅度分别为$0.02 m, 0.02 m, 0.05 m$的平移噪声和幅度为$20^{\circ}$的旋转噪声进行随机扰动，旋转方向随机。渲染和输入观察均依据扰动后的姿态进行裁剪，并调整为$160 \times 160$的尺寸后送入网络。在训练损失中（公式10），$w_{1}$和$w_{2}$均设置为1。单个训练阶段持续50个周期。为了训练效率，细化迭代设置为1；在测试时，姿态估计设置为5，跟踪设置为1。姿态细化模块的完整网络架构可以在主文献中找到（图2），其中用于图像特征嵌入的网络架构在图9中展示。在变压器编码器中，嵌入维度为512，头部数量为4，前向传播维度为512。

![](images/8.jpg)  
Figure 9. Network architecture for image feature embedding used in pose refinement and selection networks. The ResBlock is from ResNet-34 [17].

姿势选择。选择网络的单独训练需要 25 个周期，其中我们对细化网络执行类似的姿势扰动，姿势假设的数量 $K = 5$。在端到端的微调过程中，姿势假设来自细化网络的输出。在训练损失（公式 11）中，$\alpha$ 设置为 0.1。有效正样本的旋转阈值 $d$ 设置为 $10^{\circ}$。姿势细化模块的完整网络架构可以在主要论文中找到（图 2），用于图像特征嵌入的网络架构在图 9 中进行了说明。在进行两级层次比较时，我们对两个自注意力模块使用相同的架构。具体来说，嵌入维度为 512，头的数量为 4，前馈维度为 512。姿势跟踪。我们的框架可以轻松适应姿势跟踪任务，同时利用时间线索。为此，在每个时间戳，我们将裁剪后的当前帧和使用先前姿势渲染的图像发送给姿势细化模块。细化后的姿势成为当前姿势输出。这个操作沿着视频序列重复进行。第一帧的姿势可以通过我们的姿势估计模型初始化。

合成数据。Objaverse 资产在对象大小和网格复杂性方面差异极大。因此，我们进一步对对象进行规范化，并根据网格边缘连接图自动去除不相连的组件，以使对象适合学习姿态估计。为创建每个场景，我们随机抽取 70 到 90 个对象，并将它们投放到一个有隐形墙的平面上，直到对象的速度小于阈值。我们将对象的尺寸随机缩放至 5 到 $30 \mathrm {cm}$，并随机选择平台的大小在 1 到 1.5 米之间。采用 LLM 辅助纹理增强技术，对来自 Objaverse [6] 的每个对象施加 3 到 5 个不同种子的多样风格。为了生成多样和逼真的图像，我们随机创建 0 到 5 个尺寸、颜色、强度、温度和曝光度各异的光源，并在距离平台 0.2 到 3.0 米的半球上布置 $N_{c} = 2$ 个摄像头。我们还随机化了对象和平台的材质属性，包括金属感和反射，以及纹理。对于环境，我们创建了一个随机方向的穹顶光，并从 Poly Haven [16] 获取 $662 \mathrm {HDR}$ 图像中抽样背景。除了 RGBD 渲染，我们还存储了对应的对象分割、相机参数和对象姿态，类似于 [26, 32]。总的来说，我们的数据集大约有 60 万个场景和 120 万张图像。数据集将在项目页面上发布，待接受后。创建参考图像。在无模型的少样本设置中，类似于 [22]，在 YCB-Video 和 LINEMOD 数据集上，我们从训练集 $\mathbb { S } _ { t }$ 中选择参考图像子集 $\mathbb { S } _ { r }$。为此，我们首先通过根据掩码选择像素数量最多的图像来初始化选择集。接下来，对于剩余的每幅图像，我们计算其与所有选定参考图像之间的旋转测地距离，并根据以下依据选择剩余的帧：

$$
i ^ { * } = \underset { i \in \mathbb { S } _ { t } , i \notin \mathbb { S } _ { r } } { \mathrm { a r g m a x } } \left( \underset { j \in \mathbb { S } _ { r } } { \operatorname* { m i n } } D ( \pmb { R } _ { i } , \pmb { R } _ { j } ) \right) ,
$$

其中 $D ( \cdot , \cdot )$ 表示 $\mathbb { S O } ( 3 )$ 上的测地线距离。我们会重复这个过程，直到获得足够数量的参考图像，通常设置为 16，如参考文献 [22] 所示。在现实应用中，当真实的物体姿态不可用时，我们可以利用现成的 SLAM 算法 [54, 59, 71] 从视频中计算姿态。有关相关结果，请参见我们的补充视频。

# 5.3. 姿态更新的解耦表示细节。

![](images/9.jpg)  
Figure 10. Illustration of disentangled representation for pose updates.

正如主要论文中所提到的，我们将平移和旋转进行解耦有两个原因。首先，$\Delta t \in \mathbb { R } ^ { 3 }$ 和 $\Delta R \in \mathbb { S O } ( 3 )$ 是两个不同空间中的变量。因此，与在最后使用单一线性投影共同预测它们相比，早期的解耦有利于学习过程。其次，解耦使我们能够在相机的坐标框架中表示 $\Delta t$ 和 $\Delta R$，使得 $\Delta t$ 独立于 $\Delta R$。这一点在图10的二维示例中得到了说明。顶 row 显示了常用的齐次表示，其中姿态更新为：$x ^ { \prime } = \Delta T x \ : = \ : \Delta R x + \Delta t$。因此，$\Delta t$ 是基于应用了 $\Delta R$ 后更新的圆盘（物体）的局部坐标系统进行应用的，这样旋转会影响平移。相比之下，底 row 显示了 $\Delta t$ 和 $\Delta R$ 的解耦，这解决了依赖性问题并稳定了训练。

![](images/10.jpg)  
Figure 11. Failure mode. Under the combination of multiple challenges including texture-less, severe occlusion, and limited edge cues, our method fails to estimate the correct orientation.

# 5.4. 限制因素

与相关工作 [2, 19, 22, 32, 58, 76] 类似，我们的方法专注于 6D 姿态估计和跟踪，并依赖于外部的 2D 检测，这些检测是通过 CNOS [47] 或 Mask-RCNN [18] 等方法获得的。我们观察到，虚假或缺失的检测通常会成为 6D 姿态估计的瓶颈。在未来的工作中，一个端到端的框架，用于新对象检测、6D 姿态估计和跟踪将会引起关注。此外，由于多种挑战的组合而导致的另一种典型失败模式在图 11 中进行了描述。

# 5.5. 致谢

我们感谢曹天时的宝贵讨论；感谢NVIDIA Isaac Sim和Omniverse团队在合成数据生成方面的支持。