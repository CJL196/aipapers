# GNFactor：具有可泛化神经特征场的多任务真实机器人学习

闫杰 $\mathbf { Z e ^ { 1 * } }$ 戈 $\mathbf { Y a n ^ { 2 * } }$ 岳华 $\mathbf { W _ { u } } ^ { 2 * }$ 安娜贝拉·马卡鲁索 2 余莹 $\mathbf { G e ^ { 3 } }$ 江龙 $\mathbf { Y e ^ { 2 } }$ 尼克拉斯·汉森 2 李尔然 $\mathbf { L i } ^ { 4 }$ 小龙·王 2 上海交通大学 $^ 2 \mathrm { U C }$ 圣地亚哥 3 香港大学 4 亚马逊AWS AI \*同等贡献 # yanjieze.com/GNFactor

摘要：在机器人领域，开发能够在非结构化的真实环境中执行多样化操作任务的智能体一直是一个长期存在的问题。为了实现这一目标，机器人需要全面理解场景的3D结构和语义。在本研究中，我们提出了GNFactor，一种用于多任务机器人操作的视觉行为克隆智能体，基于可泛化神经特征场。GNFactor共同优化了一个可泛化神经场（GNF）作为重建模块，以及一个感知变换器作为决策模块，利用共享的深度3D体素表示。为了在3D中融入语义，重建模块利用视觉-语言基础模型（例如，Stable Diffusion）将丰富的语义信息提炼到深度3D体素中。我们在3个真实机器人任务上评估了GNFactor，并在10个RLBench任务上进行了详细的消融实验，演示了有限示例下的表现。我们观察到GNFactor在已见和未见任务上相较于当前最先进的方法有显著改善，展示了GNFactor强大的泛化能力。关键词：机器人操作、神经辐射场、行为克隆

# 1 引言

引入学习到机器人操作中的一个主要目标是使机器人能够有效地处理未见过的物体，并成功应对新环境中的各种任务。本文重点关注在多任务操作中使用有限的演示进行模仿学习。使用模仿学习有助于避免复杂的奖励设计，并且可以直接在真实机器人上进行训练，而无需在模拟中创建其数字双胞胎。这使得基于用户指令在复杂环境中进行多样任务的策略学习成为可能。然而，使用有限演示进行工作在泛化能力方面面临巨大挑战。这些挑战主要源于理解场景的三维结构、理解物体的语义和功能，以及根据视觉线索有效地遵循任务指令的需求。因此，机器人的观察结果的全面和信息丰富的视觉表示为泛化提供了关键基础。

机器人学习中的视觉表示的发展主要集中在二维平面上的学习。自监督目标被用于预训练来自二维图像观察的表示 [6, 7, 8] 或与策略梯度共同优化 [9, 10, 11]。虽然这些方法提高了样本效率并导致了更稳健的策略，但它们大多应用于相对简单的操作任务。为了应对需要几何理解（例如，物体形状和姿态）且涉及遮挡的更复杂任务，最近在机器人学习中采用了三维视觉表示学习 [11, 12]。例如，Driess 等 [12] 使用 NeRF 和视图合成来提供监督，训练三维场景表示。虽然它在处理需要几何推理的任务（如悬挂杯子）时显示出了有效性，但仅限于在单任务设置中处理简单场景结构，并且存在严重的遮挡问题。更重要的是，在没有对场景的语义理解的情况下，机器人很难遵循用户的语言指令。

![](images/1.jpg)  
Figure:Left:Three camera views used in the real robot setup to reconstruct the feature field generated by Stable Diffsion [].We segment the foreground feature or better illustration. Right:Three langageconditioned real robot tasks across two different kitchens.

在本文中，我们引入了一种利用新颖表示法学习语言条件策略的方法，该方法结合了3D和语义信息，以实现多任务操作。我们训练了一种可泛化的神经特征场（GNF），该模型将预训练的语义特征从2D基础模型提炼到神经辐射场（NeRF）。我们在该表示上进行策略学习，形成我们的模型GNFactor。值得注意的是，GNFactor学习一个编码器，以前馈的方式提取场景特征，而不是在NeRF中进行逐场景优化。给定单个RGB-D图像观测，我们的模型将其编码为3D语义体积特征，然后通过Perceiver Transformer [13]架构进行动作预测。为了进行多任务学习，Perceiver Transformer接受语言指令以获取任务嵌入，并推理语言与视觉语义之间的关系以进行操作。我们的框架中有两条训练分支（见图3）：(i) GNF训练。根据收集的演示数据，我们使用体积渲染进行视图合成来训练可泛化的神经特征场。除了渲染RGB像素外，我们还在二维空间中渲染基础模型的特征。GNF同时从像素和特征重建中学习。为了为特征重建提供监督，我们应用视觉基础模型（例如，预训练的Stable Diffusion模型 [5]）从输入视图中提取2D特征作为真实标注数据。通过这种方式，我们能够将语义特征提炼到GNF的3D空间中。(ii) GNFactor联合训练。在由GNF学习目标共同优化的3D体积特征基础上，我们进行行为克隆，以端到端地训练整个模型。为了评估，我们在两个不同厨房的三个不同任务上进行了真实机器人实验（见图1）。我们成功训练出一种单一策略，能够有效解决这些不同场景中的任务，相较于基线方法PerAct [3]显著提高了性能。我们还使用10个RLBench模拟任务 [14]和6个设计的泛化任务进行了全面评估。我们观察到，GNFactor的表现超过了PerAct，平均提升为$1.55 \mathrm{x}$和$1.57 \mathrm{x}$，与现实机器人实验中观察到的显著提升一致。

# 2 相关工作

多任务机器人操作。最近的多任务机器人操作研究在复杂任务执行和向新场景泛化能力方面取得了显著进展 [15, 2, 1, 16, 17, 3, 18, 19]。显著的方法通常涉及使用大量交互数据来训练多任务模型 [2, 1, 16, 17]。例如，RT-1 [1] 强调了任务无关训练的优势，展示了在各种数据集上的真实机器人任务中优越的表现。为了减少对大量演示的需求，利用编码运动启动的关键帧的方法已被证明有效 [20, 21, 22, 23, 24]。PerAct [3] 采用了 Perceiver Transformer [13] 来编码语言目标和体素观察，并在真实机器人实验中展示了其有效性。在本研究中，我们采用与 PerAct 相同的动作预测框架，同时专注于通过在有限数据下学习可泛化的体积表示来提高该框架的泛化能力。

![](images/2.jpg)  

Figure 2: Simulation environments and the real robot setup. We show the RGB observations for our 10 RLBench tasks in Figure (a), the sampled views for GNF in Figure (b), and the real robot setup in Figure ().

用于强化/模仿学习（RL/IL）的3D表示。为了通过利用视觉信息改善操作策略，众多研究集中在提升2D视觉表示上[8, 7, 6, 25]，而在处理更复杂任务时，利用3D表示变得至关重要。Ze等人[11]在运动控制中引入了一种基于深度体素的3D自编码器，相较于2D表示学习方法，展现了更高的样本效率。Driess等人[12]提出首先通过NeRF学习状态表示，然后将该冻结状态用于下游RL任务。尽管该研究展示了在RL中利用NeRF的初步成功，但由于多种限制，其在现实场景中的适用性受到限制，例如，需要物体掩膜、缺乏机械臂及场景结构的缺失。与我们工作最接近的是SNeRL[26]，它同样在NeRF中利用了视觉基础模型。然而，类似于NeRF-RL[12]，SNeRL对场景结构进行了掩蔽以确保功能性，并且物体掩膜的需求依然存在，这对其在真实机器人场景中的应用造成了挑战。我们提出的GNFactor则处理了具有挑战性的多任务现实场景，展示了在真实机器人应用方面的潜力。

神经辐射场（NeRF）。近年来，神经场在新视角合成和场景表示学习方面取得了巨大成功[27, 28, 29, 30, 31, 32]，近期的研究也开始将神经场应用于机器人领域[33, 34, 35, 12, 26]。NeRF[29]因通过学习场景的隐式函数实现了照片级真实感的视角合成而突出，但它需要对每个场景进行优化，因此难以推广。许多后续方法[36, 37, 38, 39, 40, 41, 42]提出了更具通用性的NeRF。PixelNeRF[43]和CodeNeRF[37]将二维图像编码为NeRF的输入，而TransINR[36]利用视觉变换器直接推断NeRF参数。一系列近期的研究[44, 45, 46, 47, 48, 49]除了RGB图像外，还利用了预训练的视觉基础模型，如DINO[50]和CLIP[51]作为监督，从而使NeRF能够学习通用特征。在本研究中，我们结合通用NeRF重建来自预训练的稳定扩散模型[5]的不同视角的RGB和嵌入。



在本节中，我们详细介绍了提议的 GNFactor，它是一个具有三维体积表示的多任务智能体，旨在实现真实世界的机器人操控。GNFactor 由一个体积渲染模块和一个三维策略模块组成，二者共享相同的深度体积表示。体积渲染模块学习通用神经特征场（GNF），以从摄像头重建 RGB 图像和来自视觉-语言基础模型（例如 Stable Diffusion [5]）的嵌入。视觉-语言嵌入的任务无关特性使得体积表示能够通过神经渲染学习通用特征，从而帮助三维策略模块更好地处理。

![](images/3.jpg)  

Figure 3: Overview of GNFactor. GNFactor takes an RGB-D image as input and encodes it using a voxel encoder to transform it into a feature in deep 3D volume.This volume is then shared by two modules:volumetric rendering (Renderer) and robot action prediction (Perceiver). These two modules are jointly trained, which optimizes the shared features to not only reconstruct vision-language embeddings (Diffusion Feature) and other views (RGB), but also to estimate accurate $\mathrm { Q }$ values $Q _ { \mathrm { t r a n s } }$ , $Q _ { \mathrm { r o t } }$ $Q _ { \mathrm { c o l l i d e } }$ , $Q _ { \mathrm { o p e n . } }$ . multi-task robotic manipulation. The task description is encoded with CLIP [51] to obtain the task embedding $T$ An overview of GNFactor is shown in Figure 3.

# 3.1 问题定义

为了有效解决复杂的现实世界机器人问题，我们将观察空间构建为一个 3D 体素空间 $\mathcal { O } \in \mathbb { R } ^ { 1 0 0 ^ { 3 } \times 3 }$，与常用的 2D 图像相对 [1, 2, 7, 8]。3D 体素观察源自具有已知外部和内部参数的单前置摄像头捕获的 RGB-D 图像，确保我们的方法在现实世界中的实用性。除了用于策略训练的前置摄像头视图外，我们还收集额外的 $k$ 个视图用于训练 GNF。对于这些额外的视图，我们仅收集 RGB 图像，而不是 RGB-D 图像。在现实场景中，我们使用 $k = 2$，而在模拟环境中，我们设定 $k = 1 9$。

机器人手臂的动作通过平移 $a _ { \mathrm { t r a n s } } \in \mathbb { R } ^ { 3 }$、旋转 $a _ { \mathrm { r o t } } \in$ $\mathbb { R } ^ { ( 3 6 0 / 5 ) \times 3 }$、夹具开度 $a _ { \mathrm { o p e n } } \in [ 0 , 1 ]$ 和避碰 $a _ { \mathrm { c o l l i s i o n } } \in [ 0 , 1 ]$ 来表示。对于旋转 $a _ { \mathrm { r o t } }$，每个旋转轴被离散化为 $R = 5$ 个桶。避碰参数指示运动规划器关于避开碰撞的必要性，这一点至关重要，因为我们的任务涵盖了接触和非接触两种运动。由于连续动作预测的低效以及随之而来的大量数据需求，我们将行为克隆问题重新构造为关键帧预测问题 [3, 52]。我们首先使用以下度量从专家演示中提取关键帧：当关节速度接近零且夹具的开状态保持不变时，轨迹中的帧即为关键帧。模型接着被训练以根据当前观测来预测后续关键帧。这一构造有效地将连续控制问题转变为离散化的关键帧预测问题，将内部过程委托给模拟中的 RRT-connect 运动规划器 [53] 和现实世界中的线性运动规划器 xArm7 机器人。

# 3.2 使用可泛化神经特征场学习体积表示

在我们的初步步骤中，我们将RGB-D图像转化为一个 $1 0 0 ^ { 3 }$ 体素。然后，3D体素编码器对这个3D体素进行编码，并输出我们的体积表示 $v \in \mathbb { R } ^ { 1 0 0 ^ { 3 } \times 1 2 8 }$。为了通过结构知识和语言语义增强体积表示 $v$，我们学习一个可泛化的神经特征场（GNF），该特征场以深度体积 $v$ 作为场景表示，模型通过重建额外视角和由2D视觉语言基础模型预测的特征进行学习[5]。整个神经渲染过程如下所述。

我们用 $v _ { \mathbf { x } } \in \mathbb { R } ^ { 1 2 8 }$ 表示通过体积表示 $v$ 对三维点 $\mathbf { x }$ 采样得到的三维特征 $v _ { \mathbf { x } }$。由于体积 $v$ 的离散性质，$v _ { \mathbf { x } }$ 是通过三线性插值形成的。我们的 GNF 主要由三个函数组成：（i）一个密度函数 $\sigma ( \mathbf { x } , v _ { \mathbf { x } } ) : \mathbb { R } ^ { 3 + 1 2 8 } \mapsto \mathbb { R } _ { + }$，将三维点 $\mathbf { x }$ 和三维特征 $v _ { \mathbf { x } }$ 映射到密度 $\sigma$，（ii）一个 RGB 函数 $\mathbf { c } ( \mathbf { x } , \mathbf { d } , v _ { \mathbf { x } } ) : \mathbb { R } ^ { 3 + 3 + 1 2 8 } \mapsto \mathbb { R } ^ { 3 }$，将三维点 $\mathbf { x }$、视角方向 $\mathbf { d }$ 和三维特征 $v _ { \mathbf { x } }$ 映射到颜色，(iii）一个视觉-语言嵌入函数 $\mathbf { f } ( \mathbf { x } , \mathbf { d } , v _ { \mathbf { x } } ) : \mathbb { R } ^ { 3 + 3 + 1 2 8 } \mapsto \mathbb { R } ^ { 5 1 2 }$，将三维点 $\mathbf { x }$、视角方向 $\mathbf { d }$ 和三维特征 $v _ { \mathbf { x } }$ 映射到视觉-语言嵌入。在图 3 中，展示了这三个函数的相应组件。给定一个像素的射线方程 $\mathbf { r } ( t ) = \mathbf { o } + t \mathbf { d }$，它由摄像机原点 $o \in \mathbb { R } ^ { 3 }$、视角方向 $\mathbf { d }$ 和深度 $t$ 定义，深度范围为 $[ t _ { n } , t _ { f } ]$，射线的估计颜色和嵌入可以通过以下方式计算：

$$
\begin{array} { l } { { \displaystyle { \hat { \mathbf { C } } } ( \mathbf { r } , v ) = \int _ { t _ { n } } ^ { t _ { f } } T ( t ) \sigma ( \mathbf { r } ( t ) , v _ { \mathbf { x } ( t ) } ) \mathbf { c } ( \mathbf { r } ( t ) , \mathbf { d } , v _ { \mathbf { x } ( t ) } ) \mathrm { d } t } , } \\ { { \displaystyle { \hat { \mathbf { F } } } ( \mathbf { r } , v ) = \int _ { t _ { n } } ^ { t _ { f } } T ( t ) \sigma ( \mathbf { r } ( t ) , v _ { \mathbf { x } ( t ) } ) \mathbf { f } ( \mathbf { r } ( t ) , \mathbf { d } , v _ { \mathbf { x } ( t ) } ) \mathrm { d } t } , } \end{array}
$$

其中 $\begin{array} { r } { T ( t ) = \exp \left( - \int _ { t _ { n } } ^ { t } \sigma ( s ) \mathrm { d } s \right) } \end{array}$ 是 T ipleation。我们的目标是通过最小化以下损失函数，从多个视角和不同场景中优化 RGB 嵌入。

$$
{ \mathcal { L } } _ { \mathrm { r e c o n } } = \sum _ { \mathbf { r } \in { \mathcal { R } } } \| \mathbf { C } ( \mathbf { r } ) - { \hat { \mathbf { C } } } ( \mathbf { r } ) \| _ { 2 } ^ { 2 } + \lambda _ { \mathrm { f e a t } } \| \mathbf { F } ( \mathbf { r } ) - { \hat { \mathbf { F } } } ( \mathbf { r } ) \| _ { 2 } ^ { 2 } ,
$$

其中 $\mathbf { C } ( \mathbf { r } )$ 是真实标注的颜色，$\mathbf { F } ( \mathbf { r } )$ 是由稳定扩散生成的真实标注视觉-语言嵌入，$\mathcal { R }$ 是从相机位姿生成的光线集合，$\lambda _ { \mathrm { f e a t } }$ 是嵌入重构损失的权重。为了提高效率，我们在一个目标视图下采样 $b _ { \mathrm { r a y } }$ 条光线，而不是重构整个图像。为了帮助 GNF 的训练，我们使用了粗到精的层次结构，类似于原始 NeRF [29]，并在“精细”网络中应用深度引导采样 [54]。

# 3.3 基于体积表示的动作预测

体积表示 $v$ 的优化不仅旨在重建 GNF 模块，还旨在预测在 3D 策略中完成操作任务所需的动作。因此，我们共同训练表示 $v$ 以满足 GNF 和 3D 策略模块的双重目标。在本节中，我们详细阐述训练目标和 3D 策略的架构。

我们采用 Perceiver Transformer 来处理高维多模态输入，即 3D 体积、机器人的本体感知和语言特征。我们首先使用卷积核大小和步幅为 5 的 3D 卷积层，将共享的体积表示 $v$ 压缩为大小为 $2 0 ^ { 3 } \times 1 2 8$ 的体积，然后经过 ReLU 函数处理，并将 3D 体积展平为一系列大小为 $8 0 0 0 \times 1 2 8$ 的小立方体。机器人的本体感知被投影到 128 维空间，并与每个立方体的体积序列拼接，形成大小为 $8 0 0 0 \times 2 5 6$ 的序列。接着，我们将来自 CLIP 的语言标记特征投影到相同维度 $( 7 7 \times 256)$，并将这些特征与 3D 体积、机器人的本体感知状态和 CLIP 标记嵌入的组合进行拼接。结果是一个维度为 $8 0 7 7 \times 2 5 6$ 的序列。

该序列与可学习的位置编码相结合，并通过Perceiver Transformer传递，输出一个相同大小的序列。为了便于体素化，我们去掉最后77个特征，并将序列重新塑形为大小为 $2 0 ^ { 3 } \times 2 5 6$ 的体素。该体素随后通过三线性插值放大到 $1 0 0 ^ { 3 } \times 1 2 8$ ，并称为 $v _ { \mathrm { P T } }$。$v _ { \mathrm { P T } }$ 在图3中与三个动作预测头 $Q _ { \mathrm { o p e n } }$ 、 $Q _ { \mathrm { t r a n s } }$ 、 $Q _ { \mathrm { r o t } }$ 、 $Q _ { \mathrm { c o l l i d e } }$ 共享，以确定与观察空间相同尺度的最终机器人动作。为了保留来自GNF训练的学习特征，我们在体积表示 $v$ 和 $v _ { \mathrm { P T } }$ 之间创建了一个跳跃连接。组合的体积特征 $( v , v _ { \mathrm { P T } } )$ 用于预测3D Q函数 $\mathcal { Q } _ { \mathrm { t r a n s } }$ 用于平移，以及其他机器人操作的 Q 函数，如抓取器开口 $( \mathcal { Q } _ { \mathrm { o p e n } } )$ 、旋转 $( \mathcal { Q } _ { \mathrm { r o t } } )$ 和避碰 $( \mathcal { Q } _ { \mathrm { c o l l i d e } } )$ 。这里的 $\mathcal { Q }$ -函数表示一个时间步的动作值，与传统的用于多个时间步的强化学习中的 $\mathcal { Q }$ -函数不同。例如，在每个时间步中，3D $\mathcal { Q } _ { \mathrm { t r a n s } }$ 值对于最可能的下一个体素为1，对于其他体素为0。然后，模型像分类器一样优化交叉熵损失，其中 $\lambda _ { \mathrm { { r e c o n } } }$ 是重建损失的权重，用于平衡不同目标的规模。为了训练GNFactor，我们采用联合训练方法，同时优化GNF和3D策略模块，而无需任何预训练。从我们的经验观察来看，这种方法可以在学习共享特征时更好地融合两个模块的信息。

$$
\begin{array} { r } { \mathcal { L } _ { \mathrm { a c t i o n } } = - \mathbb { E } _ { Y _ { \mathrm { t r a s } } } \left[ \log \mathcal { V } _ { \mathrm { t r a n s } } \right] - \mathbb { E } _ { Y _ { \mathrm { r o t } } } \left[ \log \mathcal { V } _ { \mathrm { r o t } } \right] - \mathbb { E } _ { Y _ { \mathrm { o p e n } } } \left[ \log \mathcal { V } _ { \mathrm { o p e n } } \right] - \mathbb { E } _ { Y _ { \mathrm { c o l i d e } } } \left[ \log \mathcal { V } _ { \mathrm { c o l i d e } } \right] , } \end{array}
$$

![](images/4.jpg)  

Figure 4: Main experiment results. We present the average sucess rates in both the multi-task and generalization settings across RLBench tasks and real robot tasks.The error bar represents one standard deviation. The number in the bracket denotes the number of tasks. where $\mathcal { V } _ { i } = \operatorname { s o f t m a x } ( \mathcal { Q } _ { i } )$ for $\mathcal { Q } _ { i } \in [ \mathcal { Q } _ { \mathrm { t r a n s } } , \mathcal { Q } _ { \mathrm { o p e n } } , \mathcal { Q } _ { \mathrm { r o t } } , \mathcal { Q } _ { \mathrm { c o l l i d e } } ]$ and $Y _ { i } \in [ Y _ { \mathrm { t r a n s } } , Y _ { \mathrm { r o t } } , Y _ { \mathrm { o p e n } } , Y _ { \mathrm { c o l l i d e } } ]$ is the ground truth one-hot encoding. The overall learning objective for GNFactor is as follows:

$$
{ \mathcal { L } } _ { \mathrm { G N F a c t o r } } = { \mathcal { L } } _ { \mathrm { a c t i o n } } + \lambda _ { \mathrm { r e c o n } } { \mathcal { L } } _ { \mathrm { r e c o n } } ,
$$

# 4 实验

在本节中，我们进行实验以回答以下问题：（i）GNFactor能否在模拟环境中超越基线模型？（ii）GNFactor能否在模拟中对新场景进行泛化？（iii）GNFactor是否学习了能够处理两个不同厨房中真实机器人任务的优越策略，同时使用了嘈杂且有限的真实世界数据？（iv）GNFactor中哪些关键因素确保了整个系统的功能？我们得出的结果见图4。

# 4.1 实验设置

为了保证可重复性和基准测试，我们在RLBench模拟任务中进行主要实验。此外，为了展示GNFactor在现实世界中的潜力，我们设计了一组覆盖两个厨房的真实机器人实验。我们将GNFactor与强大的语言条件多任务智能体PerAct [3] 在模拟和现实世界中进行比较，强调GNFactor的普遍功能性。GNFactor和PerAct均使用来自前置摄像头的单个RGB-D图像作为输入以构建体素网格。在多任务模拟实验中，我们还通过添加更多摄像机视角作为输入，创建了PerAct的增强版本，以全面覆盖场景（如图10所示）。图2展示了我们的模拟任务和真实机器人设置。我们简要描述了任务，详细内容见附录B和附录C。 模拟。我们从RLBench任务集中选择了10个具有挑战性的语言条件操作任务 [14]。每个任务至少有两种变体，总计166种变体。这些变体涵盖了多种类型，例如形状和颜色的变体。因此，为了在非常有限的演示下实现高成功率，智能体需要学习关于操作的一般化知识，而不仅仅是过拟合给定的演示。我们使用来自单个前置摄像头的RGB-D图像，尺寸为$1 2 8 \times 1 2 8 \times 3$作为观察输入。为了训练GNF，我们还添加了19个额外的摄像机视角，以提供RGB图像作为监督。 真实机器人。在真实机器人实验中，我们使用xArm7机器人和一个并联抓手。我们设置了两个玩具厨房环境，使智能体能够在场景之间泛化操作技能，并设计了三个操作任务，包括打开微波炉门、转动水龙头和重新安置茶壶，如图1所示。我们在机器人周围设置了三台RealSense相机。三台相机中，前置相机捕获RGB-D观察用于策略训练，左/右相机提供RGB监督用于GNF训练。 专家演示。我们使用运动规划器为每个RLBench任务收集了20个演示。任务变体是均匀采样的。我们使用VR控制器为每个真实机器人任务收集了5个演示。收集的详细信息见附录D。 泛化任务。为了进一步展示GNFactor的泛化能力，我们基于原始训练任务设计了6个额外的模拟任务和3个真实机器人任务，并添加了任务干扰项。 训练细节。一个智能体在两块NVIDIA RTX3090 GPU上训练2天（$1 0 0 \mathbf { k }$次迭代），批量大小为2。GNFactor的共享体素编码器实现为轻量级3D

Table 1: Multi-task test results on RLBench. We evaluate 25 episodes for each checkpoint on 10 tasks across 3 seeds and report the success rates $( \% )$ of the final checkpoints. Our method outperforms the most competitive baseline PerAct [3] with an average improvement of $\mathbf { 1 . 5 5 x }$ and even still largely surpasses PerAct with 4 cameras as input. The additional camera views are visualized in Figure 10.   

<table><tr><td>Method / Task</td><td>close jar</td><td>open drawer</td><td>sweep to dustpan</td><td>meat off grill</td><td>turn tap</td><td>Average</td></tr><tr><td>PerAct</td><td>18.7±8.2</td><td>54.7±18.6</td><td>0.0±0.0</td><td>40.0±17.0</td><td>38.7±6.8</td><td rowspan="4"></td></tr><tr><td>PerAct (4 Cameras)</td><td>21.3±7.5</td><td>44.0±11.3</td><td>0.0±0.0</td><td>65.3±13.2</td><td>46.7±3.8</td></tr><tr><td>GNFactor</td><td>25.3±6.8</td><td>76.0±5.7</td><td>28.0±15.0</td><td>57.3±18.9</td><td>50.7±8.2</td></tr><tr><td>Method / Task</td><td>slide block</td><td>put in drawer</td><td>drag stick</td><td>push buttons</td><td>stack blocks</td></tr><tr><td>PerAct</td><td>18.7±13.6</td><td>2.7±3.3</td><td>5.3±5.0</td><td>18.7±12.4</td><td>6.7±1.9</td><td>20.4</td></tr><tr><td>PerAct (4 Cameras)</td><td>16.0±14.2</td><td>6.7±6.8</td><td>12.0±3.3</td><td>9.±1.9</td><td>5.3±1.9</td><td>22.7</td></tr><tr><td>GNFactor</td><td>20.0±15.0</td><td>0.0±0.0</td><td>37.3±13.2</td><td>18.7±10.0</td><td>4.0±3.3</td><td>31.7</td></tr></table>

Table 2: Generalization to unseen tasks on RLBench. We evaluate 20 episodes for each task with the final chepot across 3 seeds. We denote "" as a large object, ""as a saer object "N as a new positi "D" as adding a distractor. Our method outperforms PerAct with an average improvement of $\mathbf { 1 . 5 7 x }$ .   

<table><tr><td>Method / Task</td><td>drag (D)</td><td>slide (L)</td><td>slide (S)</td><td>open (n)</td><td>turn (N)</td><td>push (D)</td><td>Average</td></tr><tr><td>PerAct</td><td>6.6±4.7</td><td>33.3±4.7</td><td>5.0±4.1</td><td>25.0±10.8</td><td>18.3±6.2</td><td>20.0±7.1</td><td>18.0</td></tr><tr><td>GNFactor</td><td>46.7±30.6</td><td>25.0±4.1</td><td>6.7±6.2</td><td>31.7±6.2</td><td>28.3±2.4</td><td>31.7±2.4</td><td>28.3</td></tr></table>

仅有 0.3M 参数的 UNet。Perceiver Transformer 的参数数量与 PerAct [3] 保持一致（25.2M 参数），这使得我们与 PerAct 的比较公平。

# 4.2 仿真结果

我们在表1中报告了RLBench多任务测试的成功率，在表2中报告了对新环境的泛化能力。我们的观察总结如下：GNFactor在多任务学习中优于PerAct。正如表1和图4所示，GNFactor在各种任务中的成功率高于PerAct，尤其是在具有挑战性的长时程任务中表现突出。例如，在扫掸垃圾到垃圾盘的任务中，机器人需要先拿起扫帚并使用扫帚将灰尘扫入垃圾盘。我们发现GNFactor的成功率为28.0%，而PerAct完全没有成功。在像打开抽屉这样的简单任务中，机器人只需拉出抽屉，GNFactor和PerAct的表现都相当不错，成功率分别为76.0%和54.7%。此外，我们观察到增强PerAct以增加额外的相机视角并未带来显著改善。这突显了有效利用可用相机视角的重要性。GNFactor对新任务的泛化能力。在表2中，我们观察到环节中如干扰物的变化会对所有智能体产生负面影响，而GNFactor在6个任务中的5个任务中显示出比PerAct更好的泛化能力。我们还发现对于一些具有挑战性的变体，例如任务滑动中的小块，GNFactor和PerAct都难以应对。这进一步强调了强健的泛化能力的重要性。消融实验。我们在表4中总结了GNFactor中对体积表示成功起关键作用的主要组件。从消融研究中，我们获得了几个见解：我们的GNF重建模块在多任务机器人学习中起着至关重要的作用。此外，RGB损失对于学习一致的3D特征是必不可少的，尤其是因为从基础模型中衍生的特征并不天然具有3D一致性。

Table 4: Ablations. We report the averaged success rates on $1 0 \ \mathrm { R L } \cdot$ . Bench tasks. "DGS" is short for depth-guided sampling. $\ddot { \cdots } \xrightarrow { \infty } \vec { \mathbf { \zeta } }$ means replacing.   

<table><tr><td>Ablation</td><td>Success Rate (%)</td></tr><tr><td>GNFactor</td><td>36.8</td></tr><tr><td>w/o. GNF objective</td><td>24.2</td></tr><tr><td>w/o. RGB objective</td><td>27.2</td></tr><tr><td>w/o. Diffusion</td><td>30.0</td></tr><tr><td>Diffusion → DINO</td><td>30.4</td></tr><tr><td>Diffusion → CLIP</td><td>32.0</td></tr><tr><td>w/o. DGS</td><td>29.2</td></tr><tr><td>w/o. skip connection</td><td>27.6</td></tr><tr><td>k = 19 → 9</td><td>33.2</td></tr><tr><td>λfeat = 0.01 → 1.0</td><td>35.2</td></tr><tr><td>λrecon = 0.01 → 1.0</td><td>35.2</td></tr></table>

体积表示利用了扩散特征和深度引导采样，其中深度先验被用来提高神经渲染中的采样质量。一个直观的解释是，当 GNF 与 DGS 结合时，更擅长于学习深度和三维结构信息。这种增强的理解使得三维表示能够更好地集中于物体的表面，而不是整个体积。此外，用 DINO [50] 或 CLIP [51] 替代稳定扩散并不会轻易导致类似的改进，这表明我们视觉-语言特征的重要性。

Table 3:Multi-task test results on real robot.We evaluate 10 episodes for each task and report the resulting success rate $( \% )$ . We denote "door" as "open door", "faucet" as "turn faucet", and "teapot" as "relocate teapot" The number in the parenthesis suggests the kitchen ID and "d" suggests testing with distractors.   

<table><tr><td>Method / Task</td><td>door (1)</td><td>faucet (1)</td><td>teapot (1)</td><td>door (1,d)</td><td>faucet (1,d)</td><td>teapot (1,d)</td><td>Average</td></tr><tr><td>PerAct</td><td>30</td><td>80</td><td>0</td><td>10</td><td>50</td><td>0</td><td rowspan="3"></td></tr><tr><td>GNFactor</td><td>40</td><td>80</td><td>40</td><td>30</td><td>500</td><td>30</td></tr><tr><td>Method / Task</td><td>door (2)</td><td>faucet (2)</td><td>teapot (2)</td><td>door (2,d)</td><td>faucet (2,d)</td><td>teapot (2,d)</td></tr><tr><td>PerAct</td><td>10</td><td>50</td><td>0</td><td>10</td><td>30</td><td>0</td><td>22.5</td></tr><tr><td>GNFactor</td><td>50</td><td>70</td><td>40</td><td>20</td><td>40</td><td>30</td><td>43.3</td></tr></table>

(iii) 尽管跳跃连接的使用并不新颖，我们只是遵循了 PerAct 的结构，但去除跳跃连接的结果表明，我们的体素表征在提取基础模型特征方面起到了关键作用，从而对最终动作的预测至关重要。 (iv) 在神经渲染损失和动作预测损失之间取得谨慎平衡对于获得最佳性能至关重要，而我们 GNF 模块利用来自多个视角的信息被证明对单视角决策模块是有益的。此外，我们提供了由 GNFactor 生成的现实世界视图合成，如图 5 和图 6 所示。我们还给出了通过 PSNR [29] 测量的定量评估。我们观察到，渲染的视图有些模糊，因为 GNFactor 学习的体积表示是最小化神经渲染损失和动作预测损失的优化结果。当去掉行为复制损失，仅训练 GNF 时，渲染质量有了显著提高。值得注意的是，对于现实世界的视图合成，我们既没有用于训练也没有用于测试的真实点云数据。相反，点云来自 RealSense 摄像头，因此并不完美。尽管在实现精确的像素级重建结果方面存在局限性，我们更注重从提取的扩散特征中学习整个场景的语义理解，这对于策略学习更为重要。

# 4.3 真实机器人实验

我们在表3中总结了真实机器人实验的结果。从实验来看，GNFactor在几乎所有任务上都优于PerAct基线。特别是在茶壶任务中，智能体需要准确确定抓取位置并从正确的角度处理茶壶，PerAct未能完成任务，并在两个厨房中获得了零成功率。我们观察到，仅依靠5个示例来学习一个精细的策略确实具有挑战性。然而，通过结合视觉-语言模型的嵌入表示，GNFactor获得了对物体的理解。因此，GNFactor并不只是简单地过拟合于给定的示例。第二个厨房（图1）由于其较小的尺寸带来了更多挑战，要求更高的准确性来有效操作物体。在第二个厨房中，GNFactor与基线PerAct之间的性能差距变得更显著。重要的是，我们的方法在从第一个厨房过渡到第二个厨房时并没有遭受相同的性能下降，不同于基线。我们还通过Grad-CAM [55]可视化我们的3D策略模块，如图7所示。我们使用来自感知变换器后3D卷积层的梯度和3D特征图来计算Grad-CAM。我们观察到，目标物体明显受到我们的策略关注，尽管训练信号仅为单个体素的Q值。

# 5 结论与局限性

在本工作中，我们提出了GNFactor，一种用于现实世界多任务机器人操控的视觉行为克隆智能体。GNFactor利用可推广的神经特征场（GNF）学习3D体积表示，该表示也用于动作预测模块。我们采用基础模型Stable Diffusion的视觉语言特征以及RGB特征来监督GNF的训练，并观察到由GNF增强的体积表示对决策制定有所帮助。GNFactor在仿真和现实世界中均取得了优异的结果，涵盖10个RLBench任务和3个真实机器人任务，展示了GNFactor在现实场景中的潜力。GNFactor的一个主要限制是对GNF训练所需的多个视角，这在现实世界中难以扩展。目前，我们使用三台固定摄像头来支持GNFactor，但探索使用手机随机收集摄像头视角将是有趣的，其中摄像头姿态的估计将是一大挑战。致谢。本研究得到亚马逊研究奖、思科教师奖和高通的资助支持。

# References

[1] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog, J. Hsu, t al. Rt-1: Robotics transformer for real-world control at scale. arXiv, 2022.   
[2] E. Jang, A. Irpan, M. Khansari, D. Kappler, F. Ebert, C. Lynch, S. Levine, and C. Finn. Bc-z: Zero-shot task generalization with robotic imitation learning. In CoRL, 2022.   
[3] M. Shridhar, L. Manuelli, and D. Fox. Perceiver-actor: A multi-task transformer for robotic manipulation. In CoRL, 2023.   
[4] M. Dalal, A. Mandlekar, C. Garrett, A. Handa, R. Salakhutdinov, and D. Fox. Imitating task and motion planning with visuomotor transformers. arXiv, 2023.   
[5] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022.   
[6] S. Parisi, A. Rajeswaran, S. Purushwalkam, and A. Gupta. The unsurprising effectiveness of pre-trained vision models for control. In ICML, 2022.   
[7] S. Nair, A. Rajeswaran, V. Kumar, C. Finn, and A. Gupta. R3m: A universal visual representation for robot manipulation. arXiv, 2022.   
[8] I. Radosavovic, T. Xiao, S. James, P. Abbeel, J. Malik, and T. Darrell. Real-world robot learning with masked visual pre-training. In CoRL, 2023.   
[9] M. Laskin, A. Srinivas, and P. Abbeel. Curl: Contrastive unsupervised representations for reinforcement learning. In ICML, 2020.   
[10] N. Hansen and X. Wang. Generalization in reinforcement learning by soft data augmentation. In ICRA, 2021.   
[11] Y. Ze, N. Hansen, Y. Chen, M. Jain, and X. Wang. Visual reinforcement learning with selfsupervised 3d representations. RA-L, 2023.   
[12] D. Driess, I. Schubert, P. Florence, Y. Li, and M. Toussaint. Reinforcement learning with neural radiance fields. NeurIPS, 2022.   
[13] A. Jaegle, F. Gimeno, A. Brock, O. Vinyals, A. Zisserman, and J. Carreira. Perceiver: General perception with iterative attention. In ICML, 2021.   
[14] S. James, Z. Ma, D. R. Arrojo, and A. J. Davison. Rlbench: The robot learning benchmark & learning environment. RA-L, 2020.   
[15] R. Rahmatizadeh, P. Abolghasemi, L. Bölöni, and S. Levine. Vision-based multi-task manipulation for inexpensive robots using end-to-end learning from demonstration. In 2018 IEEE international conference on robotics and automation (ICRA), pages 37583765. IEEE, 2018.   
[16] M. Shridhar, L. Manuelli, and D. Fox. Cliport: What and where pathways for robotic manipulation. In CoRL, 2022.   
[17] D. Kalashnikov, A. Irpan, P. Pastor, J. Ibarz, A. Herzog, E. Jang, D. Quillen, E. Holly, M. Kalakrishnan, V. Vanhoucke, et al. Qt-opt: Scalable deep reinforcement learning for visionbased robotic manipulation. arXiv, 2018.   
[18] T. Yu, D. Quillen, Z. He, R. Julian, K. Hausman, C. Finn, and S. Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. In Conference on robot learning, pages 10941100. PMLR, 2020.   
[19] R. Yang, H. Xu, Y. Wu, and X. Wang. Multi-task reinforcement learning with soft modularization. Advances in Neural Information Processing Systems, 33:47674777, 2020.   
[20] S. Song, A. Zeng, J. Lee, and T. Funkhouser. Grasping in the wild: Learning 6dof closedloop grasping from low-cost demonstrations. IEEE Robotics and Automation Letters, 5(3): 49784985, 2020.   
[21] A. Murali, A. Mousavian, C. Eppner, C. Paxton, and D. Fox. 6-dof grasping for target-driven object manipulation in clutter. In 2020 IEEE International Conference on Robotics and Automation (ICRA), pages 62326238. IEEE, 2020.   
[22] A. Mousavian, C. Eppner, and D. Fox. 6-dof graspnet: Variational grasp generation for object manipulation. In ICCV, 2019.   
[23] Z. Xu, Z. He, and S. Song. Universal manipulation policy network for articulated objects. RA-L, 2022.   
[24] Y. Li, S. Agrawal, J.-S. Liu, S. K. Feiner, and S. Song. Scene editing as teleoperation: A case study in 6dof kit assembly. In IROS, 2022.   
[ N. Hane, Z.Yuan, Y.Ze, T. Mu, A. Rajea, H. Su, H. Xu, a X. Wag.On efor visuo-motor control: Revisiting a learning-from-scratch baseline. In ICML, 2022.   
[26] D. Shim, S. Lee, and H. J. Kim. Snerl: Semantic-aware neural radiance fields for reinforcement learning. ICML, 2023.   
[27] Y. Chen, S. Liu, and X. Wang. Learning continuous image representation with local implicit image function. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 86288638, 2021.   
[28] L. Mescheder, M. Oechsle, M. Niemeyer, S. Nowozin, and A. Geiger. Occupancy networks: Learning 3d reconstruction in function space. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 44604470, 2019.   
[29] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021.   
[30] M. Niemeyer, L. Mescheder, M. Oechsle, and A. Geiger. Occupancy flow: 4d reconstruction by learning particle dynamics. In Proceedings of the IEEE/CVF international conference on computer vision, pages 53795389, 2019.   
[31] J. J. Park, P. Florence, J. Straub, R. Newcombe, and S. Lovegrove. Deepsdf: Learning continuous signed distance functions for shape representation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 165174, 2019.   
[32] V. Sitzmann, M. Zollhöfer, and G. Wetzstein. Scene representation networks: Continuous 3d-structure-aware neural scene representations. Advances in Neural Information Processing Systems, 32, 2019.   
[33] Z. Jiang, Y. Zhu, M. Svetlik, K. Fang, and Y. Zhu. Synergies between affordance and geometry: 6-dof grasp detection via implicit representations. arXiv preprint arXiv:2104.01542, 2021.   
[34] Y.-C. Lin, P. Florence, A. Zeng, J. T. Barron, Y. Du, W.-C. Ma, A. Simeonov, A. R. Garcia, .IMM boc I RoboL pages 19161927. PMLR, 2023.

[35] Y. Li, S. Li, V. Sitzmann, P. Agrawal, and A. Torralba. 3d neural scene representations for visuomotor control. In Conference on Robot Learning, pages 112123. PMLR, 2022.

[36] Y. Chen and X. Wang. Transformers as meta-learners for implicit neural representations. In Computer VisionECCV 2022: 17th European Conference, Tel Aviv, Israel, October 2327, 2022, Proceedings, Part XVII, pages 170187. Springer, 2022.   
[37] W. Jang and L. Agapito. Codenerf: Disentangled neural radiance fields for object categories. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 12949 12958, 2021.   
[38] K.-E. Lin, Y.-C. Lin, W.-S. Lai, T.-Y. Lin, Y.-C. Shih, and R. Ramamoorthi. Vision transformer for nerf-based view synthesis from a single input image. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 806815, 2023.   
[39] J. Reizenstein, R. Shapovalov, P. Henzler, L. Sbordone, P. Labatut, and D. Novotny. Common objects in 3d: Large-scale learning and evaluation of real-life 3d category reconstruction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10901 10911, 2021.   
[40] K. Rematas, R. Martin-Brualla, and V. Ferrari. Sharf: Shape-conditioned radiance fields from a single view. arXiv preprint arXiv:2102.08860, 2021.   
[4 A. Trevithick and B. Yang.Gr: Learning a general radiance feld for 3d representation and rendering. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1518215192, 2021.   
[42] Q. Wang, Z. Wang, K. Genova, P. P. Srinivasan, H. Zhou, J. T. Barron, R. Martin-Brualla, N. Snavely, and T. Funkhouser. Ibrnet: Learning multi-view image-based rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 46904699, 2021.   
[43] A. Yu, V. Ye, M. Tancik, and A. Kanazawa. pixelnerf: Neural radiance fields from one or few images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 45784587, 2021.   
[44] V. Tschernezki, I. Laina, D. Larlus, and A. Vedaldi. Neural feature fusion fields: 3d distillation of self-supervised 2d image representations. arXiv, 2022.   
[45] S. Kobayashi, E. Matsumoto, and V. Sitzmann. Decomposing nerf for editing via feature field distillation. NeurIPS, 2022.   
[46] J. Ye, N. Wang, and X. Wang. Featurenerf: Learning generalizable nerfs by distilling foundation models. arXiv, 2023.   
[47] J. Kerr, C. M. Kim, K. Goldberg, A. Kanazawa, and M. Tancik. Lerf: Language embedded radiance fields. arXiv preprint arXiv:2303.09553, 2023.   
[48] K. M. Jatavallabhula, A. Kuwajerwala, Q. Gu, M. Omama, T. Chen, S. Li, G. Iyer, S. Saryazdi, N. Keetha, A. Tewari, et al. Conceptfusion: Open-set multimodal 3d mapping. arXiv preprint arXiv:2302.07241, 2023.   
[. Bis, FMio J.J.C, L. Ott a R. Sar. Neaplsfeature fields. arXiv preprint arXiv:2303.10962, 2023.   
[50] M. Caron, H. Touvron, I. Misra, H. Jégou, J. Mairal, P. Bojaoski, and A. Jouli. Eme properties in self-supervised vision transformers. In ICCV, 2021.   
[51] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021.   
[52] S. James, K. Wada, T. Laidlow, and A. J. Davison. Coarse-to-fine q-attention: Efficient learning for visual robotic manipulation via discretisation. In CVPR, 2022.   
[53] S. Klemm, J. Oberländer, A. Hermann, A. Roennau, T. Schamm, J. M. Zollner, and R. Dillmann. Rrt\*-connect: Faster, asymptotically optimal motion planning. In 2015 IEEE international conference on robotics and biomimetics (ROBIO), pages 16701677. IEEE, 2015.   
[54] H. Lin, S. Peng, Z. Xu, Y. Yan, Q. Shuai, H. Bao, and X. Zhou. Efficient neural radiance fields for interactive free-viewpoint video. In SIGGRAPH Asia, 2022.   
[55] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE international conference on computer vision, pages 618626, 2017.   
[. You, J. Li S. Rei, J. Hse, S. Kuar, S. Bhojanapali, X. Song, J.Deme, K. Kzr, and C.-J. Hsieh. Large batch optimization for deep learning: Training bert in 76 minutes. arXiv, 2019.

# A Visualizations

![](images/5.jpg)  
Figure 5: View synthesis of GNFactor in the real world. PSNR is computed for quantitative evaluation. Ta  he  i ai    u  T rendering is mainly because, in inference, we do not optimize per-step for rendering but just perform one feedforward to obtain the feature.

![](images/6.jpg)  
Figure 6: More novel view synthesis results. Both RGB and features are synthesized. We remove the action loss here for a better rendering quality. Videos are available on yanjieze.com/GNFactor.

![](images/7.jpg)  
Figure 7: Visualize the 3D policy module by Grad-CAM [55]. Though the supervision signal is only the $\mathrm { Q }$ valu or a single voxel during the trainig proces, we observe in visualizations that he target object ae clearly attended by our policy. Videos are available on yanjieze.com/GNFactor.

# B Task Descriptions

Simulated tasks. We select 10 language-conditioned tasks from RLBench [14], all of which involve at least 2 variations. See Table 5 for an overview. Our task variations include randomly sampled colors, sizes, counts, placements, and categories of objects, totaling 166 different variations. The set of colors have 20 instances: red, maroon, lime, green, blue, navy, yellow, cyan, magenta, silver, gray, orange, olive, purple, teal, azure, violet, rose, black, and white. The set of sizes includes 2 types: short and tall. The set of counts has 3 instances: 1, 2, 3. The placements and object categories are specific to each task. For example, open drawer has 3 placement locations: top, middle and bottom. In addition to these semantic variations, objects are placed on the tabletop at random poses within a limited range.

Table 5: Language-conditioned tasks in RLBench [14].   

<table><tr><td>Task</td><td>Variation Type</td><td># of Variations</td><td>Avg. Keyframs</td><td></td><td>Language Template</td></tr><tr><td>close jar</td><td></td><td>color</td><td>20</td><td>6.0</td><td>&quot;close the — jar&quot;</td></tr><tr><td>open drawer</td><td></td><td>placement</td><td>3</td><td>3.0</td><td>&quot;open the — drawer</td></tr><tr><td></td><td>sweep to dustpan</td><td>size</td><td>2</td><td>4.6</td><td>&quot;sweep dirt to the — dustpan&quot;</td></tr><tr><td>meat off grill</td><td></td><td>category</td><td>2</td><td>5.0</td><td>&quot;take the — off the grill&quot;</td></tr><tr><td>turn tap</td><td></td><td>placement</td><td>2</td><td>2.0</td><td>&quot;turn — tap&quot;</td></tr><tr><td>slide block</td><td></td><td>color</td><td>4</td><td>4.7</td><td>&quot;slide the block to — target&quot;</td></tr><tr><td></td><td>put in drawer</td><td>placement</td><td>3</td><td>12.0</td><td>&quot;put the item in the — drawer&quot;</td></tr><tr><td>drag stick</td><td></td><td>color</td><td>20</td><td>6.0</td><td>&quot;use the stick to drag the cube onto the — — target&quot;</td></tr><tr><td>push buttons</td><td></td><td>color</td><td>50</td><td>3.8</td><td>&quot;push the — button, [then the — button]&quot;</td></tr><tr><td></td><td>stack blocks</td><td>color, count</td><td>60</td><td>14.6</td><td>stack— - blocks&quot;</td></tr></table>

Generalization tasks in simulation. We design 6 additional tasks where the scene is changed based on the original training environment, to test the generalization ability of GNFactor. Table 6 gives an overview of these tasks. Videos are also available on yanjieze.com/GNFactor.

Table 6: Generalization tasks based on RLBench.   

<table><tr><td>Task</td><td>Base</td><td>Change</td></tr><tr><td>drag (D)</td><td>drag stick</td><td>add two colorful buttons on the table</td></tr><tr><td>slide (L)</td><td>slide block</td><td>change the block size to a larger one</td></tr><tr><td>slide (S)</td><td>slide block</td><td>change the block size to a smaller one</td></tr><tr><td>open (n)</td><td>open drawer</td><td>change the position of the drawer</td></tr><tr><td>turn (N)</td><td>turn tap</td><td>change the position of the tap</td></tr><tr><td>push (D)</td><td>push buttons</td><td>add two colorful jar on the table</td></tr></table>

Real robot tasks. In the experiments, we perform three tasks along with three additional tasks where distracting objects are present. The door task requires the agent to open the door on an mircowave, a task which poses challenges due to the precise coordination required. The faucet task requires the agent to rotate the faucet back to center position, which involves intricate motor control. Lastly, the teapot task requires the agent to locate the randomly placed teapot in the kitchen and move it on top of the stove with the correct pose. Among the three, the teapot task is considered the most challenging due to the random placement and the need for accurate location and rotation of the gripper. All 6 tasks are set up in two different kitchens, as visualized in Figure 8. The keyframes used in real robot tasks are given in Figure 9.

# C Implementation Details

Voxel encoder. We use a lightweight 3D UNet (only 0.3M parameters) to encode the input voxel $1 0 0 ^ { 3 } \times 1 0$ (RGB features, coordinates, indices, and occupancy) into our deep 3D volumetric representation of size $1 0 0 ^ { 3 } \times 1 2 8$ Due to the cluttered output from directly printing the network, we provide the PyTorch-Style pseudo-code for the forward process as follows. For each block, we use a cascading of one Convolutional Layer, one BatchNorm Layer, and one LeakyReLU layer, which is common practice in the vision community.

![](images/8.jpg)  
Figure 8: Kitchens. We give a closer view of our two kitchens for real robot experiments. The figures are captured in almost the same position to display the size difference between the two.

![](images/9.jpg)  
Figure 9: Keyframes for real robot tasks. We give the keyframes used in our 3 real robot tasks across 2 kitchens.

def forward(self, x): convO $=$ self.conv0(x) # 100^3x8 conv2 $=$ self.conv2(self.conv1(conv0)) # 50^3x16 conv4 $=$ self.conv4(self.conv3(conv2)) # 25^3x32

$\mathrm { ~ \tt ~ x ~ } =$ self.conv6(self.conv5(conv4)) # 13^3x64   
$\mathrm { ~ \tt ~ x ~ } =$ conv4 $^ +$ self.conv7(x) # 25^3x32   
$\mathrm { ~ \tt ~ x ~ } =$ conv2 $^ +$ self.conv9(x) # 50^3x16   
$\mathrm { ~ \tt ~ x ~ } =$ self.conv_out(convO $^ +$ self.conv11(x)) # 100^3x128   
return x

Generalizable Neural Field (GNF). The overall network architecture of our GNF is close to the original NeRF [29] implementation. We use the same positional encoding as NeRF and the encoding function is formally

$$
\gamma ( p ) = \left( \sin \left( 2 ^ { 0 } \pi p \right) , \cos \left( 2 ^ { 0 } \pi p \right) , \cdots , \sin \left( 2 ^ { L - 1 } \pi p \right) , \cos \left( 2 ^ { L - 1 } \pi p \right) \right) .
$$

This function is applied to each of the three coordinate values and we set $L = 6$ in our experiments. The overal position encoding is then 36-dimensional. The input of GNF is thus a concatenation of the original coordinates $( \mathbb { R } ^ { 3 } )$ , the position encoding $( \mathbb { R } ^ { 3 6 } )$ , the view directions $( \mathbb { R } ^ { 3 } )$ , and the voxel feature $( \mathbb { R } ^ { 1 2 8 } )$ , totaling 170 dimensions. Our GNF mainly consists of 5 ResnetFCBlocks, in which a skip connection is used. The input feature is first projected to 512 with a linear layer and fed into these blocks, and then projected to the output dimension 516 (RGB, density, and Diffusion feature) with a cascading of one ReLU function and one linear layer. We provide the PyTorch-Style pseudo-code for the networks as follows.

GNF Linear(in_features $\mathtt { = 1 7 0 }$ , out_features $\mathtt { \mathtt { = 5 1 2 } }$ , bias $=$ True), (0-4): 5 x ResnetFCBlocks( (fc_O): Linear(in_features $= 5 1 2$ , out_features $\mathtt { = 5 1 2 }$ , bias $\ c =$ True) (fc_1): Linear(in_features $= 5 1 2$ , out_features $\mathtt { \_ { 1 2 } }$ , bias $\ c =$ True) (activation): ReLU() ), ReLU(), Linear(in_features $\mathtt { = 5 1 2 }$ , out_features $\mathtt { = 5 1 6 }$ , bias $=$ True)   
)

Percevier Transformer. Our usage of Percevier Transformer is close to PerAct [3]. We use 6 attention blocks to process the sequence from multi-modalities (3D volume, language token, and robot proprioception) and output a sequence also. The usage of Perceiver Transformer enables us to process the long sequence with computational effciency, by only utilizing a small set of latents to attend the input. The output sequence is then reshaped back to a voxel to predict the robot action. The Q-function for translation is predicted by a 3D convolutional layer, and for the prediction of openness, collision avoidance, and rotation, we use global max pooling and spatial softmax operation to aggregate 3D volume features and project the resulting feature to the output dimension with a multi-layer perception. We could clarify that the design for the policy module is not our contribution; for more details please refer to PerAct [3] and its official implementation on https://github.com/peract/peract.

# D Demonstration Collection for Real Robot Tasks

For the collection of real robot demonstrations, we utilize the HTC VIVE controller and basestation to track the 6-DOF poses of human hand movements. We then use triad-openvr package (https://github.com/TriadSemi/triad_openvr) to employ SteamVR and accurately map human operations onto the xArm robot, enabling it to interact with objects in the real kitchen.

We record the real-time pose of xArm and $6 4 0 \times 4 8 0$ RGB-D observations with the pyrealsense2 (https://pypi.org/project/pyrealsense2/). Though the image size is different from our simulation setup, we use the same shape of the input voxel, thus ensuring the same algorithm is used across the simulation and the real world. The downscaled images $( 8 0 \times 6 0 )$ are used for neural rendering.

# E Detailed Data

Besides reporting the final success rates in our main paper, we give the success rates for the best single checkpoint (i.e., evaluating all saved checkpoints and selecting the one with the highest success rates), as shown in Table 7. Under this setting GNFactor outperforms PerAct with a larger margin. However, we do not use the best checkpoint in the main results for fairness.

We also give the detailed number of success in Table 8 for reference in addition to the success rates computed in Table 2.

Table 7:Multi-task test results on RLBench. We report the success rates for the best single checkpoint fo reference. We could observe GNFactor surpasses PerAct by a large margin.   

<table><tr><td>Method / Task</td><td>close jar</td><td>open drawer</td><td>sweep to dustpan</td><td>meat off grill</td><td>turn tap</td><td>Average</td></tr><tr><td>PerAct</td><td>22.7±5.0</td><td>62.7±13.2</td><td>0.0±0.0</td><td>46.7±14.7</td><td>36.0±9.8</td><td></td></tr><tr><td>GNFactor</td><td>40.0±5.7</td><td>77.3±7.5</td><td>40.0±11.8</td><td>66.7±8.2</td><td>45.3±3.8</td><td></td></tr><tr><td>Method / Task</td><td>slide block</td><td>put in drawer</td><td>drag stick</td><td>push buttons</td><td>stack blocks</td><td></td></tr><tr><td>PerAct</td><td>22.7±6.8</td><td>9.3±5.0</td><td>12.0±6.5</td><td>18.7±6.8</td><td>5.3±1.9</td><td>23.6</td></tr><tr><td>GNFactor</td><td>18.7±10.5</td><td>10.7±12.4</td><td>73.3±13.6</td><td>20.0±3.3</td><td>8.0±0.0</td><td>40.0</td></tr></table>

Table 8: Detailed data for generalization to novel tasks. We evaluate 20 episodes, each across 3 seeds, for the final checkpoint and report the number of successful trajectories here.   

<table><tr><td>Generalization</td><td>PerAct</td><td>GNFactor w/o. Diffusion</td><td>GNFactor</td></tr><tr><td>drag (D)</td><td>2,0, 2</td><td>15, 2, 5</td><td>18, 5, 5</td></tr><tr><td>slide (L)</td><td>6, 6, 8</td><td>1, 10, 10</td><td>6, 5, 4</td></tr><tr><td>slide (S)</td><td>0,2,1</td><td>6,1,5</td><td>0, 3, 1</td></tr><tr><td>push (D)</td><td>6, 3, 3</td><td>4,4,5</td><td>7,6, 6</td></tr><tr><td>open (N)</td><td>6,2,7</td><td>5,2,9</td><td>8, 5, 6</td></tr><tr><td>turn (N)</td><td>4,5,2</td><td>2,7,2</td><td>6, 6, 5</td></tr></table>

# F Stronger Baseline

To make the comparison between our GNFactor and PerAct fairer, we enhance Peract's input by using 4 camera views, as visualized in Figure 10. These views ensure that the scene is fully covered. It is observed in our experiment results (Table 1) that GNFactor which takes the single view as input still outperforms PerAct with more views.

# G Hyperparameters

We give the hyperparameters used in GNFactor as shown in Table 9. For the GNF training, we use a ray batch size $b _ { \mathrm { r a y } } = 5 1 2$ , corresponding to 512 pixels to reconstruct, and use $\lambda _ { \mathrm { f e a t } } = 0 . 0 1$ and $\lambda _ { \mathrm { r e c o n } } = 0 . 0 1$ to maintain major focus on the action prediction. For real-world experiment, we set the weight of the reconstruction loss to 1.0 and the weight of action loss to 0.1. This choice was based on our observation that reducing the weight of the action loss and increasing the weight of the reconstruction loss did not significantly affect convergence but did help prevent overfitting to a limited number of real-world demonstrations. We uniformly sample 64 points along the ray for the "coarse" network and sample 32 points with depth-guided sampling and 32 points with uniform sampling for the "fine" network.

![](images/10.jpg)  
Figure 10: Visualization of 4 cameras used for the stronger PerAct baseline. To enhance the PerAct baseline, we add more views as the input of PerAct. These views are pre-defined in RLBench, making sure the observation covers the entire scene.

Table 9: Hyperparameters used in GNFactor.   

<table><tr><td>Variable Name</td><td>Value</td></tr><tr><td>training iteration</td><td>100k</td></tr><tr><td>image size</td><td>128 × 128 × 3</td></tr><tr><td>input voxel size</td><td>100 × 100 × 100</td></tr><tr><td>batch size</td><td>2</td></tr><tr><td>optimizer</td><td>LAMB [56]</td></tr><tr><td>learning rate</td><td>0.0005</td></tr><tr><td>ray batch size bray</td><td>512</td></tr><tr><td>weight for reconstruction loss λrecon</td><td>0.01</td></tr><tr><td>weight for embedding loss λfeat</td><td>0.01</td></tr><tr><td>number of transformer blocks</td><td>6</td></tr><tr><td>number of sampled points for GNF</td><td>64</td></tr><tr><td>number of latents in Perceiver Transformer</td><td>2048</td></tr><tr><td>dimension of Stable Diffusion features</td><td>512</td></tr><tr><td>dimension of CLIP language features</td><td>512</td></tr><tr><td>hidden dimension of NeRF blocks</td><td>512</td></tr></table>