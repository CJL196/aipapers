# GNFactor: Multi-Task Real Robot Learning with Generalizable Neural Feature Fields

Yanjie $\mathbf { Z e ^ { 1 * } }$ Ge $\mathbf { Y a n ^ { 2 * } }$ Yueh-Hua $\mathbf { W _ { u } } ^ { 2 * }$ Annabella Macaluso2 Yuying $\mathbf { G e ^ { 3 } }$ Jianglong $\mathbf { Y e ^ { 2 } }$ Nicklas Hansen2 Li Erran $\mathbf { L i } ^ { 4 }$ Xiaolong Wang2 1Shanghai Jiao Tong University $^ 2 \mathrm { U C }$ San Diego 3University of Hong Kong 4AWS AI, Amazon \*Equal Contribution # yanjieze.com/GNFactor

Abstract: It is a long-standing problem in robotics to develop agents capable of executing diverse manipulation tasks from visual observations in unstructured real-world environments. To achieve this goal, the robot needs to have a comprehensive understanding of the 3D structure and semantics of the scene. In this work, we present GNFactor, a visual behavior cloning agent for multi-task robotic manipulation with Generalizable Neural feature Fields. GNFactor jointly optimizes a generalizable neural field (GNF) as a reconstruction module and a Perceiver Transformer as a decision-making module, leveraging a shared deep 3D voxel representation. To incorporate semantics in 3D, the reconstruction module utilizes a vision-language foundation model (e.g., Stable Diffusion) to distill rich semantic information into the deep 3D voxel. We evaluate GNFactor on 3 real robot tasks and perform detailed ablations on 10 RLBench tasks with a limited number of demonstrations. We observe a substantial improvement of GNFactor over current state-of-the-art methods in seen and unseen tasks, demonstrating the strong generalization ability of GNFactor. Keywords: Robotic Manipulation, Neural Radiance Field, Behavior Cloning

# 1 Introduction

One major goal of introducing learning into robotic manipulation is to enable the robot to effectively handle unseen objects and successfully tackle various tasks in new environments. In this paper, we focus on using imitation learning with a few demonstrations for multi-task manipulation. Using imitation learning helps avoid complex reward design and training can be directly conducted on the real robot without creating its digital twin in simulation [1, 2, 3, 4]. This enables policy learning on diverse tasks in complex environments, based on users' instructions (see Figure 1). However, working with a limited number of demonstrations presents great challenges in terms of generalization. Most of these challenges arise from the need to comprehend the 3D structure of the scene, understand the semantics and functionality of objects, and effectively follow task instructions based on visual cues. Therefore, a comprehensive and informative visual representation of the robot's observations serves as a crucial foundation for generalization.

The development of visual representation for robot learning has mainly focused on learning within a 2D plane. Self-supervised objectives are leveraged to pre-train the representation from the 2D image observation [6, 7, 8] or jointly optimized with the policy gradients [9, 10, 11]. While these approaches improve sample efficiency and lead to more robust policies, they are mostly applied to relatively simple manipulation tasks. To tackle more complex tasks requiring geometric understanding (e.g., object shape and pose) and with occlusions, 3D visual representation learning has been recently adopted with robot learning [11, 12]. For example, Driess et al. [12] train the 3D scene representation by using NeRF and view synthesis to provide supervision. While it shows effectiveness over tasks requiring geometric reasoning such as hanging a cup, it only handles the simple scene structure with heavy masking in a single-task setting. More importantly, without a semantic understanding of the scene, it would be very challenging for the robot to follow the user's language instructions.

![](images/1.jpg)  
Figure:Left:Three camera views used in the real robot setup to reconstruct the feature field generated by Stable Diffsion [].We segment the foreground feature or better illustration. Right:Three langageconditioned real robot tasks across two different kitchens.

In this paper, we introduce learning a language-conditioned policy using a novel representation leveraging both 3D and semantic information for multi-task manipulation. We train Generalizable Neural Feature Fields (GNF) which distills pre-trained semantic features from 2D foundation models into the Neural Radiance Fields (NeRFs). We conduct policy learning upon this representation, leading to our model GNFactor. It is important to note that GNFactor learns an encoder to extract scene features in a feed-forward manner, instead of performing per-scene optimization in NeRF. Given a single RGB-D image observation, our model encodes it into a 3D semantic volumetric feature, which is then processed by a Perceiver Transformer [13] architecture for action prediction. To conduct multi-task learning, the Perceiver Transformer takes in language instructions to get task embedding, and reason the relations between the language and visual semantics for manipulation. There are two branches of training in our framework (see Figure 3): (i) GNF training. Given the collected demonstrations, we train the Generalizable Neural Feature Fields using view synthesis with volumetric rendering. Besides rendering the RGB pixels, we also render the features of the foundation models in 2D space. The GNF learns from both pixel and feature reconstruction at the same time. To provide supervision for feature reconstruction, we apply a vision foundation model (e.g., pre-trained Stable Diffusion model [5]) to extract the 2D feature from the input view as the ground truth. In this way, we can distill the semantic features into the 3D space in GNF. (ii) GNFactor joint training. Building on the 3D volumetric feature jointly optimized by the learning objectives of GNF, we conduct behavior cloning to train the whole model end-to-end. For evaluation, we conduct real-robot experiments on three distinct tasks across two different kitchens (see Figure 1). We successfully train a single policy that effectively addresses these tasks in different scenes, yielding significant improvements over the baseline method PerAct [3]. We also conduct comprehensive evaluations using 10 RLBench simulated tasks [14] and 6 designed generalization tasks. We observe that GNFactor outperforms PerAct with an average improvement of $1 . 5 5 \mathrm { x }$ and $1 . 5 7 \mathrm { x }$ , consistent with the significant margin observed in the real-robot experiments.

# 2 Related Work

Multi-Task Robotic Manipulation. Recent works in multi-task robotic manipulation have led to significant progress in the execution of complex tasks and the ability to generalize to new scenarios [15, 2, 1, 16, 17, 3, 18, 19]. Notable methods often involve the use of extensive interaction data to train multi-task models [2, 1, 16, 17]. For example, RT-1 [1] underscores the benefits of taskagnostic training, demonstrating superior performance in real-world robotic tasks across a variety of datasets. To reduce the need for extensive demonstrations, methods that utilize keyframes  which encode the initiation of movement  have proven to be effective [20, 21, 22, 23, 24]. PerAct [3] employs the Perceiver Transformer [13] to encode language goals and voxel observations and shows its effectiveness in real robot experiments. In this work, we utilize the same action prediction framework as PerAct while we focus on improving the generalization ability of this framework by learning a generalizable volumetric representation under limited data.

![](images/2.jpg)  

Figure 2: Simulation environments and the real robot setup. We show the RGB observations for our 10 RLBench tasks in Figure (a), the sampled views for GNF in Figure (b), and the real robot setup in Figure ().

3D Representations for Reinforcement/Imitation Learning (RL/IL). To improve manipulation policies by leveraging visual information, numerous studies have concentrated on enhancing 2D visual representations [8, 7, 6, 25], while for addressing more complex tasks, the utilization of 3D representations becomes crucial. Ze et al. [11] incorporates a deep voxel-based 3D autoencoder in motor control, demonstrating improved sample efficiency compared to 2D representation learning methods. Driess et al. [12] proposes to first learn a state representation by NeRF and then use the frozen state for downstream RL tasks. While this work shows the initial success of utilizing NeRF in RL, its applicability in real-world scenarios is constrained due to various limitations: e.g., the requirement of object masks, the absence of a robot arm, and the lack of scene structure. The work closest to ours is SNeRL [26], which also utilizes a vision foundation model in NeRF. However, similar to NeRF-RL [12], SNeRL masks the scene structure to ensure functionality and the requirement for object masks persists, posing challenges for its application in real robot scenarios. Our proposed GNFactor, instead, handles challenging muti-task real-world scenarios, demonstrating the potential for real robot applications.

Neural Radiance Fields (NeRFs). Neural fields have achieved great success in novel view synthesis and scene representation learning these years [27, 28, 29, 30, 31, 32], and recent works also start to incorporate neural fields into robotics [33, 34, 35, 12, 26]. NeRF [29] stands out for achieving photorealistic view synthesis by learning an implicit function of the scene, while it requires per-scene optimization and is thus hard to generalize. Many following methods [36, 37, 38, 39, 40, 41, 42] propose more generalizable NeRFs. PixelNeRF [43] and CodeNeRF [37] encode 2D images as the input of NeRFs, while TransINR [36] leverages a vision transformer to directly infer NeRF parameters. A line of recent works [44, 45, 46, 47, 48, 49] utilize pre-trained vision foundation models such as DINO [50] and CLIP [51] as supervision besides the RGB image, which thus enables the NeRF to learn generalizable features. In this work, we incorporate generalizable NeRF to reconstruct different views in RGB and embeddings from a pretrained Stable Diffusion model [5].

# 3 Method

In this section, we detail the proposed GNFactor, a multi-task agent with a 3D volumetric representation for real-world robotic manipulation. GNFactor is composed of a volumetric rendering module and a 3D policy module, sharing the same deep volumetric representation. The volumetric rendering module learns a Generalizable Neural Feature Field (GNF), to reconstruct the RGB image from cameras and the embedding from a vision-language foundation model, e.g., Stable Diffusion [5]. The task-agnostic nature of the vision-language embedding enables the volumetric representation to learn generalizable features via neural rendering and thus helps the 3D policy module better handle

![](images/3.jpg)  

Figure 3: Overview of GNFactor. GNFactor takes an RGB-D image as input and encodes it using a voxel encoder to transform it into a feature in deep 3D volume.This volume is then shared by two modules:volumetric rendering (Renderer) and robot action prediction (Perceiver). These two modules are jointly trained, which optimizes the shared features to not only reconstruct vision-language embeddings (Diffusion Feature) and other views (RGB), but also to estimate accurate $\mathrm { Q }$ values $Q _ { \mathrm { t r a n s } }$ , $Q _ { \mathrm { r o t } }$ $Q _ { \mathrm { c o l l i d e } }$ , $Q _ { \mathrm { o p e n . } }$ . multi-task robotic manipulation. The task description is encoded with CLIP [51] to obtain the task embedding $T$ An overview of GNFactor is shown in Figure 3.

# 3.1 Problem Definition

To effectively address complex real-world robotic problems, we structure the observation space as a 3D voxel space $\mathcal { O } \in \mathbb { R } ^ { 1 0 0 ^ { 3 } \times 3 }$ , as oposed to the commonly used 2D images [1, 2, 7, 8]. The 3D voxel observation originates from an RGB-D image captured by a single front camera with known extrinsic and intrinsic parameters, ensuring our method's practical applicability in the real world. In addition to the front camera view used for policy training, we also gather additional $k$ views for training the GNF. We collect only RGB images for these additional views instead of RGB-D images. In real-world scenarios, we use $k = 2$ , while in simulated environments, we set $k = 1 9$ .

The action of the robot arm with a gripper is represented by translation $a _ { \mathrm { t r a n s } } \in \mathbb { R } ^ { 3 }$ , rotation $a _ { \mathrm { r o t } } \in$ $\mathbb { R } ^ { ( 3 6 0 / 5 ) \times 3 }$ , gripper openness $a _ { \mathrm { o p e n } } \in [ 0 , 1 ]$ collision avoidance $a _ { \mathrm { c o l l i s i o n } } \in [ 0 , 1 ]$ For the rotation $a _ { \mathrm { r o t } }$ ,ea rotation axis is discetized no $R = 5$ bins. The collisio avoidanc parameter colsion instructs the motion planner regarding the necessity to avoid collisions, which is crucial as our tasks encompasses both contact-based and non-contact-based motions. Due to the inefficiency of continuous action prediction and the extensive data requirements that come with it, we reformulate the behavior cloning problem as a keyframe-prediction problem [3, 52]. We first extract keyframes from expert demonstrations using the following metric: a frame in the trajectory is a keyframe when joint velocities approach zero and the gripper's open state remains constant. The model is then trained to predict the subsequent keyframe based on current observations. This formulation effectively transforms the continuous control problem into a discretized keyframeprediction problem, delegating the internal procedures to the RRT-connect motion planner [53] in simulation and Linear motion planner in real-world xArm7 robot.

# 3.2 Learning Volumetric Representations with Generalizable Neural Feature Fields

In our initial step, we transform the RGB-D image into a $1 0 0 ^ { 3 }$ voxel. Then the 3D voxel encoder encodes this 3D voxel and outputs our volumetric representation $v \in \mathbb { R } ^ { 1 0 0 ^ { 3 } \times 1 2 8 }$ . To enhance the volumetric representation $v$ with structural knowledge and language semantics, we learn a Generalizable Neural Feature Field (GNF) that takes the deep volume $v$ as the scene representation and the model is learned by reconstructing the additional views and the features predicted by a 2D visionlanguage foundation model [5]. The entire neural rendering process is described as follows.

We denote $v _ { \mathbf { x } } \in \mathbb { R } ^ { 1 2 8 }$ as the sampled 3D feature for the 3D point $\mathbf { x }$ using the volumetric representation $v$ . $v _ { \mathbf { x } }$ is formed with trilinear interpolation due to the discretized nature of the volume $v$ Our GNF primarily consists of three functions: (i) one density function $\sigma ( \mathbf { x } , v _ { \mathbf { x } } ) : \mathbb { R } ^ { 3 + 1 2 8 } \mapsto \mathbb { R } _ { + }$ that maps the 3D point $\mathbf { x }$ and the 3D feature $v _ { \mathbf { x } }$ to the density $\sigma$ , (ii) one RGB function $\mathbf { c } ( \mathbf { x } , \mathbf { d } , v _ { \mathbf { x } } ) :$ $\mathbb { R } ^ { 3 + 3 + 1 2 8 } \mapsto \mathbb { R } ^ { 3 }$ that maps the 3D point $\mathbf { x }$ , the view direction $\mathbf { d }$ and the 3D feature $v _ { \mathbf { x } }$ to color, and (iii) one vision-language embedding function $\mathbf { f } ( \mathbf { x } , \mathbf { d } , v _ { \mathbf { x } } ) : \mathbb { R } ^ { 3 + 3 + 1 2 8 } \mapsto \mathbb { R } ^ { 5 1 2 }$ that maps the 3D point $\mathbf { x }$ , the view direction $\mathbf { d }$ , and the 3D feature $v _ { \mathbf { x } }$ to the vision-language embedding. In Figure 3, the corresponding components of thee three functions are illustrated. Given a pixel's camea ry $\mathbf { r } ( t ) = \mathbf { o } + t \mathbf { d }$ , which is defined by the camera origin $o \in \mathbb { R } ^ { 3 }$ , view direction $\mathbf { d }$ and depth $t$ with bounds $[ t _ { n } , t _ { f } ]$ , the estimated color and embedding of the ray can be calculated by:

$$
\begin{array} { l } { { \displaystyle { \hat { \mathbf { C } } } ( \mathbf { r } , v ) = \int _ { t _ { n } } ^ { t _ { f } } T ( t ) \sigma ( \mathbf { r } ( t ) , v _ { \mathbf { x } ( t ) } ) \mathbf { c } ( \mathbf { r } ( t ) , \mathbf { d } , v _ { \mathbf { x } ( t ) } ) \mathrm { d } t } , } \\ { { \displaystyle { \hat { \mathbf { F } } } ( \mathbf { r } , v ) = \int _ { t _ { n } } ^ { t _ { f } } T ( t ) \sigma ( \mathbf { r } ( t ) , v _ { \mathbf { x } ( t ) } ) \mathbf { f } ( \mathbf { r } ( t ) , \mathbf { d } , v _ { \mathbf { x } ( t ) } ) \mathrm { d } t } , } \end{array}
$$

where $\begin{array} { r } { T ( t ) = \exp \left( - \int _ { t _ { n } } ^ { t } \sigma ( s ) \mathrm { d } s \right) } \end{array}$ T ipleation. Our  is the timize t enstuc heRGB m and e sn-an embedding from multiple views and diverse scenes by minimizing the following loss:

$$
{ \mathcal { L } } _ { \mathrm { r e c o n } } = \sum _ { \mathbf { r } \in { \mathcal { R } } } \| \mathbf { C } ( \mathbf { r } ) - { \hat { \mathbf { C } } } ( \mathbf { r } ) \| _ { 2 } ^ { 2 } + \lambda _ { \mathrm { f e a t } } \| \mathbf { F } ( \mathbf { r } ) - { \hat { \mathbf { F } } } ( \mathbf { r } ) \| _ { 2 } ^ { 2 } ,
$$

where $\mathbf { C } ( \mathbf { r } )$ is the ground truth color, $\mathbf { F } ( \mathbf { r } )$ is the ground truth vision-language embedding generated by Stable Diffusion, $\mathcal { R }$ is the set of rays generated from camera poses, and $\lambda _ { \mathrm { f e a t } }$ is the weight for the embedding reconstruction loss. For efficiency, we sample $b _ { \mathrm { r a y } }$ rays given one target view, instead of reconstructing the entire image. To help the GNF training, we use a coarse-to-fine hierarchical structure as the original NeRF [29] and apply depth-guided sampling [54] in the "fine" network.

# 3.3 Action Prediction with Volumetric Representations

The volumetric representation $v$ is optimized not only to achieve reconstruction of the GNF module, but also to predict the desired action for accomplishing manipulation tasks within the 3D policy. As such, we jointly train the representation $v$ to satisfy the objectives of both the GNF and the 3D policy module. In this section, we elaborate the training objective and the architecture of the 3D policy.

We employ a Perceiver Transformer [3] to handle the high-dimensional multi-modal input, i.e., the 3D volume, the robot's proprioception, and the language feature. We first condense the shared volumetric representation $v$ into a volume of size $2 0 ^ { 3 } \times 1 2 8$ using a 3D convolution layer with a kernel size and stride of 5, followed by a ReLU function, and flatten the 3D volume into a sequence of small cubes of size $8 0 0 0 \times 1 2 8$ . The robot's proprioception is projected into a 128-dimensional space and concatenated with the volume sequence for each cube, resulting in a sequence of size $8 0 0 0 \times 2 5 6$ . We then project the language token features from CLIP into the same dimensions $( 7 7 \times$ 256) and concatenate these features with a combination of the 3D volume, the robot's proprioception state, and the CLIP token embedding. The result is a sequence with dimensions of $8 0 7 7 \times 2 5 6$ .

This sequence is combined with a learnable positional encoding and passed through the Perceiver Transformer, which outputs a sequence of the same size. We remove the last 77 features for the ease of voxelization [3] and reshape the sequence back to a voxel of size $2 0 ^ { 3 } \times 2 5 6$ This voxel is then upscaled to $1 0 0 ^ { 3 } \times 1 2 8$ with trilinear interpolation and referred to as $v _ { \mathrm { P T } } . ~ v _ { \mathrm { P T } }$ is shared across three action prediction heads $Q \mathrm { { _ { o p e n } } }$ , $Q _ { \mathrm { t r a n s } }$ , $Q _ { \mathrm { r o t } }$ , $Q _ { \mathrm { c o l l i d e } }$ in Figure 3) to determine the final robot actions at the same scale as the observation space. To retain the learned features from GNF training, we create a skip connection between our volumetric representation $v$ and $v _ { \mathrm { P T } }$ . The combined volume feature $( v , v _ { \mathrm { P T } } )$ is used to predict a 3D Q-function $\mathcal { Q } _ { \mathrm { t r a n s } }$ for translation, as well as Q-functions for other robot operations like gripper openness $( \mathcal { Q } _ { \mathrm { o p e n } } )$ , rotation $( \mathcal { Q } _ { \mathrm { r o t } } )$ , and collision avoidance $( \mathcal { Q } _ { \mathrm { c o l l i d e } } )$ . The $\mathcal { Q }$ -function here represents the action values of one timestep, differing from the traditional $\mathcal { Q }$ -function in RL that is for multiple timesteps. For example, in each timestep, the 3D $\mathcal { Q } _ { \mathrm { t r a n s } }$ -value would be equal to 1 for the most possible next voxel and 0 for other voxels. The model then optimizes the cross-entropy loss like a classifier, where $\lambda _ { \mathrm { { r e c o n } } }$ is the weight for the reconstruction loss to balance the scale of different objectives. To train the GNFactor, we employ a joint training approach in which the GNF and 3D policy module are optimized jointly, without any pre-training. From our empirical observation, this approach allows for better fusion of information from the two modules when learning the shared features.

$$
\begin{array} { r } { \mathcal { L } _ { \mathrm { a c t i o n } } = - \mathbb { E } _ { Y _ { \mathrm { t r a s } } } \left[ \log \mathcal { V } _ { \mathrm { t r a n s } } \right] - \mathbb { E } _ { Y _ { \mathrm { r o t } } } \left[ \log \mathcal { V } _ { \mathrm { r o t } } \right] - \mathbb { E } _ { Y _ { \mathrm { o p e n } } } \left[ \log \mathcal { V } _ { \mathrm { o p e n } } \right] - \mathbb { E } _ { Y _ { \mathrm { c o l i d e } } } \left[ \log \mathcal { V } _ { \mathrm { c o l i d e } } \right] , } \end{array}
$$

![](images/4.jpg)  

Figure 4: Main experiment results. We present the average sucess rates in both the multi-task and generalization settings across RLBench tasks and real robot tasks.The error bar represents one standard deviation. The number in the bracket denotes the number of tasks. where $\mathcal { V } _ { i } = \operatorname { s o f t m a x } ( \mathcal { Q } _ { i } )$ for $\mathcal { Q } _ { i } \in [ \mathcal { Q } _ { \mathrm { t r a n s } } , \mathcal { Q } _ { \mathrm { o p e n } } , \mathcal { Q } _ { \mathrm { r o t } } , \mathcal { Q } _ { \mathrm { c o l l i d e } } ]$ and $Y _ { i } \in [ Y _ { \mathrm { t r a n s } } , Y _ { \mathrm { r o t } } , Y _ { \mathrm { o p e n } } , Y _ { \mathrm { c o l l i d e } } ]$ is the ground truth one-hot encoding. The overall learning objective for GNFactor is as follows:

$$
{ \mathcal { L } } _ { \mathrm { G N F a c t o r } } = { \mathcal { L } } _ { \mathrm { a c t i o n } } + \lambda _ { \mathrm { r e c o n } } { \mathcal { L } } _ { \mathrm { r e c o n } } ,
$$

# 4 Experiments

In this section, we conduct experiments to answer the following questions: (i) Can GNFactor surpass the baseline model in simulated environments? (i) Can GNFactor generalize to novel scenes in simulation? (ii) Does GNFactor learn a superior policy that handles real robot tasks in two different kitchens with noisy and limited real-world data? (iv) What are the crucial factors in GNFactor to ensure the functionality of the entire system? Our concluded results are given in Figure 4.

# 4.1 Experiment Setup

For the sake of reproducibility and benchmarking, we conduct our primary experiments in RLBench simulated tasks. Furthermore, to show the potential of GNFactor in the real world, we design a set of real robot experiments across two kitchens. We compare our GNFactor with the strong languageconditioned multi-task agent PerAct [3] in both simulation and the real world, emphasizing the universal functionality of GNFactor. Both GNFactor and PerAct use the single RGB-D image from the front camera as input to construct the voxel grid. In the multi-task simulation experiments, we also create a stronger version of PerAct by adding more camera views as input to fully cover the scene (visualized in Figure 10). Figure 2 shows our simulation tasks and the real robot setup. We briefly describe the tasks and details are left in Appendix B and Appendix C. Simulation. We select 10 challenging language-conditioned manipulation tasks from the RLBench tasksuites [14]. Each task has at least two variations, totaling 166 variations. These variations encompass several types, such as variations in shape and color. Therefore, to achieve high success rates with very limited demonstrations, the agent needs to learn generalizable knowledge about manipulation instead of merely overfitting to the given demonstrations. We use the RGB-D image of size $1 2 8 \times 1 2 8 \times 3$ from the single front camera as the observation. To train the GNF, we also add additional 19 camera views to provide RGB images as supervision. Real robot. We use the xArm7 robot with a parallel gripper in real robot experiments. We set up two toy kitchen environments to make the agent generalize manipulation skills across the scenes and designed three manipulation tasks, including open the microwave door, turn the faucet, and relocate the teapot, as shown in Figure 1. We set up three RealSense cameras around the robot. Among the three cameras, the front one captures the RGB-D observations for the policy training and the left/right one provides the RGB supervision for the GNF training. Expert Demonstrations. We collect 20 demonstrations for each RLBench task with the motion planner. The task variation is uniformly sampled. We collect 5 demonstrations for each real robot task using a VR controller. Details for collection remain in Appendix D. Generalization tasks. To further show the generalization ability of GNFactor, we design additional 6 simulated tasks and 3 real robot tasks based on the original training tasks and add task distractors. Training details. One agent is trained with two NVIDIA RTX3090 GPU for 2 days ( $1 0 0 \mathbf { k }$ iterations) with a batch size of 2. The shared voxel encoder of GNFactor is implemented as a lightweight 3D

Table 1: Multi-task test results on RLBench. We evaluate 25 episodes for each checkpoint on 10 tasks across 3 seeds and report the success rates $( \% )$ of the final checkpoints. Our method outperforms the most competitive baseline PerAct [3] with an average improvement of $\mathbf { 1 . 5 5 x }$ and even still largely surpasses PerAct with 4 cameras as input. The additional camera views are visualized in Figure 10.   

<table><tr><td>Method / Task</td><td>close jar</td><td>open drawer</td><td>sweep to dustpan</td><td>meat off grill</td><td>turn tap</td><td>Average</td></tr><tr><td>PerAct</td><td>18.7±8.2</td><td>54.7±18.6</td><td>0.0±0.0</td><td>40.0±17.0</td><td>38.7±6.8</td><td rowspan="4"></td></tr><tr><td>PerAct (4 Cameras)</td><td>21.3±7.5</td><td>44.0±11.3</td><td>0.0±0.0</td><td>65.3±13.2</td><td>46.7±3.8</td></tr><tr><td>GNFactor</td><td>25.3±6.8</td><td>76.0±5.7</td><td>28.0±15.0</td><td>57.3±18.9</td><td>50.7±8.2</td></tr><tr><td>Method / Task</td><td>slide block</td><td>put in drawer</td><td>drag stick</td><td>push buttons</td><td>stack blocks</td></tr><tr><td>PerAct</td><td>18.7±13.6</td><td>2.7±3.3</td><td>5.3±5.0</td><td>18.7±12.4</td><td>6.7±1.9</td><td>20.4</td></tr><tr><td>PerAct (4 Cameras)</td><td>16.0±14.2</td><td>6.7±6.8</td><td>12.0±3.3</td><td>9.±1.9</td><td>5.3±1.9</td><td>22.7</td></tr><tr><td>GNFactor</td><td>20.0±15.0</td><td>0.0±0.0</td><td>37.3±13.2</td><td>18.7±10.0</td><td>4.0±3.3</td><td>31.7</td></tr></table>

Table 2: Generalization to unseen tasks on RLBench. We evaluate 20 episodes for each task with the final chepot across 3 seeds. We denote "" as a large object, ""as a saer object "N as a new positi "D" as adding a distractor. Our method outperforms PerAct with an average improvement of $\mathbf { 1 . 5 7 x }$ .   

<table><tr><td>Method / Task</td><td>drag (D)</td><td>slide (L)</td><td>slide (S)</td><td>open (n)</td><td>turn (N)</td><td>push (D)</td><td>Average</td></tr><tr><td>PerAct</td><td>6.6±4.7</td><td>33.3±4.7</td><td>5.0±4.1</td><td>25.0±10.8</td><td>18.3±6.2</td><td>20.0±7.1</td><td>18.0</td></tr><tr><td>GNFactor</td><td>46.7±30.6</td><td>25.0±4.1</td><td>6.7±6.2</td><td>31.7±6.2</td><td>28.3±2.4</td><td>31.7±2.4</td><td>28.3</td></tr></table>

UNet with only 0.3M parameters. The Perceiver Transformer keeps the same number of parameters as PerAct [3] (25.2M parameters), making our comparison with PerAct fair.

# 4.2 Simulation Results

We report the success rates for multi-task tests on RLBench in Table 1 and for generalization to new environments in Table 2. We conclude our observations as follows: Dominance of GNFactor over PerAct for multi-task learning. As shown by Table 1 and Figure 4, GNFactor achieves higher success rates across various tasks compared to PerAct, particularly excelling in challenging long-horizon tasks. For example, in sweep to dustpan task, the robot needs to first pick up the broom and use the broom to sweep the dust into the dustpan. We find that GNFactor achieves a success rate of $2 8 . 0 \%$ , while PerAct could not succeed at all. In simpler tasks like open drawer where the robot only pulls the drawer out, both GNFactor and PerAct perform reasonably well, with success rates of $7 6 . 0 \%$ and $5 4 . 7 \%$ respectively. Furthermore, we observe that enhancing PerAct with extra camera views does not result in significant improvements. This underscores the importance of efficiently utilizing the available camera views. Generalization ability of GNFactor to new tasks. In Table 2, we observe that the change made on the environments such as distractors impacts all the agents negatively, while GNFactor shows better capability of generalization on 5 out of 6 tasks compared to PerAct. We also find that for some challenging variations such as the smaller block in the task slide (S), both GNFactor and PerAct struggle to handle. This further emphasizes the importance of robust generalization skills. Ablations. We summarize the key components in GNFactor that contribute to the success of the volumetric representation in Table 4. From the ablation study, we gained several insights (i) Our GNF reconstruction module plays a crucial role in multitask robot learning. Moreover, the RGB loss is essential for learning a consistent 3D feature in addition to the feature loss, especially since the features derived from foundation models are not inherently 3D consistent.

Table 4: Ablations. We report the averaged success rates on $1 0 \ \mathrm { R L } \cdot$ . Bench tasks. "DGS" is short for depth-guided sampling. $\ddot { \cdots } \xrightarrow { \infty } \vec { \mathbf { \zeta } }$ means replacing.   

<table><tr><td>Ablation</td><td>Success Rate (%)</td></tr><tr><td>GNFactor</td><td>36.8</td></tr><tr><td>w/o. GNF objective</td><td>24.2</td></tr><tr><td>w/o. RGB objective</td><td>27.2</td></tr><tr><td>w/o. Diffusion</td><td>30.0</td></tr><tr><td>Diffusion → DINO</td><td>30.4</td></tr><tr><td>Diffusion → CLIP</td><td>32.0</td></tr><tr><td>w/o. DGS</td><td>29.2</td></tr><tr><td>w/o. skip connection</td><td>27.6</td></tr><tr><td>k = 19 → 9</td><td>33.2</td></tr><tr><td>λfeat = 0.01 → 1.0</td><td>35.2</td></tr><tr><td>λrecon = 0.01 → 1.0</td><td>35.2</td></tr></table>

(ii) The volumetric representation benefits from Diffusion features and depth-guided sampling, where the depth prior is utilized to enhance the sampling quality in neural rendering. An intuitive explanation is that GNF, when combined with DGS, becomes more adept at learning depth and 3D structure information. This enhanced understanding allows the 3D representation to better concentrate on the surfaces of objects rather than the entire volume. Moreover, replacing Stable Diffusion with DINO [50] or CLIP [51] would not result in similar improvements easily, indicating the importance of our vision-language feature.

Table 3:Multi-task test results on real robot.We evaluate 10 episodes for each task and report the resulting success rate $( \% )$ . We denote "door" as "open door", "faucet" as "turn faucet", and "teapot" as "relocate teapot" The number in the parenthesis suggests the kitchen ID and "d" suggests testing with distractors.   

<table><tr><td>Method / Task</td><td>door (1)</td><td>faucet (1)</td><td>teapot (1)</td><td>door (1,d)</td><td>faucet (1,d)</td><td>teapot (1,d)</td><td>Average</td></tr><tr><td>PerAct</td><td>30</td><td>80</td><td>0</td><td>10</td><td>50</td><td>0</td><td rowspan="3"></td></tr><tr><td>GNFactor</td><td>40</td><td>80</td><td>40</td><td>30</td><td>500</td><td>30</td></tr><tr><td>Method / Task</td><td>door (2)</td><td>faucet (2)</td><td>teapot (2)</td><td>door (2,d)</td><td>faucet (2,d)</td><td>teapot (2,d)</td></tr><tr><td>PerAct</td><td>10</td><td>50</td><td>0</td><td>10</td><td>30</td><td>0</td><td>22.5</td></tr><tr><td>GNFactor</td><td>50</td><td>70</td><td>40</td><td>20</td><td>40</td><td>30</td><td>43.3</td></tr></table>

(iii) While the use of skip connection is not a new story and we merely followed the structure of PerAct, the result of removing the skip connection suggests that our voxel representation, which distills features from the foundation model, plays a critical role in predicting the final action. (iv) Striking a careful balance between the neural rendering loss and the action prediction loss is critical for optimal performance and utilizing information from multiple views by our GNF module proves to be beneficial for the single-view decision module. Furthermore, we provide the view synthesis in the real world, generated by GNFactor in Figure 5 and Figure 6. We also give the quantitative evaluation measured by PSNR [29]. We observe that the rendered views are somewhat blurred since the volumetric presentation learned by GNFactor is optimized to minimize both the neural rendering loss as well as the action prediction loss, and the rendering quality is largely improved when the behavior cloning loss is removed and only the GNF is trained. Notably, for the view synthesis in the real world, we do not have access to ground-truth point clouds for either training or testing. Instead, the point clouds are sourced from RealSense cameras and are therefore imperfect. Despite the limitations in achieving accurate pixel-level reconstruction results, we focus on learning semantic understanding of the whole scene from distilling Diffusion features, which is more important for policy learning.

# 4.3 Real Robot Experiments

We summarize the results of our real robot experiment in Table 3. From the experiments, GNFactor outperforms the PerAct baseline on almost all tasks. Notably, in the teapot task where the agent is required to accurately determine the grasp location and handle the teapot from a correct angle, PerAct fails to accomplish the task and obtains a zero success rate across two kitchens. We observed that it is indeed challenging to learn a delicate policy from only 5 demonstrations. However, by incorporating the representation from the embedding of a vision-language model, GNFactor gains an understanding of objects. As such, GNFactor does not simply overfit to the given demonstrations. The second kitchen (Figure 1) presents more challenges due to its smaller size compared to the first kitchen. This requires higher accuracy to manipulate the objects effectively. The performance gap between GNFactor and the baseline PerAct becomes more significant in the second kitchen. Importantly, our method does not suffer the same performance drop transitioning from the first kitchen to the second, unlike the baseline. We also visualize our 3D policy module by Grad-CAM [55], as shown in Figure 7. We use the gradients and the 3D feature map from the 3D convolution layer after the Perceiver Transformer to compute Grad-CAM. We observe that the target objects are clearly attended by our policy, though the training signal is only the Q-value for a single voxel.

# 5 Conclusion and Limitations

In this work, we propose GNFactor, a visual behavior cloning agent for real-world multi-task robotic manipulation. GNFactor utilizes a Generalizable Neural Feature Field (GNF) to learn a 3D volumetric representation, which is also used by the action prediction module. We employ the visionlanguage feature from the foundation model Stable Diffusion besides the RGB feature to supervise the GNF training and observe that the volumetric representation enhanced by the GNF is helpful for decision-making. GNFactor achieves strong results in both simulation and the real world, across 10 RLBench tasks and 3 real robot tasks, showcasing the potential of GNFactor in real-world scenarios. One major limitation of GNFactor is the requirement of multiple views for the GNF training, which can be challenging to scale up in the real world. Currently, we use three fixed cameras for GNFactor, but it would be interesting to explore using a cell phone to randomly collect camera views, where the estimation of the camera poses would be a challenge. Acknowledgment. This work was supported, in part, by the Amazon Research Award, Cisco Faculty Award and gifts from Qualcomm.

# References

[1] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog, J. Hsu, t al. Rt-1: Robotics transformer for real-world control at scale. arXiv, 2022.   
[2] E. Jang, A. Irpan, M. Khansari, D. Kappler, F. Ebert, C. Lynch, S. Levine, and C. Finn. Bc-z: Zero-shot task generalization with robotic imitation learning. In CoRL, 2022.   
[3] M. Shridhar, L. Manuelli, and D. Fox. Perceiver-actor: A multi-task transformer for robotic manipulation. In CoRL, 2023.   
[4] M. Dalal, A. Mandlekar, C. Garrett, A. Handa, R. Salakhutdinov, and D. Fox. Imitating task and motion planning with visuomotor transformers. arXiv, 2023.   
[5] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022.   
[6] S. Parisi, A. Rajeswaran, S. Purushwalkam, and A. Gupta. The unsurprising effectiveness of pre-trained vision models for control. In ICML, 2022.   
[7] S. Nair, A. Rajeswaran, V. Kumar, C. Finn, and A. Gupta. R3m: A universal visual representation for robot manipulation. arXiv, 2022.   
[8] I. Radosavovic, T. Xiao, S. James, P. Abbeel, J. Malik, and T. Darrell. Real-world robot learning with masked visual pre-training. In CoRL, 2023.   
[9] M. Laskin, A. Srinivas, and P. Abbeel. Curl: Contrastive unsupervised representations for reinforcement learning. In ICML, 2020.   
[10] N. Hansen and X. Wang. Generalization in reinforcement learning by soft data augmentation. In ICRA, 2021.   
[11] Y. Ze, N. Hansen, Y. Chen, M. Jain, and X. Wang. Visual reinforcement learning with selfsupervised 3d representations. RA-L, 2023.   
[12] D. Driess, I. Schubert, P. Florence, Y. Li, and M. Toussaint. Reinforcement learning with neural radiance fields. NeurIPS, 2022.   
[13] A. Jaegle, F. Gimeno, A. Brock, O. Vinyals, A. Zisserman, and J. Carreira. Perceiver: General perception with iterative attention. In ICML, 2021.   
[14] S. James, Z. Ma, D. R. Arrojo, and A. J. Davison. Rlbench: The robot learning benchmark & learning environment. RA-L, 2020.   
[15] R. Rahmatizadeh, P. Abolghasemi, L. Bölöni, and S. Levine. Vision-based multi-task manipulation for inexpensive robots using end-to-end learning from demonstration. In 2018 IEEE international conference on robotics and automation (ICRA), pages 37583765. IEEE, 2018.   
[16] M. Shridhar, L. Manuelli, and D. Fox. Cliport: What and where pathways for robotic manipulation. In CoRL, 2022.   
[17] D. Kalashnikov, A. Irpan, P. Pastor, J. Ibarz, A. Herzog, E. Jang, D. Quillen, E. Holly, M. Kalakrishnan, V. Vanhoucke, et al. Qt-opt: Scalable deep reinforcement learning for visionbased robotic manipulation. arXiv, 2018.   
[18] T. Yu, D. Quillen, Z. He, R. Julian, K. Hausman, C. Finn, and S. Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. In Conference on robot learning, pages 10941100. PMLR, 2020.   
[19] R. Yang, H. Xu, Y. Wu, and X. Wang. Multi-task reinforcement learning with soft modularization. Advances in Neural Information Processing Systems, 33:47674777, 2020.   
[20] S. Song, A. Zeng, J. Lee, and T. Funkhouser. Grasping in the wild: Learning 6dof closedloop grasping from low-cost demonstrations. IEEE Robotics and Automation Letters, 5(3): 49784985, 2020.   
[21] A. Murali, A. Mousavian, C. Eppner, C. Paxton, and D. Fox. 6-dof grasping for target-driven object manipulation in clutter. In 2020 IEEE International Conference on Robotics and Automation (ICRA), pages 62326238. IEEE, 2020.   
[22] A. Mousavian, C. Eppner, and D. Fox. 6-dof graspnet: Variational grasp generation for object manipulation. In ICCV, 2019.   
[23] Z. Xu, Z. He, and S. Song. Universal manipulation policy network for articulated objects. RA-L, 2022.   
[24] Y. Li, S. Agrawal, J.-S. Liu, S. K. Feiner, and S. Song. Scene editing as teleoperation: A case study in 6dof kit assembly. In IROS, 2022.   
[ N. Hane, Z.Yuan, Y.Ze, T. Mu, A. Rajea, H. Su, H. Xu, a X. Wag.On efor visuo-motor control: Revisiting a learning-from-scratch baseline. In ICML, 2022.   
[26] D. Shim, S. Lee, and H. J. Kim. Snerl: Semantic-aware neural radiance fields for reinforcement learning. ICML, 2023.   
[27] Y. Chen, S. Liu, and X. Wang. Learning continuous image representation with local implicit image function. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 86288638, 2021.   
[28] L. Mescheder, M. Oechsle, M. Niemeyer, S. Nowozin, and A. Geiger. Occupancy networks: Learning 3d reconstruction in function space. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 44604470, 2019.   
[29] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021.   
[30] M. Niemeyer, L. Mescheder, M. Oechsle, and A. Geiger. Occupancy flow: 4d reconstruction by learning particle dynamics. In Proceedings of the IEEE/CVF international conference on computer vision, pages 53795389, 2019.   
[31] J. J. Park, P. Florence, J. Straub, R. Newcombe, and S. Lovegrove. Deepsdf: Learning continuous signed distance functions for shape representation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 165174, 2019.   
[32] V. Sitzmann, M. Zollhöfer, and G. Wetzstein. Scene representation networks: Continuous 3d-structure-aware neural scene representations. Advances in Neural Information Processing Systems, 32, 2019.   
[33] Z. Jiang, Y. Zhu, M. Svetlik, K. Fang, and Y. Zhu. Synergies between affordance and geometry: 6-dof grasp detection via implicit representations. arXiv preprint arXiv:2104.01542, 2021.   
[34] Y.-C. Lin, P. Florence, A. Zeng, J. T. Barron, Y. Du, W.-C. Ma, A. Simeonov, A. R. Garcia, .IMM boc I RoboL pages 19161927. PMLR, 2023. [35] Y. Li, S. Li, V. Sitzmann, P. Agrawal, and A. Torralba. 3d neural scene representations for visuomotor control. In Conference on Robot Learning, pages 112123. PMLR, 2022.

[36] Y. Chen and X. Wang. Transformers as meta-learners for implicit neural representations. In Computer VisionECCV 2022: 17th European Conference, Tel Aviv, Israel, October 2327, 2022, Proceedings, Part XVII, pages 170187. Springer, 2022.   
[37] W. Jang and L. Agapito. Codenerf: Disentangled neural radiance fields for object categories. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 12949 12958, 2021.   
[38] K.-E. Lin, Y.-C. Lin, W.-S. Lai, T.-Y. Lin, Y.-C. Shih, and R. Ramamoorthi. Vision transformer for nerf-based view synthesis from a single input image. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 806815, 2023.   
[39] J. Reizenstein, R. Shapovalov, P. Henzler, L. Sbordone, P. Labatut, and D. Novotny. Common objects in 3d: Large-scale learning and evaluation of real-life 3d category reconstruction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10901 10911, 2021.   
[40] K. Rematas, R. Martin-Brualla, and V. Ferrari. Sharf: Shape-conditioned radiance fields from a single view. arXiv preprint arXiv:2102.08860, 2021.   
[4 A. Trevithick and B. Yang.Gr: Learning a general radiance feld for 3d representation and rendering. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1518215192, 2021.   
[42] Q. Wang, Z. Wang, K. Genova, P. P. Srinivasan, H. Zhou, J. T. Barron, R. Martin-Brualla, N. Snavely, and T. Funkhouser. Ibrnet: Learning multi-view image-based rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 46904699, 2021.   
[43] A. Yu, V. Ye, M. Tancik, and A. Kanazawa. pixelnerf: Neural radiance fields from one or few images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 45784587, 2021.   
[44] V. Tschernezki, I. Laina, D. Larlus, and A. Vedaldi. Neural feature fusion fields: 3d distillation of self-supervised 2d image representations. arXiv, 2022.   
[45] S. Kobayashi, E. Matsumoto, and V. Sitzmann. Decomposing nerf for editing via feature field distillation. NeurIPS, 2022.   
[46] J. Ye, N. Wang, and X. Wang. Featurenerf: Learning generalizable nerfs by distilling foundation models. arXiv, 2023.   
[47] J. Kerr, C. M. Kim, K. Goldberg, A. Kanazawa, and M. Tancik. Lerf: Language embedded radiance fields. arXiv preprint arXiv:2303.09553, 2023.   
[48] K. M. Jatavallabhula, A. Kuwajerwala, Q. Gu, M. Omama, T. Chen, S. Li, G. Iyer, S. Saryazdi, N. Keetha, A. Tewari, et al. Conceptfusion: Open-set multimodal 3d mapping. arXiv preprint arXiv:2302.07241, 2023.   
[. Bis, FMio J.J.C, L. Ott a R. Sar. Neaplsfeature fields. arXiv preprint arXiv:2303.10962, 2023.   
[50] M. Caron, H. Touvron, I. Misra, H. Jégou, J. Mairal, P. Bojaoski, and A. Jouli. Eme properties in self-supervised vision transformers. In ICCV, 2021.   
[51] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021.   
[52] S. James, K. Wada, T. Laidlow, and A. J. Davison. Coarse-to-fine q-attention: Efficient learning for visual robotic manipulation via discretisation. In CVPR, 2022.   
[53] S. Klemm, J. Oberländer, A. Hermann, A. Roennau, T. Schamm, J. M. Zollner, and R. Dillmann. Rrt\*-connect: Faster, asymptotically optimal motion planning. In 2015 IEEE international conference on robotics and biomimetics (ROBIO), pages 16701677. IEEE, 2015.   
[54] H. Lin, S. Peng, Z. Xu, Y. Yan, Q. Shuai, H. Bao, and X. Zhou. Efficient neural radiance fields for interactive free-viewpoint video. In SIGGRAPH Asia, 2022.   
[55] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE international conference on computer vision, pages 618626, 2017.   
[. You, J. Li S. Rei, J. Hse, S. Kuar, S. Bhojanapali, X. Song, J.Deme, K. Kzr, and C.-J. Hsieh. Large batch optimization for deep learning: Training bert in 76 minutes. arXiv, 2019.

# A Visualizations

![](images/5.jpg)  

Figure 5: View synthesis of GNFactor in the real world. PSNR is computed for quantitative evaluation. Ta  he  i ai    u  T rendering is mainly because, in inference, we do not optimize per-step for rendering but just perform one feedforward to obtain the feature.

![](images/6.jpg)  

Figure 6: More novel view synthesis results. Both RGB and features are synthesized. We remove the action loss here for a better rendering quality. Videos are available on yanjieze.com/GNFactor.

![](images/7.jpg)  

Figure 7: Visualize the 3D policy module by Grad-CAM [55]. Though the supervision signal is only the $\mathrm { Q }$ valu or a single voxel during the trainig proces, we observe in visualizations that he target object ae clearly attended by our policy. Videos are available on yanjieze.com/GNFactor.

# B Task Descriptions

Simulated tasks. We select 10 language-conditioned tasks from RLBench [14], all of which involve at least 2 variations. See Table 5 for an overview. Our task variations include randomly sampled colors, sizes, counts, placements, and categories of objects, totaling 166 different variations. The set of colors have 20 instances: red, maroon, lime, green, blue, navy, yellow, cyan, magenta, silver, gray, orange, olive, purple, teal, azure, violet, rose, black, and white. The set of sizes includes 2 types: short and tall. The set of counts has 3 instances: 1, 2, 3. The placements and object categories are specific to each task. For example, open drawer has 3 placement locations: top, middle and bottom. In addition to these semantic variations, objects are placed on the tabletop at random poses within a limited range.

Table 5: Language-conditioned tasks in RLBench [14].   

<table><tr><td>Task</td><td>Variation Type</td><td># of Variations</td><td>Avg. Keyframs</td><td></td><td>Language Template</td></tr><tr><td>close jar</td><td></td><td>color</td><td>20</td><td>6.0</td><td>&quot;close the — jar&quot;</td></tr><tr><td>open drawer</td><td></td><td>placement</td><td>3</td><td>3.0</td><td>&quot;open the — drawer</td></tr><tr><td></td><td>sweep to dustpan</td><td>size</td><td>2</td><td>4.6</td><td>&quot;sweep dirt to the — dustpan&quot;</td></tr><tr><td>meat off grill</td><td></td><td>category</td><td>2</td><td>5.0</td><td>&quot;take the — off the grill&quot;</td></tr><tr><td>turn tap</td><td></td><td>placement</td><td>2</td><td>2.0</td><td>&quot;turn — tap&quot;</td></tr><tr><td>slide block</td><td></td><td>color</td><td>4</td><td>4.7</td><td>&quot;slide the block to — target&quot;</td></tr><tr><td></td><td>put in drawer</td><td>placement</td><td>3</td><td>12.0</td><td>&quot;put the item in the — drawer&quot;</td></tr><tr><td>drag stick</td><td></td><td>color</td><td>20</td><td>6.0</td><td>&quot;use the stick to drag the cube onto the — — target&quot;</td></tr><tr><td>push buttons</td><td></td><td>color</td><td>50</td><td>3.8</td><td>&quot;push the — button, [then the — button]&quot;</td></tr><tr><td></td><td>stack blocks</td><td>color, count</td><td>60</td><td>14.6</td><td>stack— - blocks&quot;</td></tr></table>

Generalization tasks in simulation. We design 6 additional tasks where the scene is changed based on the original training environment, to test the generalization ability of GNFactor. Table 6 gives an overview of these tasks. Videos are also available on yanjieze.com/GNFactor.

Table 6: Generalization tasks based on RLBench.   

<table><tr><td>Task</td><td>Base</td><td>Change</td></tr><tr><td>drag (D)</td><td>drag stick</td><td>add two colorful buttons on the table</td></tr><tr><td>slide (L)</td><td>slide block</td><td>change the block size to a larger one</td></tr><tr><td>slide (S)</td><td>slide block</td><td>change the block size to a smaller one</td></tr><tr><td>open (n)</td><td>open drawer</td><td>change the position of the drawer</td></tr><tr><td>turn (N)</td><td>turn tap</td><td>change the position of the tap</td></tr><tr><td>push (D)</td><td>push buttons</td><td>add two colorful jar on the table</td></tr></table>

Real robot tasks. In the experiments, we perform three tasks along with three additional tasks where distracting objects are present. The door task requires the agent to open the door on an mircowave, a task which poses challenges due to the precise coordination required. The faucet task requires the agent to rotate the faucet back to center position, which involves intricate motor control. Lastly, the teapot task requires the agent to locate the randomly placed teapot in the kitchen and move it on top of the stove with the correct pose. Among the three, the teapot task is considered the most challenging due to the random placement and the need for accurate location and rotation of the gripper. All 6 tasks are set up in two different kitchens, as visualized in Figure 8. The keyframes used in real robot tasks are given in Figure 9.

# C Implementation Details

Voxel encoder. We use a lightweight 3D UNet (only 0.3M parameters) to encode the input voxel $1 0 0 ^ { 3 } \times 1 0$ (RGB features, coordinates, indices, and occupancy) into our deep 3D volumetric representation of size $1 0 0 ^ { 3 } \times 1 2 8$ Due to the cluttered output from directly printing the network, we provide the PyTorch-Style pseudo-code for the forward process as follows. For each block, we use a cascading of one Convolutional Layer, one BatchNorm Layer, and one LeakyReLU layer, which is common practice in the vision community.

![](images/8.jpg)  

Figure 8: Kitchens. We give a closer view of our two kitchens for real robot experiments. The figures are captured in almost the same position to display the size difference between the two.

![](images/9.jpg)  

Figure 9: Keyframes for real robot tasks. We give the keyframes used in our 3 real robot tasks across 2 kitchens.

def forward(self, x): convO $=$ self.conv0(x) # 100^3x8 conv2 $=$ self.conv2(self.conv1(conv0)) # 50^3x16 conv4 $=$ self.conv4(self.conv3(conv2)) # 25^3x32 $\mathrm { ~ \tt ~ x ~ } =$ self.conv6(self.conv5(conv4)) # 13^3x64   
$\mathrm { ~ \tt ~ x ~ } =$ conv4 $^ +$ self.conv7(x) # 25^3x32   
$\mathrm { ~ \tt ~ x ~ } =$ conv2 $^ +$ self.conv9(x) # 50^3x16   
$\mathrm { ~ \tt ~ x ~ } =$ self.conv_out(convO $^ +$ self.conv11(x)) # 100^3x128   
return x Generalizable Neural Field (GNF). The overall network architecture of our GNF is close to the original NeRF [29] implementation. We use the same positional encoding as NeRF and the encoding function is formally

$$
\gamma ( p ) = \left( \sin \left( 2 ^ { 0 } \pi p \right) , \cos \left( 2 ^ { 0 } \pi p \right) , \cdots , \sin \left( 2 ^ { L - 1 } \pi p \right) , \cos \left( 2 ^ { L - 1 } \pi p \right) \right) .
$$

This function is applied to each of the three coordinate values and we set $L = 6$ in our experiments. The overal position encoding is then 36-dimensional. The input of GNF is thus a concatenation of the original coordinates $( \mathbb { R } ^ { 3 } )$ , the position encoding $( \mathbb { R } ^ { 3 6 } )$ , the view directions $( \mathbb { R } ^ { 3 } )$ , and the voxel feature $( \mathbb { R } ^ { 1 2 8 } )$ , totaling 170 dimensions. Our GNF mainly consists of 5 ResnetFCBlocks, in which a skip connection is used. The input feature is first projected to 512 with a linear layer and fed into these blocks, and then projected to the output dimension 516 (RGB, density, and Diffusion feature) with a cascading of one ReLU function and one linear layer. We provide the PyTorch-Style pseudo-code for the networks as follows. GNF Linear(in_features $\mathtt { = 1 7 0 }$ , out_features $\mathtt { \mathtt { = 5 1 2 } }$ , bias $=$ True), (0-4): 5 x ResnetFCBlocks( (fc_O): Linear(in_features $= 5 1 2$ , out_features $\mathtt { = 5 1 2 }$ , bias $\ c =$ True) (fc_1): Linear(in_features $= 5 1 2$ , out_features $\mathtt { \_ { 1 2 } }$ , bias $\ c =$ True) (activation): ReLU() ), ReLU(), Linear(in_features $\mathtt { = 5 1 2 }$ , out_features $\mathtt { = 5 1 6 }$ , bias $=$ True)   
)

Percevier Transformer. Our usage of Percevier Transformer is close to PerAct [3]. We use 6 attention blocks to process the sequence from multi-modalities (3D volume, language token, and robot proprioception) and output a sequence also. The usage of Perceiver Transformer enables us to process the long sequence with computational effciency, by only utilizing a small set of latents to attend the input. The output sequence is then reshaped back to a voxel to predict the robot action. The Q-function for translation is predicted by a 3D convolutional layer, and for the prediction of openness, collision avoidance, and rotation, we use global max pooling and spatial softmax operation to aggregate 3D volume features and project the resulting feature to the output dimension with a multi-layer perception. We could clarify that the design for the policy module is not our contribution; for more details please refer to PerAct [3] and its official implementation on https://github.com/peract/peract.

# D Demonstration Collection for Real Robot Tasks

For the collection of real robot demonstrations, we utilize the HTC VIVE controller and basestation to track the 6-DOF poses of human hand movements. We then use triad-openvr package (https://github.com/TriadSemi/triad_openvr) to employ SteamVR and accurately map human operations onto the xArm robot, enabling it to interact with objects in the real kitchen. We record the real-time pose of xArm and $6 4 0 \times 4 8 0$ RGB-D observations with the pyrealsense2 (https://pypi.org/project/pyrealsense2/). Though the image size is different from our simulation setup, we use the same shape of the input voxel, thus ensuring the same algorithm is used across the simulation and the real world. The downscaled images $( 8 0 \times 6 0 )$ are used for neural rendering.

# E Detailed Data

Besides reporting the final success rates in our main paper, we give the success rates for the best single checkpoint (i.e., evaluating all saved checkpoints and selecting the one with the highest success rates), as shown in Table 7. Under this setting GNFactor outperforms PerAct with a larger margin. However, we do not use the best checkpoint in the main results for fairness. We also give the detailed number of success in Table 8 for reference in addition to the success rates computed in Table 2.

Table 7:Multi-task test results on RLBench. We report the success rates for the best single checkpoint fo reference. We could observe GNFactor surpasses PerAct by a large margin.   

<table><tr><td>Method / Task</td><td>close jar</td><td>open drawer</td><td>sweep to dustpan</td><td>meat off grill</td><td>turn tap</td><td>Average</td></tr><tr><td>PerAct</td><td>22.7±5.0</td><td>62.7±13.2</td><td>0.0±0.0</td><td>46.7±14.7</td><td>36.0±9.8</td><td></td></tr><tr><td>GNFactor</td><td>40.0±5.7</td><td>77.3±7.5</td><td>40.0±11.8</td><td>66.7±8.2</td><td>45.3±3.8</td><td></td></tr><tr><td>Method / Task</td><td>slide block</td><td>put in drawer</td><td>drag stick</td><td>push buttons</td><td>stack blocks</td><td></td></tr><tr><td>PerAct</td><td>22.7±6.8</td><td>9.3±5.0</td><td>12.0±6.5</td><td>18.7±6.8</td><td>5.3±1.9</td><td>23.6</td></tr><tr><td>GNFactor</td><td>18.7±10.5</td><td>10.7±12.4</td><td>73.3±13.6</td><td>20.0±3.3</td><td>8.0±0.0</td><td>40.0</td></tr></table>

Table 8: Detailed data for generalization to novel tasks. We evaluate 20 episodes, each across 3 seeds, for the final checkpoint and report the number of successful trajectories here.   

<table><tr><td>Generalization</td><td>PerAct</td><td>GNFactor w/o. Diffusion</td><td>GNFactor</td></tr><tr><td>drag (D)</td><td>2,0, 2</td><td>15, 2, 5</td><td>18, 5, 5</td></tr><tr><td>slide (L)</td><td>6, 6, 8</td><td>1, 10, 10</td><td>6, 5, 4</td></tr><tr><td>slide (S)</td><td>0,2,1</td><td>6,1,5</td><td>0, 3, 1</td></tr><tr><td>push (D)</td><td>6, 3, 3</td><td>4,4,5</td><td>7,6, 6</td></tr><tr><td>open (N)</td><td>6,2,7</td><td>5,2,9</td><td>8, 5, 6</td></tr><tr><td>turn (N)</td><td>4,5,2</td><td>2,7,2</td><td>6, 6, 5</td></tr></table>

# F Stronger Baseline

To make the comparison between our GNFactor and PerAct fairer, we enhance Peract's input by using 4 camera views, as visualized in Figure 10. These views ensure that the scene is fully covered. It is observed in our experiment results (Table 1) that GNFactor which takes the single view as input still outperforms PerAct with more views.

# G Hyperparameters

We give the hyperparameters used in GNFactor as shown in Table 9. For the GNF training, we use a ray batch size $b _ { \mathrm { r a y } } = 5 1 2$ , corresponding to 512 pixels to reconstruct, and use $\lambda _ { \mathrm { f e a t } } = 0 . 0 1$ and $\lambda _ { \mathrm { r e c o n } } = 0 . 0 1$ to maintain major focus on the action prediction. For real-world experiment, we set the weight of the reconstruction loss to 1.0 and the weight of action loss to 0.1. This choice was based on our observation that reducing the weight of the action loss and increasing the weight of the reconstruction loss did not significantly affect convergence but did help prevent overfitting to a limited number of real-world demonstrations. We uniformly sample 64 points along the ray for the "coarse" network and sample 32 points with depth-guided sampling and 32 points with uniform sampling for the "fine" network.

![](images/10.jpg)  

Figure 10: Visualization of 4 cameras used for the stronger PerAct baseline. To enhance the PerAct baseline, we add more views as the input of PerAct. These views are pre-defined in RLBench, making sure the observation covers the entire scene.

Table 9: Hyperparameters used in GNFactor.   

<table><tr><td>Variable Name</td><td>Value</td></tr><tr><td>training iteration</td><td>100k</td></tr><tr><td>image size</td><td>128 × 128 × 3</td></tr><tr><td>input voxel size</td><td>100 × 100 × 100</td></tr><tr><td>batch size</td><td>2</td></tr><tr><td>optimizer</td><td>LAMB [56]</td></tr><tr><td>learning rate</td><td>0.0005</td></tr><tr><td>ray batch size bray</td><td>512</td></tr><tr><td>weight for reconstruction loss λrecon</td><td>0.01</td></tr><tr><td>weight for embedding loss λfeat</td><td>0.01</td></tr><tr><td>number of transformer blocks</td><td>6</td></tr><tr><td>number of sampled points for GNF</td><td>64</td></tr><tr><td>number of latents in Perceiver Transformer</td><td>2048</td></tr><tr><td>dimension of Stable Diffusion features</td><td>512</td></tr><tr><td>dimension of CLIP language features</td><td>512</td></tr><tr><td>hidden dimension of NeRF blocks</td><td>512</td></tr></table>