# Enhancing Graph Contrastive Learning with Reliable and Informative Augmentation for Recommendation

Hongyu Lu WeChat, Tencent Guangzhou, China luhy94@gmail.com Bowen Zheng   
Renmin University of   
China   
Beijing, China   
bwzheng0324@ruc.edu.cn   
Junjie Zhang   
Renmin University of   
China   
Beijing, China   
junjie.zhang@ruc.edu.cn   
Yu Chen   
WeChat, Tencent   
Beijing, China   
nealcui@tencent.com   
Ming Chen   
WeChat, Tencent   
Guangzhou, China   
mingchen@tencent.com   
Wayne Xin Zhao   
Renmin University of China Beijing, China   
batmanfly@gmail.com   
Ji-Rong Wen   
Renmin University of   
China   
Beijing, China   
jrwen@ruc.edu.cn

# ABSTRACT

Graph neural network (GNN) has been a powerful approach in collaborative filtering (CF) due to its ability to model high-order user-item relationships. Recently, to alleviate the data sparsity and enhance representation learning, many efforts have been conducted to integrate contrastive learning (CL) with GNNs. Despite the promising improvements, the contrastive view generation based on structure and representation perturbations in existing methods potentially disrupts the collaborative information in contrastive views, resulting in limited effectiveness of positive alignment.

To overcome this issue, we propose CoGCL, a novel framework that aims to enhance graph contrastive learning by constructing contrastive views with stronger collaborative information via discrete codes. The core idea is to map users and items into discrete codes rich in collaborative information for reliable and informative contrastive view generation. To this end, we initially introduce a multi-level vector quantizer in an end-to-end manner to quantize user and item representations into discrete codes. Based on these discrete codes, we enhance the collaborative information of contrastive views by considering neighborhood structure and semantic relevance respectively. For neighborhood structure, we propose virtual neighbor augmentation by treating discrete codes as virtual neighbors, which expands an observed user-item interaction into multiple edges involving discrete codes. Regarding semantic relevance, we identify similar users/items based on shared discrete codes and interaction targets to generate the semantically relevant view. Through these strategies, we construct contrastive views with stronger collaborative information and develop a triple-view graph contrastive learning approach. Extensive experiments on four public datasets demonstrate the effectiveness of our proposed approach. Moreover, detailed analyses highlight our contribution in enhancing graph CL for recommendation. Our code is available at https://github.com/RUCAIBox/CoGCL

# CCS CONCEPTS

Information systems Recommender systems.

# KEYWORDS

Recommendation, Collaborative Filtering, Graph Contrastive Learning

# ACM Reference Format:

Bowen Zheng, Junjie Zhang, Hongyu Lu, Yu Chen, Ming Chen, Wayne Xin Z oWe 0.  Ga C with Reliable and Informative Augmentation for Recommendation. In Proceedings of Make sure to enter the correct conference title from your rights confirmation emai (Conference acronym 'XX). ACM, New York, NY, USA, 13 pages. https://doi.org/XXXXXXX.XXXXXXX

# 1 INTRODUCTION

In the literature of recommender systems, collaborative filtering (CF) based on graph neural network (GNN) has showcased significant success in recommendation systems due to its ability to model high-order user-item relationships [11, 15, 48]. This approach typically involves organizing user-item interaction data into a bipartite graph and learning node representations that contain collaborative knowledge from the graph structure. However, given the sparsity of user behaviors, GNN-based methods often struggle with limited graph edges and insufficient supervision signals. This challenge hinders the ability to develop high-quality user and item representations [45, 51, 52], which are vital for improving recommendation. To address this challenge, recent studies propose to integrate contrastive learning (CL) [7, 12, 20] with GNN-based CF to incorporate self-supervised signals.

According to how the contrastive views are constructed, existing Graph CL-based methods can be divided into two categories: structure augmentation and representation augmentation. Structure augmentation perturbs the graph structure to create augmented graphs, which are subsequently used by the GNN to generate contrastive node representations [4, 27, 35, 51]. As a representative method, SGL [51] adopts stochastic node/edge dropout to construct augmented graphs as contrastive views. Representation augmentation involves encoding additional representations of nodes from the interaction graph for CL [26, 29, 40, 53, 58, 60]. Particularly, SimGCL [60] perturbs the node embedding by adding random noise to generate contrastive views. Despite their effectiveness, existing approaches still suffer from unexpected self-supervised signals [4, 27]. Contrastive view generation based on perturbations potentially disrupt collaborative information within contrastive views. More precisely, in recommendation scenarios where user behaviors are scarce, structural perturbations may lose key interactions of sparse users [4, 27, 60]. And the random noise added to node embeddings may interfere with the implicit collaborative semantics in node representations [4, 55]. In addition, the empirical analysis in Section 2.2 confirms that the alignment between positive pairs based on perturbations is not as effective as expected, and the model performance significantly relies on the representation uniformity across different instances facilitated by CL.

![](images/1.jpg)  

Figure 1: Comparison of current graph CL-based methods (e.g., SGL [51], SimGCL [60]) that disrupt collaborative information within contrastive views and the proposed approach that enhances collaborative information.

Considering these issues, we aim to construct higher-quality contrastive views to enhance collaborative information. Specifically, we strive to maintain both reliability and informativeness for contrastive view generation. For reliability, we anticipate that the structural information introduced by graph augmentation is wellfounded rather than arbitrary, that is, based on the observed useritem interactions. Our idea is to represent each user or item as a tuple of discrete IDs (called codes in this paper) associated with collaborative information. Given the user and item codes, as shown in Figure 1, we can naturally expand a ${ } ^ { * } \cup { } - \dot { \beth } { } ^ { s }$ interaction edge to several "u-codes (i)" and "codes (u) $- \overset { \cdot } { \beth } ^ { \mathfrak { n } }$ edges. For informativeness, this code-based augmentation can enhance neighborhood structure and effectively alleviate the sparsity of the interaction graph by treating the codes as virtual neighbors. Furthermore, sharing discrete codes between different users/items indicates their relevance of collaborative semantics, such as $u$ and $u ^ { + }$ in Figure 1. To develop our methodology, we focused on (a) how to elegantly learn discrete codes associated with rich collaborative information and (b) how to integrate the learned discrete codes into the graph CL framework to improve recommendation.

In this paper, we propose CoGCL, a reliable and informative graph CL approach aiming to construct contrastive views that imply stronger collaborative information by introducing discrete codes. To map users and items into discrete codes rich in collaborative information, we learn a multi-level vector quantizer in an end-toend manner to quantize user and item representations encoded by GNN into discrete codes. Subsequently, the learned discrete codes are adopted to enhance the collaborative information of contrastive views in two aspects: neighborhood structure and semantic relevance. For neighborhood structure, we conduct virtual neighbor augmentation by treating discrete codes as virtual neighbors based on existing interactions. This process serves to enhance the node's neighbor information and alleviate interaction sparsity in contrasting views. For semantic relevance, we identify users/items that share discrete codes or interaction targets as semantically similar for positive sampling. By aligning users/items with semantic relevance via CL, we can further enhance the integration of collaborative semantics. Through the above strategies, we can generate various contrastive views with stronger collaborative information. Finally, a triple-view graph contrastive learning approach is proposed to achieve alignment across the augmented nodes and similar users/items. The contributions in this paper can be summarized as follows: •We present a reliable and informative graph CL approach, namely CoGCL, which constructs contrastive views that imply stronger collaborative information via discrete codes. •We propose an end-to-end method to elegantly learn discrete codes for users and items. These discrete codes are employed to enhance the collaborative information of contrastive views in terms of both neighborhood structure and semantic relevance. •Extensive experiments on four public datasets show that our approach consistently outperforms baseline models. Further in-depth analyses illustrate the crucial role that our designed components play in enhancing graph CL for recommendation.

# 2 PRELIMINARY AND EMPIRICAL ANALYSIS

In this section, we first overview the common paradigm of graph CL for recommendation. Subsequently, we conducted a brief empirical analysis to further explore how graph CL works in CL.

# 2.1 Graph CL for Recommendation

Given user and item sets $\mathcal { U }$ and $\boldsymbol { \mathcal { T } }$ respectively, let $\mathbf { R } \in \{ 0 , 1 \} ^ { | \mathcal { U } | \times | \mathcal { I } | }$ represent the user-item interaction matrix, where ${ \bf R } _ { u , i } = 1$ if there is an observed interaction between user $u$ and item $i$ ,otherwise $\mathbf { R } _ { u , i } = 0$ Based on the interaction data R, GNN-based CF methods construct a bipartite graph $\mathcal { G } = ( \mathcal { V } , \mathcal { E } )$ , where the node set $\mathbf { \nabla } _ { \mathbf { \gamma } } \mathbf { \mathbf { \Phi } } _ { \mathbf { \mathcal { V } } } =$ $\{ { \mathcal { U } } \cup T \}$ includes all users and items, and $\mathcal { E } = \{ ( u , i ) | u \in \mathcal { U } , i \in$ $\mathcal { T } , \mathbf { R } _ { u , i } = 1 \}$ denotes the set of interaction edges. Typically, GNN-based CF methods [15, 48] utilize the neighbor aggregation scheme on $\mathcal { G }$ to obtain informative node representations, which can be formulated as follows:

$$
\begin{array} { r } { \boldsymbol { \mathbf { Z } } ^ { l } = \boldsymbol { \mathbf { \check { G } } } \boldsymbol { \mathbf { N } } \boldsymbol { \mathbf { N } } ( \boldsymbol { \mathbf { Z } } ^ { l - 1 } , \boldsymbol { \mathbf { \check { \phi } } } ) , \quad \boldsymbol { \mathbf { Z } } \mathrm { ~ = ~ \boldsymbol { \ R e a d o u t } ( \boldsymbol { \mathbf { \check { Z } } } ^ 0 , \boldsymbol { \mathbf { Z } } ^ 1 , \ldots , \boldsymbol { \mathbf { Z } } ^ { L } ] ) , } } \end{array}
$$

![](images/2.jpg)  

Figure 2: Performance comparison of different graph CLbased methods with their variants.

where $L$ denotes the number of GNN layers, and $Z ^ { l } ~ \in ~ \mathbb { R } ^ { | \mathcal { V } | \times d }$ denotes the node representations at the $l$ th GNN layer, capturing the $l$ -hop neighbor information. Here, $\mathbf { Z } ^ { 0 }$ is the trainable ID embedding matrix. The readout function Readout $( \cdot )$ is used to summarize all representations for prediction. Then, the predicted score is defined as the similarity between the user and item representations (e.g., inner product, $\hat { y } _ { u i } = z _ { u } ^ { T } z _ { i } )$ . For the recommendation optimization objective, most studies use the pairwise Bayesian Personalized Ranking (BPR) [36] loss for model training, denoted as $\mathcal { L } _ { b p r }$ . In addition, the graph CL-based methods [4, 29, 51, 60] propose to further improve the recommendation performance by performing contrastive learning between two contrastive views. Specifically, given two view representations $\mathbf { z } _ { v } ^ { \prime }$ and $\mathbf { z } _ { \boldsymbol { v } } ^ { \prime \prime }$ of a node (e.g., obtained by two augmented graphs [51]), the optimization objective of CL based on InfoNCE [42] loss is:

$$
\mathcal { L } _ { c l } = - \log \frac { e ^ { s ( \mathbf { z } _ { v } ^ { \prime } , \mathbf { z } _ { v } ^ { \prime \prime } ) / \tau } } { e ^ { s ( \mathbf { z } _ { v } ^ { \prime } , \mathbf { z } _ { v } ^ { \prime \prime } ) / \tau } + \sum _ { \tilde { v } \in \mathcal { V } _ { \mathrm { n e g } } } e ^ { s ( \mathbf { z } _ { v } ^ { \prime } , \mathbf { z } _ { \tilde { v } } ^ { \prime \prime } ) / \tau } } ,
$$

where $s ( \cdot )$ denotes the cosine similarity function, $\tau$ is the temperature coefficient, $v$ is a user/item, and $\mathcal { V } _ { \mathrm { n e g } }$ denotes the set of negative samples, such as in-batch negatives. Finally, the joint learning scheme of graph CL-based CF is outlined as follows:

$$
\mathcal { L } = \mathcal { L } _ { b p r } + \mu \mathcal { L } _ { c l } ,
$$

where $\mu$ is a hyperparameter for balance between two objectives.

# 2.2 Alignment between Perturbed Views is Ineffective

To further emphasize our motivation, we proceed with an empirical analysis to explore the limitations of existing methods that disrupt collaborative information. Following previous works [12, 46], when the number of negative examples is large, the asymptotics of the InfoNCE loss (2) can be expressed by the following equation:

$$ - \frac { 1 } { \tau } \underset { ( \mathbf { z } , \mathbf { z } ^ { + } ) \sim p _ { \mathrm { p o s } } } { \mathbb { E } } \left[ s ( \mathbf { z } , \mathbf { z } ^ { + } ) \right] + \underset { \mathbf { z } \sim p _ { \mathrm { d a t a } } } { \mathbb { E } } \left[ \log _ { \mathbf { z } ^ { - } \sim p _ { \mathrm { d a t a } } } \left[ e ^ { s ( \mathbf { z } , \mathbf { z } ^ { - } ) / \tau } \right] \right] ,
$$ where $ { p _ { \mathrm { p o s } } }$ denotes the tribution  positive pairs, an $ { p _ { \mathrm { d a t a } } }$ denotes the overall data distribution. Intuitively, the first term maintains the similarity of positive pairs, whereas the second term pushes negative pairs apart. These are formally defined as the alignment and uniformity of representations on the unit hypersphere [46]. Here, we try to investigate the contributions of the above two terms by individually disabling their effects. Specifically, we conduct experiments on three representative graph CL-based CF models: SGL [51], SimGCL [60], and LightGCL [4]. For each model, we introduce two variants: (a) w/o U stops the gradient of similarity calculations for negative pairs in Eq. (2) (using det ach function in Pytorch), which leads to the breakdown of uniformity in Eq. (4). (b) w/o A stops the gradient between positive pairs in Eq. (2), resulting in the breakdown of alignment in Eq. (4). From the results in Figure 2, we can observe the following two phenomena:

![](images/3.jpg)  

Figure 3: The overall framework of our CoGCL, which enhances graph CL by constructing contrastive views that imply stronger collaborative information via discrete codes.

Disabling uniformity and only pulling the positive pairs together does not yield a significant improvement compared to LightGCN. Furthermore, SGL w/o U produces a decrease in performance. Disabling alignment leads to minimal negative impact and might even result in a slight performance improvement. Generally, alignment between positive examples in the above methods could be ineffective or potentially harmful. We argue that perturbation methods such as stochastic edge/node dropout (i.e. SGL), random noise (i.e.,, SimGCL), and incomplete reconstruction of adjacency matrix by SVD (i.e., LightGCL) could disrupt the collaborative information within contrastive views [27, 29, 55], and alignment based on these contrastive views may mislead model learning in graph CL.

# 3 METHODOLOGY

In this section, we present our proposed CoGCL, a novel framework to enhance graph CL by constructing contrastive views that imply stronger collaborative information via discrete codes. The overall framework of our proposed approach is illustrated in Figure 3.

# 3.1 Approach Overview

As mentioned in Sections 1 and 2, our basic idea is to enhance contrastive view generation and improve graph CL by introducing discrete codes associated with rich collaborative information. To this end, we make efforts in the following aspects: •End-To-End Discrete Code Learning (Section 3.2): In order to elegantly learn discrete codes associated with rich collaborative information to represent users and items, we present an end-toend multi-level vector quantizer, which quantizes user and item representations encoded by GNN into discrete codes. Reliable and Informative Contrastive View Generation (Section 3.3): Given the learned discrete codes, we use them for reliable and informative contrastive views by proposing virtual neighbor augmentation and semantic relevance sampling, respectively. Triple-View Graph Contrastive Learning (Section 3.4): Based on the generated contrastive views, we finally introduce triple-view graph contrastive learning to achieve alignment across multiple contrastive views, so as to integrate the stronger collaborative information contained in these views into model learning.

# 3.2 End-To-End Discrete Code Learning

As introduced before, we aim to learn discrete codes rich in collaborative information for users and items to enhance contrastive view generation. This involves (a) encoding user and item representations via GNN (Section 3.2.1), and (b) learning end-to-end multi-level vector quantizer to map the encoded representations into discrete codes (Section 3.2.2). 3.2.1 Representation Encoding via GNN. In line with previous works [29, 51, 60], we adopt LightGCN [15] as the GNN encoder in our framework to propagate neighbor information across interaction graph due to its simplicity and effectiveness. Notably, unlike previous implementations, we incorporate dropout on the input representation of each layer (instead of edge dropout on the graph structure) to mitigate overfitting. The process can be written as:

$$
\mathbf { Z } ^ { l } = \mathrm { G N N } ( \rho ( \mathbf { Z } ^ { l - 1 } ) , G ) ,
$$

where $\rho ( \cdot )$ denotes the dropout operation. As for the readout function, we follow SimGCL [60] to skip $\mathbf { Z } ^ { 0 }$ , which shows slight performance improvement in graph CL-based CF. Subsequently, the user and item representations are denoted as $z _ { u }$ and $z _ { i }$ , respectively, which will be applied for joint learning of the recommendation task and multi-level code. 3.2.2 End-To-End Multi-Level Code Learning. Given user and item representations, common approaches for learning discrete codes include hierarchical clustering [32, 39], semantic hashing [5], and vector quantization [14, 34]. Our CoGCL adopts the multi-level vector quantization (vQ) method in an end-to-end manner, such as residual quantization (RQ) [9] and product quantization (PQ) [21]. Next, we take discrete code learning for users as an example, and icm ods c milgonly A eachi b $h$   
$C ^ { h } = \{ { \bf e } _ { k } ^ { h } \} _ { k = 1 } ^ { K }$ $\mathbf { e } _ { k } ^ { h }$   
$H$   
quantization process can be expressed as:

$$
c _ { u } ^ { h } = \underset { k } { \arg \operatorname* { m a x } } P ( k | \mathbf { z } _ { u } ^ { h } ) , \quad P ( k | \mathbf { z } _ { u } ^ { h } ) = \frac { e ^ { s ( \mathbf { z } _ { u } ^ { h } , \mathbf { e } _ { k } ^ { h } ) / \tau } } { \sum _ { j = 1 } ^ { K } e ^ { s ( \mathbf { z } _ { u } ^ { h } , \mathbf { e } _ { j } ^ { h } ) / \tau } } ,
$$

where $c _ { u } ^ { h }$ is the $h$ -th code for the user, $\mathbf { z } _ { u } ^ { h }$ denotes user representation at the $h$ -th level. RQ calculates residuals as representations for each level, denoted by ${ \bf z } _ { u } ^ { h + 1 } = { \bf z } _ { u } ^ { h } - { \bf e } _ { c _ { h } } ^ { h }$ and $\mathbf { z } _ { u } ^ { 1 } = \mathbf { z } _ { u }$ Psplits $\mathbf { z } _ { u }$ into $H$ sub-vectors $\mathbf { z } _ { u } = \left[ \mathbf { z } _ { u } ^ { 1 } ; \ldots ; \mathbf { z } _ { u } ^ { H } \right]$ , each of dimension $d / H$ Here we do not adopt the Euclidean distance commonly used in prior VQ works [14, 34, 44, 64] but cosine similarity, which is to synchronize with the similarity measure in CL (Eq. (2)). Our optimization objective is to maximize the likelihood of assigning representations to their corresponding centers via CrossEntropy (CE) loss. Formally, the training loss for user discrete code learning is:

$$
\mathcal { L } _ { c o d e } ^ { U } = - \frac { 1 } { H } \sum _ { h = 1 } ^ { H } \log P ( c _ { u } ^ { h } | \mathbf { z } _ { u } ^ { h } ) ,
$$

where LU $\mathcal { L } _ { c o d e } ^ { U }$ denotes the discrete code loss on the user side, and the loss for items is defined similarly, denoted by $\mathcal { L } _ { c o d e } ^ { I }$ The total cete c ss i $\mathcal { L } _ { c o d e } = \mathcal { L } _ { c o d e } ^ { U } + \mathcal { L } _ { c o d e } ^ { \bar { I } }$ ·

# 3.3 Reliable and Informative Contrastive View Generation

Compared to previous methods [4, 51, 60] involving information disruption, our motivation to strengthen collaborative information requires us to develop a reliable and informative approach for contrastive view generation via the learned discrete codes. Below, we introduce virtual neighbor augmentation (Section 3.3.1) and semantic relevance sampling (Section 3.3.2) to enhance the neighborhood structure and semantic relevance of contrastive views, respectively. 3.3.1 Virtual Neighbor Augmentation via Discrete Codes. In order to generate reliable contrastive views with enhanced neighborhood structure, we use discrete codes for virtual neighbor augmentation in the graph. For instance, considering user $u$ , we select nodes from the user's neighbors $N _ { u }$ with a probability of $\mathcal { P }$ to create augmented data, denoted as $\mathcal { N } _ { u } ^ { \mathrm { a u g } }$ Then we design two operators on graph structure to augment the node neighbors, i.e., "replace" and "add". The former replaces the neighbor items with their corresponding codes, without retaining the original edges, while the latter directly adds the codes as virtual neighbors. All augmentation operations strictly rely on observed interactions to ensure reliability. Formally, the augmented edge of $u$ can be expressed as:

$$
\begin{array} { r l } & { { \mathcal E } _ { u } ^ { c } = \left\{ ( u , c _ { i } ^ { h } ) | i \in N _ { u } ^ { \mathrm { a u g } } , h \in \{ 1 , . . . , H \} \right\} , } \\ & { { \mathcal E } _ { u } ^ { r } = \left\{ ( u , i ) | i \in ( N _ { u } \setminus N _ { u } ^ { \mathrm { a u g } } ) \right\} \cup { \mathcal E } _ { u } ^ { c } , } \\ & { { \mathcal E } _ { u } ^ { a } = \{ ( u , i ) | i \in N _ { u } \} \cup { \mathcal E } _ { u } ^ { c } , } \end{array}
$$

where $\mathcal { E } _ { u } ^ { c }$ denotes the edges between user $u$ and discrete codes, $\mathcal { E } _ { u } ^ { r }$ is all interaction edges of the user with "replace" augmentation, and $\mathcal { E } _ { u } ^ { a }$ is edges with "add" augmentation. In this case, discrete codes can be regarded as virtual neighbors of the user. The operations described above, which entail either replacing the original neighbor with several virtual neighbors or adding extra virtual neighbors, can bring richer neighbor information and effectively alleviate the sparsity of the graph. The graph augmentation for items can be symmetrically performed. To acquire a pair of augmented nodes for CL, we perform two rounds of virtual neighbor augmentation. The augmented graphs are depicted as follows:

$$
\mathcal { G } ^ { 1 } = ( \widetilde { \mathcal { V } } , \mathcal { E } ^ { o _ { 1 } } ) , \quad \mathcal { G } ^ { 2 } = ( \widetilde { \mathcal { V } } , \mathcal { E } ^ { o _ { 2 } } ) , \quad o _ { 1 } , o _ { 2 } \in \{ r , a \}
$$

where the node set $\widetilde { \mathcal { V } } = \{ \mathcal { U } \cup C ^ { U } \cup \mathcal { I } \cup C ^ { I } \}$ comprises all users, items and their corresponding discrete codes. Two stochastic operators $o _ { 1 }$ and $o _ { 2 }$ are selected from "replace" $( i . e . , r )$ and "add" (i.e., a). ${ \varepsilon } ^ { o _ { 1 } }$ and $\varepsilon ^ { o _ { 2 } }$ denote the edge sets resulting from the aforementioned virtual neighbor augmentation for all users and items. The augmented nodes in the two graphs possess abundant (extensive virtual neighbors) and homogeneous (substantial common neighbors) neighbor structural information. Alignment between the two augmented nodes is helpful to introduce more neighbor structure information into the model. Following SGL [51], we update the discrete codes and augmented graphs once per epoch during training.

3.3.2 Semantic Relevance Sampling via Discrete Codes. In our framework, we not only consider different augmented views of the same node as positive samples, but also regard distinct users/items with similar semantics as mutually positive, which leads to a more informative contrastive view. This emphasizes the alignment of similar instances, rather than indiscriminately distancing different ones [53, 60]. Notably, different from NCL [29], which learns cluster centers based on the EM algorithm as anchors, we measure semantic relevance in a more fine-grained manner based on discrete codes. Specifically, we assess the semantic relevance of users in two ways: (a) Shared codes: The discrete codes we learned are correlated with the collaborative semantics of user representations. Sharing codes between two users indicates fine-grained semantic relevance. Thus, we identify users who share at least H-1 codes as positive. (b) Shared target: When two users share a common interacted target, that is, they possess the same prediction label in the dataset, we also consider them to be relevant. This supervised positive sampling method has shown its effectiveness in various scenarios, including sentence embedding [12] and sequential recommendation [33]. Given the positive set combined by the instances from the above two groups, we pair a sampled relevant instance with each user for CL. Furthermore, semantically relevant positives of items can also be obtained in a symmetrical way. By performing CL within the sampled instances above, we aim to enhance the clustering among similar users/items and improve semantic learning.

# 3.4 Triple-View Graph Contrastive Learning

After the above contrastive view generation methods, we can obtain three contrastive views with stronger collaborative information for each node through virtual neighbor augmentation and semantic relevance sampling: two augmented nodes with more abundant neighborhood structure and a semantically relevant user/item. In this part, we first introduce how to encode multi-view node representations, and then present our triple-view graph contrastive learning approach to integrate structural and semantic information effectively. 3.4.1 Multi-View Representation Encoding. For the two augmented graphs, we introduce additional learnable embeddings of user and item discrete codes to serve as supplemental inputs, denoted as $\mathbf { Z } ^ { c } \in \mathbb { R } ^ { ( | C ^ { U } | + | C ^ { I } | ) \times d }$ Theinput embedding matrix for augmented graphs is formed by concatenating ID embeddings with code embeddings, denoted as $\dot { \bf Z } ^ { 0 } = [ { \bf Z } ^ { 0 } ; { \bf Z } ^ { c } ]$ Then we obtain representations of different views based on the same GNN encoder in Section 3.2.1:

$$
\begin{array} { r } { \mathbf { Z } _ { 1 } ^ { l } = \mathrm { G N N } ( \rho ( \mathbf { Z } _ { 1 } ^ { l - 1 } ) , \mathcal { G } ^ { 1 } ) , \quad \mathbf { Z } _ { 2 } ^ { l } = \mathrm { G N N } ( \rho ( \mathbf { Z } _ { 2 } ^ { l - 1 } ) , \mathcal { G } ^ { 2 } ) , } \end{array}
$$

where the initial representations are set as ${ \bf Z } _ { 1 } ^ { 0 } = { \bf Z } _ { 2 } ^ { 0 } = \widetilde { { \bf Z } } ^ { 0 }$ After applying the readout function, we denote the representations of these two views as $\mathbf { Z ^ { \prime } }$ , and $\mathbf { Z } ^ { \prime \prime }$ , respectively. As for the semantically relevant user/item, we directly adopt the node representation obtained based on the initial interaction graph in Section 3.2.1 due to no structural augmentation. Moreover, the representation dropout we introduced can also be regarded as a minor data augmentation. The distinct dropout masks applied during the two forward propagations result in different features [12, 33, 56, 66]. 3.4.2 Alignment Between Neighbor Augmented Views. As detailed in Section 3.3.1, the two augmented nodes resulting from two rounds of virtual neighbor augmentation possess abundant neighbor structures. Therefore, we aim to incorporate more structural information and improve model efficacy by aligning these neighbor augmented views. Formally, the alignment objective on the user side is as follows:

$$
\mathcal { L } _ { a u g } ^ { U } = - \left( \log \frac { e ^ { s ( \mathbf { z } _ { u } ^ { \prime } , \mathbf { z } _ { u } ^ { \prime \prime } ) / \tau } } { \sum _ { \tilde { u } \in \mathcal { B } } e ^ { s ( \mathbf { z } _ { u } ^ { \prime } , \mathbf { z } _ { \tilde { u } } ^ { \prime \prime } ) / \tau } } + \log \frac { e ^ { s ( \mathbf { z } _ { u } ^ { \prime \prime } , \mathbf { z } _ { u } ^ { \prime } ) / \tau } } { \sum _ { \tilde { u } \in \mathcal { B } } e ^ { s ( \mathbf { z } _ { u } ^ { \prime \prime } , \mathbf { z } _ { \tilde { u } } ^ { \prime } ) / \tau } } \right) ,
$$

where $u$ and $\tilde { u }$ are users in batch data $\mathcal { B }$ . $\mathbf { z } _ { u } ^ { \prime }$ and $\mathbf { z } _ { u } ^ { \prime \prime }$ denote two different user representations after virtual neighbor augmentations. The loss consists of two terms, representing the bidirectional alignment of the two views. Analogously, we calculate the CL loss for the item side as $\mathcal { L } _ { a u g } ^ { I }$ Te o   e o ntedi $\mathcal { L } _ { a u g } = \mathcal { L } _ { a u g } ^ { U } + \mathcal { L } _ { a u g } ^ { I }$ 3.4.3 Alignment Between Semantically Relevant Users/Items. Following the semantics relevance sampling method in Section 3.3.2, we randomly select a positive example with similar collaborative semantics for each user $u$ , denoted as $u ^ { + }$ Then we align these relevant users to incorporate more collaborative semantic information into the model. The alignment loss can be written as:

$$
\mathcal { L } _ { s i m } ^ { U } = - \left( \log \frac { e ^ { s ( \mathbf { z } _ { u } ^ { \prime } , \mathbf { z } _ { u ^ { + } } ) / \tau } } { \sum _ { \tilde { u } \in \widetilde { \mathcal { B } } } e ^ { s ( \mathbf { z } _ { u } ^ { \prime } , \mathbf { z } _ { \tilde { u } } ) / \tau } } + \log \frac { e ^ { s ( \mathbf { z } _ { u } ^ { \prime \prime } , \mathbf { z } _ { u ^ { + } } ) / \tau } } { \sum _ { \tilde { u } \in \widetilde { \mathcal { B } } } e ^ { s ( \mathbf { z } _ { u } ^ { \prime \prime } , \mathbf { z } _ { \tilde { u } } ) / \tau } } \right) ,
$$

where $( u , u ^ { + } )$ is a positive user pair, and $\widetilde { \mathcal B }$ is the sampled data in a batch. The two components of the equation correspond to the alignment between two augmented views and the similar user, respectively. Furthermore, combining the symmetric alignment loss on the item side, the total alignment loss between similar users/items is $\mathcal { L } _ { s i m } = \mathcal { L } _ { s i m } ^ { U } + \mathcal { L } _ { s i m } ^ { I }$ m Lim 3.4.4 Overall Optimization. In the end, by combining the recommendation loss(i.e., BPR loss), discrete code learning objective (Eq. (7)) and all contrastive learning loss (Eq. (13) and Eq. (14)), our CoGCL is jointly optimized by minimizing the following overall loss:

$$
\mathcal { L } = \mathcal { L } _ { b p r } + \lambda \mathcal { L } _ { c o d e } + \mu \mathcal { L } _ { a u g } + \eta \mathcal { L } _ { s i m } ,
$$

where $\lambda , \mu$ and $\eta$ are hyperparameters for the trade-off between various objectives.

# 3.5 Discussion

In this section, we make a brief comparison with existing graph CL-based CF methods to highlight the novelty and contributions of CoGCL. According to how to construct contrast views, existing methods can be divided into two categories: structure augmentation and representation augmentation.

Table 1: Statistics of the preprocessed datasets.   

<table><tr><td>Datasets</td><td>#Users</td><td>#Items</td><td>#Interactions</td><td>Sparsity</td></tr><tr><td>Instrument</td><td>48,453</td><td>21,413</td><td>427,674</td><td>99.959%</td></tr><tr><td>Office</td><td>181,878</td><td>67,409</td><td>1,477,820</td><td>99.988%</td></tr><tr><td>Gowalla</td><td>29,858</td><td>40,988</td><td>1,027,464</td><td>99.916%</td></tr><tr><td>iFashion</td><td>300,000</td><td>81,614</td><td>1,607,813</td><td>99.993%</td></tr></table>

Structural augmentation methods typically generate contrastive views by perturbing the graph structure like stochastic node/edge dropout [51]. Several recent efforts attempt to use well-founded methods for structural perturbations, such as SVD-based adjacency matrix reconstruction [4] and graph rationale discovery based on masked autoencoding [27]. However, perturbations on sparse graphs can not construct more informative contrastive views. As a comparison, our approach is both reliable and informative, leveraging discrete codes as virtual neighbors to reliably enhance node neighborhood structure and alleviate data sparsity. The alignment between two augmented nodes with abundant neighbors is beneficial for the integration of further collaborative information. Representation augmentation methods involve modeling additional node representations as contrastive views, such as learning hypergraph representations [53] and adding random noise [60]. However, limited by the low-rank hypergraph matrix and the noise perturbation, the generated contrastive views also suffer from the semantic disruption issue. Besides, these methods typically indiscriminately distinguish representations of different instances. In contrast, we consider users/items with shared codes or interaction targets as semantically relevant. By aligning users/items with similar collaborative semantics, we can further unleash the potential of CL and enhance the semantic learning of the model.

# 4 EXPERIMENT

# 4.1 Experiment Setup

4.1.1 Dataset. We evaluate our proposed approach on four public datasets: Instrument and Office subsets from the most recent Amazon2023 benchmark [17], Gowalla [10], Alibaba-iFashion [8]. For Instrument and Office datasets, we filter out low-activity users and items with less than five interactions. For Gowalla dataset, we use 10-core filtering to ensure the data quality following prior works [15, 48]. As for the sparser iFashion dataset, we employ the data processed by [51], which randomly samples $3 0 0 \mathrm { k }$ users and their interactions. Our processed datasets vary in terms of domain, scale, and sparsity. Their statistics are summarized in Table 1. For each dataset, we split the observed interactions into training, validation, and testing sets with a ratio of 8:1:1. 4.1.2 Baseline Models. We adopt the following competitive baselines for comparison with our CoGCL, which includes traditional CF models: (1) BPR [36], (2) GCMC [41], (3) NGCF [48], (4) DGCF [49], (5) LightGCN [15], (6) SimpleX [31], as well as various representative CL-based models: (7) SLRec [56], (8) SGL [51], (9) NCL [29], (10) HCCF [53], (11) GFormer [27], (12) SimGCL [60], (13) LightGCL [4]. A more detailed introduction to the above baseline models is given in Appendix B.1. 4.1.3 Evaluation Settings. To evaluate the performance of the above models, we adopt two widely used metrics in recommendation: Recall@N and Normalized Discounted Cumulative Gain $\mathrm { \Omega } ( { \mathrm { N D C G } } ) @ N$ In this paper, we set $N$ to 5, 10, and 20. For the sake of rigorous comparison, we perform full ranking evaluation [62, 63] over the entire item set instead of sample-based evaluation.

4.1.4 Implementation Details. For all comparison models, we use Adam for optimization and set the embedding dimension to 64 uniformly. The batch size is 4096, and the number of GNN layers in GNN-based methods is set to 3. To ensure a fair comparison, we utilize grid search to obtain optimal performance according to the hyperparameter settings reported in the original papers of baseline methods. For our approach, we employ RQ as the default discrete code learning method. The number of code levels $H = 4$ , and the temperature $\tau = 0 . 2$ The codebook size $K$ is set to 256 for Instrument and Gowalla datasets, and 512 for Office and iFashion datasets due to their larger scale. The hyperparameters $\lambda$ are tuned in {5, 1, 0.5}, while $\mu$ and $\eta$ are tuned in {5, 1, 0.5, 0.2, 0.1, 0.05, 0.02, 0.01, 0.005, 0.001}. The probabilities of "replace" and "add" in virtual neighbor augmentation are tuned in $\{ 0 . 0 1 , 0 . 0 5 , 0 . 1 , 0 . 1 5 , 0 . 2 , 0 . 2 5$ 0.3, 0.4, 0.5, 0.6}. For experiments on hyperparameter tuning, please refer to Appendix B.2.

# 4.2 Overall Performance

The overall results for performance comparison between CoGCL and other baseline models are shown in Table 2. From the results, we find the following observations: The CL-based methods (e.g., SGL, NCL, SimGCL, LightGCL) show consistent superiority over the traditional MF methods (e.g., BPR, SimpleX) and GNN-based methods (e.g., NGCF, LightGCN). This performance improvement could be attributed to the selfsupervised signals brought by contrastive learning, which helps to alleviate data sparsity and enhance representation learning. Within CL-based methods, structure augmentation and representation augmentation exhibit distinct strengths in different scenarios. Specifically, SimGCL, as a typical representation augmentation method, performs better than other baseline models on Instrument and Gowalla datasets, thanks to the improved uniformity achieved by incorporating random noise. Conversely, the most competitive models for Office and iFashion datasets are GFormer and LightGCL, respectively, both of which are structure augmentation methods. In contrast, SGL tends to underperform, indicating that stochastic edge/node dropout possibly interferes with crucial structural information, leading to adverse impacts. Finally, our proposed CoGCL consistently maintains the best performance in all cases, achieving significant improvements over baseline methods. Different from these baseline models, CoGCL unleashes the potential of CL by constructing contrastive views that imply stronger collaborative information. Based on the learned discrete codes rich in collaborative information, we introduce virtual neighbor augmentation and semantic relevance sampling to enhance the neighborhood structure and semantic relevance of contrasting views, respectively. Furthermore, triple-view graph contrastive learning across the obtained contrastive views brings supplemental collaborative insights to the model. As a result, CoGCL ebis ongrbustness anefeivenes n spadtasets , Office, iFashion). a  r fnmeohraasTheenbe p indicated in bold and underlined font, respectively.   

<table><tr><td>Dataset</td><td>Metric</td><td>BPR</td><td>GCMC</td><td>NGCF</td><td>DGCF</td><td>LightGCN SimpleX</td><td></td><td>SLRec</td><td>SGL</td><td>NCL</td><td>HCCF GFormer</td><td></td><td>SimGCL</td><td>LightGCL</td><td>CoGCL</td><td>Improv.</td></tr><tr><td rowspan="5">Instrument</td><td>Recall@5</td><td>0.0293</td><td>0.0334</td><td>0.0391</td><td>0.0401</td><td>0.0435</td><td>0.0386</td><td>0.0381</td><td>0.0449</td><td>0.0449</td><td>0.0456</td><td>0.0471</td><td>0.0470</td><td>0.0468</td><td>0.0515</td><td>9.34%</td></tr><tr><td>NDCG@5</td><td>0.0194</td><td>0.0218</td><td>0.0258</td><td>0.0269</td><td>0.0288</td><td>0.0244</td><td>0.0256</td><td>0.0302</td><td>0.0302</td><td>0.0303</td><td>0.0314</td><td>0.0316</td><td>0.0310</td><td>0.0345</td><td>9.18%</td></tr><tr><td>Recall@10</td><td>0.0469</td><td>0.0532</td><td>0.0617</td><td>0.0628</td><td>0.0660</td><td>0.0631</td><td>0.0574</td><td>0.0692</td><td>0.0685</td><td>0.0703</td><td>0.0715</td><td>0.0717</td><td>0.0715</td><td>0.0788</td><td>9.90%</td></tr><tr><td>NDCG@10</td><td>0.0250</td><td>0.0282</td><td>0.0331</td><td>0.0342</td><td>0.0361</td><td>0.0324</td><td>0.0319</td><td>0.0380</td><td>0.0377</td><td>0.0383</td><td>0.0393</td><td>0.0395</td><td>0.0391</td><td>0.0435</td><td>10.13%</td></tr><tr><td>Recall@20</td><td>0.0705</td><td>0.0824</td><td>0.0929</td><td>0.0930</td><td>0.0979</td><td>0.0984</td><td>0.0820</td><td>0.1026</td><td>0.1011</td><td>0.1028</td><td>0.1041</td><td>0.1057</td><td>0.1042</td><td>0.1152</td><td>8.99%</td></tr><tr><td rowspan="7"></td><td>NDCG@20</td><td>0.0310</td><td>0.0357</td><td>0.0411</td><td>0.0419</td><td>0.0442</td><td>0.0413</td><td>0.0381</td><td>0.0466</td><td>0.0459</td><td>0.0466</td><td>0.0478</td><td>0.0482</td><td>0.0474</td><td>0.0526</td><td>9.13%</td></tr><tr><td>Recall@5</td><td>0.0204</td><td>0.0168</td><td>0.0178</td><td>0.0258</td><td>0.0277</td><td>0.0291</td><td>0.0294</td><td>0.0349</td><td>0.0293</td><td>0.0340</td><td>0.0353</td><td>0.0349</td><td>0.0338</td><td>0.0411</td><td>16.43%</td></tr><tr><td>NDCG@5</td><td>0.0144</td><td>0.0109</td><td>0.0116</td><td>0.0177</td><td>0.0186</td><td>0.0199</td><td>0.0209</td><td>0.0242</td><td>0.0201</td><td>0.0230</td><td>0.0245</td><td>0.0240</td><td>0.0232</td><td>0.0287</td><td>17.14%</td></tr><tr><td>Recall@10</td><td>0.0285</td><td>0.0270</td><td>0.0279</td><td>0.0380</td><td>0.0417</td><td>0.0422</td><td>0.0402</td><td>0.0493</td><td>0.0434</td><td>0.0489</td><td>0.0492</td><td>0.0494</td><td>0.0490</td><td>0.0582</td><td>17.81%</td></tr><tr><td>NDCG@10</td><td>0.0170</td><td>0.0141</td><td>0.0149</td><td>0.0217</td><td>0.0231</td><td>0.0241</td><td>0.0244</td><td>0.0289</td><td>0.0243</td><td>0.0282</td><td>0.0292</td><td>0.0289</td><td>0.0280</td><td>0.0343</td><td>17.47%</td></tr><tr><td>Recall@20</td><td>0.0390</td><td>0.0410</td><td>0.0438</td><td>0.0544</td><td>0.0605</td><td>0.0602</td><td>0.0534</td><td>0.0681</td><td>0.0629</td><td>0.0677</td><td>0.0672</td><td>0.0689</td><td>0.0698</td><td>0.0785</td><td>12.46%</td></tr><tr><td>NDCG@20</td><td>0.0197</td><td>0.0178</td><td>0.0189</td><td>0.0258</td><td>0.0279</td><td>0.0287</td><td>0.0277</td><td>0.0336</td><td>0.0292</td><td>0.0331</td><td>0.0338</td><td>0.0337</td><td>0.0332</td><td>0.0393</td><td></td><td>14.18%</td></tr><tr><td rowspan="6">Gowalla</td><td>Recall@5</td><td>0.0781</td><td>0.0714</td><td>0.0783</td><td>0.0895</td><td>0.0946</td><td>0.0782</td><td>0.0689</td><td>0.1047</td><td>0.1040</td><td>0.0836</td><td>0.1042</td><td>0.1047</td><td>0.0947</td><td>0.1092</td><td>4.30%</td></tr><tr><td>NDCG@5</td><td>0.0707</td><td>0.0633</td><td>0.0695</td><td>0.0801</td><td>0.0854</td><td>0.0712</td><td>0.0613</td><td>0.0955</td><td>0.0933</td><td>0.0749</td><td>0.0935</td><td>0.0959</td><td>0.0860</td><td>0.0995</td><td>3.75%</td></tr><tr><td>Recall@10</td><td>0.1162</td><td>0.1089</td><td>0.1150</td><td>0.1326</td><td>0.1383</td><td>0.1187</td><td>0.1045</td><td>0.1520</td><td>0.1508</td><td>0.1221</td><td>0.1515</td><td>0.1525</td><td>0.1377</td><td>0.1592</td><td>4.39%</td></tr><tr><td>NDCG@10</td><td>0.0821</td><td>0.0749</td><td>0.0808</td><td>0.0932</td><td>0.0985</td><td>0.0834</td><td>0.0722</td><td>0.1092</td><td>0.1078</td><td>0.0866</td><td>0.1085</td><td>0.1100</td><td>0.0988</td><td>0.1145</td><td>4.09%</td></tr><tr><td>Recall@20</td><td>0.1695</td><td>0.1626</td><td>0.1666</td><td>0.1914</td><td>0.2002</td><td>0.1756</td><td>0.1552</td><td>0.2160</td><td>0.2130</td><td>0.1794</td><td>0.2166</td><td>0.2181</td><td>0.1978</td><td>0.2253</td><td>3.30%</td></tr><tr><td>NDCG@20</td><td>0.0973</td><td>0.0903</td><td>0.0956</td><td>0.1100</td><td>0.1161</td><td>0.0996</td><td>0.0868</td><td>0.1274</td><td>0.1254</td><td>0.1029</td><td>0.1271</td><td>0.1286</td><td>0.1159</td><td>0.1333</td><td>3.65%</td></tr><tr><td rowspan="6">iFashion</td><td>Recall@5</td><td>0.0195</td><td>0.0240</td><td>0.0234</td><td>0.0297</td><td>0.0309</td><td>0.0345</td><td>0.0237</td><td>0.0377</td><td>0.0330</td><td>0.0419</td><td>0.0354</td><td>0.0401</td><td>0.0423</td><td>0.0463</td><td>9.46%</td></tr><tr><td>NDCG@5</td><td>0.0128</td><td>0.0156</td><td>0.0151</td><td>0.0197</td><td>0.0205</td><td>0.0231</td><td>0.0157</td><td>0.0252</td><td>0.0219</td><td>0.0280</td><td>0.0235</td><td>0.0267</td><td>0.0284</td><td>0.0310</td><td>9.15%</td></tr><tr><td>Recall@10</td><td>0.0307</td><td>0.0393</td><td>0.0384</td><td>0.0459</td><td>0.0481</td><td>0.0525</td><td>0.0361</td><td>0.0574</td><td>0.0501</td><td>0.0636</td><td>0.0540</td><td>0.0608</td><td>0.0641</td><td>0.0696</td><td>8.58%</td></tr><tr><td>NDCG@10</td><td>0.0164</td><td>0.0206</td><td>0.0199</td><td>0.0249</td><td>0.0260</td><td>0.0289</td><td>0.0198</td><td>0.0315</td><td>0.0274</td><td>0.0350</td><td>0.0294</td><td>0.0334</td><td>0.0354</td><td>0.0386</td><td>9.04%</td></tr><tr><td>Recall@20</td><td>0.0470</td><td>0.0623</td><td>0.0608</td><td>0.0685</td><td>0.0716</td><td>0.0770</td><td>0.0535</td><td>0.0846</td><td>0.0742</td><td>0.0929</td><td>0.0790</td><td>0.0897</td><td>0.0932</td><td>0.1010</td><td>8.37%</td></tr><tr><td>NDCG@20</td><td>0.0206</td><td>0.0264</td><td>0.0256</td><td>0.0307</td><td>0.0320</td><td>0.0351</td><td>0.0242</td><td>0.0384</td><td>0.0335</td><td>0.0425</td><td>0.0358</td><td>0.0407</td><td>0.0428</td><td>0.0465</td><td>8.64%</td></tr></table>

![](images/4.jpg)  

Figure 4: Ablation study of data augmentation methods.

# 4.3 Ablation Study

In this part, we first investigate the contribution of various contrastive view generation methods in the proposed approach, and then conduct an in-depth ablation analysis of alignment and uniformity of CL. 4.3.1 Ablation Study of Data Augmentation. In order to explore the contribution of data augmentation methods involved in CoGCL. we evaluate the performance of the following variants: (1) w/o Replace removes the "replace" operator in virtual neighbor augmentation. (2) w/o Add removes the "add" operator in virtual neighbor augmentation. (3) w/o Shared-C removes similar users/items shared codes in semantic relevance sampling. (4) w/o Shared-T removes similar users/items shared interaction target in semantic relevance sampling. The results are shown in Figure 4. We can observe that the exclusion of any data augmentation method would lead to a decrease in performance, which demonstrates that all data augmentation methods employed for contrastive view generation in CoGCL are useful for performance improvement.

Table 3: Performance analysis of alignment and uniformity in CoGCL.   

<table><tr><td rowspan="2">Methods</td><td colspan="2">Instrument</td><td colspan="2">Office</td></tr><tr><td>Recall@10</td><td>NDCG@10</td><td>Recall@10</td><td>NDCG@10</td></tr><tr><td>LightGCN</td><td>0.0660</td><td>0.0361</td><td>0.0417</td><td>0.0231</td></tr><tr><td>CoGCL</td><td>0.0788</td><td>0.0435</td><td>0.0582</td><td>0.0343</td></tr><tr><td>w/o A</td><td>0.0726</td><td>0.0401</td><td>0.0490</td><td>0.0280</td></tr><tr><td>w/o U</td><td>0.0703</td><td>0.0384</td><td>0.0465</td><td>0.0267</td></tr><tr><td>w/o AA</td><td>0.0741</td><td>0.0411</td><td>0.0536</td><td>0.0315</td></tr><tr><td>w/o AU</td><td>0.0762</td><td>0.0421</td><td>0.0542</td><td>0.0306</td></tr><tr><td>w/o SA</td><td>0.0767</td><td>0.0422</td><td>0.0554</td><td>0.0329</td></tr><tr><td>w/o SU</td><td>0.0779</td><td>0.0429</td><td>0.0574</td><td>0.0336</td></tr></table>

4.3.2 Ablation Study of Triple-View Graph Contrastive Learning. Apart from the above techniques, we further investigate how the alignment and uniformity of CL affect our approach. We disable these two terms respectively in the CL losses (i.e., $\mathcal { L } _ { a u g }$ and $\mathcal { L } _ { s i m }$ in Section 3.4) by applying the same gradient-stopping operations in empirical analysis (Section 2.2). Specifically, we construct the following variants for detailed exploration: (1) $\underline { { \mathbf { W } / \mathbf { o } ~ \mathbf { A } } }$ and (2) w/o U are consistent with Section 2.2, denoting disabling alignment and uniformity in CL respectively, including both $\mathcal { L } _ { a u g }$ and $\mathcal { L } _ { s i m }$ (3) w/o AA and (4) w/o AU only involve disabling the above two terms of $\mathcal { L } _ { a u g }$ while keeping $\mathcal { L } _ { s i m }$ constant. (5) w/o SA and (6) w/o SU are analogous variants for $\mathcal { L } _ { s i m }$ and do not change $\mathcal { L } _ { s i m }$ .

As shown in Table 3, the absence of alignment (i.e., ${ \underline { { \mathbf { W } / \mathbf { 0 } } } } \mathbf { A } )$ or uniformity (i.e., $\underline { { \mathbf { w } / \mathbf { o } \mathbf { U } } } )$ within both $\mathcal { L } _ { a u g }$ and $\mathcal { L } _ { s i m }$ leads to a notable performance degradation. This observation verifies that the joint effect of these two elements is crucial for the effectiveness of the proposed approach, rather than relying solely on uniformity. Furthermore, individually disabling uniformity within $\mathcal { L } _ { a u g }$ (i.e., w/o AU) and $\mathcal { L } _ { s i m }$ (i.e., w/o SU) does not result in the significant adverse impact as conjectured. It could be attributed to the shared uniformity effect between the two CL losses in $\mathrm { C o G C L } ,$ which may mutually reinforce each other. In contrast, the individual deactivation of alignment within $\mathcal { L } _ { a u g }$ (i.e., $\underline { { \mathbf { w } / \mathbf { o } \mathbf { A A } } } )$ and $\mathcal { L } _ { s i m }$ (i.e., w/o SA) incurs a pronounced decrease in performance. This provides further evidence that our proposed alignment between the two types of positives brings enhanced collaborative information beyond uniformity.

![](images/5.jpg)  

Figure 5: Performance comparison of different discrete code learning methods.

# 4.4 Further Analysis

4.4.1 Performance Comparison w.r.t. Different Discrete Code Learning Methods. To verify the advancedness of the proposed end-toend discrete code learning method, we compare it with the following three variants: (1) Non-Learnable Code uses Faiss library [23] to generate discrete codes based on trained LightGCN embeddings. The generated codes are non-learnable and remain unchanged during model training. (2) Euclidean Code adopts Euclidean distance to measure the similarity between user/item representations and codebook vectors in Eq. (6), which is consistent with the original RQ method [9]. (3) PQ Code employs PQ instead of RQ as a multi-level quantizer for discrete code learning. We conduct experiments on Instrument and Office datasets, and the results are shown in Figure 5. It can be seen that Non-Learnable Code is less robust compared to the end-to-end learned discrete codes, which may stem from the inability to continuously improve the collaborative information within discrete codes while optimizing the model. In comparison to Euclidean Code and PQ Code, our proposed approach shows superior performance. Unlike Euclidean Code, our method utilizes cosine similarity to synchronize with the similarity measure in CL. Compared with PQ Code, the RQ we applied establishes conditional probability relationships among codes at each level instead of treating them as independent, which is conducive to the semantic modeling of various granularities. 4.4.2 Performance Comparison w.r.t. Data Sparsity. To verify the merit of our approach in alleviating data sparsity, we evaluate CoGCL on user groups with different sparsity levels. Specifically, following prior works [4, 29], we divide users into five groups according to their number of interactions, while keeping the same number of users in each group constant. Subsequently, we evaluate the performance of these five groups of users, and the results are shown in Figure 6. We can see that CoGCL consistently outperforms the baseline model across all sparsity levels. Furthermore, our model shows superior performance and significant improvement in the highly sparse user groups. This phenomenon indicates that CoGCL can achieve high-quality recommendation in scenarios with sparse interactions, which benefits from the additional insights brought by CL between contrastive views with stronger collaborative information.

![](images/6.jpg)  

Figure 6: Performance comparison on user groups with different sparsity levels.

# 5 RELATED WORK

GNN-Based Collaborative Filtering. Graph Neural Networks (GNNs) have become prominent in collaborative filtering (CF) due to their effectiveness in modeling user-item relationships [11, 52]. The core approach involves organizing user-item interaction data into a bipartite graph and learning node representations from the graph structure. Earlier efforts [2, 13] extract the graph information using random walk strategies. With the development of GNNs, the common studies has shifted towards designing effective messagepassing mechanisms to propagate user/item embeddings over the graph [41, 48, 57]. Subsequently, LightGCN [15] and LR-GCCF [6] propose eliminating transformation and non-linear activation to simplify GNNs while improving performance. Furthermore, recent studies are also devoted to enhancing GNNs with various advanced techniques, such as disentangled representation learning [49, 50], hypergraph learning [22, 59] and contrastive learning [4, 29, 51, 60].

Contrastive Learning for Recommendation. Recently, contrastive learning (CL) has demonstrated significant potential in various recommendation scenarios like sequential recommendation [33, 54, 65] and knowledge graph-enhanced recommendation [67, 68]. In the context of GNN-based CF, existing efforts can be categorized into two main approaches according to how the contrastive views are constructed. The first approach is to perform data augmentation over graph structure [4, 27, 35, 51] For instance, SGL [51] randomly drops nodes/edges within the interaction graph to construct augmented graphs. The second approach is to model additional view representations of users and items for CL [26, 29, 40, 53, 58, 60]. Particularly, SimGCL [60] generates contrastive views by adding random noise to node embeddings. Despite their success, the collaborative information within contrastive views may be disrupted in these methods, and thus the potential of CL has not been fully exploited. In this paper, we propose to unleash the potential of CL by constructing contrastive views with stronger collaborative information via discrete codes.

User/Item ID Discretization in Recommendation. ID discretization involves employing a tuple of discrete codes as identifier to represent a user/item instead of the vanilla single ID, achieved through methods like semantic hashing [5, 19, 37], vector quantization [14, 44], etc. These methods allow similar users/items to share certain codes, which can offer valuable prior knowledge for subsequent recommendation models. Initially, the focus was on developing memory- and time-efficient recommendation algorithms by sharing code embeddings [1, 24, 25, 28, 38]. Recently, discrete codes have gained popularity for improving recommendation quality in various scenarios. They are particularly beneficial in alleviating data sparsity and offering prior semantics, which has proven advantageous in transferable recommendation [16], generative sequential recommendation [30, 34, 39, 47] and LLM-based recommendation [18, 64]. Different from these studies, our work aims to employ discrete codes for virtual neighbor augmentation and semantic similarity sampling to enhance graph CL in CF.

# 6 CONCLUSION

In this paper, we proposed a novel framework to enhance graph CL by constructing reliable and informative contrastive views that imply stronger collaborative information. The core idea is to learn discrete codes associated with rich collaborative information for users and items to generate contrastive views. Specifically, we present an end-to-end multi-level vector quantizer to map users and items into discrete codes. These codes are used to enhance the neighborhood structure and semantic relevance of contrastive views. Firstly, we generate dual augmented nodes with abundant neighborhood structures by replacing node neighbors with discrete codes or adding them as virtual neighbors relying on the observed interactions. Secondly, we consider users/items with shared discrete codes as semantically relevant and select similar positive examples based on this semantic relevance. Finally, we introduce a tripleview graph contrastive learning approach to align two augmented nodes and the sampled similar user/item. Extensive experiments on four public datasets demonstrate the effectiveness of our proposed CoGCL. As future work, we attempt to improve the scalability of our framework to extend it to other recommendation scenarios, such as click-through rate prediction and sequential recommendation.