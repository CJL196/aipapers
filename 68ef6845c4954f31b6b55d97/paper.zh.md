# 自我强制：弥合自回归视频扩散中的训练-测试差距

# 荀黄1

郑琦李1 关德 $\mathbf { H e } ^ { 2 }$ 明远周2 埃利·谢赫特曼1 1Adobe Research 2德克萨斯大学奥斯汀分校

https://self-forcing.github.io/

# 摘要

我们介绍了自我强制（Self Forcing），一种针对自回归视频扩散模型的新训练范式。它解决了长期存在的曝光偏差问题，即在推理过程中，模型在真实上下文上训练后，必须基于自身不完美的输出生成序列。与之前基于真实上下文帧去噪未来帧的方法不同，自我强制在训练期间通过执行自回归展开和关键值（KV）缓存，使每一帧的生成依据先前自生成的输出进行条件化。这一策略通过在视频层面进行整体损失监督，直接评估整个生成序列的质量，而不仅仅依赖传统逐帧目标。为了确保训练效率，我们采用了少步扩散模型，并结合随机梯度截断策略，有效平衡了计算成本和性能。我们进一步引入了动态KV缓存机制，使高效自回归视频外推成为可能。大量实验表明，我们的方法在单个GPU上实现了实时流媒体视频生成，延迟不足一秒，同时生成质量匹配甚至超过了显著较慢的非因果扩散模型。

# 1 引言

近年来，视频合成技术取得了巨大的进展，最先进的系统现在能够生成非常真实的内容，并具有复杂的时间动态。然而，这些结果通常是通过扩散变压器（DiT）实现的，它们使用双向注意机制同时对所有帧进行去噪。这种设计允许未来影响过去，并要求一次性生成整个视频，这从根本上限制了它们在实时流媒体应用中的适用性，因为在生成当前帧时未来信息是未知的。

相比之下，自回归（AR）模型[17, 27, 38, 94, 104]按顺序生成视频，这种范式自然与时间媒体的因果结构对齐。这种方法不仅显著减少了生成视频的观看延迟，还解锁了许多应用，包括实时交互内容创作[9, 46]、游戏模拟[11, 61, 78, 102]和机器人学习[42, 96, 101]。然而，由于依赖有损向量量化技术[79]，AR模型往往难以匹配最先进的视频扩散模型所取得的视觉保真度。

为了结合两全其美的优势，最近出现了两种技术，旨在为视频扩散模型提供增强现实生成能力：教师强制（Teacher Forcing，TF）和扩散强制（Diffusion Forcing，D）。教师强制是一种在序列建模中被广泛认可的范式，训练模型根据真实标签预测下一个标记。当应用于视频扩散时，TF涉及使用干净的真实上下文帧对每一帧进行去噪（图1（a）），这种策略通常被称为下帧预测。相对而言，扩散强制对噪声水平独立采样的每一帧视频进行训练，根据带噪声的上下文帧对每一帧进行去噪（图1（b））。这确保了自回归推理场景，其中上下文帧是干净的，而当前帧是有噪声的，已被训练分布覆盖。

![](images/1.jpg)  

Figure 1: Training paradigms for AR video diffusion models. (a) In Teacher Forcing, the model is trained to denoise each frame conditioned on the preceding clean, ground-truth context frames. (b) In Diffusion Forcing, the model is trained to denoise each frame conditioned on the preceding c fams with varyg noi level.oh ( nd  gneate outus that do ot e to the distribution the model generates during inference. (c) Our Self Forcing approach performs autoregressive self-rollout during training, denoising the next frame based on previous context frames generated by itself. A distribution-matching loss (e.g., SiD, DMD, GAN) is computed on the final output video to align the distribution of generated videos with that of real videos. Our training paradigm closely mirrors the inference process, thereby bridging the train-test distribution gap.

然而，使用TF或DF训练的模型在自回归生成过程中往往会遭受误差累积，导致视频质量随时间下降[84, 100, 105]。这个问题通常被称为曝光偏差[60, 71]，即模型完全基于真实上下文进行训练，但在推理时不得不依赖自己的不完善预测，导致在生成过程中出现分布不匹配，从而累积错误。虽然一些方法尝试通过在推理时引入噪音上下文帧来减轻视频扩散模型中的这一问题[8, 11, 105]，但这种设计牺牲了时间一致性，增加了KV缓存设计的复杂性，增加了生成延迟，并未从根本上解决曝光偏差问题。

在这项工作中，我们提出了自我强迫（SF）算法，这是一个解决自回归视频生成中的曝光偏差的新颖算法。受到早期递归神经网络时代序列建模技术的启发，我们的方法通过在训练过程中显式展开自回归生成，弥补了训练和测试分布之间的差距，生成每一帧都是以之前自生成的帧为条件，而不是以真实帧为条件。这使得我们能够对完整生成的视频序列应用整体分布匹配损失进行监督。通过强迫模型遇到并从自身的预测错误中学习，自我强迫有效地减轻了曝光偏差并减少了错误积累。

虽然自我强迫由于其顺序特性可能看起来在计算上具有得天独厚的限制，阻止并行训练，但我们证明它可以在后训练阶段有效实现为一种算法，在此阶段模型不需要大量的梯度更新即可收敛。通过采用几步扩散基础结构和精心设计的梯度截断策略，自我强迫在相同的实际训练时间内比替代的并行策略高效得多，取得了更优的性能。此外，我们引入了一种滚动KV缓存机制，以提高视频外推的效率。

广泛的实验表明，我们的模型能够在单个H100 GPU上以17帧每秒的速度实现实时视频生成，延迟低于一秒，同时与最近的慢速双向和自回归视频扩散模型相比，生成质量具有竞争力或更优。这些进展为真正的互动视频生成用例打开了大门——如直播、游戏和世界模拟——其中延迟预算以毫秒而非分钟来衡量。

# 2 相关文献

视频生成的GANs。早期的视频生成方法主要依赖于生成对抗网络（GANs）[18]，要么使用卷积网络并行生成整个视频[5, 68, 82]，要么采用递归架构逐帧生成[14, 44, 49, 77, 81]。最近，GANs也被应用于提炼视频扩散模型[47, 56, 91, 108]。由于GANs中的生成器在训练和推理过程中遵循相同的过程，因此本质上避免了曝光偏差。我们的工作受这一基本GAN原理的启发，直接优化生成器输出分布与目标分布之间的对齐。

自回归/扩散模型用于视频生成。现代视频生成模型大多数已转向扩散或自回归模型，因为它们具有更强的扩展能力。视频扩散模型通常采用双向注意机制，以同时去噪所有视频帧 [3, 4, 6, 13, 2326, 39, 64, 80, 83, 97]。自回归模型则与下一个标记预测目标进行训练，在推理时顺序生成时空标记 [7, 38, 66, 86, 88, 94]。

自回归-扩散混合模型。最近，整合自回归与扩散框架的混合模型作为视频生成建模的一个有前景的方向开始出现[8, 16, 20, 22, 28, 33, 45, 50, 52, 89, 100, 106, 107]，以及其他序列领域[1, 12, 43, 53, 59, 90, 110]。它们通常依赖于一个长的、迭代的预测链（在时间上是自回归的，空间上是迭代去噪的），这可能导致显著的错误积累。我们的工作通过以自身预测为条件训练模型，并教导其纠正自身错误来解决这个问题。

滚动扩散及其变体。另一项研究工作[35, 67, 69, 76, 93, 105]采用渐进噪声调度训练视频扩散模型，这种方法中，噪声水平从早期帧到后期帧逐渐增加。虽然这些方法支持以较少累积误差的顺序生成长视频，有时也被称为自回归，但它们并不严格遵循自回归链式规则分解。因此，在交互式应用中，它们会表现出显著的延迟，因为在当前帧呈现给用户之前，未来帧会被部分预生成。这种过早的承诺限制了实时用户注入控制的影响，导致立即后续帧的响应性有限。

CausVid。我们的工作与CausVid [100] 最为相关，该方法使用DF方案和分布匹配蒸馏（DMD）训练少步自回归扩散模型。然而，CausVid存在一个关键缺陷，即其训练输出（通过DF生成）并不来自模型在推理时生成的分布，因此DMD损失匹配了错误的分布。我们定位了这个问题，并提出了一个解决方案，旨在匹配真实的模型分布。

# 3 自我强制：通过整体后训练弥合训练与测试的差距

我们首先在第3.1节提供自回归视频扩散模型的正式定义，并描述标准的训练方法。在第3.2节中，我们介绍我们的自强训练算法的主要部分，并描述如何使用几个步骤的扩散模型高效地实现它。在第3.3节中，我们描述各种整体视频级分布匹配训练目标的选择。最后，在第3.4节中，我们介绍一个滚动键值缓存机制，使得能够高效生成任意长度的视频。

# 3.1 初步知识：自回归视频扩散模型

自回归视频扩散模型是一种混合生成模型，它结合了自回归链规则分解和去噪扩散模型用于视频生成。具体而言，给定一系列 $N$ 个视频帧 $\boldsymbol { x } ^ { 1 : N } = ( x ^ { \tilde { 1 } } , x ^ { 2 } , \dots , x ^ { N } )$，有 $\begin{array} { r } { p ( x ^ { 1 : N } ) \ = \ \prod _ { i = 1 } ^ { N } p ( x ^ { i } | x ^ { < i } ) } \end{array}$。每一个条件 $p ( x ^ { i } | x ^ { < i } )$ 通过扩散过程进行建模，在该过程中，通过逐步去噪初始高斯噪声并考虑先前生成的帧来生成一个帧。这种表述结合了自回归模型和扩散模型的优势，以捕捉序列依赖性，同时实现连续值视觉信号的高质量生成。在实践中，我们也可以选择一次生成一组帧，而不是每次生成单个帧[69，100]。然而，为了简化符号表示，我们在本节中继续将每一组帧称为一个帧。

![](images/2.jpg)  

Figure 2: Attention mask configurations. Both Teacher Forcing (a) and Diffusion Forcing (b) train the model on the entire video in parallel, enforcing causal dependencies with custom attention masks. In contrast, our Self-Forcing Training (c) mirrors the autoregressive (AR) inference process with KV caching and does not rely on special attention masks. For illustration purposes, we show a scenario where the video contains 3 frames, and each frame consists of 2 tokens.

大多数现有的自回归视频扩散模型是在教师强迫（TF）或扩散强迫（DF）范式下使用逐帧去噪损失进行训练的。具体而言，每一帧 $x ^ { i }$ 是通过 $q _ { t ^ { i } | 0 } \big ( x _ { t ^ { i } } ^ { i } | x _ { 0 } ^ { i } \big )$ 生成的，使得 $x _ { t ^ { i } } ^ { i } = \bar { \Psi } ( x ^ { i } , \epsilon ^ { i } , \dot { t } ^ { i } ) = \alpha _ { t ^ { i } } x ^ { i } + \sigma _ { t ^ { i } } \epsilon ^ { i }$，其中 $\alpha _ { t ^ { i } } , \sigma _ { t ^ { i } }$ 是在有限时间范围 $t ^ { i } \in [ 0 , 1000 ]$ 内预定义的噪声调度，$\epsilon ^ { i } \sim \mathcal { N } ( 0 , I )$ 是高斯噪声。在TF中，时间步 $t ^ { i }$ 通常在所有帧之间共享，而在DF中，它们是为每一帧独立采样的。通过正向过程的时间反向，学习生成模型，其中每一个去噪步骤可以通过预测添加到每一帧的噪声 $\epsilon ^ { i }$ 来实现，使用神经网络 $\hat { \epsilon } _ { \theta } ^ { i } : = \breve { G } _ { \theta } ( x _ { t ^ { i } } ^ { \dot { i } } , t ^ { i } , c )$，条件是上下文 $c$ 和 DF 中的上下文 $x ^ { < i }$ $x _ { t ^ { j } } ^ { j < i }$。模型的训练目标是最小化预测噪声与真实添加噪声之间的逐帧均方误差（MSE）：$\mathcal { L } _ { \theta } ^ { \mathrm { D M } } = \mathbb { E } _ { \boldsymbol { x } ^ { i } , t ^ { i } , \epsilon ^ { i } } \left[ w _ { t ^ { i } } \lVert \hat { \epsilon } _ { \boldsymbol { \theta } } ^ { i } - \epsilon ^ { i } \rVert _ { 2 } ^ { 2 } \right]$，其中 $w _ { t ^ { i } }$ 是加权函数。

我们关注于基于变压器的扩散模型架构（为清晰起见省略公式），该架构使用由因果3D变分自编码器（VAE）编码的压缩潜在空间进行文本条件处理。自回归链规则分解通过因果注意力实现。图2（a）和（b）展示了教师强制和扩散强制方法的注意力掩码配置。对于教师强制，我们描述了一种高效的变体，使用块稀疏注意力掩码并行处理所有帧，而不是在每次训练迭代中去噪一帧。这种设计已在基于MAR的自回归视频生成和其他自回归视频扩散模型中同时使用。

# 3.2 自回归扩散后训练通过自我回放

Self Forcing 的核心思想是在训练过程中通过自回归自展开生成视频 $\{ x _ { \theta } ^ { 1 : N } \} \sim p _ { \theta } \overline { { { ( x ^ { 1 : N } ) } } } =$ $\textstyle \prod _ { i = 1 } ^ { N } p _ { \theta } ( x ^ { i } | x ^ { < i } )$，其中每一帧 $x ^ { i }$ 是通过对自生成的输出进行迭代去噪条件生成的，包括过去的干净上下文帧和当前时间步的噪声帧。与大多数只在推理期间利用 KV 缓存的自回归模型不同，我们的 Self Forcing 方法创新性地在训练期间采用 KV 缓存，如图 2 (c) 所示。

然而，使用标准多步扩散模型实现自我强制在计算上是不可行的，因为这需要通过长去噪链展开和反向传播。因此，我们选择使用几步扩散模型 $G _ { \theta }$ 来近似自回归分解中的每个条件分布 $p _ { \theta } ( x ^ { i } | x ^ { < i } )$。考虑 $\{ t _ { 0 } = 0 , t _ { 1 } , \ldots , t _ { T } = 1 0 0 0 \}$ 是时间步长 $[ 0 , . . . , 1 0 0 0 ]$ 的一个子序列，在每个去噪步骤 $t _ { j }$ 和帧索引 $i$ 中，模型基于之前的干净帧 $x ^ { < i }$ 对中间的噪声帧 $\boldsymbol { x } _ { t _ { j } } ^ { i }$ 进行去噪。然后，它通过正向过程 $\Psi$ 向去噪帧注入较低噪声水平的高斯噪声以

# 算法 1 自我强制训练

<table><tr><td>算法 1 自我强制训练 需要: 去噪时间步 {t1, . . . , tT }</td></tr><tr><td>需要: 视频帧数 N</td></tr><tr><td>需要: 自回归扩散模型 Gθ (通过 GKV 返回KV嵌入)</td></tr><tr><td>1: 循环 2: 初始化模型输出 Xθ ← []</td></tr><tr><td>3: 初始化 KV 缓存 KV ← []</td></tr><tr><td>4: 采样 s ∼ Uniform(1, 2, . . . , T )</td></tr><tr><td>5: 对于 i = 1, . . . , N 做</td></tr><tr><td>6:</td></tr><tr><td>初始化 xiT ∼ N (0, I) 7: 对于 j = T , . . . , s 做</td></tr><tr><td>8: 如果 j = s 则</td></tr><tr><td>9: 启用梯度计算</td></tr><tr><td>10: 设置 x0 ← Gθ(xi j; tj, KV)</td></tr><tr><td>11: Xθ.append(x0)</td></tr><tr><td>12: 禁用梯度计算</td></tr><tr><td>13: 缓存 kvi ← GK (x0; 0, KV)</td></tr><tr><td>14: KV.append(kvi)</td></tr><tr><td>15: 否则</td></tr><tr><td>16: 禁用梯度计算</td></tr><tr><td>17: 设置 xi ← Gθ(xij ; tj, KV)</td></tr><tr><td>18: 采样 € ~ N (0, I)</td></tr><tr><td>设置 xtj−1 ← Ψ(x0, ,tj−1) 19:</td></tr><tr><td>20: 结束如果</td></tr><tr><td>21: 结束循环</td></tr><tr><td>22: 结束循环</td></tr><tr><td>23: 通过分布匹配损失更新 θ 24: 结束循环</td></tr></table>

<table><tr><td>算法 2 带滚动 KV 缓存的自回归扩散推断</td></tr><tr><td>需要: 大小为 L 帧的 KV 缓存 需要: 去噪时间步 {t1, . . . , tT }</td></tr><tr><td>需要: 生成帧数 M</td></tr><tr><td>需要: 自回归扩散模型 Gθ (通过 GKv 返回 KV 嵌入)</td></tr><tr><td>1: 初始化模型输出 Xθ ← []</td></tr><tr><td>2: 初始化 KV 缓存 KV ← []</td></tr><tr><td>3: 对于 i = 1, . . . , M 做 4:</td></tr><tr><td>初始化 xiT ∼ N (0, I) 5: 对于 j = T , . . . , 1 做</td></tr><tr><td>6: 设置 xi ← Gθ(xi j; tj, KV)</td></tr><tr><td>7: 如果 j = 1 则</td></tr><tr><td>8: Xθ.append(xi)</td></tr><tr><td>9: 缓存 kvi ← GKV (x0; 0, KV)</td></tr><tr><td>10: 如果 |KV| = L 则</td></tr><tr><td>11: KV.pop(0) 缓存淘汰</td></tr><tr><td>12: 结束如果</td></tr><tr><td>13: KV.append(kv)</td></tr><tr><td>14: 否则</td></tr><tr><td>15: 采样 ~ N (0, I)</td></tr><tr><td>16: 设置 xij−1 ← Ψ(x0, , tj−1)</td></tr><tr><td>17: 结束如果</td></tr><tr><td></td></tr><tr><td>18: 结束循环</td></tr><tr><td>19: 结束循环 20: 返回 Xθ</td></tr></table>

获取噪声帧 j−a 作为下一个去噪步骤的输入，遵循少步扩散模型的标准实践 [74, 98]。模型分布 $p _ { \theta } ( x ^ { i } | x ^ { < i } )$ 被隐式定义为 $f _ { \theta , t _ { 1 } } \circ f _ { \theta , t _ { 2 } } \circ . . . \circ f _ { \theta , t _ { T } } ( x _ { t _ { T } } ^ { i } )$，其中 $\bar { f _ { \theta , t _ { j } } ( x _ { t _ { j } } ^ { i } ) } = \Psi ( G _ { \theta } ( x _ { t _ { j } } ^ { i } , t _ { j } , x ^ { < i } ) , \dot { \epsilon _ { t _ { j - 1 } } } , \dot { t _ { j - 1 } } )$，且 $x _ { t _ { T } } ^ { i } \sim \mathcal { N } ( 0 , I )$

即使在少步模型中，天真地通过整个自回归扩散过程进行反向传播仍会导致过度的内存消耗。为了解决这个挑战，我们提出了一种限制每帧每步传播的策略。此外，我们不是在推理时总是使用 $T$ 次去噪步骤，而是在每次训练迭代中随机从 $[ 1 , T ]$ 中抽取一个去噪步 $s$，并使用第 $s$ 步的去噪输出作为最终输出。这种随机抽样的方法确保所有中间去噪步骤都能接收到监督信号。我们还通过限制梯度流入 KV 缓存嵌入，将上一帧的梯度与当前帧在训练期间分离。有关训练过程的完整描述，请参见算法 1。

# 3.3 整体分布匹配损失

自回归自滚动直接从推理时模型分布生成样本，使我们能够应用整体的视频级损失，将生成视频的分布 $p _ { \theta } ( x ^ { 1 : N } )$ 与真实视频的分布 $p _ { \mathrm { d a t a } } ( x ^ { 1 : N } )$ 对齐。为了确保稳定性 [32]，我们在两个分布中注入噪声，并定义 $p _ { \theta , t } ( x _ { t } ^ { 1 : N } )$ 和 $p _ { \mathrm { d a t a } , t } \big ( x _ { t } ^ { 1 : N } \big )$，其中 ea 表示在应用前向扩散过程后的各自分布：$p _ { \cdot , t } ( x _ { t } ^ { 1 : N } ) = \begin{array} { r } { \int q _ { t | 0 } ( \bar { x } _ { t } ^ { 1 : N } | x ^ { 1 : N } ) p . ( x ^ { \bar { 1 } : N } ) \mathrm { d } x ^ { 1 : N } } \end{array}$。我们实际上基于可用的真实度量和分布匹配框架，并在本文中考虑三种方法：

• 分布匹配蒸馏 (DMD) [98, 99]：这种方法通过利用分布之间的得分差异来指导梯度更新，从而最小化反Kullback-Leibler散度 $\mathbb { E } _ { t } \big [ D _ { \mathrm { K L } } \big ( p _ { \theta , t } \| p _ { \mathrm { d a t a } , t } \big ) \big ]$。

得分身份蒸馏 (SiD) [112, 113]：该方法通过Fisher散度 $\mathbb { E } _ { t , p _ { \theta , t } } [ \| \nabla \log p _ { \theta , t } - \nabla \log p _ { \mathrm { d a t a } , t } \| ^ { 2 } ]$ 进行分布匹配。

• 生成对抗网络（GANs）[18]：它通过生成器（我们的自回归扩散模型）和一个区分真实视频与生成视频的鉴别器之间的最小最大博弈来近似詹森-香农散度。

重要的是，我们的训练目标与整个视频序列的整体分布相匹配，即数据分布 ${ \mathsf { \tilde { D } } } ( { p } _ { \mathrm { d a t a } } ( x ^ { 1 : N } ) \| p _ { \theta } ( x ^ { 1 : N } ) )$。相比之下，TF/DF可以理解为 $\begin{array} { r } { \mathbb { E } _ { \{ x < i \} \sim p _ { \mathrm { d a t a } } } D _ { K L } ( p _ { \mathrm { d a t a } } ( x ^ { i } | x ^ { < i } ) | | p _ { \theta } ( x ^ { i } | x ^ { < i } ) ) ^ { 1 } } \end{array}$，其中DF在 $\{ x ^ { < i } \} \sim \tilde { p } _ { \mathrm { d a t a } }$ 处。我们的公式从根本上改变了训练动态——上下文帧 $\{ x ^ { < i } \}$ 是从模型自身的分布 $p _ { \theta }$ 中采样，而不是从数据分布（干净或嘈杂）中采样。这种训练与推理分布之间的对齐有效解决了曝光偏差，并迫使模型从自身的不完美中学习，从而发展出对错误积累的鲁棒性。

虽然这三个目标在扩散模型的时间步蒸馏中都被使用，但我们的主要动机与蒸馏根本不同：我们旨在通过分布匹配来解决曝光偏差，从而提高自回归视频生成的质量，而不仅仅是加速采样。这个区分使得其他流行的蒸馏方法[74]不适用于我们的框架，因为它们只关注时间步减少，而没有直接对齐生成器输出分布。尽管CausVid [100] 同样使用DMD来匹配生成视频的分布，但它在训练期间优化的分布（使用扩散强制输出）偏离了实际推理时间的分布，显著削弱了其有效性。

# 3.4 使用滚动 KV 缓存生成长视频

自回归模型相对于标准视频扩散模型的一个关键优势是它们的外推能力，这原则上允许通过滑动窗口推理生成无限长度的视频。虽然使用扩散强制训练的双向注意力模型也可以自回归地生成视频，但它们不支持KV缓存，要求对每个新帧完全重新计算注意力矩阵。这导致了计算复杂度过高，为 $O ( T L ^ { 2 } )$（其中 $T$ 表示去噪步骤的数量，$L$ 是窗口大小），如图3（a）所示。

另一方面，具有因果注意力的模型可以利用KV缓存来提高效率。然而，现有的实现[69, 100]需要重新计算连续滑动窗口之间重叠帧的KV缓存，如图3(b)所示。这导致在使用密集滑动窗口时复杂度为$\stackrel { \ldots } { O ( } L ^ { \overleftarrow { 2 } } + T L )$。因此，以前的实现采用较大的步幅和最小重叠以降低计算成本，这牺牲了时间的一致性，因为每个窗口开头的帧依赖于显著减少的历史上下文。

受大型语言模型研究的启发，我们提出了一种用于自回归扩散模型的滚动KV缓存机制，允许无限长的视频生成而无需重新计算KV缓存。如图3(c)所示，我们维护一个固定大小的KV缓存，存储最近$L$帧的令牌KV嵌入。当生成新帧时，我们首先检查KV缓存是否已满。如果已满，我们会在添加新条目之前删除最旧的KV缓存条目。这种方法使得帧生成无限进行，时间复杂度为$O ( T L )$，同时在生成每个新帧时仍能保持足够的上下文长度。算法2提供了我们带有滚动KV缓存的自回归长视频生成算法的详细描述。

然而，这种机制的简单实现导致了由于分布不匹配而产生严重的闪烁伪影。具体来说，第一帧的潜在特征与其他帧的统计特性不同：它只编码了第一张图像，而没有进行时间压缩。模型在训练时始终将第一帧视为图像潜在特征，当图像潜在特征在滚动KV缓存场景中不再可见时，它未能很好地推广。虽然我们尝试了类似于StreamingLLM的策略，保持第一帧固定，同时滚动其他帧的KV缓存，但这些对视频生成无效。我们的解决方案简单但有效：在训练过程中，我们限制注意力窗口，使模型在去噪最后一块时无法关注第一块，从而模拟长视频生成过程中遇到的条件。

![](images/3.jpg)  

Figure 3: Efficiency comparisons for video extrapolation. When performing video extrapolation through sliding window inference, (a) bidirectional diffusion models trained with TF/DF [10, 73] do not support KV cache. (b) Prior causal diffusion models [69, 100] require re-computing KV when shifting the window. (c) Our method does not recompute KV and enables more effcient extrapolation.

# 4 实验

实施。我们采用 Self Forcing 和 Wan2.1-T2V-1.3B [83]，一个基于流匹配 [48] 的模型，生成 5 秒的视频，帧率为 16 FPS，分辨率为 $8 3 2 \times 4 8 0$。按照 CausVid 的初始化协议 [100]，我们首先通过在从基础模型采样的 16k ODE 解决方案对上进行因果注意掩码的微调。对于 ODE 初始化和 Self Forcing 训练，我们从 VidProM 的过滤和 LLM 扩展版本 [85] 中提取文本提示。我们使用 4 步扩散，并实现了逐帧和块级自回归变体，其中后者每次生成 3 幅潜在帧。我们采用 R3GAN [29] 目标，该目标由相对配对 GAN 损失 [34] 和 ${ \bf R } 1 + { \bf R } 2$ 正则化 [58] 组成。我们使用 14B 基础模型生成 70k 视频作为训练 GANs [70] 和微调多步 TF/DF AR 扩散基线的数据集。值得注意的是，我们算法的 DMD/SiD 实现保持无数据，能够在没有任何视频训练数据的情况下将预训练的视频扩散模型转换为自回归模型。附录 A 中提供了更多实施细节。

评估指标。我们采用VBench [31]和用户偏好研究来评估视觉质量和语义对齐。我们还严格评估我们方法在实时应用中的效率。虽然一些最新的研究声称基于吞吐量具有“实时”视频生成能力 [24, 109]，但我们认为真正的实时性能需要足够的吞吐量（超过视频播放速率）和低于感知阈值的延迟，这可能依赖于应用 [41]。因此，我们评估吞吐量和首帧延迟，以提供对实时能力的全面评估，所有速度测试均在单个NVIDIA H100 GPU上进行。

与现有基线的比较。我们将我们的模型与相似规模的相关开源视频生成模型进行比较。我们的比较包括两个扩散模型：Wan2.1-1.3B [83]（我们的初始化权重）和LTXVideo [24]（以高效著称）。我们还与几个自回归模型进行比较，包括Pyramid Flow [33]、NOVA [13]、SkyReelsV2 [10]、MAGI-1 [69]和CausVid [100]（也以Wan-1.3B为初始化）。

如表1所示，我们的分块自回归模型在所有比较模型中获得了最高的VBench分数，同时实现了实时吞吐量（17.0 FPS）和亚秒级延迟，这对于某些实时应用程序（如实时视频流）来说是足够低的[2]。图4显示了用户研究结果，将我们的分块自我强制模型与几个重要的基线进行比较。我们的方案始终优于所有替代方案，包括我们模型初始化的多步扩散模型Wan2.1。我们的逐帧变体在保持强大的生成质量的同时提供了最低的延迟（0.45秒），特别适合对延迟敏感的实时应用程序。这里的结果是使用DMD损失目标得到的；

![](images/4.jpg)  

Figure 4: User preference study. Self Forcing outperforms all baselines in human preference.

Table 1: Comparison with relevant baselines. We compare Self Forcing with representative open-source video generation models of similar parameter sizes and resolutions.   

<table><tr><td rowspan="2">模型</td><td rowspan="2"></td><td rowspan="2">参数数量 分辨率 吞吐量 延迟</td><td rowspan="2">(FPS) ↑</td><td rowspan="2">(秒) ↓</td><td colspan="3">评估分数 ↑</td></tr><tr><td>总分</td><td>质量分</td><td>语义分</td></tr><tr><td colspan="8">扩散模型</td></tr><tr><td>LTX-视频 [24]</td><td>1.9B</td><td>768×512</td><td>8.98</td><td>13.5</td><td>80.00</td><td>82.30</td><td>70.79</td></tr><tr><td>Wan2.1 [83]</td><td>1.3B</td><td>832×480</td><td>0.78</td><td>103</td><td>84.26</td><td>85.30</td><td>800.09</td></tr><tr><td colspan="8">块级自回归模型</td></tr><tr><td>SkyReels-V2 [10]</td><td>1.3B</td><td>960×540</td><td>0.49</td><td>112</td><td>82.67</td><td>84.70</td><td>74.53</td></tr><tr><td>MAGI-1 [69]</td><td>4.5B</td><td>832×480</td><td>0.19</td><td>282</td><td>79.18</td><td>82.04</td><td>67.74</td></tr><tr><td>Caus Vid [100]*</td><td>1.3B</td><td>832×480</td><td>17.0</td><td>0.69</td><td>81.20</td><td>84.05</td><td>69.80</td></tr><tr><td>自我强迫（我们，块级）</td><td>1.3B</td><td>832×480</td><td>17.0</td><td>0.69</td><td>84.31</td><td>85.07</td><td>81.28</td></tr><tr><td colspan="8">自回归模型†</td></tr><tr><td>NOVA [13]</td><td>0.6B</td><td>768×480</td><td>0.88</td><td>4.1</td><td>80.12</td><td>80.39</td><td>79.05</td></tr><tr><td>金字塔流 [33]</td><td>2B</td><td>640×384</td><td>6.7</td><td>2.5</td><td>81.72</td><td>84.74</td><td>69.62</td></tr><tr><td>自我强迫（我们，帧级）</td><td>1.3B</td><td>832×480</td><td>8.9</td><td>0.45</td><td>84.26</td><td>85.25</td><td>80.30</td></tr></table>

* 我们与使用相同基础模型（Wan-1.3B）的CausVid官方实现进行比较。† AR/non-AR的区分适用于时间维度。

![](images/5.jpg)  

Figure 5: Qualitative comparisons. We visualize videos generated by Self Forcing (Ours) against those by Wan2.1 [83], SkyReels-V2 [10], and CausVid [100] at three time steps. All models share the same architecture with 1.3B parameters.

使用SiD和GAN目标训练的模型在我们的消融研究中表现出可比性能。如图5所示，CausVid遭受了错误累积问题，导致饱和度随时间增加。我们的方法在视觉质量上比Wan2.1/SkyReels-V2稍好，同时在延迟方面大约快150倍。更多示例视频请访问项目网站（https://self-forcing.github.io/）。

Table 2: Ablation study. We conduct controlled ablation studies comparing different training paradigms and distribution matching objectives under our training setup across chunk-wise (left) and frame-wise (right) AR models. Self Forcing works well with all different distribution matching objectives and consistently outperforms alternative training approaches.   

<table><tr><td>按块自回归</td><td colspan="3">评估分数 ↑ 总体质量 语义</td></tr><tr><td></td><td>分数 分数</td><td></td><td>分数</td></tr><tr><td>许多 (50×2)-步模型 扩散强制 (DF)</td><td>82.95 83.66</td><td></td><td>80.09</td></tr><tr><td>教师强制 (TF)</td><td></td><td>83.58 84.34</td><td>80.52</td></tr><tr><td colspan="4">少量 (4)-步模型 DF + DMD</td></tr><tr><td>TF + DMD 自我强制 (我们的, DMD) 84.31 自我强制 (我们的, SiD) 自我强制 (我们的, GAN) 83.88</td><td>82.76 82.32 84.07 85.06</td><td>83.49 82.73 85.07 85.52</td><td>79.85 80.67 81.28 78.24</td></tr></table>

<table><tr><td>按帧自回归</td><td colspan="3">评估分数 ↑ 总体质量 语义</td></tr><tr><td></td><td>分数 分数</td><td></td><td>分数</td></tr><tr><td colspan="4">许多 (50×2)-步模型</td></tr><tr><td>扩散强制 (DF) 教师强制 (TF)</td><td>77.24 79.72 80.34 81.34</td><td></td><td>67.33 76.34</td></tr><tr><td colspan="4">少量 (4)-步模型</td></tr><tr><td>DF + DMD TF + DMD</td><td>80.56 78.12 84.71</td><td>81.02 79.62 85.25</td><td>78.71 72.11</td></tr><tr><td colspan="3">自我强制 (我们的, DMD) 84.26 自我强制 (我们的, SiD) 83.54 自我强制 (我们的, GAN) 83.27</td><td>80.30 78.86</td></tr></table>

消融研究。我们对自我强制与其他自回归扩散训练方法进行了受控比较。我们评估了：（1）使用教师强制或扩散强制的去噪扩散损失训练的自回归扩散模型，以及（2）使用TF/DF输入训练但优化为分布匹配目标的少步自回归扩散模型。后者的DF和DMD配置本质上在我们的实现框架中复现了CausVid [100]，允许在相同的训练条件下进行直接比较。

表2显示，Self Forcing在各种分布匹配目标（DMD、SiD和GAN）上表现稳健，始终优于所有基线方法。当基线方法从分块自回归（AR）转向逐帧自回归时，由于与增加的自回归展开步骤相关的误差累积，表现出明显的质量下降，通常表现为逐渐的过饱和或过锐化（类似于附录B图5中的CausVid），而Self Forcing在两种设置中均保持一致的性能，突出其在解决曝光偏差方面的有效性。

滚动KV缓存。我们观察到，当移动滑动窗口时（图3（b）），重新计算KV缓存会显著降低吞吐量（生成10秒视频时仅为4.6 FPS）。虽然简单的滚动KV缓存保持高吞吐量，但它引入了严重的视觉伪影，如附录B中的示例所示。通过训练模型在不查看初始图像潜变量的情况下生成帧，我们有效地减少了这些伪影，同时保持了高吞吐量（16.1 FPS）。

训练效率。由于其顺序特性与变换器的可并行性范式相矛盾，人们可能会认为自我强制训练在计算上是不可行的。令人惊讶的是，我们的实验表明，自我强制在训练效率上实际上优于其他替代策略。如图6（左）所示，自我强制在每次迭代的训练时间上与教师强制和扩散强制相当。此外，图6（右）表明，在相同墙钟训练预算下，自我强制达到的质量优于这两种替代方法。每个使用DMD的自我强制实验在64个H100 GPU上大约在1.5小时内收敛。

这一反直觉的结果源于两个关键因素：首先，尽管自我强迫执行顺序展开，但它在每个单独的帧/块内仍然以并行方式处理所有标记，在训练过程中保持高GPU利用率。其次，TF和DF要求特殊的注意力掩蔽模式以强制因果依赖，即使使用像FlexAttention [15]这样的专门实现也会引入额外的计算开销。另一方面，自我强迫在训练过程中始终使用全注意力，并且可以利用高度优化的注意力内核，如FlashAttention-3 [72]。

# 5 讨论

在本节中，我们将探讨我们结果的更广泛影响，讨论其他视角，并概述未来研究的潜在方向。

![](images/6.jpg)  

Figure 6: Training efficiency comparison. Left: Per-iteration time across different chunk-wise, few-step autoregressive video diffusion training algorithms (using DMD as the distribution matching objective). Right: Video quality (VBench score) vs. wall clock training time.

可并行训练范式的基本局限性。可并行训练对于变压器的成功至关重要，因为它实现了高效的扩展。然而，这种并行性引入了基本限制。先前的研究表明，平行架构固有地限制了在顺序状态跟踪问题中的表现能力。我们的工作强调了另一个关键限制：可并行训练范式导致训练和推理分布之间的不对齐，从而导致错误随着时间的积累。我们提倡一种新的范式，即并行预训练和顺序后训练，结合了两者的优点。虽然这种范式转变在通过强化学习进行语言建模方面势头正劲，但我们的工作代表了向视频领域迈出的第一步。我们相信我们的框架是通用的，可以应用于其他序列领域，特别是在数据是连续的情况下。

自回归模型、扩散模型和生成对抗网络（GANs）在生成建模中传统上被视为不同的范式。我们的工作强调了它们的互补性，并展示了如何有效地将它们整合在一起。具体而言，自回归模型和扩散模型提供了互补的方式来分解分布（链规则与潜变量），可以以嵌套的方式组合。GANs背后的核心思想是通过从隐式生成器中抽取样本，将隐式生成器的分布与目标分布进行匹配，从而可以用于训练一个由自回归-扩散分解驱动的生成器。

局限性和未来方向。尽管我们的方法有效地减缓了训练上下文长度内的错误积累，但在生成远超过训练时所见视频的情况下，质量退化仍然可观察到。此外，我们的梯度截断策略——虽然对内存效率是必要的——可能限制了模型学习长程依赖的能力。未来的工作可以探索改进的外推技术和本质上循环的架构，如状态空间模型，来更好地平衡内存效率与长上下文建模。

# 致谢

我们感谢尹天维、陈北迪、郑凯文、张凯、戈拉夫·帕尔马尔、顾毅、毕赛和张建明的宝贵讨论。