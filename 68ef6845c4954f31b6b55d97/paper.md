# Self Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion

# Xun Huang1

Zhengqi Li1 Guande $\mathbf { H e } ^ { 2 }$ Mingyuan Zhou2 Eli Shechtman1 1Adobe Research 2The University of Texas at Austin

https://self-forcing.github.io/

# Abstract

We introduce Self Forcing, a novel training paradigm for autoregressive video diffusion models. It addresses the longstanding issue of exposure bias, where models trained on ground-truth context must generate sequences conditioned on their own imperfect outputs during inference. Unlike prior methods that denoise future frames based on ground-truth context frames, Self Forcing conditions each frame's generation on previously self-generated outputs by performing autoregressive rollout with key-value (KV) caching during training. This strategy enables supervision through a holistic loss at the video level that directly evaluates the quality of the entire generated sequence, rather than relying solely on traditional frame-wise objectives. To ensure training efficiency, we employ a few-step diffusion model along with a stochastic gradient truncation strategy, effectively balancing computational cost and performance. We further introduce a rolling KV cache mechanism that enables efficient autoregressive video extrapolation. Extensive experiments demonstrate that our approach achieves real-time streaming video generation with sub-second latency on a single GPU, while matching or even surpassing the generation quality of significantly slower and non-causal diffusion models.

# 1 Introduction

Recent years have witnessed tremendous progress in video synthesis, with state-of-the-art systems now capable of generating remarkably realistic content with complex temporal dynamics [6]. However, these results are typically achieved with diffusion transformers (DiT) [62, 83] that denoise all frames simultaneously using bidirectional attention. This design allows the future to affect the past and requires generating the entire video at once, fundamentally limiting their applicability to real-time streaming applications where future information is unknown when generating the current frame.

In contrast, autoregressive (AR) models [17, 27, 38, 94, 104] generate videos sequentially, a paradigm that naturally aligns with the causal structure of temporal media. This approach not only significantly reduces the viewing latency of generated videos but also unlocks numerous applications, including real-time interactive content creation [9, 46], game simulation [11, 61, 78, 102], and robotics learning [42, 96, 101]. However, AR models often struggle to match the visual fidelity achieved by state-of-the-art video diffusion models due to their reliance on lossy vector quantization techniques [79].

To combine the best of both worlds, two recent techniques have emerged to equip video diffusion models with AR generation capabilities: Teacher Forcing (TF) [16, 28, 33, 106] and Diffusion Forcing (D) [8, 10, 20, 69, 73, 100]. Teacher Forcing, a wel1-established paradigm in sequence modeling, trains the model to predict the next token conditioned on ground-truth tokens. When applied to video diffusion, TF involves denoising each frame using clean, ground-truth context frames (Figure 1 (a)), a strategy commonly referred to as next-frame prediction. In contrast, Diffusion Forcing trains the model on videos with noise levels independently sampled for each frame, denoising each frame based on noisy context frames (Figure 1 (b)). This ensures the autoregressive inference scenario, where context frames are clean and the current frame is noisy, is covered by the training distribution.

![](images/1.jpg)  

Figure 1: Training paradigms for AR video diffusion models. (a) In Teacher Forcing, the model is trained to denoise each frame conditioned on the preceding clean, ground-truth context frames. (b) In Diffusion Forcing, the model is trained to denoise each frame conditioned on the preceding c fams with varyg noi level.oh ( nd  gneate outus that do ot e to the distribution the model generates during inference. (c) Our Self Forcing approach performs autoregressive self-rollout during training, denoising the next frame based on previous context frames generated by itself. A distribution-matching loss (e.g., SiD, DMD, GAN) is computed on the final output video to align the distribution of generated videos with that of real videos. Our training paradigm closely mirrors the inference process, thereby bridging the train-test distribution gap.

However, models trained with TF or DF often suffer from error accumulation during autoregressive generation, leading to degraded video quality over time [84, 100, 105]. This issue is more broadly known as exposure bias [60, 71], where a model is trained exclusively on ground-truth context but must rely on its own imperfect predictions at inference time, resulting in a distributional mismatch that compounds errors as generation progresses. While some approaches attempt to mitigate this issue in video diffusion models by incorporating noisy context frames during inference [8, 11, 105], such design sacrifices temporal consistency, complicates the KV-cache design, increases generation latency, and does not fundamentally resolve the exposure bias problem.

In this work, we propose Self Forcing (SF), a novel algorithm addressing exposure bias in autoregressive video generation. Inspired by early RNN-era sequence modeling techniques [40, 65, 103], our approach bridges the train-test distribution gap by explicitly unrolling autoregressive generation during training, generating each frame conditioned on previously self-generated frames rather than groundtruth ones. This enables supervision with holistic distribution-matching losses [18, 98, 99] applied to complete generated video sequences. By forcing the model to encounter and learn from its own prediction errors, Self Forcing effectively mitigates exposure bias and reduces error accumulation.

While Self Forcing may seem computationally prohibitive due to its sequential nature preventing parallel training, we demonstrate that it can be efficiently implemented as an algorithm in the posttraining stage where the model does not require a large number of gradient updates to converge. By employing a few-step diffusion backbone and a carefully designed gradient truncation strategy, Self Forcing is surprisingly more efcient than alternative parallel strategies, achieving superior performance within the same wall-clock training time. Additionally, we introduce a rolling KV cache mechanism that enhances the efficiency of video extrapolation.

Extensive experiments demonstrate that our model enables real-time video generation at 17 FPS with sub-second latency on a single H100 GPU, while achieving competitive or superior generation quality compared to recent slow bidirectional and autoregressive video diffusion models. These advances open the door to genuinely interactive video generation use cases—live streaming, gaming, and world simulation—where latency budgets are measured in milliseconds rather than minutes.

# 2 Related Work

GANs for Video Generation. Early video generation approaches relied primarily on generative adversarial networks (GANs) [18], either using convolutional networks to generate entire videos in parallel [5, 68, 82] or employing recurrent architectures to produce frames sequentially [14, 44, 49, 77, 81]. Recently, GANs have also been applied to distill video diffusion models [47, 56, 91, 108]. Since the generator in GANs follows the same process during training and inference, it inherently avoids exposure bias. Our work draws inspiration from this fundamental GAN principle by directly optimizing the alignment between the generator's output distribution and the target distribution.

Autoregressive/Diffusion Models for Video Generation. Modern video generation models have largely shifted toward diffusion or autoregressive models due to their stronger scaling abilities. Video diffusion models typically adopt bidirectional attention mechanisms to simultaneously denoise all video frames [3, 4, 6, 13, 2326, 39, 64, 80, 83, 97]. Autoregressive models, in contrast, are trained with next-token prediction objectives and generate spatiotemporal tokens sequentially at inference time [7, 38, 66, 86, 88, 94].

Autoregressive-Diffusion Hybrid Models. Very recently, hybrid models integrating autoregressive and diffusion frameworks have emerged as a promising direction in generative modeling of videos [8, 16, 20, 22, 28, 33, 45, 50, 52, 89, 100, 106, 107] as well as other sequence domains [1, 12, 43, 53, 59, 90, 110]. They typically rely on a long, iterative prediction chain (both temporally autoregressive and spatially iterative denoising), which could lead to significant error accumulation. Our work addresses this issue by training the model conditioned on its own predictions and teaching it to correct its own mistakes.

Rolling Diffusion and Variants. Another line of work [35, 67, 69, 76, 93, 105] trains video diffusion models with a progressive noise schedule, where the noise level gradually increases from earlier to later frames. While these methods support sequential long video generation with less accumulated errors and are sometimes also referred to as autoregressive, they do not strictly follow the autoregressive chain rule decomposition. Consequently, they would exhibit significant latency in interactive applications, as future frames are partially pre-generated before the current frame is presented to the user. This premature commitment restricts the impact of real-time user-injected controls, resulting in limited responsiveness in immediately subsequent frames.

CausVid. Our work is most closely related to CausVid [100], which trains few-step autoregressive diffusion models using the DF scheme and distribution matching distillation (DMD). However, CausVid suffers from a critical flaw that its training outputs (generated via DF) do not come from the dstribtion the model produes at inerence time, therefor the DMD loss is matching the wron distribution. We pinpoint this issue and propose a solution that matches the true model distribution.

# 3 Self Forcing: Briding Train-Test Gap via Holistic Post-Training

We first provide a formal definition of autoregressive video diffusion models and describe standard training approaches in Section 3.1. In Section 3.2, we introduce the main part of our Self Forcing training algorithm and describe how it can be efficiently implemented with a few-step diffusion model. In Section 3.3, we describe various choices of holistic, video-level distribution-matching training objectives. Finally, we introduce a rolling key-value cache mechanism that enables efficient generation of arbitrarily long videos in Section 3.4.

# 3.1 Preliminaries: Autoregressive Video Diffusion Models

Autoregressive video diffusion model is a hybrid generative model that combines autoregressive chain-rule decomposition with denoising diffusion models for video generation. Specifically, given a sequence of $N$ video frames $\boldsymbol { x } ^ { 1 : N } = ( x ^ { \tilde { 1 } } , x ^ { 2 } , \dots , x ^ { N } )$ $\begin{array} { r } { p ( x ^ { 1 : N } ) \ = \ \prod _ { i = 1 } ^ { N } p ( x ^ { i } | x ^ { < i } ) } \end{array}$ Ea contional io $p ( x ^ { i } | x ^ { < i } )$ is then modeled using a diffusion process, where a frame is generated by progressively denoising an initial Gaussian noise conditioned on previously generated frames. This formulation combines the strengths of both autoregressive models and diffusion models for capturing sequential dependencies while enabling high-quality generation of continuous-valued visual signal. In practice,

we can also choose to generate one chunk of frames rather than a single frame at a time [69, 100].   
For simplicity of notation, however, we continue to denote each chunk as a frame in this section.

![](images/2.jpg)  

Figure 2: Attention mask configurations. Both Teacher Forcing (a) and Diffusion Forcing (b) train the model on the entire video in parallel, enforcing causal dependencies with custom attention masks. In contrast, our Self-Forcing Training (c) mirrors the autoregressive (AR) inference process with KV caching and does not rely on special attention masks. For illustration purposes, we show a scenario where the video contains 3 frames, and each frame consists of 2 tokens.

Most existing autoregressive video diffusion models are trained using frame-wise denoising loss within the paradigm of Teacher Forcing (TF) or Diffusion Forcing (DF). Specifically, each frame $x ^ { i }$ is cp $q _ { t ^ { i } | 0 } \big ( x _ { t ^ { i } } ^ { i } | x _ { 0 } ^ { i } \big )$ such that $x _ { t ^ { i } } ^ { i } = \bar { \Psi } ( x ^ { i } , \epsilon ^ { i } , \dot { t } ^ { i } ) = \alpha _ { t ^ { i } } x ^ { i } + \sigma _ { t ^ { i } } \epsilon ^ { i }$ where $\alpha _ { t ^ { i } } , \sigma _ { t ^ { i } }$ are pre-defined noise schedule within a finite time horizon $t ^ { i } \in [ 0 , 1 0 0 0 ]$ and $\epsilon ^ { i } \sim \mathcal { N } ( 0 , I )$ is Gaussian noise. In TF, the timesteps $t ^ { i }$ are usually shared across all frames, whereas in DF, they are sampled independently for each frame. A generative model is learned through the time-reversal of the forward process, where each denoising step can be achieved by predicting the noise $\epsilon ^ { i }$ added to each frame with a neural network $\hat { \epsilon } _ { \theta } ^ { i } : = \breve { G } _ { \theta } ( x _ { t ^ { i } } ^ { \dot { i } } , t ^ { i } , c )$ conditioned on the context $c$ The context $x ^ { < i }$ $x _ { t ^ { j } } ^ { j < i }$ in DF. The model is trained to minimize the frame-wise mean squared error (MSE) between the predicted noise and the true added noise: $\mathcal { L } _ { \theta } ^ { \mathrm { D M } } = \mathbb { E } _ { \boldsymbol { x } ^ { i } , t ^ { i } , \epsilon ^ { i } } \left[ w _ { t ^ { i } } \lVert \hat { \epsilon } _ { \boldsymbol { \theta } } ^ { i } - \epsilon ^ { i } \rVert _ { 2 } ^ { 2 } \right]$ , where $w _ { t ^ { i } }$ is a weighting function.

We focus on the transformer-based architecture [62] of diffusion models with text conditioning (omitted from equations for clarity) operating in a compressed latent space encoded by a causal 3D variational autoencoder (VAE) [37]. The autoregressive chain-rule decomposition is implemented via causal attention. Figures 2 (a) and (b) illustrate the attention mask configurations of Teacher Forcing and Diffusion Forcing approaches. For Teacher Forcing, we describe an efficient variant that processes allframes in parallel using block sparse attention masks, rather than denoising one frame at each training iteration [33]. Such design has been used in MAR-based [43] autoregressive video generation [111] and concurrently in other autoregressive video diffusion models [106, 107].

# 3.2 Autoregressive Diffusion Post-Training via Self-Rollout

The core idea of Self Forcing is to generate videos through autoregressive self-rollout during training $\{ x _ { \theta } ^ { 1 : N } \} \sim p _ { \theta } \overline { { { ( x ^ { 1 : N } ) } } } =$ $\textstyle \prod _ { i = 1 } ^ { N } p _ { \theta } ( x ^ { i } | x ^ { < i } )$ where each frame $x ^ { i }$ is generated by performing iterative denoising conditioned on self-generated outputs, including both clean context frames in the past and noisy frames at the current time step. Unlike most previous autoregressive models that only utilize KV caching during inference, our Self Forcing method innovatively employs KV caching during training, as shown in Figure 2 (c).

Nevertheless, implementing Self Forcing with standard many-step diffusion models would be computationally prohibitive, as it requires unrolling and backpropagation through long denoising chains. Therefore, we choose to use a few-step diffusion model $G _ { \theta }$ to approximate each conditional distribution $p _ { \theta } ( x ^ { i } | x ^ { < i } )$ in the autoregressive factorization. Consider $\{ t _ { 0 } = 0 , t _ { 1 } , \ldots , t _ { T } = 1 0 0 0 \}$ a subsequence of timesteps $[ 0 , . . . , 1 0 0 0 ]$ , at each denoising step $t _ { j }$ and frame index $i$ , the model denoises an intermediate noisy frame $\boldsymbol { x } _ { t _ { j } } ^ { i }$ conditioned on previous clean frames $x ^ { < i }$ . It then injects Gaussian noise with a lower noise level into the denoised frame through the forward process $\Psi$ to

# Algorithm 1 Self Forcing Training

<table><tr><td>Algorithm 1 Self Forcing Training Require: Denoise timesteps {t1, . . . , tT }</td></tr><tr><td>Require: Number of video frames N</td></tr><tr><td>Require: AR diffusion model Gθ (returns KV em</td></tr><tr><td>beddings via GKV)</td></tr><tr><td>1: loop 2: Initialize model output Xθ ← []</td></tr><tr><td>3: Initialize KV cache KV ← []</td></tr><tr><td>4: Sample s ∼ Uniform(1, 2, . . . , T )</td></tr><tr><td>5: for i = 1, . . . , N do</td></tr><tr><td>6:</td></tr><tr><td>Initialize xiT ∼ N (0, I) 7: for j = T , . . . , s do</td></tr><tr><td>8: if j = s then</td></tr><tr><td>9: Enable gradient computation</td></tr><tr><td>10: Set x0 ← Gθ(xi j; tj, KV)</td></tr><tr><td>11: Xθ.append(x0)</td></tr><tr><td>12: Disable gradient computation</td></tr><tr><td>13: Cache kvi ← GK (x0; 0, KV)</td></tr><tr><td>14: KV.append(kvi)</td></tr><tr><td>15: else</td></tr><tr><td>16: Disable gradient computation</td></tr><tr><td>17: Set xi ← Gθ(xij ; tj, KV)</td></tr><tr><td>18: Sample € ~ N (0, I)</td></tr><tr><td>Set xtj−1 ← Ψ(x0, ,tj−1) 19:</td></tr><tr><td>20: end if</td></tr><tr><td>21: end for</td></tr><tr><td>22: end for</td></tr><tr><td>23: Update θ via distribution matching loss 24: end loop</td></tr></table>

<table><tr><td>Algorithm 2 Autoregressive Diffusion Inference with Rolling KV Cache</td></tr><tr><td>Require: KV cache of size L frames Require: Denoise timesteps {t1, . . . , tT }</td></tr><tr><td>Require: Number of generated frames M</td></tr><tr><td>Require: AR diffusion model Gθ (returns KV em- beddings via GKv)</td></tr><tr><td>1: Initialize model output Xθ ← []</td></tr><tr><td>2: Initialize KV cache KV ← []</td></tr><tr><td>3: for i = 1, . . . , M do 4:</td></tr><tr><td>Initialize xiT ∼ N (0, I) 5: for j = T , . . . , 1 do</td></tr><tr><td>6: Set xi ← Gθ(xi j; tj, KV)</td></tr><tr><td>7: if j = 1 then</td></tr><tr><td>8: Xθ.append(xi)</td></tr><tr><td>9: Cache kvi ← GKV (x0; 0, KV)</td></tr><tr><td>10: if |KV| = L then</td></tr><tr><td>11: KV.pop(0) Cache eviction</td></tr><tr><td>12: end if</td></tr><tr><td>13: KV.append(kv)</td></tr><tr><td>14: else</td></tr><tr><td>15: Sample  ~ N (0, I)</td></tr><tr><td>16: Set xij−1 ← Ψ(x0, , tj−1)</td></tr><tr><td>17: end if</td></tr><tr><td></td></tr><tr><td>18: end for</td></tr><tr><td>19: end for 20: return Xθ</td></tr></table>

obtain the noisy fame j−a as the input to the next denoising step, following the standard practice in few-step diffusion models [74, 98]. The model distribution $p _ { \theta } ( x ^ { i } | x ^ { < i } )$ is implicitly defined as $f _ { \theta , t _ { 1 } } \circ f _ { \theta , t _ { 2 } } \circ . . . \circ f _ { \theta , t _ { T } } ( x _ { t _ { T } } ^ { i } )$ , where $\bar { f _ { \theta , t _ { j } } ( x _ { t _ { j } } ^ { i } ) } = \Psi ( G _ { \theta } ( x _ { t _ { j } } ^ { i } , t _ { j } , x ^ { < i } ) , \dot { \epsilon _ { t _ { j - 1 } } } , \dot { t _ { j - 1 } } )$ ,and $x _ { t _ { T } } ^ { i } \sim \mathcal { N } ( 0 , I )$

Even with few-step models, however, naively backpropagating through the entire autoregressive diffusion process would still lead to excessive memory consumption. To address this challenge, we po dint ation raty hat limits ebcproagation l e al de step of each frame. Moreover, instead of always using $T$ denoising steps (as in inference time), we randomly sample a denoising step $s$ from $[ 1 , T ]$ for each sample sequence at each training iteration, and use the denoised output of the $s$ -th step as the final output. This stochastic sampling approach ensures all intermediate denoising steps receive supervision signals. We additionally detach the gradients of the previous frames from the current frame during training by restricting gradient flow into KV cache embeddings. For a complete description of the training process, see Algorithm 1.

# 3.3 Holistic Distribution Matching Loss

Autoregressive self-rollout generates samples directly from the inference-time model distribution, enabling us to apply holistic, video-level losses that align the distribution of generated videos $p _ { \theta } ( x ^ { 1 : N } )$ with that of real videos $p _ { \mathrm { d a t a } } ( x ^ { 1 : N } )$ T sability [32], we inject noise to both distriutions and math $p _ { \theta , t } ( x _ { t } ^ { 1 : N } )$ and $p _ { \mathrm { d a t a } , t } \big ( x _ { t } ^ { 1 : N } \big )$ ,where ea represents the respective distribution after applying the forward diffusion process: $p _ { \cdot , t } ( x _ { t } ^ { 1 : N } ) =$ $\begin{array} { r } { \int q _ { t | 0 } ( \bar { x } _ { t } ^ { 1 : N } | x ^ { 1 : N } ) p . ( x ^ { \bar { 1 } : N } ) \mathrm { d } x ^ { 1 : N } } \end{array}$ Ouwr  eally plabl  ru ive measures and distribution matching frameworks, and we consider three approaches in this paper:

• Distribution Matching Distillation (DMD) [98, 99]: This approach minimizes the reverse Kullback-Leibler divergence $\mathbb { E } _ { t } \big [ D _ { \mathrm { K L } } \big ( p _ { \theta , t } \| p _ { \mathrm { d a t a } , t } \big ) \big ]$ by leveraging the score difference between distributions to guide gradient updates.

Score Identity Distillation (SiD) [112, 113]: This method performs distribution matching via Fisher divergence $\mathbb { E } _ { t , p _ { \theta , t } } [ \| \nabla \log p _ { \theta , t } - \nabla \log p _ { \mathrm { d a t a } , t } \| ^ { 2 } ]$ .

• Generative Adversarial Networks (GANs) [18]: It approximates the Jensen-Shannon divergence through a minimax game between the generator (our autoregressive diffusion model) and a discriminator that distinguishes between real and generated videos.

Importantly, our training objective matches the holistic distribution of the entire video sequence to the data distribution ${ \mathsf { \tilde { D } } } ( { p } _ { \mathrm { d a t a } } ( x ^ { 1 : N } ) \| p _ { \theta } ( x ^ { 1 : N } ) )$ . In contrast, TF/DF can be understood as perfoie $\begin{array} { r } { \mathbb { E } _ { \{ x < i \} \sim p _ { \mathrm { d a t a } } } D _ { K L } ( p _ { \mathrm { d a t a } } ( x ^ { i } | x ^ { < i } ) | | p _ { \theta } ( x ^ { i } | x ^ { < i } ) ) ^ { 1 } } \end{array}$ , where DF inalp x o at $\{ x ^ { < i } \} \sim \tilde { p } _ { \mathrm { d a t a } }$ Our formulation fundamentally transforms the training dynamics—context frames $\{ x ^ { < i } \}$ are sampled from the model's own distribution $p _ { \theta }$ rather than from the data distribution (clean or noisy). This alignment between training and inference distributions effectively addresses exposure bias and forces the model to learn from its own imperfections, thereby developing robustness to error accumulation.

While al three objectives have been used in the context of timestep distillation of diffusion models, our primary motivation differs fundamentally from distillation: we aim to enhance the quality of autoregressive video generation by addressing exposure bias via distribution matching, rather than merely accelerating sampling. This distinction makes other popular distillation methods [74] inapplicable to our framework as they only focus on timestep reduction without directly aligning the generator output distribution. Although CausVid [100] similarly employs DMD to match the distribution of generated videos, the distribution it optimizes during training (using Diffusion Forcing outputs) deviates from the actual inference-time distribution, significantly undermining its effectiveness.

# 3.4 Long Video Generation with Rolling KV Cache

A key advantage of autoregressive models over standard video diffusion models is their extrapolative ability, in principle allowing the generation of infinitely long videos via sliding-window inference. While bidirectional attention models trained with Diffusion Forcing [10, 73] can also generate videos autoregressively, they do not support KV caching, requiring complete recomputation of attention matrices for each new frame. This leads to excessive computational complexity of $O ( T L ^ { 2 } )$ (where $T$ represents the number of denoising steps and $L$ is thewindow size), as shown in Figure 3 (a).

Models with causal attention, on the other hand, can leverage KV caching to improve efficiency. However, existing implementations [69, 100] require recomputing KV cache for overlapping frames between consecutive sliding windows, as illustrated in Figure 3 (b). This leads to $\stackrel { \ldots } { O ( } L ^ { \overleftarrow { 2 } } + T L )$ complexity when employing dense sliding windows. As a result, prior implementations adopt larger strides with minimal overlap to reduce computational costs, which compromises temporal consistency since frame at the beginning of each window relies on a significantly reduced historical context.

Inspired by research in large language models [92], we propose a rolling KV cache mechanism for autoregressive diffusion models that allows infinitely long video generation without any need of recomputing the KV cache. As illustrated in Figure 3 (c), we maintain a fixed-size KV cache that stores the KV embeddings of tokens in the most recent $L$ frames. When generating a new frame, we first check if the KV cache is full. If it is, we remove the oldest KV cache entry before adding the new one. This approach enables endless frame generation with a time complexity of $O ( T L )$ , while still maintaining a sufficient context length when generating each new frame. Algorithm 2 provides a detailed description of our autoregressive long video generation algorithm with rolling KV cache.

However, naive implementation of this mechanism leads to severe flickering artifacts due to distribution mismatch. Specifically, the first latent frame has different statistical properties than other frames: it only encodes the first image without performing temporal compression. The model, having always seen the first frame as the image latent during training, fails to generalize when the image latent is no longer visible in the rolling KV cache scenario. While we experimented with strategies similar to StreamingLLM [92], keeping the first frame fixed while rolling KV cache of other frames, these provd ineffective for video generation. Our solution is straightforward but effective: during training, we restrict the attention window so the model cannot attend to the first chunk when denoising the final chunk, thereby simulating the conditions encountered during long video generation.

![](images/3.jpg)  

Figure 3: Efficiency comparisons for video extrapolation. When performing video extrapolation through sliding window inference, (a) bidirectional diffusion models trained with TF/DF [10, 73] do not support KV cache. (b) Prior causal diffusion models [69, 100] require re-computing KV when shifting the window. (c) Our method does not recompute KV and enables more effcient extrapolation.

# 4 Experiments

Implementation. We implement Self Forcing with Wan2.1-T2V-1.3B [83], a Flow Matching [48] based model that generates 5s videos at 16 FPS with a resolution of $8 3 2 \times 4 8 0$ Following CausVid's initialization protocol [100], we first finetune the base model with causal attention masking on 16k ODE solution pairs sampled from the base model. For both ODE initialization and Self Forcing training, we sample text prompts from a filtered and LLM-extended version of VidProM [85]. We use 4-step diffusion and implement both frame-wise and chunk-wise autoregressive variants, with the latter generating a chunk of 3 latent frames at a time. We adopt the R3GAN [29] objective, which consists of relativistic pairing GAN loss [34] with ${ \bf R } 1 + { \bf R } 2$ regularization [58]. We use the 14B base model to generate 70k videos as the dataset for training GANs [70] and fine-tuning many-step TF/DF AR diffusion baselines. Notably, DMD/SiD implementations of our algorithm remain data-free, capable of converting a pre-trained video diffusion model into an autoregressive model without any video training data. Additional implementation details are provided in Appendix A.

Evaluation metrics. We adopt VBench [31] and user preference study to evaluate both visual quality and semantic alignment. We also rigorously evaluate the efficiency of our method for real-time applications. While some recent works claim "real-time" video generation abilities [24, 109] based solely on throughput, we argue that true real-time performance requires both sufficient throughput (exceeding video playback rate) and lower latency than the perceptual threshold which could be application-dependent [41]. We therefore evaluate both throughput and first-frame latency to provide a comprehensive assessment of real-time capabilities, with all speed tests conducted on a single NVIDIA H100 GPU.

Comparison with existing baselines. We compare our model with relevant open-source video generation models of similar scale. Our comparisons include two diffusion models: Wan2.1- 1.3B [83] (our initialization weights) and LTXVideo [24] (known for efficiency). We also compare with several autoregressive models including Pyramid Flow [33], NOVA [13], SkyReelsV2 [10], MAGI-1 [69], and CausVid [100] (also initialized from Wan-1.3B).

As shown in Table 1, our chunk-wise autoregressive model achieves the highest VBench scores across all compared models while simultaneously delivering real-time throughput (17.0 FPS) with

sub-second latency, low enough for certain real-time applications such as live video streaming [2]. Figure 4 shows the user study results comparing our chunk-wise Self Forcing model against several important baselines. Our approach is consistently preferred over all alternatives, including the manystep diffusion model Wan2.1 that our model is initialized from. Our frame-wise variant maintains strong generation quality while providing the lowest latency (0.45s), making it particularly suitable for latency-sensitive real-time applications. Results here are obtained using the DMD loss objective;

![](images/4.jpg)  

Figure 4: User preference study. Self Forcing outperforms all baselines in human preference.

Table 1: Comparison with relevant baselines. We compare Self Forcing with representative open-source video generation models of similar parameter sizes and resolutions.   

<table><tr><td rowspan="2">Model</td><td rowspan="2"></td><td rowspan="2">#Params Resolution Throughput Latency</td><td rowspan="2">(FPS) ↑</td><td rowspan="2">(s) ↓</td><td colspan="3">Evaluation scores ↑</td></tr><tr><td>Total Score</td><td>Quality Score</td><td>Semantic Score</td></tr><tr><td colspan="8">Diffusion models</td></tr><tr><td>LTX-Video [24]</td><td>1.9B</td><td>768×512</td><td>8.98</td><td>13.5</td><td>80.00</td><td>82.30</td><td>70.79</td></tr><tr><td>Wan2.1 [83]</td><td>1.3B</td><td>832×480</td><td>0.78</td><td>103</td><td>84.26</td><td>85.30</td><td>800.09</td></tr><tr><td colspan="8">Chunk-wise autoregressive models</td></tr><tr><td>SkyReels-V2 [10]</td><td>1.3B</td><td>960×540</td><td>0.49</td><td>112</td><td>82.67</td><td>84.70</td><td>74.53</td></tr><tr><td>MAGI-1 [69]</td><td>4.5B</td><td>832×480</td><td>0.19</td><td>282</td><td>79.18</td><td>82.04</td><td>67.74</td></tr><tr><td>Caus Vid [100]*</td><td>1.3B</td><td>832×480</td><td>17.0</td><td>0.69</td><td>81.20</td><td>84.05</td><td>69.80</td></tr><tr><td>Self Forcing (Ours, chunk-wise)</td><td>1.3B</td><td>832×480</td><td>17.0</td><td>0.69</td><td>84.31</td><td>85.07</td><td>81.28</td></tr><tr><td colspan="8">Autoregressive models†</td></tr><tr><td>NOVA [13]</td><td>0.6B</td><td>768×480</td><td>0.88</td><td>4.1</td><td>80.12</td><td>80.39</td><td>79.05</td></tr><tr><td>Pyramid Flow [33]</td><td>2B</td><td>640×384</td><td>6.7</td><td>2.5</td><td>81.72</td><td>84.74</td><td>69.62</td></tr><tr><td>Self Forcing (Ours, frame-wise)</td><td>1.3B</td><td>832×480</td><td>8.9</td><td>0.45</td><td>84.26</td><td>85.25</td><td>80.30</td></tr></table>

\* We compare with the offcial implementation of CausVid that uses the same base model (Wan-1.3B). † The distinction of AR/non-AR applies to the temporal dimension.

![](images/5.jpg)  

Figure 5: Qualitative comparisons. We visualize videos generated by Self Forcing (Ours) against those by Wan2.1 [83], SkyReels-V2 [10], and CausVid [100] at three time steps. All models share the same architecture with 1.3B parameters.

models trained with SiD and GAN objectives achieve comparable performance as detailed in our ablation studies. As shown in Figure 5, CausVid suffers from the error accumulation problem that causes the saturation to increase over time. Our approach obtains slightly better visual quality than Wan2.1/SkyReels-V2, while being around 150x faster in latency. More example videos are provided in the project website (https://self-forcing.github.io/).

Table 2: Ablation study. We conduct controlled ablation studies comparing different training paradigms and distribution matching objectives under our training setup across chunk-wise (left) and frame-wise (right) AR models. Self Forcing works well with all different distribution matching objectives and consistently outperforms alternative training approaches.   

<table><tr><td>Chunk-wise AR</td><td colspan="3">Evaluation scores ↑ Total Quality Semantic</td></tr><tr><td></td><td>Score Score</td><td></td><td>Score</td></tr><tr><td>Many (50×2)-step models Diffusion Forcing (DF)</td><td>82.95 83.66</td><td></td><td>80.09</td></tr><tr><td>Teacher Forcing (TF)</td><td></td><td>83.58 84.34</td><td>80.52</td></tr><tr><td colspan="4">Few (4)-step models DF + DMD</td></tr><tr><td>TF + DMD Self Forcing (Ours, DMD) 84.31 Self Forcing (Ours, SiD) Self Forcing (Ours, GAN) 83.88</td><td>82.76 82.32 84.07 85.06</td><td>83.49 82.73 85.07 85.52</td><td>79.85 80.67 81.28 78.24</td></tr></table>

<table><tr><td>Frame-wise AR</td><td colspan="3">Evaluation scores ↑ Total Quality Semantic</td></tr><tr><td></td><td>Score Score</td><td></td><td>Score</td></tr><tr><td colspan="4">Many (50×2)-step models</td></tr><tr><td>Diffusion Forcing (DF) Teacher Forcing (TF)</td><td>77.24 79.72 80.34 81.34</td><td></td><td>67.33 76.34</td></tr><tr><td colspan="4">Few (4)-step models</td></tr><tr><td>DF + DMD TF + DMD</td><td>80.56 78.12 84.71</td><td>81.02 79.62 85.25</td><td>78.71 72.11</td></tr><tr><td colspan="3">Self Forcing (Ours, DMD) 84.26 Self Forcing (Ours, SiD) 83.54 Self Forcing (Ours, GAN) 83.27</td><td>80.30 78.86</td></tr></table>

Ablation Studies. We perform controlled comparisons of Self Forcing with alternative autoregressive diffusion training approaches. We evaluate: (1) AR Diffusion models trained with denoising diffusion loss using either Teacher Forcing or Diffusion Forcing, and (2) few-step AR Diffusion models trained with TF/DF inputs but optimized with distribution matching objectives. The latter configuration with DF and DMD essentially replicates CausVid [100] within our implementation framework, allowing direct comparison under identical training conditions.

Table 2 demonstrates that Self Forcing performs robustly across various distribution matching objectives (DMD, SiD, and GAN), consistently outperforming all baselines. While baseline methods exhibit notable quality degradation when shifting from chunk-wise to frame-wise AR due to error accumulation associated with increased AR unrolling steps, usually manifesting as progressive oversaturation or over-sharpening (similar to CausVid in Appendix B Fig. 5), Self Forcing maintains consistent performance across both setups, highlighting its effectiveness at addressing exposure bias.

Rolling KV cache. We observe that recomputing KV cache when shifting sliding window (Fig. 3 (b)) results in significantly reduced throughput (only 4.6 FPS) when generating 10-second videos. While naive rolling KV cache maintains high throughput, it introduces severe visual artifacts, as illustrated in the examples in Appendix B. By training the model to generate frames without seeing the initial image latent, we effectively mitigate these artifacts while maintaining high throughput (16.1 FPS).

Training efficiency. One might expect Self Forcing training to be computationally prohibitive given its sequential nature that contradicts the parallelizable paradigm of transformers. Surprisingly, our experiments reveal that Self Forcing actually outperforms alternative strategies in training efficiency. As shown in Fig. 6 (left), Self Forcing achieves comparable per-iteration training time to Teacher Forcing and Diffusion Forcing. Furthermore, Fig. 6 (right) demonstrates that Self Forcing achieves superior quality given same wall-clock training budgets compared to both alternative approaches. Each Self Forcing experiment with DMD converges in approximately 1.5 hours on 64 H100 GPUs.

This counter-intuitive result stems from two key factors: First, while Self Forcing performs sequential rollout, it still processes all tokens within each individual frame/chunk in parallel, maintaining high GPU utilization during training. Second, TF and DF require specialized attention masking patterns to enforce causal dependencies, introducing additional computational overhead even with specialized implementations like FlexAttention [15]. On the other hand, Self Forcing always uses full attention during training and can leverage highly optimized attention kernels such as FlashAttention-3 [72].

# 5 Discussion

In this section, we examine the broader implications of our results, discuss additional perspectives, and outline potential directions for future research.

![](images/6.jpg)  

Figure 6: Training efficiency comparison. Left: Per-iteration time across different chunk-wise, few-step autoregressive video diffusion training algorithms (using DMD as the distribution matching objective). Right: Video quality (VBench score) vs. wall clock training time.

Fundamental limitation of the parallelizable training paradigm. Parallelizable training has been pivotal to transformers' success by enabling efficient scaling. However, this parallelism introduces fundamental limitations. Prior research [57] demonstrates that parallel architectures inherently limit expressiveness in sequential state-tracking problems. Our work highlights another critical limitation: parallelizable training paradigms creates misalignment between training and inference distributions, leading to the accumulation of errors over time. We advocate a new paradigm of parallel pre-training and sequential post-training that combines the best of both worlds. While this paradigm shift is gaining momentum in language modeling through reinforcement learning [21], our work represents the first step towards this direction for the video domain. We believe our framework is general and can be applied to other sequence domains, especially where the data is continuous.

Interplay between AR, Diffusion, and GANs. Autoregressive models, diffusion models, and GANs have traditionally been viewed as distinct paradigms in generative modeling. Our work highlights their complementary nature and demonstrates how they can be effectively integrated. Specifically, autoregressive and diffusion models provide complementary ways to factorize distributions (chain-rule vs. latent-variable), which can be composed in a nested manner. The core idea behind GANs—matching the distribution of an implicit generator to the target distribution by drawing samples from the implicit generator—can be employed to train a generator powered by autoregressive-diffusion factorization.

Limitation and future directions. While our method effectively mitigates error accumulation within the training context length, quality degradation remains observable when generating videos substantially longer than those seen during training. Additionally, our gradient truncation strategies—while necessary for memory efficiency—may limit the model's ability to learn long-range dependencies. Future work could explore both improved extrapolation techniques and inherently recurrent architectures like state-space models [19, 63] that better balance memory efficiency with long-context modeling.

# Acknowledgments

We thank Tianwei Yin, Beidi Chen, Kaiwen Zheng, Kai Zhang, Gaurav Parmar, Yi Gu, Sai Bi, and Jianming Zhang for valuable discussions.