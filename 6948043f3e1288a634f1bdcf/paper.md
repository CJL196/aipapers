# Revisiting Feature Prediction for Learning Visual Representations from Video

Ain Bardes1uentin GarridoJea Ponce356 Xinlei ChenMicael RabbatYann $^ { 1 , 5 , 6 }$ , Mahmoud AssranNicolas Ballas1 1FAIR at Meta, 2Inria, $^ 3$ École normale supérieure, CNRS, PSL Research University, $^ 4$ Univ. Gustave Eiffel, CNRS, LIGM, $^ 5$ Courant Institute, New York Univry, $_ 6$ Center for Data Science, New York University Joint last author This paper explores feature prediction as a stand-alone objective forunsupervised learning from video and intrucVJEPAcollectionisn mode raiesoely usinature prediction jectiviu theu reraimagecders, x nativxaple rucion hesursupes. The models aretrained on 2 million videos collcted from publi datasets and are evaluated on downstream image and video tasks.Our results show that learning by predicting video features leads to versatile visual representations that perform well on both motion and appearance-based tasks, without adaption of the model's parameters; e.g., using a frozen backbone. Our largest model, a ViT-H/16 trained only on videos, obtains $8 1 . 9 \%$ on Kinetics-400, $7 2 . 2 \%$ on Something-Something-v2, and $7 7 . 9 \%$ on ImageNet1K. Date: April 15, 2024 Correspondence: {abardes, massran, ballasn}@meta.com Code: https://github.com/facebookresearch/jepa Blogpost: Click here

# 1 Introduction

Humans possess the remarkable ability to map low-level signals originating from the retina into a semantic spatiotemporal understanding of the world; synthesizing notions such as objects and global motion (Spelke et al., 1995). A long-standing goal of the machine learning community is to identify the principles or objectives that may guide such unsupervised learning in humans (Field, 1994; Berkes and Wiskott, 2005; Hinton, 1989). One related hypothesis is based on the predictive feature principle (Rao and Ballard, 1999), which posits that representations of temporally adjacent sensory stimuli should be predictive of each other.

In this work, we revisit feature prediction as a standalone objective for unsupervised learning of visual representations from video. Numerous advances in the field — such as the standard use of transformer architectures in vision (Dosovitskiy et al., 2020), the maturing of masked autoencoding frameworks (Xie et al., 2021; Bao et al., 2021; He et al., 2021), query-based feature pooling (Chen et al., 2022), joint-embedding predictive architectures (JEPA) (LeCun, 2022; Assran et al., 2023; Baevski et al., 2022b), and larger datasets — form a unique arsenal of tools, which we integrate in a modern and conceptually simple method, the video joint-embedding predictive architecture or V-JEPA, which is based solely on feature prediction, without using pretrained image encoders, text, negative examples, human annotations, or pixel level reconstruction.

![](images/1.jpg)  
Figure 1 V-JEPA models pretrained on video learn versatile visual representations. It performs well on motion-based tasks (Something-Something-v2) and appearance-based tasks (Kinetics 400) without adaptation of the model's parameters, i.e., using the same frozen backbone for both tasks.

We seek to answer the simple question: How effective is feature prediction as a standalone objective for unsupervised learning from video with modern tools? To that end, we pretrain a family of V-JEPA models on a dataset of 2 million videos collected from publicly available datasets by combining a masked modeling prediction task with a joint-embedding predictive architecture (see Figure 2). We measure performance on several downstream image and video tasks, using both frozen evaluation and end-to-end fine-tuning. Our findings suggest that feature prediction can indeed serve as an effective stand-alone objective for unsupervised learning from video, while using significantly shorter training schedules than pixel prediction methods. Specifically: • Feature prediction leads to versatile visual representations that perform well across downstream image and video tasks without adaption of the model's weights; i.e., using a frozen backbone. V-JEPA achieves the best performance among methods we consider ( $+ 6 \%$ accuracy) on the SomethingSomething-v2 task, which requires finegrained temporal understanding. V-JEPA is also competitive on tasks like Kinetics400, where appearance-based features are sufficient and hence state-of-the-art image models such as DINOv2 excel (Figure 1 and Table 6). • Models trained with feature prediction are superior to pixel prediction approaches under a frozen evaluation protocol (attentive probing) and are competitive with pixel prediction under full fine-tuning, while using significantly shorter training schedules (Tables 5 and 6). variance and explores feature prediction using masked modeling.

Predictive Features. Going beyond local invariance, a family of works trains a predictor network to map the representation of a frame or clip at one time-step to a distinct representation at another time-step. Srivastava et al. (2015); Vondrick et al. (2016); Wang et al. (2023b) train such a video feature predictor network on top of a frozen pretrained image or video encoder. Unfreezing the target feature extractor, several methods train the video encoder and the predictor network simultaneously, while preventing collapse by using a supervised action forecasting loss (Girdhar and Grauman, 2021), or by using the representations of distant clips as negative samples in a contrastive loss (Han et al., 2019, 2020; Tan et al., 2023), often focusing on small convolutional encoders (Han et al., 2019, 2020). The idea of learning a representation by predicting missing information in feature space is also core to the joint-embedding predictive architecture (JEPA) (LeCun, 2022), which combines a siamese encoder with a predictor network. JEPAs have been successfully instantiated in several modalities, such as with audio data (Baevski et al., 2022b) and image data (Zhou et al., 2021; Oquab et al., 2023; Assran et al., 2023). In this work, we extend this paradigm to video data by leveraging recent advances in self-supervised learning. • Models trained with feature prediction are more label-efficient than pixel prediction approaches. Decreasing the available number of labeled examples results in an increase in the performance gap between V-JEPA and pixel-reconstruction models (Table 7).

# 2 Related Works

Slow Features. One way to encourage temporally adjacent representations to be predictive of each other is to ensure that they vary slowly over time. Early works targeting predictive features encouraged representations of individual video frames to be locally temporally invariant, while preventing representation collapse by using spectral methods, as in SFA (Wiskott and Sejnowski, 2002), SSA (Kayser et al., 2001), and Simulated Fixations (Zou et al., 2012). More recently, Goroshin et al. (2015); Wang et al. (2010) train a siamese convolutional network to map the representations of two subsequent frames to the same point, while encouraging distant frames to have diverse representations via a pairwise margin loss and a triplet loss, respectively. Other works (Oord et al., 2018; Surís et al., 2021; Feichtenhofer et al., 2021) implement temporal invariance using noisecontrastive estimation (Gutmann and Hyvärinen, 2012). Our exploration in this paper goes beyond temporal in

Advances in Self-Supervised Learning. The use of vision transformers (Dosovitskiy et al., 2020; Li et al., 2022) has become standard practice in self-supervised learning with joint-embedding architectures (Chen et al., 2021; Caron et al., 2021; Oquab et al., 2023; Zhou et al., 2021; Assran et al., 2022), and unlocked masked image modeling in pixel space by parameterizing the pixel decoder as a transformer with learnable mask tokens (Dosovitskiy et al., 2020; Xie et al., 2021; He et al., 2021; Bao et al., 2021), demonstrating a step-change in the representation quality of autoencoding methods (Vincent et al., 2010). This line of generative methods was subsequently extended to video data using spatio-temporal masking (Tong et al., 2022; Feichtenhofer et al., 2022; Wang et al., 2023a; Kalluri et al., 2023; Gupta et al., 2023). It was also recently shown that the representations of masked image autoencoders could be significantly improved by using learnable pooling mechanisms based on cross-attention (Chen et al., 2022). Finally, through careful selection of design choices, the non-contrastive collapse prevention strategy in BYOL (Grill et al., 2020) was recently made to work with image feature prediction methods (Baevski et al., 2022b; Assran et al., 2023), which demonstrated the ability to learn representations that can be leveraged for various downstream tasks without relying on invariance to hand-crafted image transformations.

# Feature Prediction versus Pixel Reconstruction.

Approaches that predict in pixel space must dedicate significant model capacity and compute to capture all the low-level detail in the visual input. By contrast, approaches that predict in latent space have the flexibility to eliminate irrelevant or unpredictable pixel-level details from the target representation (Vondrick et al., 2016). Predicting in representation space has been shown to lead to versatile representations that perform well across many downstream tasks through linear probing or lowshot adaptation (Assran et al., 2023; Oquab et al., 2023; Assran et al., 2022), while demonstrating an efficiency gain during pretraining compared to pixel level reconstruction (Assran et al., 2023; Baevski et al., 2022b,a). The works of Baevski et al. (2022a,b) additionally show that predicting in representation space results in competitive end-to-end fine-tuning performance in the image, audio and text domains. In this work, we extend these findings to the video modality.

# 3 Methodology: Video-JEPA

![](images/2.jpg)  
Figure 2 Joint-Embedding Predictive Architectures are trained to predict the representation of an input $_ y$ from the representation of another input $x$ . The additional variable $z$ provides the predictor with information about the transformation that computes $_ y$ from $_ x$ .

Our goal is to explore the effectiveness of feature prediction as a stand-alone objective for learning visual representations from video. To that end, we use a joint-embedding predictive architecture (JEPA) (LeCun, 2022); see Figure 2. The main idea behind a JEPA is to learn by predicting the representation of an input $y$ from the representation of another input $x$ . The basic architecture is made up of an encoder, $E _ { \theta } ( \cdot )$ , which computes the representation of the inputs, and a predictor, $P _ { \phi } ( \cdot )$ , which predicts the representation of $y$ from the representation of $x$ , conditioned on a variable $z$ indicating the transformation (or corruption) between $x$ and $y$ . Conditioning on $z$ enables the generation of distinct predictions for various transformations of $x$ .

# 3.1 Training Objective

We train our visual encoder $E _ { \theta } ( \cdot )$ to satisfy the constraint that representations computed from one part of the video, $y$ , should be predictable from representations computed from another part of the video, $x$ . The predictor network $P _ { \phi } ( \cdot )$ , which maps the representation of $x$ to the representation of $y$ , is trained simultaneously with the encoder, and is provided specification of the spatio-temporal positions of $y$ through the conditioning variable $z  \Delta _ { y }$ .

Naively implementing the objective using the regression would admit a trivial solution, where the encoder outputs a constant representation, regardless of its input. In practice, we use the following modified objective to prevent representation collapse, where $\operatorname { s g } ( \cdot )$ denotes a stop-gradient operation, which does not backpropagate through its argument, and $\overline { { E } } _ { \theta } ( \cdot )$ is an exponential moving average of the network $E _ { \theta } ( \cdot )$ The use of an exponential-moving average feature extractor along with a stop-gradient and a predictor has been used as a collapse prevention strategy for image pretraining (Grill et al., 2020), and studied empirically (Xie et al., 2021) and theoretically (Tian et al., 2021). In fact, the objective in equation (1) is similar to the loss of Assran et al. (2023) used for image pretraining, but we modify it to use an $\ell _ { 1 }$ regression, which we found to be more stable.

$$
\begin{array} { r l } { \operatornamewithlimits { m i n i m i z e } _ { \theta , \phi } } & { { } \| P _ { \phi } ( E _ { \theta } ( x ) , \Delta _ { y } ) - E _ { \theta } ( y ) \| _ { 1 } , } \end{array}
$$

$$
\begin{array} { r l } { \operatorname * { m i n i m i z e } _ { \boldsymbol { \theta } , \boldsymbol { \phi } } } & { { } \| P _ { \boldsymbol { \phi } } ( E _ { \boldsymbol { \theta } } ( \boldsymbol { x } ) , \Delta _ { \boldsymbol { y } } ) - \mathrm { s g } ( \overline { { E } } _ { \boldsymbol { \theta } } ( \boldsymbol { y } ) ) \| _ { 1 } , } \end{array}
$$

Theoretical motivation. A theoretical motivation for the effectiveness of this collapse prevention strategy was proposed in Grill et al. (2020) for the BYOL method. We provide a simple adaptation of their analysis for our $\ell _ { 1 }$ loss. For ease of exposition, we will disregard the effect of the conditioning variable $z$ and consider one dimensional representations. Denote the representation $\overline { { E } } _ { \theta } ( y )$ by a random variable $Y$ . The optimal predictor under equation (1) is thus given by the following functional expression,

$$
\begin{array} { r l } & { P ^ { \star } ( E _ { \theta } ( x ) ) = \operatorname * { a r g m i n } _ { P } \| P ( E _ { \theta } ( x ) ) - Y \| _ { 1 } } \\ & { \qquad = \operatorname * { m e d i a n } ( Y | E _ { \theta } ( x ) ) . } \end{array}
$$

Substituting this expression for the optimal predictor into the loss function and evaluating the expected gradient of the encoder gives where MAD(. $| E _ { \theta } ( x ) )$ is the median absolute deviation of a random variable conditioned on $E _ { \theta } ( x )$ . Thus, in the case where the predictor is optimal, the encoder must learn to capture as much information about the video as possible to minimize the deviation of the target. The hypothesis is that incorporating an exponential moving average to compute the representation of $y$ ensures that the predictor evolves faster than the encoder and remains close to optimal, thereby preventing collapse.

$$
\nabla _ { \boldsymbol { \theta } } \mathbb { E } \| P ^ { \star } ( E _ { \boldsymbol { \theta } } ( \boldsymbol { x } ) ) - Y \| _ { 1 } = \nabla _ { \boldsymbol { \theta } } \mathrm { M A D } ( Y | E _ { \boldsymbol { \theta } } ( \boldsymbol { x } ) ) ,
$$

![](images/3.jpg)  
Figure 3 V-JEPA. Training operates on a video clip of $T$ frames with spatial resolution $H \times W$ , flattened into a sequence of $L$ tokens. (Left to right): We first obtain the input of the $x$ -encoder by dropping tokens from the video clip. The $x$ r he pro he ask i ntpu  e vororu tokeNex te outputs of the $x$ -encoder are concatenated with a set of learnable mask tokens containing positional embeddings of the masked T each mask token. The outputs of the predictor are then regressed to the prediction targets using an $L _ { 1 }$ loss. The prediction targets correspond to the output of the $_ y$ -encoder.

# 3.2 Prediction Task: Predicting $y$ from $x$

The feature prediction task is based on a masked modeling formulation (He et al., 2021; Tong et al., 2022); i.e., regions $x$ and $y$ from the video are sampled using masking. To sample $y$ from a video, we sample several (possibly overlapping) spatially continuous blocks with various aspect ratios and repeat the spatial blocks across the entire temporal dimension of the video; $x$ is taken to be the complement. Masking a large continuous block that covers the full temporal dimension limits information leakage due to the spatial and temporal redundancy of videos, and results in a harder prediction task (Tong et al., 2022). We leverage two types of masks: short-range masks, where we take the union of 8 randomly sampled target blocks covering $1 5 \%$ of each frame, and long-range masks, where we take the union of 2 randomly sampled target blocks covering 70% of each frame. In both cases, the aspect ratio for all sampled blocks is randomly chosen in the range (0.75, 1.5). Given that both short-range and long-range masks are produced by sampling many blocks and taking their union, the result is an average masking ratio of $\sim 9 0 \%$ . We refer to our masking strategy as multi-block, and compare it to other possible masking strategies in Section 4.

# 3.3 Network Parameterization

We use a Vision Transformer (ViT) (Dosovitskiy et al., 2020; Arnab et al., 2021) as our video backbone. To process a video with a transformer network, we split the video clip into a 3D grid of $L$ spatio-temporal patches, where a patch consists of a $1 6 \times 1 6$ pixel block spanning 2 consecutive frames; we refer to these spatio-temporal patches as tokens. This sequence of tokens is then directly processed by the stack of transformer blocks. Inputs $x$ and $y$ correspond to masked regions of a video, we apply the video masks by simply dropping a subset of the tokens. We apply masking at the input of the $x$ -encoder, and at the output of the $y$ -encoder to construct contextualized targets (Baevski et al., 2022b). The encoder is parameterized using standard ViT networks, while the predictor is a narrow transformer implemented using 12 blocks with an embedding dimension of 384. Taking inspiration from masked autoencoders (He et al., 2021), our predictor takes as input the sequence of embeddings produced by the $x$ -encoder as well as a sequence of learnable mask tokens with positional embeddings indicating the spatio-temporal positions of the $y$ tokens. The output of the predictor is an embedding vector for each mask token; see Figure 3 and refer to Appendix B for more details.

# 3.4 Pretraining Data and Evaluation Setup

Pretraining. We combine several public datasets to construct an unsupervised video pretraining dataset, which we refer to as VideoMix2M. Specifically, we combine the videos from HowTo100M (HT) (Miech et al., 2019), Kinetics-400/600/700 (K710) (Kay et al., 2017), and Something-Something-v2 (SSv2) (Goyal et al., 2017), and remove any overlap with the validation sets of Kinetics-400/600/700 and Something-Something-v2, resulting in approximately 2 million videos. We train a ViT-L/16, a ViT-H/16, and a ViT-H/16384 transformer model on VideoMix2M. We use a batch size of 3072 for the ViT-L/16 and ViT-H/16 models, and a batch size of 2400 for the ViT-H/16384 model. Each model takes as input a video clip of 16 frames sampled with a frameskip of 4, corresponding to roughly 3 second clips on average. The ViT-L/16 and ViT-H/16 process the video at a spatial resolution of 224, while the ViT-H/16384 uses an input resolution of 384; cf. Appendix C. m Vix0 ba0ulo W a consistent improvement over pixel space prediction.   

<table><tr><td></td><td></td><td colspan="3">Frozen Evaluation</td><td>Fine-Tuning</td></tr><tr><td>Target</td><td>Arch.</td><td>K400 (16×1×1)</td><td>SSv2 (16×1×1)</td><td>IN1K</td><td>K400-ft (16×5×3)</td></tr><tr><td>Pixels</td><td>ViT-L/16</td><td>68.6</td><td>66.0</td><td>73.3</td><td>85.4</td></tr><tr><td>Features</td><td>ViT-L/16</td><td>73.7</td><td>66.2</td><td>74.8</td><td>85.6</td></tr></table>

ab o across tasks increases with the pretraining dataset size.

<table><tr><td></td><td></td><td></td><td colspan="3">Frozen Evaluation</td><td></td></tr><tr><td>Arch.</td><td>Data</td><td>#Samples</td><td>K400 (16×1×1)</td><td>SSv2 (16×1×1)</td><td>IN1K</td><td>Avg.</td></tr><tr><td rowspan="4">ViT-L/16</td><td>K710</td><td>700K</td><td>75.8</td><td>63.2</td><td>73.7</td><td>70.9</td></tr><tr><td>K710+SSv2</td><td>900K</td><td>72.9</td><td>67.4</td><td>72.8</td><td>71.0</td></tr><tr><td>K710+HT</td><td>1900K</td><td>74.5</td><td>64.2</td><td>74.8</td><td>71.1</td></tr><tr><td>VideoMix2M</td><td>2000K</td><td>73.7</td><td>66.2</td><td>74.8</td><td>71.5</td></tr><tr><td rowspan="2">ViT-H/16</td><td>K710+SSv2</td><td>900K</td><td>75.7</td><td>66.8</td><td>73.7</td><td>72.0</td></tr><tr><td>VideoMix2M</td><td>2000K</td><td>74.0</td><td>68.5</td><td>75.9</td><td>72.8</td></tr></table>

Evaluations. Pretrained models are evaluated on downstream video and image tasks. On video tasks, we use a subset of the VideoGLUE benchmark (Yuan et al., 2023) to test for various capabilities; specifically, we investigate action recognition on Kinetics400 (K400) (Kay et al., 2017), motion classification on Something-Something-v2 (SSv2) (Goyal et al., 2017), and action localization on AVA (Gu et al., 2018). Action classification on Kinetics evaluates the appearance-based understanding of the model, as many action classes in the dataset can be inferred from the presence of specific objects in the video (Sevilla-Lara et al., 2021). Motion classification on Something-Something-v2 evaluates the temporal understanding of the model, as action classes in the dataset are decoupled from the appearance/presence of specific objects in the video (Goyal et al., 2017). Finally, action localization on AVA evaluates the ability of the model to understand and localize motions in the video. We follow standard practice and report accuracy on K400 and SSv2 by sampling several spatial and temporal views. For static image tasks, we explore object recognition on ImageNet (Russakovsky et al., 2015), scene classification on Places205 (Zhou et al., 2014), and fine-grained recognition on iNaturalist 2021 (Van Horn et al., 2018).

# 4 What Matters for Learning Representations from Video?

In this section we isolate the contributions of several design choices, including: a) the use of a feature prediction versus pixel prediction objective, b) the construction of the pretraining data distribution, c) the feature pooling strategy for leveraging the model's representations in downstream tasks, and d) the masking strategy, towards identifying: what to predict from what?

# 4.1 Predicting Representations versus Pixels

We first ablate the effect of computing the prediction loss in representation space. We train a pair of ViT-L/16 models using either a V-JEPA feature prediction loss, or a mean-squared error loss with the normalized pixel values, as in masked autoencoders (He et al., 2021), and perform a sweep over the learning rate and weight decay schedules for both approaches. All models are pretrained on VideoMix2M for 90K iterations with a batch size of 3072 using multi-block masking. We examine performance on Kinetics-400 (K400), Something-Something-v2 (SSv2), and ImageNet-1K (IN1K), using a frozen backbone with an attentive probe, and report top-1 accuracy using a single center view. We also examine end-to-end fine-tuning performance of the models on Kinetics-400. Results of this comparison are reported in Table 1 and indicate that predicting in feature space provides a consistent performance improvement over pixel space prediction in both frozen evaluation of the video backbone, as well as end-to-end fine-tuning.

# 4.2 Pretraining Data Distribution

Next we study the impact of the pretraining data distribution in Table 2. Leveraging large scale datasets has been critical for enabling the surge of advancements in other modalities, such as text and images (Kaplan et al., 2020; Cherti et al., 2023). We investigate whether a similar trend holds for video data. To control for the possible confounding variable of compute budget, we pretrain all models in Table 2 for 90K iterations using a batch-size of 3072. We report downstream results on K400, SSv2, and IN1K using a frozen backbone with an attentive probe, and report top-1 accuracy using a single center view. Table 3 Average Pooling vs. Adaptive Pooling. We pool the feature map output by the frozen V-JEPA encoder using an attentive probe, which is then fed into a linear classifier for downstream supervised tasks (K400 and SSv2). We evaluate two pooling strategies: 1) average pooling (Avg.), and attentive pooling (Att.). Results are reported using a single center view. Using adaptive pooling with a crossattention layer leads to improvements of $+ 1 7 . 3$ points on K400 and $+ 1 6 . 1$ points on SSv2.   

<table><tr><td></td><td></td><td colspan="4">Frozen Evaluation</td></tr><tr><td></td><td></td><td colspan="2">K400</td><td colspan="2">SSv2</td></tr><tr><td>Method</td><td>Arch.</td><td>(16×1×1) Avg.</td><td>Att.</td><td>(16×1×1)</td><td>Att.</td></tr><tr><td>V-JEPA</td><td>ViT-L/16</td><td>56.7</td><td>73.7</td><td>Avg. 50.1</td><td>66.2</td></tr></table>

Table 2 shows that average performance across tasks monotonically increases as we increase the size of the pretraining dataset, but the best task-specific performance is obtained by independently selecting the pretraining data for each specific downstream task. For instance, the L/16 obtains its best SSv2 performance when pretrained on K710+SSv2, its best K400 performance when pretrained only on K710, and its best IN1K performance when pretrained only on K710+HT. The best average performance across all tasks is achieved by pretraining VideoMix2M, which combines all the data sources. Similarly, the H/16 pretrained on K710+SSv2 achieves a greater K400 score than the H/16 pretrained on VideoMix2M, however, the top performing H/16 on average is pretrained on VideoMix2M.

# 4.3 Evaluation: Attentive Probing

Next we explore the feature pooling strategy for applying the model's representations in downstream tasks. Since the prediction objective in equation (1) is unnormalized, there is no a priori reason for the encoder to yield a linearly separable subspace (Chen et al., 2020). Thus, rather than using a linear operation (averaging) to pool the features output of the frozen backbone, we explore a learnable non-linear pooling strategy. Specifically, when evaluating the frozen pretrained backbone on downstream tasks, we learn a cross-attention layer with a learnable query token. The output of the crossattention layer is then added back to the query token (residual connection), and then fed into two-layer MLP with a single GeLU activation, followed by a LayerNorm, and finally a linear classifier.

<table><tr><td></td><td colspan="3">Frozen Evaluation</td></tr><tr><td>Masking</td><td>K400 (16×1×1)</td><td>SSv2 (16×1× 1)</td><td>IN1K</td></tr><tr><td>random-tube[0.9]</td><td>51.5</td><td>46.4</td><td>55.6</td></tr><tr><td>causal multi-block[6]</td><td>61.3</td><td>49.8</td><td>66.9</td></tr><tr><td>causal multi-block[12]</td><td>71.9</td><td>63.6</td><td>72.2</td></tr><tr><td>multi-block</td><td>72.9</td><td>67.4</td><td>72.8</td></tr></table>

In Table 3 we see that using adaptive pooling with a learnable cross-attention layer leads to a significant improvement of +17 points on K400 and $+ 1 6 . 1$ points on SSv2. Using an attentive-probe is also beneficial for other baseline models as reported in Appendix E.

# 4.4 Prediction Task: Predicting $y$ from $x$

We conduct an ablation on the masking strategy used in V-JEPA pretraining. We examine the following masking strategies: random-tube[r] in which $x$ is obtained by removing a random fraction $r$ of tubes (spatial patches extended across the entire temporal duration) from the video, causal multi-block[p] in which $x$ is restricted to the first $p$ frames of the 16-frame video, which are then masked with a random set of spatio-temporal blocks, and multi-block in which $x$ obtained by masking a random set of spatio-temporal blocks from the entire video. Spatio-temporal blocks are sampled using the parameters described in Section 3.2; an ablation on the size and quantity of masked spatio-temporal blocks is provided in Appendix E.4. Table 4 indicates that the best results are obtained by sampling $x$ using a multi-block strategy, wherein the network is forced to make predictions after removing large continuous blocks in the video. When $x$ is only sampled from the first few frames of the video, as in the causal multi-block strategy, we observe a decrease in downstream performances. Finally, the random-tube strategy, wherein 90% of the tubes in the video are randomly masked, leads to features of low-semantic quality when combined with our feature prediction objective. TableComparison with Pixel Predicion MethodsWe compare V-JEPA wit mniMA (Girdhar t al 2023) VideoMA H VT-riteurarab HieWevauatheprceremaask Pa0, Al lvo a tasks, except ImageNet, where the model achieves $7 4 . 8 \%$ compared to $7 5 . 1 \%$ of an OmniMAE model trained directly on mNeJEPhive e estu oo - aeheH SSv2. The V-JEPA results are achieved while processing significantly fewer examples during pretraining.   

<table><tr><td rowspan="2"></td><td rowspan="2">#Samples</td><td rowspan="2"></td><td rowspan="2"></td><td colspan="6">Frozen Evaluation w/ Att. Pooling</td><td colspan="2">Fine-Tuning</td></tr><tr><td>K400 (16×8×3)</td><td>SSv2 (16×2×3)</td><td>AVA</td><td>IN1K</td><td>Places205</td><td>iNat21</td><td>K400-ft (16×5× 3)</td><td>SSv2-ft (16×2× 3)</td></tr><tr><td>Methods pretrained using pixel prediction</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>OmniMAE</td><td>ViT-L/16</td><td>2400M</td><td>1170K</td><td>65.6</td><td>60.6</td><td>14.4</td><td>75.1</td><td>59.8</td><td>66.1</td><td>84.0</td><td>74.2</td></tr><tr><td>VideomMAE</td><td> ViT-L/16</td><td>410M</td><td>400K</td><td>77.8</td><td>65.5</td><td>21.6</td><td>71.1</td><td>59.3</td><td>64.6</td><td>85.4</td><td>74.3</td></tr><tr><td>Hiera</td><td>iera-L</td><td>770M</td><td>500K</td><td>755</td><td>64.2</td><td>15.8</td><td>68.9</td><td>558.5</td><td>56.9</td><td>87.3</td><td>75.1</td></tr><tr><td>V-JEPA</td><td>ViT-L/16</td><td>270M</td><td>90K</td><td>80.8</td><td>69.5</td><td>25.6</td><td>74.8</td><td>60.3</td><td>67.8</td><td>85.6</td><td>75.1</td></tr></table>

TabahroeJa-he we proeasks N1 Pace20at21nviask(K400Sv AVAAe ava o exct -JEP1JEP hivaeiveo 38On K400 and Sv2 we follow the standar practicef reportin accuracy rom several spatial and temporalviews frohiparehbsels hinvm crss. CexJEont v tasks requiring motion understanding ( $+ 2 1$ points on SSv2), and reduces the gap between video and image models on tasks requiring static appearance-based features.

<table><tr><td></td><td></td><td></td><td></td><td colspan="3">Video Tasks</td><td colspan="3">Image Tasks</td></tr><tr><td>Method</td><td>Arch.</td><td>Params.</td><td>Data</td><td>K400 (16×8×3)</td><td>SSv2 (16×2×3)</td><td>AVA</td><td>IN1K</td><td>Places205</td><td>iNat21</td></tr><tr><td colspan="10">Methods pretrained on Images</td></tr><tr><td>I-JEPA</td><td>ViT-H/16512</td><td>630M</td><td>IN22K</td><td>79.7</td><td>50.0</td><td>19.8</td><td>84.4</td><td>66.5</td><td>85.7</td></tr><tr><td>OpenCLIP</td><td>Vi-G/14</td><td>1800M</td><td>LAION</td><td>81.8</td><td>34.8</td><td>23.2</td><td>85.3</td><td>70.2</td><td>83.6</td></tr><tr><td>DINOv2</td><td>ViT-g/14</td><td>1100M</td><td>LVD-142M</td><td>83.4</td><td>50.6</td><td>24.3</td><td>86.2</td><td>68.4</td><td>88.8</td></tr><tr><td colspan="10">Methods pretrained on Videos</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>MVD</td><td>ViT-L/16</td><td>200M</td><td>IN1K+K400</td><td>79.4</td><td>66.5</td><td>19.7</td><td>73.3</td><td>59.4 660.6</td><td>65.7 72.4</td></tr><tr><td>OmniMAE</td><td>ViT-H/16 ViIT-H/16</td><td>630M 630M</td><td>IN1K+SSv2 K400</td><td>71.4 79.8</td><td>65.4 66.2</td><td>16.0 20.7</td><td>76.3 72.3</td><td>59.1</td><td>65.5</td></tr><tr><td>VideoMAE VideoMAEv2</td><td>ViT-g/14</td><td>1100M</td><td>Un.Hybrid</td><td>71.2</td><td>61.2</td><td>12.9</td><td>71.4</td><td>60.6</td><td>68.3</td></tr><tr><td>Hiera</td><td>Hiera-H</td><td>670M</td><td>K400</td><td>77.0</td><td>64.7</td><td>17.5</td><td>71.4</td><td>59.5</td><td>61.7</td></tr><tr><td rowspan="3">V-JEPA</td><td></td><td>200M</td><td></td><td>80.8</td><td>69.5</td><td>25.6</td><td>74.8</td><td>60.3</td><td>67.8</td></tr><tr><td>ViT-L/16 ViIT-H/16</td><td>630M</td><td>VideoMix2M</td><td>82.0</td><td>71.4</td><td>25.8</td><td>75.9</td><td>61.7</td><td>67.9</td></tr><tr><td>ViT-H/16384</td><td>630M</td><td></td><td>81.9</td><td>72.2</td><td>25.0</td><td>77.4</td><td>62.8</td><td>72.6</td></tr></table>

# 5 Comparison with Prior Work

In Section 5.1, we investigate the impact of feature prediction by comparing V-JEPA with video approaches that rely on pixel prediction, while using a similar architecture for all baselines. Subsequently, in Section 5.2, we remove the architectural constraint and report the best performance across architectures for self-supervised video and image pretraining approaches. Finally, we explore the label-efficiency of V-JEPA relative to other selfsupervised video pretraining approaches in Section 5.3. We further detail the evaluation setup in Appendix D.

# 5.1 Comparison with Pixel Prediction

To investigate the effectiveness of feature prediction pretraining, we first compare V-JEPA to video masked modeling models relying on a pixel prediction loss. We control for the possible confounding factor of model architecture by evaluating all models using either a ViT-L/16 encoder, or a Hiera-L encoder, which has a similar number of parameters. For the pixel prediction baselines we consider VideoMAE (Tong et al., 2022; Wang et al., 2023a), which trains vision transformer autoencoders exclusively on video, Hiera (Ryali et al., 2023), which trains a hierarchical transformer autoencoder on video, and OmniMAE (Girdhar et al., 2023), which trains a vision transformer autoencoder on static images and video simultaneously. Table 5 examines both frozen evaluation with an attentive probe on downstream video and image tasks, as well as end-to-end fine-tuning. In frozen evaluation, V-JEPA outperforms the baselines on all downstream tasks, except ImageNet, where we achieve $7 4 . 8 \%$ compared to $7 5 . 1 \%$ of an OmniMAE model trained directly on ImageNet; hence, V-JEPA achieves comparable ImageNet performance despite only pretraining on video.

![](images/4.jpg)  
Figure 4 SSv2 fine-tuning performance vs. Samples Seen. We report SSv2 fine-tuning for V-JEPA and pixel-reconstruction baselines using a ViT-L/16 or Hiera-L architecture. V-JEPA outperforms all pixel-reconstruction methods using a ViTL/16 and matches the Hiera-L performance while seeing significantly less samples during pretraining.

Under the fine-tuning protocol, V-JEPA also achieves the best performance of any model trained with a ViT-L/16, and matches the performance of the Hiera-L on SSv2, which benefits from a hierachical prior (Ryali et al., 2023). The V-JEPA models achieve this result while processing significantly fewer samples during pretraining (Figure 4), demonstrating the efficiency of feature prediction as a learning principle.

# 5.2 Comparison with State-of-the-Art

Next, in Table 6, we inspect how the V-JEPA models pretrained on video stack up next to the largest stateof-the-art self-supervised image and video models when freezing the backbone encoder and training an attentive probe on top. Our image pretrained baselines include OpenCLIP (Cherti et al., 2023), DINOv2 (Oquab et al., 2023), and I-JEPA (Assran et al., 2023). The OpenCLIP model is trained with a contrastive image-text alignment objective, DINOv2 and I-JEPA are trained with self-supervision. These models are known to excel in their frozen-evaluation performance (Oquab et al., 2023); i.e., their ability to produce visual features that can be applied to many downstream tasks simultaneously, without end-to-end fine-tuning, and thus provide highly competitive baselines. Our video pretrained baselines include VideoMAE (Tong et al., 2022), OmniMAE (Girdhar et al., 2023), Hiera (Ryali et al., 2023), VideoMAEv2 (Wang et al., 2023a), and MVD (Wang et al., 2023b). The OpenCLIP, DINOv2 and VideoMAEv2 models are parameterized as Giant/Gigantic vision transformer architectures containing over 1B parameters trained on large-scale image or video datasets.

Comparison with video models. Compared to large-scale video baselines, the V-JEPA models outperform all previous models on every downstream video and image task with notable margin (see Table 6). Our H/16 model outperforms the largest publicly available VideoMAE, VideoMAEv2, OmniMAE, MVD, and Hiera models by at least $+ 5$ points in motion understanding (Something-Something-v2), $+ 2$ points in action recognition (Kinetics-400), $+ 5$ points on action detection (AVA), $+ 1$ point on object recognition (ImageNet-1K), $+ 2$ points in scene recognition (Places205), and $+ 0 . 2$ points on finegrained recognition (iNaturalist). Moreover, when comparing pretraining wallclock time in Figure 5, we see that V-JEPA achieves this performance with a roughly 2 $\times$ speedup compared to the large pixel prediction models.

![](images/5.jpg)  
Figure 5 SSv2 frozen-evaluation performance vs. Pretraining Time. Wallclock times for all methods are measured on a single GPU with a batch size of 10 clips, using the official codebases for VideoMAE and VideoMAEv2, and linearly extrapolated assuming a global batch size of 2400 samples. However, note that the SSv2 accuracies of video pixel prediction methods are actually obtained with small batch sizes and significantly longer training schedules. V-JEPA outperforms pixel-reconstruction methods while training significantly faster.

Comparison with image models. On tasks that require a fine-grained understanding of motion (SomethingSomething-v2), the V-JEPA models provide a major improvement (over +21 points) compared to large-scale image baselines, such as DINOv2, OpenCLIP, and IJEPA. Self-supervised pretraining from videos allows to model dynamic concepts that are not easily learned from static image datasets. Similarly, we observe that the V-JEPA models outperform image-based pretraining on action localization. On Kinetics-400, we find image models to perform well; e.g., while DINOv2 (Oquab et al., 2023) previously reported $7 8 . 4 \%$ on K400 with a linear probe, we improve the frozen evaluation of the g/14 model to $8 3 . 4 \%$ by using an attentive probe. In this case, our H/16 model achieves 82.0% top-1 accuracy. It is worth noting that the label for many Kinetics videos can be inferred using appearance-based cues, without requiring an understanding of motion (Sevilla-Lara et al., 2021). The V-JEPA models narrow the gap with image models on image classification tasks. In particular, V-JEPA achieves a score of $7 7 . 4 \%$ on ImageNet using a onelayer attentive probe, which can be further improved to $\mathbf { 7 7 . 9 \% }$ using a two-layer attentive probe. More generally, we hypothesize that the datasets used to train V-JEPA and other video models are too constrained and lack the visual diversity of the internet-scale pretraining data used by the images models; as such, there is value in focusing future work on building diverse publicly available video datasets. TabeLowShot rozvaaio.parinVJEPthervidomode nozevaluatonninei-400 probe. We train the probes in several low-shot settings: using either $5 \%$ of the train set, $1 0 \%$ , or $5 0 \%$ , and take 3 random te0v be between V-JEPA and the baselines.   

<table><tr><td></td><td>K400</td><td colspan="6">Frozen Evaluation</td></tr><tr><td></td><td colspan="5">(16×8×3)</td><td colspan="3">SSv2 (16×2× 3)</td></tr><tr><td>Method</td><td>Arch.</td><td>5% (∼29 samples per class)</td><td>10% (∼58 samples per class)</td><td>50% (∼287 samples per class)</td><td>5% (∼48 samples per class)</td><td>10% (∼96 samples per class)</td><td>50% (~440 samples per class)</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>MVD VideoMAE</td><td>ViT-L/16 ViT-H/16</td><td>62.6 ± 0.2 62.3 ± 0.3</td><td>68.3 ± 0.2 68.5 ± 0.2</td><td>77.2 ± 0.3 78.2 ± 0.1</td><td>42.9 ± 0.8 41.4 ± 0.8</td><td>49.5 ± 0.6 48.1 ± 0.2</td><td>61.0 ± 0.2 60.5 ± 0.4</td></tr><tr><td>VideoMAEv2</td><td>ViT-g/14</td><td>37.0 ± 0.3</td><td>48.8 ± 0.4</td><td>67.8 ± 0.1</td><td>28.0 ± 1.0</td><td>37.3 ± 0.3</td><td>54.0 ± 0.3</td></tr><tr><td>V-JEPA</td><td>ViT-H/16</td><td>67.0 ± 0.2</td><td>72.1 ± 0.1</td><td>80.2 ± 0.2</td><td>51.9 ± 0.3</td><td>57.5 ± 0.4</td><td>67.3 ± 0.2</td></tr><tr><td></td><td>ViT-H/16384</td><td>68.2 ± 0.2</td><td>72.8 ± 0.2</td><td>80.6 ± 0.2</td><td>54.0 ± 0.2</td><td>59.3 ± 0.5</td><td>67.9 ± 0.2</td></tr></table>

# 5.3 Label-efficiency

We examine the label-efficiency of V-JEPA compared to other self-supervised video models by measuring the ability of the pretrained backbones to adapt to downstream tasks with few labels. Specifically, we investigate the performance of the frozen models on Kinetics-400 and Something-Something-v2 as we vary the percentage of labeled examples from each dataset available for training the attentive probe. We train the probes in several lowshot settings: using either $5 \%$ of the train set, $1 0 \%$ , or $5 0 \%$ , and take 3 random splits in each setting to obtain more robust metrics, resulting in 9 different evaluation experiments for each model. Table 7 reports the mean performances and standard deviation using the K400 and SSv2 validation sets.

We find V-JEPA to be more label-efficient than other self-supervised video models: decreasing the available number of labeled examples for training the attentive probe results in an increase in the performance gap between V-JEPA and the other models. In particular, the performance of the largest V-JEPA model on K400 drops by $1 2 \%$ to $6 8 . 2 \%$ top-1 when we reduce the number of labeled examples by a factor of $1 0 \times$ (from roughly 287 examples per class to 29 examples per class). By contrast, VideoMAEv2 drops by $3 0 \%$ to $3 7 . 0 \%$ top-1, VideoMAE drops by $1 5 . 9 \%$ to $6 2 . 3 \%$ top-1, and MVD drops by $1 4 . 6 \%$ to $6 2 . 6 \%$ top-1. Similar observations hold on SSv2. The performance of the largest V-JEPA model on SSv2 drops by $1 3 . 9 \%$ to $5 4 . 0 \%$ top-1 when we reduce the number of labeled examples by a factor of $1 0 \times$ (from roughly 440 examples per class to 48 examples per class). By contrast, VideoMAEv2 drops by $2 6 \%$ to $2 8 . 0 \%$ top-1, VideoMAE drops by $1 9 . 1 \%$ to $4 1 . 4 \%$ top-1, and MVD drops by $1 8 . 1 \%$ to $4 2 . 9 \%$ top-1.

# 6 Evaluating the Predictor

Next, we seek to qualitatively inspect the V-JEPA models. Recall that the predictor network in V-JEPA predicts the representations of a masked spatio-temporal region $y$ from a visible region $x$ , given the positional information of the masked regions (see Section 3). To qualitatively investigate the grounding of the feature-space predictions, we freeze the pretrained encoder and predictor networks and train a conditional diffusion decoder to map the V-JEPA predictions to interpretable pixels. Notably, the decoder is only fed the representations predicted for the missing regions of the video, and does not have access to the unmasked regions of the video (see Figure 6a). Given a masked video, we use the V-JEPA pretrained models to predict the representations of the missing regions, and then use the decoder to project the representations to pixel space. Figure 6b shows decoder outputs for various random seeds. Qualities that are common across samples represent information that is contained in the predictor representation. Figure 6b shows that the V-JEPA feature predictions are indeed grounded, and exhibit spatio-temporal consistency with the unmasked regions of the video. Specifically, the samples in Figure 6b show that the V-JEPA predictor correctly captures positional uncertainty and produces a variety of visual objects at various locations with consistent motion. Some of the samples also demonstrate an understanding of object-permanence, as the visual objects remain consistent after partial occlusion.

# Frozen

![](images/6.jpg)

)Vzy z of the video.

![](images/7.jpg)

iszatisrsRowMaski use putheVJEPAmodel  ra T-/c cnd rork.OhBo nv mpy oh t video. The predictions also capture consistent motion through time. Figure 6 Qualitative Analysis. Offine visualizations of the V-JEPA feature-space predictions.

# 7 Conclusion

In this work, we explored the effectiveness of feature prediction as a stand-alone objective for unsupervised learning from video and introduced V-JEPA, a collection of vision models trained solely using a self-supervised feature prediction objective. The V-JEPA models demonstrate the ability to solve various downstream image and video tasks without adaption of the model parameters, and outperform previous video representation learning approaches in frozen evaluation on action recognition, spatio-temporal action detection, and image classification tasks. Additionally, we show that pretraining VJEPA on videos is particularly effective for solving downstream tasks requiring fine-grained motion understanding, while large-scale image models trained on internet scale datasets fall short on such tasks. Finally, we empirically observed that V-JEPA models are label-efficient learners, and exhibit good performance on downstream tasks, even when only few labeled examples are available.

# References

Hassan Akbari, Liangzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin Cui, and Boqing Gong. Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text. Advances in Neural Information Processing Systems, 34:2420624221, 2021. Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lucic, and Cordelia Schmid. Vivit: A video vision transformer. In Proceedings of the IEEE international conference on computer vision, 2021. Mahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Florian Bordes, Pascal Vincent, Armand Joulin, Michael Rabbat, and Nicolas Ballas. Masked siamese networks for label-efficient learning. arXiv preprint arXiv:2204.07141, 2022. Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann LeCun, and Nicolas Ballas. Self-supervised learning from images with a joint-embedding predictive architecture. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1561915629, 2023. Alexei Baevski, Arun Babu, Wei-Ning Hsu, and Michael Auli. Efficient self-supervised learning with contextualized target representations for vision, speech and language. arXiv preprint arXiv:2212.07525, 2022a. Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, and Michael Auli. Data2vec: A general framework for self-supervised learning in speech, vision and language. arXiv preprint arXiv:2202.03555, 2022b. Hangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv preprint arXiv:2106.08254, 2021. Pietro Berkes and Laurenz Wiskott. Slow feature analysis yields a rich repertoire of complex cell properties. Journal of vision, 5(6):99, 2005. Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. arXiv preprint arXiv:2006.09882, 2020. Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jé- gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. arXiv preprint arXiv:2104.14294, 2021. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. preprint arXiv:2002.05709, 2020. Xiaokang Chen, Mingyu Ding, Xiaodi Wang, Ying Xin, Shentong Mo, Yunhao Wang, Shumin Han, Ping Luo, Gang Zeng, and Jingdong Wang. Context autoencoder for self-supervised representation learning. arXiv preprint arXiv:2202.03026, 2022. Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. arXiv preprint arXiv:2104.02057, 2021. Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive language-image learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 28182829, 2023. Ekin Dogus Cubuk, Barret Zoph, Vijay Mane, Dandelion andVasudevan, and Quoc V. Le. Autoaugment: Learning augmentation policies from data. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. Christoph Feichtenhofer, Haoqi Fan, Bo Xiong, Ross Girshick, and Kaiming He. A large-scale study on unsupervised spatiotemporal representation learning. Proceedings of the IEEE conference on computer vision and pattern recognition, 2021. Christoph Feichtenhofer, Yanghao Li, Kaiming He, et al. Masked autoencoders as spatiotemporal learners. Advances in neural information processing systems, 35:3594635958, 2022. David J Field. What is the goal of sensory coding? Neural computation, 6(4):559601, 1994. Spyros Gidaris, Andrei Bursuc, Nikos Komodakis, Patrick Pérez, and Matthieu Cord. Learning representations by predicting bags of visual words. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 69286938, 2020. Rohit Girdhar and Kristen Grauman. Anticipative video transformer. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1350513515, 2021. Rohit Girdhar, Alaaeldin El-Nouby, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. Omnimae: Single model masked pretraining on images and videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1040610417, 2023. Ross Goroshin, Joan Bruna, Jonathan Tompson, David Eigen, and Yann LeCun. Unsupervised learning of spatiotemporally coherent metrics. In Proceedings of the IEEE international conference on computer vision, pages 40864093, 2015. Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The" something something" video database for learning and evaluating visual common sense. In Proceedings of the IEEE international conference on computer vision, pages 58425850, 2017. Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent: A new approach to self-supervised learning. arXiv preprint arXiv:2006.07733, 2020. Chunhui Gu, Chen Sun, David A Ross, Carl Vondrick, Caroline Pantofaru, Yeqing Li, Sudheendra Vijayanarasimhan, George Toderici, Susanna Ricco, Rahul Sukthankar, et al. Ava: A video dataset of spatio-temporally localized atomic visual actions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 60476056, 2018. Agrim Gupta, Jiajun Wu, Jia Deng, and Li Fei-Fei. Siamese masked autoencoders. arXiv preprint arXiv:2305.14344, 2023. Michael U Gutmann and Aapo Hyvärinen. Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics. Journal of machine learning research, 13(2), 2012. Tengda Han, Weidi Xie, and Andrew Zisserman. Video representation learning by dense predictive coding. In Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops, pages 00, 2019. Tengda Han, Weidi Xie, and Andrew Zisserman. Memoryaugmented dense predictive coding for video representation learning. In European conference on computer vision, pages 312329. Springer, 2020. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are scalable vision learners. arXiv preprint arXiv:2111.06377, 2021. Geoffrey E Hinton. Connectionist learning procedures. In Machine learning, pages 555610. Elsevier, 1989. Tarun Kalluri, Deepak Pathak, Manmohan Chandraker, and Du Tran. Flavr: Flow-agnostic video representations for fast frame interpolation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 20712082, 2023. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv preprint arXiv:1705.06950, 2017. Christoph Kayser, Wolfgang Einhäuser, Olaf Dümmer, Peter König, and Konrad Körding. Extracting slow subspaces from natural videos leads to complex cells. In Artificial Neural Networks—ICANN 2001: International Conference Vienna, Austria, August 2125, 2001 Proceedings 11, pages 10751080. Springer, 2001. Gustav Larsson, Michael Maire, and Gregory Shakhnarovich. Learning representations for automatic colorization. 2016. Gustav Larsson, Michael Maire, and Gregory Shakhnarovich. Colorization as a proxy task for visual understanding. 2017. Yann LeCun. A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. 2022. Hsin-Ying Lee, Jia-Bin Huang, Maneesh Singh, and MingHsuan Yang. Unsupervised representation learning by sorting sequences. In Proceedings of the IEEE international conference on computer vision, pages 667676, 2017. Kunchang Li, Yali Wang, Peng Gao, Guanglu Song, Yu Liu, Hongsheng Li, and Yu Qiao. Uniformer: Unified transformer for efficient spatiotemporal representation learning. arXiv preprint arXiv:2201.04676, 2022. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. Howto100m: Learning a text-video embedding by watching hundred million narrated video clips. In Proceedings of the IEEE/CVF international conference on computer vision, pages 26302640, 2019. Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles. In European conference on computer vision, pages 6984. Springer, 2016. Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. Nikhil Parthasarathy, SM Eslami, João Carreira, and Olivier J Hénaff. Self-supervised video pretraining yields strong image representations. arXiv preprint arXiv:2210.06433, 2022. Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. Context encoders: Feature learning by inpainting. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 25362544, 2016. Silvia L Pintea, Jan C van Gemert, and Arnold WM Smeulders. Déja vu: Motion prediction in static images. In Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part III 13, pages 172187. Springer, 2014. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. Rajesh PN Rao and Dana H Ballard. Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects. Nature neuroscience, 2(1):7987, 1999. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115(3): 211252, 2015. Chaitanya Ryali, Yuan-Ting Hu, Daniel Bolya, Chen Wei, Haoqi Fan, Po-Yao Huang, Vaibhav Aggarwal, Arkabandhu Chowdhury, Omid Poursaeed, Judy Hoffman, et al. Hiera: A hierarchical vision transformer without the bellsand-whistles. arXiv preprint arXiv:2306.00989, 2023. Laura Sevilla-Lara, Shengxin Zha, Zhicheng Yan, Vedanuj Goswami, Matt Feiszli, and Lorenzo Torresani. Only time can tell: Discovering temporal data for temporal modeling. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 535544, 2021. Elizabeth S Spelke, Peter Vishton, and Claes Von Hofsten. Object perception, object-directed action, and physical knowledge in infancy. 1995. Nitish Srivastava, Elman Mansimov, and Ruslan Salakhudinov. Unsupervised learning of video representations using lstms. In International conference on machine learning, pages 843852. PMLR, 2015. Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid. Videobert: A joint model for video and language representation learning. In Proceedings of the IEEE/CVF international conference on computer vision, pages 74647473, 2019. Dídac Surís, Ruoshi Liu, and Carl Vondrick. Learning the predictability of the future. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1260712617, 2021. Reuben Tan, Matthias De Lange, Michael Iuzzolino, Bryan A Plummer, Kate Saenko, Karl Ridgeway, and Lorenzo Torresani. Multiscale video pretraining for long-term activity forecasting. arXiv preprint arXiv:2307.12854, 2023. Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. arXiv preprint arXiv:1703.01780, 2017. Yuandong Tian, Xinlei Chen, and Surya Ganguli. Understanding self-supervised learning dynamics without contrastive pairs. In International Conference on Machine Learning, pages 1026810278. PMLR, 2021. Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training. Advances in neural information processing systems, 35:1007810093, 2022. Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classification and detection dataset. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 87698778, 2018. Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and PierreAntoine Manzagol. Extracting and composing robust features with denoising autoencoders. In Proceedings of the 25th International Conference on Machine Learning, ICML '08, page 10961103, 2008. Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, Pierre-Antoine Manzagol, and Léon Bottou. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. Journal of machine learning research, 11(12), 2010. Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Anticipating visual representations from unlabeled video. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 98106, 2016. Fei Wang, Ping Li, and Arnd Christian Konig. Learning a bi-stochastic data similarity matrix. In 2010 IEEE International Conference on Data Mining, pages 551560. IEEE, 2010.

Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yinan He, Yi Wang, Yali Wang, and Yu Qiao. Videomae v2: Scaling video masked autoencoders with dual masking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1454914560, 2023a. Rui Wang, Dongdong Chen, Zuxuan Wu, Yinpeng Chen, Xiyang Dai, Mengchen Liu, Lu Yuan, and Yu-Gang Jiang. Masked video distillation: Rethinking masked feature modeling for self-supervised video representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 63126322, 2023b. Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun Wang, et al. Internvideo: General video foundation models via generative and discriminative learning. arXiv preprint arXiv:2212.03191, 2022. Laurenz Wiskott and Terrence J Sejnowski. Slow feature analysis: Unsupervised learning of invariances. Neural computation, 14(4):715770, 2002. Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-parametric instance discrimination. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 37333742, 2018. Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: A simple framework for masked image modeling. arXiv preprint arXiv:2111.09886, 2021. Dejing Xu, Jun Xiao, Zhou Zhao, Jian Shao, Di Xie, and Yueting Zhuang. Self-supervised spatiotemporal learning via video clip order prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1033410343, 2019. Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko, Armen Aghajanyan, Florian Metze, Luke Zettlemoyer, and Christoph Feichtenhofer. Videoclip: Contrastive pretraining for zero-shot video-text understanding. arXiv preprint arXiv:2109.14084, 2021. Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive captioners are image-text foundation models. arXiv preprint arXiv:2205.01917, 2022. Liangzhe Yuan, Nitesh Bharadwaj Gundavarapu, Long Zhao, Hao Zhou, Yin Cui, Lu Jiang, Xuan Yang, Menglin Jia, Tobias Weyand, Luke Friedman, et al. Videoglue: Video general understanding evaluation of foundation models. arXiv preprint arXiv:2307.03166, 2023. Rowan Zellers, Jiasen Lu, Ximing Lu, Youngjae Yu, Yanpeng Zhao, Mohammadreza Salehi, Aditya Kusupati, Jack Hessel, Ali Farhadi, and Yejin Choi. Merlot reserve: Neural script knowledge through vision and language and sound. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1637516387, 2022. Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba, and Aude Oliva. Learning deep features for scene recognition using places database. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 27. Curran Associates, Inc., 2014. https://proceedings.neurips.cc/paper/2014/file/ 3fe94a002317b5f9259f82690aeea4cd-Paper.pdf. Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. Ibot: Image bert pre-training with online tokenizer. arXiv preprint arXiv:2111.07832, 2021. Will Zou, Shenghuo Zhu, Kai Yu, and Andrew Ng. Deep learning of invariant features via simulated fixations in video. Advances in neural information processing systems, 25, 2012.

# Appendix

# A Extended Related Works e rst evpes rlari isal eption om statiage boisusi atgor rom video.

# Weakly-Supervised Learning from Static Images

One family  appraches orlearning visual perception fromstaticmages trains  visualencoder to predict the reprnatins text aptinste upanyafomheWe,  LIP(Raor 02 C (Yu t al., 2022). The largest open source LIP model to date, numbering 2B parameters and trainedn over 2B web-scrapeimages (Chert al 023) demonstrate ipressive performance n a widerange ownsrem iniotasksotabyh sievusihegh-weaptatiskspeche referre s oze-evaluatin,an ds ot reqiexpensiventoene-tuning  the prtainmoe.

# Self-Supervised Learning from Static Images

Ohc staavesjcvInl ork eu aas aia exsk u 1, rotatio prediction (Gidaris e al, 2020), and jgsaws (Norozi and Favaro, 2016). More recent appraches leverage ivnbecivaiiscervnthanraansW et al., 2018; Chen et al., 2020). Anormi meho lreretai usisuce Vicnl 08); popur sti  thi e (athak l01 Mont maskersHe train an encoder-decoder transormerto predict missing pixe famaskeimage.Followup work adresses the indeeri peetruction  explornsantiaion maskmaemodelinlatensa (Bk et al 2022b; Assran et al, 2023; Baevski et al., 2022a).These approaches can be seen as applications  the predictive feature principle in the image modality. Thereare alsovarius methods that combine bothmaske image modeing an invariancecriter tolearvisual repreentations from statiages, such as iBOT (Zhou e al 2021) and DINOv2 Zhou et al, 2021; Oquabl, hiv to a model with over 1.1B parameters trained on a curated dataset of 142M images.

# Weakly-Supervised Learning from Videos

Onfamipoisu petios wakl-eio captioning, often computed from an ASRtranscription of audio data accompanying internet videos. For instance, ViBERT Su 01; Xu al 021)traivier  preic mask spans  hetextualc captions.Similarly, VideoCLIP (Xu et al, 2021) trains a video encoder to predict the representation of video capns cputed by text ncoder. Folow-up work such as MERLOT Zeers et al 2022), VATT (Akbarl, an InternVidWang 0extendeVidCLIP yncoratidiialnsupervisejeive.

# Self-Supervised Learning from Videos

Similar ounsupervis larningfrommage,  family  unsupervisvidoreprentationlearng ppe enforces a spatio-temporal representation of a video clip to be invariant to hand-crafted spatio-temporaldata aumetations (Parthasarathy  al 02 However, nebvious insights that the temporal rderinsal i sh onunsupervised video learning.Toward leveraging temporalinformation as supervision, some approaches train a visl encoder by predictin thetemporalrderin f frames (Xu al, 2019; Lee  al 017 Othe ppres se  c v til l 0  ei in viams ushamenteolati jciv (Kall )enstnr To et al., 2022; Feichtenhofer et al., 2022; Wang et al., 2023a).

# B Extended Description of V-JEPA

In this section, we provide an in-depth description of our approach V-JEPA that is illustrated in Figure 3.

Inpu.Unls state herwise, duriduri pretraining we lways randomly sample a cip  16frame from ipu wioal sritwmpamepuidoheeover intoalouy   ivvi at3a peseconWetheresz heid dimensions to $2 2 4 \times 2 2 4$ , resulting in an overall shape of $1 6 \times 2 2 4 \times 2 2 4 \times 3$ for the entire clip. Since ViT networks p   toes v  uvo   tk T  e 3D convolution comprising $d$ filters of size $2 \times 1 6 \times 1 6$ with a temporal stride of 2 and a spatial stride of 16, resulting in a tensor of shape $8 \times 1 4 \times 1 4 \times d$ . Next we add absolute 3D sin-cos positional embeddings to the spatio-temporal feature map and fatten it, resulting in a 1D token sequence of shape $1 5 6 8 \times d$ . This process is demonstrated in Figure 7.

![](images/8.jpg)  
JAenese $1 6 \times 2 2 4 \times 2 2 4 \times 3$ into a 1D token sequence, we apply a 3D convolution comprising $d$ filters of size $2 \times 1 6 \times 1 6$ with a temporal stride of 2 and a spatial stride of 16, resulting in a tensor of shape $8 \times 1 4 \times 1 4 \times d$ . Next we add absolute 3D sin-cos positional embeddings to the spatio-temporal feature map and flatten it, resulting in a 1D token sequence of shape $1 5 6 8 \times d$ .

V-JEPA.We sample bot a video cp, and a video mask in each iteration We denotea video p repreented as a 1D token sequence of length $L = 1 5 6 8$ by $x _ { L } = ( x _ { 1 } , \dots , x _ { L } )$ . Similarly, given a mask of $M < L$ patches, leaving $N = L - M$ patches unmasked, we denote the indices of masked patches by $( i _ { 1 } , \dots , i _ { M } )$ and its complement (the indices of unmasked patches) by $( j _ { 1 } , \dots , j _ { N } )$ . Computing the $x$ -representations. To compute the V-JEPA loss, we first produce the $x$ -representations by masking the video clip and feeding it into the $x$ -encoder; we denote the masked video by $x _ { N } = ( x _ { j _ { 1 } } , \dots , x _ { j _ { N } } )$ . Applying the $x$ . encoder $E _ { \theta } ( \cdot )$ to the masked clip gives a sequence of patch representations, denoted as $z _ { N } = E _ { \theta } ( x _ { N } ) = ( z _ { j _ { 1 } } , . . . , z _ { j _ { N } } )$ .

Predicting the target. Next, the V-JEPA predictor network $P _ { \phi } ( \cdot , \cdot )$ takes as input the tokens produced by the $x$ nr nd predicts he missiregions  the video cp, whicare specfed by  se of learable masktkens. Specifically, the mask tokens are parameterized as the sum of a shared learnable vector and an absolute 3D sin-cos positional embedding, denoted by $m _ { M } = ( m _ { i _ { 1 } } , \dots , m _ { i _ { M } } )$ . The output of the predictor is thus given by, $\hat { s } _ { M } = P _ { \phi } ( z _ { N } , m _ { M } ) = ( \hat { s } _ { i _ { 1 } } , \dots , \hat { s } _ { i _ { M } } )$ , corresponding to a $d$ -dimensional output for each of the $M$ masked patches. Computing the $y$ -representations. Finally to compute the prediction targets, the entire unmasked video clip is processed by the $y$ -encoder to obtain a set of target representations, denoted by $s _ { L } = \overline { { { E } } } _ { \theta } ( x _ { L } ) = ( s _ { 1 } , \dots , s _ { L } )$ The V-JEPA loss is now computed as which is simply the average $L _ { 1 }$ distance between the output of the predictor and the $y$ -encoder. We then compute a gradient update with respect to the parameters of the $x$ -encoder, $\theta$ , and the predictor, $\phi$ , and subsequently update the parameters of the $y$ -encoder as an exponential moving average of the context encoder weights (Polyak average).

$$
\mathrm { L o s s } = \frac { 1 } { M } \sum _ { k \in ( i _ { 1 } , \ldots , i _ { M } ) } \lVert \hat { s } _ { k } - s _ { k } \rVert _ { 1 } ,
$$

Table 8 pretraining hyper-parameters for V-JEPA.

<table><tr><td>Hyper-parameter</td><td>ViT-L/16224 ViT-H/16224</td><td></td><td>ViT-H/16384</td></tr><tr><td colspan="4">data</td></tr><tr><td>datasets</td><td>VideoMix2M VideoMix2M VideoMix2M</td><td></td><td></td></tr><tr><td>resolution</td><td>224</td><td>224</td><td>384</td></tr><tr><td>num frames</td><td>16</td><td>16</td><td>16</td></tr><tr><td>temporal_stride</td><td>4</td><td>4</td><td>4</td></tr><tr><td>horizontaI_flip</td><td>true</td><td>true</td><td>true</td></tr><tr><td>random_resize _scale</td><td>(0.3, 1.0)</td><td>(0.3, 1.0)</td><td>(0.3, 1.0)</td></tr><tr><td>random resize aspect _ratio</td><td>(0.75, 1.35)</td><td>(0.75, 1.35)</td><td>(0.75, 1.35)</td></tr><tr><td colspan="4">masking</td></tr><tr><td>block_aspect _ratio</td><td>(0.75, 1.5)</td><td>(0.75, 1.5)</td><td>(0.75, 1.5)</td></tr><tr><td>shortrange_mask _num_blocks</td><td>8</td><td>8</td><td>8</td></tr><tr><td>shortrange ∞ mask spatial_scale</td><td>0.15</td><td>0.15</td><td>0.15</td></tr><tr><td>longrange mask _num_blocks</td><td>2</td><td>2</td><td>2</td></tr><tr><td>longrange mask_spatial_scale</td><td>0.7</td><td>0.7</td><td>0.7</td></tr><tr><td colspan="4">optimization</td></tr><tr><td>batch_size</td><td>3072</td><td>3072</td><td>2400</td></tr><tr><td>total ¯number of iterations</td><td>90000</td><td>90000</td><td>90000</td></tr><tr><td>warmup_iterations</td><td>12000</td><td>12000</td><td>12000</td></tr><tr><td>br</td><td>6.25e-4</td><td>6.25×10−4</td><td>6.25×10−4</td></tr><tr><td>start lr</td><td>2×10−4</td><td>2×10-4</td><td>2×10−4</td></tr><tr><td>final_lr</td><td>1×10-6</td><td>1×10-6</td><td>1×10−6</td></tr><tr><td>start momentum</td><td>0.998</td><td>0.998</td><td>0.998</td></tr><tr><td>final momentum</td><td>1.0</td><td>1.0</td><td>1.0</td></tr><tr><td>start_weight _decay</td><td>0.04</td><td>0.04</td><td>0.04</td></tr><tr><td>final_weight_decay</td><td>0.4</td><td>0.4</td><td>0.4</td></tr><tr><td>scheduler_scale _factor</td><td>1.25</td><td>1.25</td><td>1.25</td></tr><tr><td colspan="4">architecture</td></tr><tr><td>patch _size</td><td>16</td><td>16</td><td>16</td></tr><tr><td>tubelet _size</td><td>2</td><td>2</td><td>2</td></tr><tr><td>pred_depth</td><td>12</td><td>12</td><td>12</td></tr><tr><td>pred_embed_dim</td><td>384</td><td>384</td><td>384</td></tr><tr><td colspan="4">hardware</td></tr><tr><td>dtype</td><td>bfloat16 A100 80G</td><td>bfloat16 A100 80G</td><td>bfloat16 A100 80G</td></tr><tr><td>accelerator</td><td></td><td></td><td></td></tr></table>

Multi-Mask Prediction.To increase the efficiency of V-JEPA, we use a multi-masking strategy (Caron  al, 2020; Baevski et al., 2022a), which enables us to amortize the cost of the target computation.As mentionedin Sc   iv video  wep  ft mask, rtagend -angeWhil  e propagate the $x$ -encoder and predictor separately for each mask, we only need to compute the $y$ -representation once.

# C Pretraining details n section, we report V-JEPA pretraining details. Table 8 summarizes the main hyperparameters used during pretraining.

Architectures. We use Vision Transformer (Dosovitskiy et al., 2020) (ViT) architectures for the $x$ -encoder and $y$ -encder. We train three V-JEPA encoders: a ViT-L/16224, a ViT-H/16224 and a ViT-H/16384. Allthree encders korl The 224 and 384, indicate the spatial resolution f the video clp.-JEPAfattes the vido cip into sequ non-overlapping spatio-temporal patches of size $1 6 \times 1 6 \times 2$ (see Figure 7). For all three models, the predictor is designed as a narrow ViT architecture, consistin of 12 transformer blocks wit anmbedding dimension of 38.For simpliciy wekeep the number  selattentin head  the predicr equal tothat thebackone use  the context-encoder/target-encoder. V-JEPA is pretrained without using a [cls] token. Optimization. We use AdamW (Loshchilov and Hutter, 2017) to optimize the $x$ -encoder and predictor weights. The ViT-L/ $1 6 _ { 2 2 4 }$ and ViT-H/16224 models use a batch size of 3072 while the ViT-H/16384 uses a batch size of 2400. Models are trained for a total of 90,000 iterations. The learning rate is linearly increased from $2 \times 1 0 ^ { - 4 }$ to $6 . 2 5 \times 1 0 ^ { - 4 }$ during the first $1 2 , 0 0 0$ iterations of pretraining, and decayed to $1 0 ^ { - 6 }$ following a cosine schedule. Table 9 Frozen Evaluation hyper-parameters.   

<table><tr><td>Hyper-parameter</td><td>K400</td><td>SSv2</td><td>IN1K</td><td>Place205</td><td>iNat21</td></tr><tr><td>data</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>clips num</td><td>8</td><td>1</td><td>N.A.</td><td>N.A.</td><td>N.A.</td></tr><tr><td>num frames</td><td>16</td><td>16</td><td>N.A.</td><td>N.A.</td><td>N.A.</td></tr><tr><td>temporal_stride</td><td>4</td><td>4</td><td>N.A.</td><td>N.A.</td><td>N.A.</td></tr><tr><td>horizontal_flip random resize scale</td><td>true (0.08, 1.0)</td><td>true (0.08, 1.0)</td><td>true (0.08, 1.0)</td><td>true (0.08, 1.0)</td><td>true (0.08, 1.0)</td></tr><tr><td>random resize auto augment</td><td>false</td><td>false</td><td>_aspect_ratio (0.75, 1.33) (0.75, 1.33) (0.75, 1.33) true</td><td>(0.75, 1.33) true</td><td>(0.75, 1.33)</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td>true</td></tr><tr><td>optimization</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>batch _size</td><td>256</td><td>256</td><td>1024</td><td>1024</td><td>1024</td></tr><tr><td></td><td>20</td><td>20</td><td>20</td><td>20</td><td>20</td></tr><tr><td>epochs</td><td>1e-3</td><td>1e-3</td><td>1e-3</td><td>1e-3</td><td></td></tr><tr><td>lr</td><td></td><td></td><td></td><td></td><td>1e-3</td></tr><tr><td>final lr weight_decay</td><td>0 0.01</td><td>0 0.01</td><td>0 0.01</td><td>0 0.01</td><td>0 0.01</td></tr></table>

Weight-decay is also linearly increased from 0.04 to 0.4 throughout pretraining. The $y$ -encoder weights are initialized identically to the $x$ -encoder, and subsequently updated as an exponential moving average (EMA) (Tarvainen and Valpola, 2017) of the $x$ -encoder weights using a momentum value which starts at 0.998 and is linearly increased to 1.0 during training (Caron et al., 2021; Assran et al., 2022). We scale all hyper-parameter schedules $2 5 \%$ beyond the actualtraini schedule.Specificaly the learning rate chedule, wight-decay schedule, and EMA sheule are computed assuming atraining length of112,500 iterations even though we nly train ur model for90,00 iterations. We found the last $2 5 \%$ of the default scheduler period to update hyper-parameters too aggressively, and simply truncating the schedulers improved performance. Masking. As described in Section 3, we propose a 3D Multi-Block masking strategy. We use two type of masks: shor-rangemasks, wherew takethenion  8 randomly sampledtaret blocks wia spatial scale 0.1an loragmaskshereakheionany mpareblocks wispatl sale .In cases, the aspect ratio for all sampled blocks is randomly chosen in the range (0.75, 1.5).

# D Evaluation details

# D.1 Frozen classification

Attentive Probing. Given an input video, $x _ { L }$ , the V-JEPA target encoder $\overline { { E } } _ { \theta } ( \cdot )$ outputs a sequence of $L$ tokens, $E _ { \theta } ( x _ { L } ) = ( s _ { 1 } , . . . , s _ { L } )$ , where $s _ { i } \in \mathbb { R } ^ { d }$ . To pool this sequence of tokens into a single feature vector, we apply a liwe o-iossattntblwhiclacheeattenieatirnorl cross attention. Specifically, the cross-attention performs the following computation:

$$
\sum _ { i = 1 } ^ { L } \frac { \exp (  { \boldsymbol { q } } ^ { \top } \mathbf { W } _ { \mathbf { k } } s _ { i } ) } { \sum _ { j } \exp (  { \boldsymbol { q } } ^ { \top } \mathbf { W } _ { \mathbf { k } } s _ { j } ) } \mathbf { W } _ { \mathbf { v } } s _ { i } ,
$$

where $\mathbf { W _ { k } } , \mathbf { W _ { v } } \in \mathbf { R ^ { d \times d } }$ are the key and value matrices, and $q \in R ^ { d }$ is a learnable query token. The output of the cr-attenion s then aded backthe quey k (reiuaceion, nd then intowo-ayer L GeL vaiLaeNordfasThe he bloc are jointly leared with that the near classierorthe ownstream task, while the encoder parameers ar kept oz Note that, i pratic wectualuentive probe wi1head, mnsion In Appendix $\mathrm { E }$ we show that baselines benefit from the attentive probing protocol. Optimization. For al the tasks, we use AdamW optimizer with a cosine scheduler (no warmup) that decays the learnng rate from 0.001 to 0. We use a fixed weight-decay of 0.01 and apply simple dataaugmentations (random resizcrops andhorizontal is)duritraiing  the ttentiv probe except  agetasks, wherewpy AutoAugment (Dogus Cubuk et al., 2019). Table 9 reports the hyperparameters for each downstream evaluation. Extension tmultpleclis.Unles state herwise,ourattentive probe takes 8 cps of 16frames s iu Kinetics, and 2 clips of 16 frames on Something-Somethingv2 to increase the temporal coverage of the videc Table 10 Frozen Detection hyper-parameters.   

<table><tr><td>Hyper-parameter</td><td>ViT-L/16</td><td>ViT-H/16</td></tr><tr><td>out layers</td><td>[18, 20, 22, 24] [26, 28, 30, 32]</td><td></td></tr><tr><td>batch size</td><td>64</td><td>64</td></tr><tr><td>epochs</td><td>30</td><td>30</td></tr><tr><td>opt</td><td>AdamW</td><td>AdamW</td></tr><tr><td>opt_eps</td><td>0.00000001</td><td>0.00000001</td></tr><tr><td>momentum</td><td>0.9</td><td>0.9</td></tr><tr><td>weight _decay</td><td>0.05</td><td>0.05</td></tr><tr><td>br</td><td>0.0001</td><td>0.0001</td></tr><tr><td>warmup_lr</td><td>0.000001</td><td>0.000001</td></tr><tr><td>min lr</td><td>0.000001</td><td>0.000001</td></tr><tr><td>warmup epochs</td><td>2</td><td>2</td></tr><tr><td>warmup steps</td><td>1</td><td>1</td></tr></table>

Specifically, we first divide a video in 8 (or 2) equal-length temporal segments, and sample 1 clip at random per segment. The video encoder $\overline { { E } } _ { \theta }$ processes each clip separately and produces a clip-level feature map. The feature maps for each clip are then concatenated together and fed to the attentive probe. At test time, we average the prediction of 3 spatial views following standard practice in video classification. Application of video models to images.To evaluate the video models on image tasks, we simply duplicate input imag to generate stivideo clips f 1frames.We perform this duplication operation simply r convenienin evaluation of the video models, however we find this step to be unnecessary in general.Given a vido tokenizer implemented s3-co wi temporal srid  i suent sipl uplicate heage int video cp.This would resul n the sameumber  input token  that produced by  tatimage model w a 2D-conv tokenizer. Application of image models to videos.To evaluate image models such as DINOv2 and OpenCLIP on video tasks, we simply process each frame independently with the image encoder to produce frame-level feature map. The fapsenhe r  ieve feature maps when evaluating video models.

# D.2 Frozen detection

WaA l rai nd lt ietsolloe pel poool Feto, and use precomputed masks from a pretrained Faster-RCNN adapted to videos, which uses a ResNeXt-101-FPN ban a to classify the extracted regions of interest and report mean Average Precision (mAP) on the 60 most common class. Hypepreerse provideTabeurozatur aine nnatihe o theransormerencoder i thretermediate layers.Weus batch siz 4and pretrain or 30 epocs wh AdamW using a learning rate of 0.0001 with 2 epochs of warmup and a weight decay of 0.05.

# D.3 Finetuning o Ton ( wenetuenear aye  oprmode usi ayr dey hend as the data augmentation pipeline. We provide all hyper-parameters for both K400 and SSv2 in Table 11.

# E Extra Results

# E.1 Frozen Evaluation.

Linear vs.Attentive probe Table 12 shows that V-JEPA and VideoMAE benefit from using a non-inear attentive probe and multiple clips on the K400 and SSv2 downstream tasks. Additionally, Table 13 shows that attentive probing leads to better performance on average or DINOv2 and OpenCLIP models. Since atentive probing and multiclips eval iproves the perormanceof all models, we uset as our default protocol infrozen evaluation. Table 11 Finetuning Evaluation hyper-parameters.   

<table><tr><td>Hyper-parameter</td><td colspan="4">K400</td></tr><tr><td>data</td><td colspan="4"></td></tr><tr><td>num segments</td><td colspan="4">1 16</td></tr><tr><td>num frames</td><td colspan="4"></td></tr><tr><td>sampling_rate</td><td colspan="4">4</td></tr><tr><td>resolution</td><td colspan="4">224</td></tr><tr><td>model</td><td></td><td></td><td></td><td></td></tr><tr><td>model_name</td><td colspan="4">ViT-L/16 ViT-H/16</td></tr><tr><td>drop_path</td><td>0.1</td><td>0.2</td><td>ViT-L/16 ViT-H/16 0.2</td><td>0.2</td></tr><tr><td>head_drop</td><td>0.</td><td>0.</td><td>0.5</td><td>0.5</td></tr><tr><td>_rate</td><td></td><td></td><td></td><td></td></tr><tr><td>optimization</td><td></td><td></td><td></td><td>256</td></tr><tr><td>batch_size</td><td>256 35</td><td>1024 25</td><td>256</td><td>15</td></tr><tr><td>epochs</td><td></td><td></td><td>15</td><td></td></tr><tr><td>opt</td><td colspan="4">adamw</td></tr><tr><td>opt_eps</td><td colspan="4">0.00000001 0.9</td></tr><tr><td>momentum</td><td colspan="4"></td></tr><tr><td>weight _decay</td><td></td><td>0.05</td><td></td><td></td></tr><tr><td>lr</td><td>0.002</td><td>0.0005</td><td>0.0005</td><td>0.0005</td></tr><tr><td>layer _decay</td><td>0.75</td><td>0.75</td><td>0.75</td><td>0.75 1e-6</td></tr><tr><td>warmup_lr</td><td>1e-6</td><td>1e-8</td><td>1e-6</td><td></td></tr><tr><td>min_lr</td><td>1e-6</td><td>1e-5</td><td>1.5e-4</td><td>1.5e-3</td></tr><tr><td>warmup_epochs</td><td></td><td>5</td><td></td><td></td></tr><tr><td>augmentations</td><td></td><td></td><td></td><td></td></tr><tr><td>color_jitter horizontal_flip</td><td>True</td><td>0.4</td><td></td><td>False</td></tr><tr><td></td><td></td><td>True</td><td>False</td><td></td></tr><tr><td>num_sample</td><td></td><td>2</td><td></td><td></td></tr><tr><td>aa</td><td></td><td>rand-m7-n4-mstd0.5-inc1</td><td></td><td></td></tr><tr><td>smoothing</td><td></td><td>0.1</td><td></td><td></td></tr><tr><td>train</td><td></td><td>bicubic</td><td></td><td></td></tr><tr><td>interpolation</td><td></td><td></td><td></td><td>2</td></tr><tr><td>test_ num _segment crop</td><td>5 3</td><td>5</td><td>2</td><td></td></tr><tr><td>test num</td><td></td><td>3</td><td>3</td><td>3</td></tr><tr><td>erase</td><td></td><td></td><td></td><td></td></tr><tr><td>prob mode</td><td></td><td>0.25</td><td></td><td></td></tr><tr><td>count</td><td></td><td>pixel</td><td></td><td></td></tr><tr><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>split</td><td></td><td>False</td><td></td><td></td></tr><tr><td>mixup</td><td></td><td></td><td></td><td></td></tr><tr><td>mixup</td><td></td><td></td><td></td><td></td></tr><tr><td>cutmix</td><td></td><td></td><td></td><td></td></tr><tr><td>mixup_prob</td><td></td><td></td><td>1.0</td><td></td></tr><tr><td></td><td></td><td></td><td>1.0 0.5</td><td></td></tr><tr><td>mixup_switch_prob</td><td></td><td></td><td></td><td></td></tr><tr><td>mixup _mode</td><td></td><td>batch</td><td></td><td></td></tr></table>

Tab  inear Attentiv robvaluatio or-JEan VideoMA.Weevalahe efr i and attentive (Att.) probing when adapting V-JEPA to the K400 ( $1 6 \times 5 \times 3 .$ and SSv2( $1 6 \times 2 \times 2$ tasks. V-JEPA and VideoMAE benefit from using a non-linear attentive probe.

<table><tr><td rowspan="2">Method Arch.</td><td rowspan="2"></td><td colspan="2">K400</td><td colspan="2">SSv2</td></tr><tr><td>Lin.</td><td>Att.</td><td>Lin.</td><td>Att.</td></tr><tr><td>VideoMAE</td><td>ViT-L/16</td><td>52.5</td><td>77.8</td><td>41.3</td><td>61.2</td></tr><tr><td>V-JEPA</td><td>ViT-L/16</td><td>56.7</td><td>80.8</td><td>50.1</td><td>69.5</td></tr></table>

Tabinearttentiv robevaluaion orINOv2 n OpenCLP.Weevaluatheecnri iv iv strategy. Results shown ingrayare reported from the linear probe evaluation in Oquab et al. (2023).

<table><tr><td></td><td></td><td colspan="2">K400</td><td colspan="2">SSv2</td><td colspan="2">IN1K</td><td colspan="2">Place205</td><td colspan="2">iNat21</td></tr><tr><td>Method</td><td>Arch.</td><td>Lin.</td><td>Att.</td><td>Lin.</td><td>Att.</td><td>Lin.</td><td>Att.</td><td>Lin.</td><td>Att.</td><td>Lin.</td><td>Att.</td></tr><tr><td>DINOv2</td><td>ViT-g/14</td><td>78.4</td><td>83.4</td><td>38.3</td><td>50.0</td><td>86.5</td><td>86.2</td><td>67.5</td><td>68.4</td><td>85.7</td><td>88.8</td></tr><tr><td>OpenCLIP</td><td>ViT-G/14</td><td>78.3</td><td>81.8</td><td>35.8</td><td>34.8</td><td>86.2</td><td>85.3</td><td>69.8</td><td>70.2</td><td>76.0</td><td>83.6</td></tr></table>

OCi MuplsWeehepa anheoalcovemor evaluation n K400action classification. In Table 1, we evaluate VideoMAE and VJEPA models using anatteniv proe wia the the atue maprnd mpefomhevidrthe tenat map o 8 clips randomly sampled from the video.To sample 8 clips from a video, we first divide thevideo into8 equal length temporal segments, and sample 1 clip at random from each segment. A single clip corresponds to $\approx 2$ seconds of a video on average, while 8 clips correspond to $\approx 1 6$ seconds. The video encoders processes each clip separatelyproduc-evl atue map whic rethen onatenateat heput theentive pbe Incrasing the temporal coverage from 1 clip per video to 8 clips improves the performance of both V-JEPA and VideoMAE on K400 action classification. We therefore use the multiclip attentive probing setup as our default evaluation pipeline.

# E.2 Finetuning

In Table 15, we evaluate V-JEPA using finetuning (separately) on K400 and SSv2. We compare V-JEPA with VidoMAEv2 (Wang e al., 2023a), VideoMAE (Tong etal., 2022) and MVD (Wang et al., 2023b) using a ViT-L/16 o  ViT-/16 architecture.V-JEPA obtains competitive performance using afnetuning protocol With a ViTiH/16 architecture, V-JEPA outperforms by 1.2% VideoMAE and $+ 0 . 3 \%$ VideoMAEv2 on the SSv2 dataset, while obtaining comparable performance on K400. V-JEPA also obtains performance similar to MVD on the SSv2 dataset. The MVD model achieves the best performance across models on the K400 dataset, and is trained using the image dataset ImageNet1K, i contrast to theother methods in the table which only use videodataAdditionally MVD reqes the procesing  snicanty more sample duri pretrainigdue to the cost  training the tacer encoder networks in a pre-pre-training step.

# E.3 Sample Efficiency of pretraining

We compare the sample eficiency  pretraini varius stateo-the-art mageand videomodels.Specifically, we look at the number  samples imager video cips) processe by the network durig pretraini, whic iarer than the iz  the preri ataset r uli-epoc tainiNotablyur sults with VJEPA area while processing a rder magnitude ewer samples han previous method, and notably twoorders magnitude fewer samples than OpenCLIP. We believe that further investment towards improving the vido pretraining data distribution could lead to substantial gains in downstream image and video tasks.

# E.4 Masking Strategy

An iportant component of the V-JEPA pretraini strategy is the 3D clip masking strategy. In this sectin, we detail 26 ablation experients exploring dfferent masks. Forll the experiments, we pretrain aViT-B/16 preaie on K400. Figure 8 presents a summary of those results. Fgureshows the efec f changing the spatial and temporal masking ratiFigure  ablates the nube sampled blocks used to construct the asks given aixed effective masking ratio o 90%. Finally, inFigure  w a ulask rateyndn that smplimasksoc  onrangen hort-ang t oe more effective than sampling just a single mask for each clip. TaboviWvaalve probe on K400 using either 1 clip ( $\approx 2$ seconds of a video) or 8 clips ( $\approx 1 6$ seconds of a video). To sample $N$ clips, we first divide a video in $N$ equal-length temporal segments and sample one clip at random per segment. The video encoder processes t e and V-JEPA.   

<table><tr><td>Method</td><td>Arch.</td><td>1 Clip</td><td>8 Clips</td></tr><tr><td>VideoMAE</td><td>ViT-L/16</td><td>69.4</td><td>77.8</td></tr><tr><td>V-JEPA</td><td>ViT-L/16</td><td>73.7</td><td>80.9</td></tr></table>

Tab5Fnun resultsWealuateVJEAmode wi he e proocol  he K400 and Sv ats using 16 frames per clip and multi-view fusion ( $5 \times 3$ or $2 \times 3$ ) for inference. The #Samples Seen entry corresponds to the o Avu finetuning protocol.

<table><tr><td>Method</td><td>Arch.</td><td>Pretraining Data</td><td>#Samples Seen</td><td>K400 (16×5×3)</td><td>SSv2 (16×2×3)</td></tr><tr><td>VideoMAEv1</td><td>ViT-L/16</td><td>K400|SSv2</td><td>380M|410M</td><td>85.4</td><td>74.3</td></tr><tr><td rowspan="3">VideoMAEv2</td><td>ViT-H/16</td><td>K400SSv2</td><td>380M|410M</td><td>86.6</td><td>74.8</td></tr><tr><td>ViT-H/16</td><td>Un.Hybrid</td><td>1600M</td><td>86.9</td><td>76.8</td></tr><tr><td>ViT-L/16</td><td>K400+IN1K</td><td>2400M</td><td>86.4</td><td>76.7</td></tr><tr><td>MVD</td><td>ViT-H/16</td><td>K400+IN1K</td><td>2400M</td><td>87.2</td><td>77.3</td></tr><tr><td rowspan="2">V-JEPA</td><td>ViT-L/16</td><td>VideoMix2M</td><td>270M</td><td>85.6</td><td>75.1</td></tr><tr><td>ViT-H/16</td><td>VideoMix2M</td><td>270M</td><td>86.6</td><td>77.0</td></tr></table>

In Figurec, we explore differet average spatial and temporal maskig rati, ithe spatil/temporal rat the area that is covered b  mask  averageora cp.Recll that each mask isconstructedby smplievral (pyoverlapiblocks n taki theWe hangehe verespatlrtmpoalmaskira y changing a block spatial r temporal size, as well as the overall numberof blocks.We found that low spatial or temporal coverage results in a trivial prediction task, which degrades downstream performance. Basedon those results, we sample masks that remove roughly 90% of the frame and exten along the enti temporaldimensio f the clip by default. In Figure 8b , we explore different block size given an effective spatial masking ratio of $9 0 \%$ and temporal ratio of $1 0 0 \%$ . We keep the masking ratio approximately constant by changing the block size and the number of block at the same time. We find that sampling several blocks to perform better than sampling a single large block.Figure9 visually illustrates the effect of sampling several smaller blocks to construct a mask. In Figure a, we explore the effect of sampling various number of masks per samples. We fnd that samplng two msk    beiv. We hypothesize that this masking strategy induces complementary tasks. In our experiment, we use this as our default masks sampling. a. T S paper are obtained while processing an order of magnitude fewer samples than previous methods.   

<table><tr><td>Method</td><td>Arch.</td><td>Data</td><td>#Samples Seen</td></tr><tr><td>OpenCLIP</td><td>ViT-G/14</td><td>LAION-2B</td><td>39000M</td></tr><tr><td>DINOv2</td><td>ViT-g/14</td><td>LVD 142M</td><td>1900M</td></tr><tr><td>VideoMAEv2</td><td>ViT-g/14</td><td>UnlabeledHybrid</td><td>1600M</td></tr><tr><td>V-JEPA</td><td>ViT-H/16384</td><td>VideoMix2M</td><td>210M</td></tr></table>

![](images/9.jpg)  
Figue8 Masking StrategyAblation. Evaluating alnear probe on aViT-B/16 pretrained with V-JEPAn K400under ulBaskt lsk sk $1 0 0 \%$ masking ratio during pretraining.

![](images/10.jpg)  
(c) Num. Blocks: 2, Spatial Block Size: $1 6 0 \times 1 6 0$

overlapping) blocks and taking their union.