# V-JEPA 2：自监督视频模型实现理解、预测与规划

Mahmoud Assran1，Adrien Bardes1，David Fan1*，Quentin Garrido1，Russell Howes1，Mojtaba Komeili1，Matthew Muckley，Ammar Rizvi，Claire Roberts1，Koustuv Sinha1，Artem Zholus1,2，Sergio Arnaud1*，Abha Geji1*，Ada Martin1*，Francois Robert Hogan $^{1,*}$，Daniel Dugas1*，Piotr Bojanowski1，Vasil Khalidov1，Patrick Labatut1，Francisco Massa1，Marc Szafraniec1，Kapil Krishnakumar1，Yong Li1，Xiaodong $\mathsf{M} \mathsf{a}^{1}$，Sarath Chandar2，Franziska Meier $^{1,*}$，Yann LeCun $^{1,*}$，Michael Rabbat $^{1,*}$，Nicolas Ballas1* 1Meta 的 FAIR，$^{2}$Mila 魁北克人工智能研究所及蒙特利尔理工大学 *核心团队

现代人工智能面临的一个主要挑战是通过观察学习理解世界和行动（LeCun, 2022）。本文探讨了一种自监督方法，该方法将互联网规模的视频数据与少量交互数据（机器人轨迹）相结合，以开发能够理解、预测和规划物理世界的模型。我们首先在一个包含超过100万小时互联网视频的视频和图像数据集上预训练了一种无动作的联合嵌入预测架构V-JEPA 2。V-JEPA 2在运动理解方面表现出色（在Something-Something v2上达到77.3的top-1准确率），并在人体动作预测方面获得了最先进的表现（在Epic-Kitchens-100上达到39.7的召回率），超越了之前的任务特定模型。此外，在将V-JEPA 2与大型语言模型对齐后，我们在多个视频问答任务中展示了最先进的表现，以80亿参数规模进行（例如，在PerceptionTest上达到84.0，在TempCompass上达到76.9）。最后，我们展示了如何通过后训练一个潜在动作条件的世界模型V-JEPA 2-AC，将自监督学习应用于机器人规划任务，该模型使用来自Droid数据集的不到62小时的无标签机器人视频进行训练。我们在两个不同实验室中对Franka机械臂进行零-shot部署V-JEPA 2-AC，并利用规划与图像目标进行物体的抓取与放置。值得注意的是，这一切是在没有从这些环境中的机器人收集任何数据的情况下实现的，也没有进行任何任务特定的训练或奖励。这项工作展示了如何通过从网络规模的数据和少量机器人交互数据中进行自监督学习，产生能够在物理世界中进行规划的世界模型。日期：2025年6月13日 联系人：Nicolas Ballas <ballasn@meta.com> 和 Michael Rabbat <mikerabbat@meta.com> 代码： https://github.com/facebookresearch/vjepa2 博客文章： https://ai.meta.com/blog/v-jepa-2-world-model-benchmarks

# 1 引言

人类在面对新任务和陌生环境时，具有适应和概括的能力。几种认知学习理论认为，人类通过整合低级感官输入来学习世界的内部模型，以表示和预测未来状态（Craik, 1967；Rao 和 Ballard, 1999），并进一步认为这一世界模型在任何时刻塑造了我们的感知，在帮助我们理解现实方面发挥着关键作用（Friston, 2010；Clark, 2013；Nortmann 等, 2015）。此外，我们预测自身行为对未来世界状态影响的能力对于目标导向的规划同样至关重要（Sutton 和 Barto, 1981, 1998；Ha 和 Schmidhuber, 2018；Wolpert 和 Ghahramani, 2000）。构建从感官数据（如视频）学习世界模型的人工智能体，可以使其理解物理世界、预测未来状态，像人类一样在新情境中有效规划，从而形成能够应对未曾遇到过的任务的系统。

之前的研究探讨了从包含状态-动作序列的交互数据中开发预测世界模型，通常还依赖于来自环境的显式奖励反馈来推断目标（Sutton 和 Barto, 1981；Fragkiadaki 等, 2015；Ha 和 Schmidhuber, 2018；Hafner 等, 2019b；Hanseal 然而，交互数据的可用性限制了这些方法的能力。为了解决这一局限性，最近的研究利用互联网规模的视频和交互数据，训练针对机器人控制的动作条件视频生成模型，但在使用基于模型的控制进行机器人执行时仅显示出有限的结果（Hu 等, 2023；Yang 等, 2024b；Brt 等, 202；Agarwal l 02）。特别是，这项新研究强调了对预测的忠实度和视觉质量的评估，而不是规划能力，这可能是由于通过生成视频进行规划的计算成本。

![](images/1.jpg)  
Figure1 V-JEPA 2 Overview. Leveraging 1hours of internet-scale vido and 1M images, we pretrain the V-JEPA 2 videmodel usinga visual mask denoising objective (Bardes et al., 2024; Assran et al, 2023), and leverage this ml orwstrem task su sacti aation, jerenition,actantipation, an Vid Qu Answering by aligning the model with an LLM backone.After pretraining, we can als freeze the video encoder and train a new action-conditioned predictor with asmall amount of robot interaction data n top thelarned representations, and leverage this action-conditioned model, V-JEPA 2-AC, for downstream robot manipulation tasks using planning within a model predictive control loop.

在本研究中，我们基于自监督假设构建世界模型，以从观察中捕捉世界的背景知识。具体而言，我们利用联合嵌入预测架构（JEPA）（LeCun, 2022），该架构通过在学习的表示空间中进行预测来学习。与仅依赖交互数据的学习方法不同，自监督学习使我们能够利用互联网规模的视频——展示状态序列而没有直接的动作观察——来学习表示视频观察，并在这一学习的表示空间中学习世界动态的预测模型。此外，与基于视频生成的方法不同，JEPA方法专注于学习场景中可预测的方面（例如运动物体的轨迹），而忽略生成目标所强调的不可预测细节，因为后者进行像素级预测（例如，田野中每一根草的确切位置，或树上每一片叶子的具体位置）。通过扩大JEPA的预训练，我们证明它能够产生具有最先进理解和预测能力的视频表示，并且这些表示可以作为动作条件预测模型的基础，支持零-shot 规划。

我们的方法 V-JEPA 2 采用逐阶段的训练过程，首先在互联网规模的视频上进行无动作预训练，然后基于少量交互数据进行后训练（见图1）。在第一阶段，我们使用掩码去噪特征预测目标（Assran 等，2023；Bardes 等, 2024），模型在学习的表示空间中预测视频的掩码片段。我们训练的 V-JEPA 2 编码器参数量高达 10 亿，并使用了超过 100 万小时的视频。我们的实验确认，扩展自监督视频预训练提升了编码器的视觉理解能力，包括广泛的运动和外观识别能力，通过基于探针的评估以及将编码器与语言模型对齐进行视频问答（Krojer 等，2024；Ptrucean 等，2023；Liu 等，2024c；Cai 等，2024；Shangguan 等，2024）。在互联网规模视频的预训练后，我们利用第一阶段学习到的表示，在一小组交互数据上训练一个以动作为条件的世界模型 V-JEPA 2-AC。我们的动作条件世界模型是一个拥有 3 亿参数的变换器网络，采用块因果注意机制，自回归地预测下一个视频帧的表示，条件是某个动作和之前的状态。仅使用来自 Droid 数据集（Khazatsky 等，2024）62 小时的无标签交互数据，我们验证了潜在世界模型的可行性，给定子目标能够在 Franka 机器人臂上规划动作，并在新环境中以零样本方式执行抓取操作。总结而言，我们展示了从视频中学习的联合嵌入预测架构可以用于构建世界模型，从而理解物理世界、预测未来状态并有效规划情境；这是通过利用互联网规模的视频和少量的交互数据实现的。具体而言：理解——基于探针的分类：扩展自监督视频预训练使得视频表示适用于许多任务。V-JEPA 2 在编码细粒度运动信息方面表现出色，在需要运动理解的任务中，如 Something-Something v2，使用注意探针实现了 77.3 的 top-1 准确率。理解——视频问答：V-JEPA 2 编码器可用于训练多模态大型语言模型，以应对视频问答任务。我们观察到在多个需要物理世界理解和时间推理的基准测试中，8B 语言模型类别达到最先进的性能，如 MVP（44.5 配对准确率）、PerceptionTest（84.0 测试集准确率）、TempCompass（76.9 多选准确率）、TemporalBench（36.7 多二元短问答准确率）和 TOMATO（40.3 准确率）。特别地，我们展示了一个在没有语言监督的情况下预训练的视频编码器可以与语言模型对齐并达到最先进性能，这与传统观点相悖（Yuan 等，2025；Wang 等，2024b）。预测：大规模自监督视频预训练增强了预测能力。V-JEPA 2 在使用注意探针的 Epic-Kitchens-100 人类动作预期任务上达到了最先进的性能，回收率为 39.7，这比以前最佳模型有 44% 的相对提升。规划：我们展示了通过对 V-JEPA 2 进行后训练，仅使用来自流行的 Droid 数据集的 62 小时无标签机器人操作数据，获得的 V-JEPA 2-AC 能够在新环境中部署，以给定子目标解决抓取操作任务。在未对我们实验室中的任何额外数据进行训练且不进行特定任务训练或奖励的情况下，该模型成功完成了抓取和拾放等抓取操作任务，使用新物体和在新环境中进行。

本文其余部分组织如下：第二节描述了 V-JEPA 2 的预训练过程，包括使规模超越 Bardes 等人（2024）原始 V-JEPA 配方的关键要素。第三节介绍了我们的任务无关的基于动作条件的世界模型 V-JEPA 2-AC 的训练方法，该方法利用了预训练的 V-JEPA 2 模型。第四节展示了如何通过基于模型的规划使用 V-JEPA 2-AC 进行机器人控制。由于 V-JEPA 2-AC 在学习到的表示空间中建模世界动态，因此其能力根本上依赖于在 V-JEPA 2 表示空间中捕获的信息，因此我们在第五节进一步探索 V-JEPA 2 在视频理解中的表现，以及第六节中的预测任务。最后，在第七节，我们展示 V-JEPA 2 如何与语言模型对齐，以实现视频问答。第八节讨论相关工作，第九节为总结。

![](images/2.jpg)  
Figure2 Multistage training. (Left) We first pretrain the V-JEPA 2 vido encoder on internet-scale image and atiea ioe nask s pl yopi se theoesTheerh pr mask viduc nutputs  bedivectorornu token. Next, theutput  thenc cntenate wise learnable mask tokenshat spehe posiithe aske pat, n subsey processed by the predictor. The outputs of the predictor are then regressed t the prediction targets using an L1 loss. The prediction targets are computed by an ema-encoder, the weights of which are defined as an exponential moving averageof the encoder weights. (Right) After pretraining, we freeze the video encoder and learn a new action-conditioned predictor, V-JEPA 2-AC, on top of the learned representation. We leverage an autoregressive faprcijecivavol riciherationsuimntine p vi cnsann-fat.Our-cpeorublc-cualtnpt a p  ne a from current and previous time steps.

# 2 V-JEPA 2：规模化自监督视频预训练

我们在一个包含超过100万小时视频的视觉数据集上对V-JEPA 2进行了预训练。自监督训练任务基于表示空间中的掩蔽去噪，构建在V-JEPA框架之上（Bardes等人，2024）。在本文中，我们通过探索更大规模的模型、增加预训练数据的规模，以及引入空间和时间渐进分辨率训练策略，扩展了V-JEPA框架，使我们能够有效地对超出短16帧视频剪辑的模型进行预训练。

# 2.1 方法论

表示空间中的掩码去噪。V-JEPA 目标旨在从被掩码的视频视图 $x$ 中预测学习到的视频表示 $y$，即从中随机删除了一些块的视图（如图 2 左侧所示）。任务的元架构由一个编码器 $E_{ \theta }( \cdot )$ 组成，用于提取视频表示，和一个预测器 $P_{ \phi }( \cdot )$，用于预测被掩码视频部分的表示。编码器和预测器使用该目标同时训练，其中 $\Delta_{ y }$ 是一个可学习的掩码标记，指示删除块的位置。损失函数使用停止梯度操作 $\operatorname{s g}( \cdot )$ 和编码网络权重 $\theta$ 的指数移动平均 $\overline{{ \theta }}$，以防止表示崩溃。损失仅应用于对被掩码块的预测。

$$
\begin{array} { r l } { \operatorname * { m i n i m i z e } _ { \theta , \phi , \Delta _ { y } } } & { { } \parallel P _ { \phi } ( \Delta _ { y } , E _ { \theta } ( x ) ) - \mathrm { s g } ( E _ { \overline { { \theta } } } ( y ) ) \parallel _ { 1 } , } \end{array}
$$

架构。编码器 $E_{ \theta } ( \cdot )$ 和预测器 $P_{ \phi } ( \cdot )$ 都被参数化为视觉变换器 (Dosovitskiy 等, 2020)（或 ViT）。为了在视觉变换器中编码相对位置信息，我们利用 RoPE（旋转位置嵌入），而不是 Bardes 等 (2024) 中使用的绝对正弦余弦位置嵌入。我们通过将特征维度划分为三段大致相等的部分（对应时间、高度和宽度轴），并分别对每个轴的片段应用 1D 旋转，使用传统 1D-RoPE 的 3D 扩展 (Su 等, 2024)。我们发现，使用 3D-RoPE 替代绝对正弦余弦位置嵌入 (Vaswani 等, 2017) 有助于为最大模型稳定训练。为了处理视频，我们首先将其切分为 $2 \times 16 \times 16$ 的大小的 tubelet 序列 $( T \times H \times W )$，并采用与 Bardes 等 (2024) 相同的多块掩码策略。关键缩放成分。在本节中，我们引入并研究四个额外的关键成分，这些成分使 V-JEPA 预训练原则扩展以获得我们的 V-JEPA 2 模型。1. 数据扩展：通过利用和策划额外的数据源，我们将数据集规模从 200 万增加到 200 万视频。2. 模型扩展：我们将编码器架构从 3 亿参数扩展到超过 10 亿参数，从 ViT-L 转变为 ViT-g (Zhai 等, 2022)。3. 更长的训练：采用预热-常数衰减学习率调度简化超参数调优，使我们能够将训练从 9 万次扩展到 25.2 万次，有效利用额外的数据。4. 更高的分辨率：我们利用预热-常数衰减调度，通过在预热和常数阶段训练较短、低分辨率的剪辑，然后在最终衰减阶段提高分辨率和/或剪辑长度，来高效扩展到更高分辨率的视频和更长的视频剪辑。本节的其余部分将更详细地描述每个成分，并使用下面描述的评估协议量化每个成分的影响。评估协议。我们对模型预训练的目标是将一般视觉理解注入到我们的编码器中。因此，我们通过评估模型在一组六个运动和外观分类任务上的学习表示质量，来评估我们的模型和数据设计选择：Something-Something v2 (Goyal 等, 2017)、Diving-48 (Li 等, 2018)、Jester (Materzynska 等, 2019)、Kinetics (Kay 等, 2017)、COIN (Tang 等, 2019) 和 ImageNet (Deng 等, 2009)。我们使用冻结评估协议：我们冻结编码器权重，并在其表示上训练一个任务特定的 4 层注意力探头以输出预测类。在本节中，我们主要关注六个理解任务的平均准确率。有关任务、评估协议和结果的更多细节，请参阅第 5 节。

# 2.2 扩展自监督视频学习

我们首先总结了Scaling分析中的关键发现，调查了四个关键因素对下游任务平均性能的影响。图3展示了这些缩放干预对6个分类任务平均准确率的影响，以使用V-JEPA目标预训练于200万视频的ViT-L/16模型作为基线。将数据集从200万增加到2200万视频（VM22M）带来了1.0个百分点的提升。将模型从3亿参数扩展到10亿参数（ViT-g/16）提供了额外1.5个百分点的增益。将训练次数从90K延长至252K次又贡献了0.8个百分点的改善。最后，同时增强空间分辨率（$256\to384$）和时间持续性（$1664$帧），在预训练和评估期间，将性能提升至$88.2\%$，相较于ViT-L/16基线总共提高了4.0个百分点。每个单独的变化都产生了积极的影响，确认了视频自监督学习（SSL）中缩放的潜力。

![](images/3.jpg)  
Figure 3 Scaling Ingredients. The effects of scaling interventions on average accuracy across 6 image and video classification tasks (SSv2, Diving-48, Jester, Kinetics, COIN, ImageNet) using a ViT-L/16 model as baseline.

TablVideix2（rtraiDatat.Tbuilserat preraidatae，基于我们的检索式策展在YT1B上减少噪声内容（例如，卡通或剪贴画风格）。

<table><tr><td>Source</td><td>Samples</td><td>Type</td><td>Total Hours</td><td>Apply Curation</td><td>Weight</td></tr><tr><td>SSv2 (Goyal et al., 2017)</td><td>168K</td><td>EgoVideo</td><td>168</td><td>No</td><td>0.056</td></tr><tr><td>Kinetics (Carreira et al., 2019)</td><td>733K</td><td>ExoVideo</td><td>614</td><td>No</td><td>0.188</td></tr><tr><td>Howto100M (Miech et al., 2019)</td><td>1.1M</td><td>ExoVideo</td><td>134K</td><td>No</td><td>0.318</td></tr><tr><td>YT-Temporal-1B (Zellers et al., 2022)</td><td>19M</td><td>ExoVideo</td><td>1.6M</td><td>Yes</td><td>0.188</td></tr><tr><td>ImageNet (Deng et al., 2009)</td><td>1M</td><td>Images</td><td>n/a</td><td>No</td><td>0.250</td></tr></table>

# 2.3 预训练数据集

接下来，我们将描述构成我们预训练数据集的视频和图像来源，以及我们整理数据集的方法。

扩展数据集规模。我们通过结合公开可用的数据源构建了一个大规模的视频数据集。在本研究中使用公开可用的数据源使其他研究人员能够重现这些结果。总体数据集包括来自Goyal等人（2017）推出的Something-Something v2数据集（SSv2）的自我中心视频，从Kinetics 400、600和700数据集（Kay等人，2017；Carreira等人，2018, 2019）的外部中心动作视频，以及来自HowTo10oM（Miech等人，2019）的YouTube教程视频和来自YT-Temporal-1B（Zellers等人，2022）的通用YouTube视频，我们将其称为YT1B。我们还包括来自ImageNet数据集（Deng等人，2009）的图像，以增加预训练数据的视觉覆盖范围。为了实现图像和视频的联合预训练，我们在时间上重复一幅图像，将其视为一个16帧的视频，其中所有帧都是相同的。在训练期间，我们从每个数据源中按经验确定的权重系数进行抽样。最终的数据集，我们称之为VideoMix22M（或VM22M），包含2200万个样本。表1列出了这些数据源及其权重。图4（左）比较了在VM22M上预训练的ViT-L/16与在Bardes等人（2024）的小型（200万）VideoMix2M数据集上训练的类似模型的性能。与VM2M相比，在VM22M上的训练在视觉理解任务的平均性能上提高了1个百分点。性能提升在以外观为基础的任务（如Kinetics-400、COIN和ImageNet）上更为显著，显示了增加这些任务的视觉覆盖范围的重要性。

数据策展。YT1B是一个大型视频数据集，由140万小时的视频组成，与较小的视频数据集（如Kinetics和Something-Something v2）相比，缺乏策展和过滤。由于未经策展和不平衡的数据会妨碍模型性能（Assran等，2022；Oquab等，2023），我们通过调整现有的检索基础策展流程来过滤YT1B视频。具体而言，我们从YT1B视频中提取场景，为每个场景计算嵌入向量，然后使用基于聚类的检索过程（Oquab等，2023）根据目标分布选择视频场景，该目标分布由Kinetics、Something-Something v2、COIN和EpicKitchen训练数据集组成。我们在附录A.2中详细描述了数据集构建过程。与Oquab等（2023）类似，我们确保目标验证集中的视频不包含在初始的未策展数据池中。在图4（右）中，我们比较了在视觉理解评估中，基于未经策展的YT-1B数据预训练的ViT-L模型与基于我们的Curated-YT-1B数据集训练的可比模型之间的平均性能。使用策划数据集训练相较于未经策展的基线模型平均性能提升了1.4点。值得注意的是，基于Curated-YT-1B训练的模型在ViT-L规模上与完整的VM22M数据集相较表现出竞争力。然而，较大规模的模型从VM22M训练中获益更多（见附录A.2），这表明将Curated-YT-1B与其他数据源结合能够增强可扩展性。

![](images/4.jpg)  
Figure4 Data Scaling & Curation. We train and compare models on different data-mixes. Models are ViT-L/16 trai for90K iterations using a cosie learni schedule followiBardes al. (024).(Let) We compare the performance of a ViT-L/16 model pretrained on the VM2M dataset and our VM22M dataset. Training on the VM22M dataset leads to a $+ 1$ point improvement in average performance. Performance improvement is more pronounced on appearance-based tasks such as Kinetics-400, COIN, and ImageNet (Right) We compare the performance of a ViT-L/16 model pretrained on YT1B and a model pretrained on our Curated-YT1B dataset, which leverages our clustr-base cration.Trainnhe curatedataset lea . po provementaverg perr, showing the effectiveness of data-curation.

# 2.4 预训练方案

模型规模扩展。为了探索我们模型的扩展行为，我们训练了一系列编码器模型，参数量从3亿（ViT-L）到10亿（ViT-g）。所有编码器架构的详细信息见附录中的表12。请注意，所有编码器均使用与ViT-small类似的相同预测器架构。我们在图5（左侧）中报告了这些编码器在视觉理解任务上的平均性能。将模型规模从3亿（ViT-L）扩展到10亿（ViT $^ \mathrm { g }$）参数，平均性能提升了+1.5分。运动和外观理解任务均从扩展中受益，其中SSv2提高了+1.6分，Kinetics提高了+1.5分（参见表4）。这些结果证实，自监督视频预训练有效利用了更大的模型容量，达到10亿参数的ViT-g。

训练计划。V-JEPA 2模型训练采用预热-恒定学习率计划，随后进入冷却阶段（Zhai et al., 2022；Hägele et al., 2024）。与Hägele et al.（2024）类似，我们发现该计划的性能与半余弦计划（Loshchilov和Hutter，2016）相当；同时，这也使得探索较长的训练过程更具成本效益，因为多个冷却阶段的训练可以从恒定阶段的不同检查点开始。我们简化了Bardes et al.（2024）的配方，通过保持固定的教师EMA和权重衰减系数，而不是使用逐步增加的学习率计划，因为这些变化对下游理解任务的影响微乎其微。图3显示，将训练计划从90K延长至252K迭代可以使ViT-g模型的平均性能提高0.8，这验证了延长训练时间的好处。该计划还促进了渐进式训练，在冷却阶段显著提高视频分辨率。

高效渐进分辨率训练。尽管大多数先前的视频编码器专注于短片段（16帧，约1秒）(Bardes等, 2024; Wang等, 2024b, 2023)，我们探索使用较长的片段进行训练，最长可达64帧（16秒），并提高空间分辨率。然而，随着持续时间和分辨率的增加，训练时间显著增加——在 $6 4 \times 3 8 4 \times 3 8 4$ 输入上训练我们的ViT-g模型大约需要60个GPU年（见图5，中部）。为了减少这一点，我们采用渐进的下游性能训练。我们的训练过程从热身阶段开始，在此阶段我们在16帧，$2 5 6 \times 2 5 6$分辨率的视频上进行线性学习率热身，持续12K次迭代，然后进入主训练阶段，此阶段的学习率保持不变，持续228K次迭代。接着，在冷却阶段，我们增加视频持续时间和分辨率，同时在12K次迭代中线性衰减学习率。因此，训练长时间、高分辨率视频所带来的额外计算开销仅在最终冷却阶段产生。这种方法使得高效的高分辨率训练成为可能：如图所示（中部），我们在GPU时间上实现了$8.4 \times$的减少，该模型可以处理64帧，$3 8 4 \times 3 8 4$分辨率的输入，而与整个训练阶段从零开始训练这样一个模型相比，始终保持全分辨率。此外，我们仍然观察到可以处理较长持续时间和更高分辨率输入的模型的利益，如下文所述。

![](images/5.jpg)  
Fiure5 Model Scaling.We explore the impact of scaling model size and input videoresolution.All models are traie on the VideoMix22M pretrainigdataset.(Left) Average performance across ix understanding tasks as a fnomo slMol rai wnsan ar teun pplatus tasks. We then cool down the model using 64 frames at $2 5 6 \times 2 5 6$ resolution and report post-cooldown performance. Scaling the model size from 300M to 1B parameters yields a $+ 1 . 7$ point average improvement. (Middle) Training times (GPU-days) for ViT $\mathbf { g }$ on A100 GPUs when training videos at $3 8 4 \times 3 8 4$ resolution with different numbers of frames per clip. We compare progressive resolution training (252K iterations at 16 frames / $2 5 6 \times 2 5 6$ resolution, followed by 12K cooldown iterations at $3 8 4 \times 3 8 4$ resolution) to the projected time for full-resolution training. Progressive training provides up to $8 \times$ speedup, significantly reducing the pretraining compute requirement. (Right) Effect of inscreasing vide duration at cooldown on downstream performance or ViT-gEven when onlyusing 16-frameclips uealatincsviratidurhecolo hasraprovver performance by $+ 0 . 7$ points.

缩放视频的时间和空间分辨率。图5考察了输入视频分辨率如何影响下游任务性能。当在预训练过程中将剪辑时长从16帧增加到64帧，同时保持固定的16帧评估时长时，我们观察到平均性能提高了0.7个百分点（图5，右侧）。此外，我们发现，在评估过程中增加视频时长和分辨率会显著改善各个任务的表现（详见表4和附录A.4.2）。这些结果表明，视频自监督预训练在训练和评估过程中都受益于增强的时间分辨率。尽管我们尝试将视频剪辑扩展到更长（128和256帧），但在这组理解任务中并未观察到超过64帧的进一步改善。

# 3 V-JEPA 2-AC：学习一个基于动作的世界模型

在预训练之后，V-JEPA 2 模型能够对视频中缺失的部分进行预测。然而，这些预测并没有直接考虑智能体可能采取的因果作用。在本节描述的下一阶段训练中，我们专注于通过利用少量交互数据使模型适用于规划。为此，我们在冻结的 V-JEPA 2 视频编码器基础上学习一个帧因果的动作条件预测器（图 2，右）。我们在来自 Droid 数据集（Khazatsky 等，2024）上的数据进行模型训练，该数据集由通过遥操作收集的桌面 Franka Panda 机器人臂实验数据组成。我们将得到的动作条件模型称为 V-JEPA 2-AC，在第 4 节中我们展示 V-JEPA 2-AC 可以在模型预测控制规划循环中用于在新环境中规划动作。

# 3.1 动作条件下的世界模型训练

我们的目标是利用预训练后的 V-JEPA 2 模型获取一个潜在的世界模型，能够通过闭环模型预测控制来驱动具身智能系统。为此，我们训练了 V-JEPA 2-AC，这是一个自回归模型，预测基于控制动作和自我感知观察的未来视频观测表示。在本节中，我们描述了一个具体的框架实例，用于具有固定外部摄像头的桌面机械臂，其控制动作对应于末端执行器命令。该模型的训练使用了大约 62 小时的未标记视频，数据来源于原始 Droid 数据集，包含短视频，通常长度为 34 秒，视频中是一台配有双指夹持器的 7 自由度 Franka Emika Panda 机械臂。在这里，未标记视频是指我们不使用额外的元数据来指明任何奖励、每个演示正在执行的任务类型，或是演示是否成功完成了所尝试的任务。相反，我们仅使用数据集中的原始视频和末端执行器状态信号（数据集中的每个视频都有元数据，指明每帧中的末端执行器状态——位置的三个维度、方向的三个维度和夹持器状态的一个维度）。

模型输入。在每次训练迭代中，我们随机从Droid数据集中抽取4秒的迷你视频片段，并为简化起见，丢弃任何少于4秒的视频，从而得到一个较小的子集，该子集包含不足62小时的视频。视频片段的分辨率为$256 \times 256$，帧率为每秒4帧（fps），生成的16帧片段表示为$(x_k)_{k \in [1 6]}$，其中每个$x_k$表示单个视频帧。机器人末端执行器在每次观测中的状态用序列$(s_k)_{k \in [1 6]}$表示，其中$s_k$是相对于机器人基座定义的实值7维向量。$s_k$的前三个维度编码末端执行器的笛卡尔位置，接下来的三个维度以外部欧拉角的形式编码其朝向，最后一个维度编码夹具状态。我们通过计算相邻帧之间末端执行器状态的变化，构造一个动作序列$(a_k)_{k \in [1 5]}$。具体而言，每个动作$a_k$是一个实值7维向量，表示帧$k$和帧$k + 1$之间末端执行器状态的变化。我们对抽取的视频片段应用随机缩放裁剪增强，纵横比在(0.75, 1.35)范围内抽样。

损失函数。我们使用 V-JEPA 2 编码器 $E ( \cdot )$ 作为图像编码器，并独立地编码给定片段中的每帧，以获得一系列特征图 $( z _ { k } ) _ { k \in [ 1 6 ] }$ ，其中 $z _ { k } := E ( x _ { k } ) \in \mathbb { R } ^ { H \times W \times D }$，$H \times W$ 表示特征图的空间分辨率，$D$ 为嵌入维度。实际上，我们的特征图是使用 ViT-g 编码器编码的，形状为 $1 6 \times 1 6 \times 1 4 0 8$。请注意，在此后训练阶段，编码器保持冻结状态。这一系列特征图、末端执行器状态和动作在时间上交错表示为 $( a _ { k } , s _ { k } , z _ { k } ) _ { k \in [ 1 5 ] }$，并通过变换器预测网络 $P _ { \phi } ( \cdot )$ 处理，以获得一系列下一状态表示预测 $\left( \hat { z } _ { k + 1 } \right) _ { k \in [ 1 5 ] }$。标量值的教师强制损失函数最终计算为 $T = 1 5$。我们还计算了一个两步推演损失，以提高模型在推理时进行自回归推演的能力。为便于表述，并略微重载符号，设 $P _ { \phi } ( \hat { a } _ { 1 : T } ; s _ { k } , z _ { k } ) \in \mathbb { R } ^ { H \times W \times D }$ 表示通过自回归运行 V-JEPA 2-AC 得到的最终预测状态表示，从动作序列 $( \hat { a } _ { i } ) _ { i \in [ T ] }$ 开始，起始状态为 $\left( \boldsymbol { s } _ { k } , ~ \boldsymbol { z } _ { k } \right)$。我们现在可以将推演损失表示为：

$$
\mathcal { L } _ { \mathrm { t e a c h e r - f o r c i n g } } ( \phi ) : = \frac { 1 } { T } \sum _ { k = 1 } ^ { T } \lVert \hat { z } _ { k + 1 } - z _ { k + 1 } \rVert _ { 1 } = \frac { 1 } { T } \sum _ { k = 1 } ^ { T } \left. P _ { \phi } \left( \left( a _ { t } , s _ { t } , E ( x _ { t } ) \right) _ { t \leq k } \right) - E ( x _ { k + 1 } ) \right. _ { 1 } ,
$$

$$
\mathcal { L } _ { \mathrm { r o l l o u t } } ( \phi ) : = \| P _ { \phi } ( a _ { 1 : T } , s _ { 1 } , z _ { 1 } ) - z _ { T + 1 } \| _ { 1 } .
$$

在实际应用中，我们使用 $T = 2$ 来计算推演损失，这样我们仅通过一个递归步骤对预测器进行求导。因此，总体训练目标如下，且相对于预测器权重 $\phi$ 进行最小化。为了说明，训练过程如图6所示，其中教师强制和推演损失的 $T = 4$。

$$
L ( \phi ) : = \mathcal { L } _ { \mathrm { t e a c h e r - f o r c i n g } } ( \phi ) + \mathcal { L } _ { \mathrm { r o l l o u t } } ( \phi ) ,
$$

![](images/6.jpg)  
VJEPA 2-AC Teacher Forcing Loss   
VJEPA 2-AC Rollout Loss   
Figue 6 V-JEPA 2-AC training.V-JEPA 2-AC is trained in a autorressive fshion, utilizing a teachr rci lo n aolout osLe) I thetea r os the preicr akes edin  the  me rerai unar echeathexte.R)Theolou f error accumulation during rollouts.

架构。预测网络 $P _ { \phi } ( \cdot )$ 是一个参数约为 $300M$ 的变换器网络，具有 24 层、16 个头、1024 的隐藏维度，以及 GELU 激活函数。输入到预测器的动作、末端效应器状态和扩展特征图经过单独的可学习仿射变换处理，以映射到预测器的隐藏维度。同样，预测器最后一个注意力模块的输出也经过可学习的仿射变换，以将其映射回编码器的嵌入维度。我们使用 3D-RoPE 实现来表示扩展特征图中每个视频块的时空位置，同时仅对动作和姿态词元应用时间旋转位置嵌入。我们在预测器中使用块因果注意力模式，以便在给定时间步的每个块特征可以关注来自同一时间步的动作、末端效应器状态和其他块特征，以及来自前面时间步的特征。

# 3.2 通过规划推断行动

能量最小化。给定目标状态的图像，我们通过规划利用 V-JEPA 2-AC 进行下游任务。具体而言，在每个时间步骤中，我们通过最小化一个与目标相关的能量函数，为固定的时间范围规划一系列动作。然后我们执行第一个动作，观察新状态，并重复该过程。设 $s _ { k }$ 表示当前末端执行器状态，$x _ { k }$ 和 $x _ { g }$ 分别表示当前观察到的帧和目标图像，这两个图像通过视频编码器分别编码以获得特征图 $z _ { k }$ 和 $z _ { g }$。给定规划时间范围 $T$，我们通过最小化一个与目标相关的能量函数来优化一系列机器人动作 $( a _ { i } ^ { \star } ) _ { i \in [ T ] }$，使得 $\begin{array} { r } { ( a _ { i } ^ { \star } ) _ { i \in [ T ] } : = \operatorname * { a r g m i n } _ { \hat { a } _ { 1 : T } } \mathcal { E } ( \hat { a } _ { 1 : T } ; \ z _ { k } , s _ { k } , z _ { g } ) } \end{array}$。如图7所示，模型通过选择一条轨迹来推断动作序列 $( a _ { i } ^ { \star } ) _ { i \in [ T ] }$，以最小化世界模型预想状态表示与其目标表示之间的 L1 距离，时间前进 $T$ 步。实际上，我们在每个规划步骤中使用交叉熵方法（Rubinstein, 1997）最小化（5），并在重新规划之前仅在机器人上执行第一个动作，类似于逐步延伸控制。

$$
\mathcal { E } ( \hat { a } _ { 1 : T } ; \ z _ { k } , s _ { k } , z _ { g } ) : = \| P ( \hat { a } _ { 1 : T } ; s _ { k } , z _ { k } ) - z _ { g } \| _ { 1 } ,
$$

![](images/7.jpg)  
Figure 7 Planning. We plan an action sequence for a fixed time horizon $T$ by minimizing the L1 distance between the world model's imagined state representation $T$ steps into the future and its goal representation. The L1 loss is optimized with respect to the actions $( a _ { k } ) _ { k \in [ T ] }$ using the cross-entropy method (Rubinstein, 1997). Specifically, in each pla epe samle het corat   pon he plahoi omq Gu isulzzan The atiaheo-r T pv before finally returning the mean of the sequence of Gaussians as the selected action trajectory.

# 4 规划：零-shot 机器人控制

在本节中，我们展示了如何使用 V-JEPA 2-AC 实现基本的机器人技能，如伸手、抓取和拾取放置，采用模型预测控制的方法。我们重点关注具有视觉目标规范的任务，并显示 V-JEPA 2-AC 能够在零-shot 的情况下推广到新的环境。

# 4.1 实验设置

基线对比。我们将V-JEPA 2-AC的性能与两个基线进行比较，其中一个是通过行为克隆训练的视觉-语言-动作模型，另一个是基于视频生成的世界模型。

第一个基线模型基于Octo视频语言动作模型，支持目标图像条件（Octo Mode Team等，2024）。我们从octo-base-1.5版本模型的开源权重开始，该模型在包含超过1M轨迹的Open-X Embodiment数据集上预训练。我们使用行为克隆在整个Droid数据集上微调Octo模型，采用事后重标记（Andrychowicz等，2017；Ghosh等，2019），使用图像目标和末端效应器状态。具体而言，我们在训练过程中随机采样Droid数据集中的轨迹片段，并均匀采样目标图像，最多20种。我们调整Droid优化超参数，并利用单侧图像视图输入，分辨率为$256 \times 256$，上下文包括前两个帧，以及未来4个动作的时间范围。

我们比较的第二个基线是基于Cosmos视频生成模型（Agarwal等，2025）。我们从无动作的Cosmos模型（肉体扩散-7B与连续标记器）的开源权重开始，该模型是在2000万小时的视频上训练的，然后我们使用官方发布的动作条件微调代码在Droid上微调该模型。为了在Droid上训练时提高性能，我们（i）将学习率降低至与视频条件Cosmos配方中使用的学习率相匹配，（ii）去除了视频条件中的dropout，以改善训练动态，以及（iii）将噪声水平提高了$e^{2}$倍，因为我们观察到使用较低噪声因子的模型在利用条件帧中的信息时遇到困难。尽管Cosmos技术报告（Agarwal等，2025）提到将世界模型用于规划或模型预测控制作为未来的应用，但据我们所知，这是首次报告使用Cosmos模型进行机器人控制的尝试。

![](images/8.jpg)  
Figure 8 Single-Goal Reaching.Single-goal reaching involves moving the end-effector to a desired location in sabasn  sg lageThis tasasuresor basdeandictins as we  3 understanding f the scene including depth,from the onocular RGB camera In each step, we use V-JEPA 2-ACto plan ie san tweodeatur aeere an  ereati the al amThe s ct h ee boe -pan he ex im e. Duri plani w smle dividalctons he LBalrads 0.07 ctere at the rTu h maximum achievable decrease in cartesian distance to the goal in a single step is 0.13 (\~13 cm).

机器人部署。所有模型在具有RobotiQ抓手的Franka Emika Panda机械臂上进行零-shot部署，分布在两个不同的实验室中，而这两个实验室都没有出现在Droid数据集中。视觉输入通过一台未校准的低分辨率单目RGB相机提供。机器人使用相同的模型权重和推理代码，并基于操作空间控制的相似低级控制器。我们对V-JEPA 2-AC世界模型和Cosmos世界模型使用阻塞控制（即系统在发送新动作到控制器之前，等待最后一个命令的动作完成），并在Octo上实验阻塞和非阻塞控制，报告两者选项中的最佳性能。在使用V-JEPA 2-AC和Cosmos进行规划时，我们将每个采样动作限制在以原点为中心的半径为0.075的L1-球内，这对应于每个单独动作最大末端效应器位移约为13厘米，因为对于模型而言，大动作相对而言属于分布外。

# 4.2 结果

单目标到达。首先，我们评估单目标到达任务，该任务涉及根据单个目标图像将末端执行器移动到期望的空间位置。此任务测量对动作的基本理解以及从单目 RGB 相机对场景（包括深度）的三维空间理解。图 8 显示了在机器人执行三个不同的单目标到达任务期间末端执行器与其目标位置之间的欧几里得距离。在所有情况下，模型能够将末端执行器移动到距离其目标位置不到 4 厘米的范围内，并选择导致误差单调减少的动作。这可以视为一种视觉伺服（Hill, 1979），其中使用相机的视觉反馈来控制机器人的运动。然而，与传统的视觉伺服方法不同，V-JEPA 2-AC 是通过对未标记的真实世界视频数据进行训练来实现的。在图 9 中，我们可视化了 V-JEPA 2-AC 能量景观，基于方程（5）对于 $\Delta y$ 到达任务，作为单个笛卡尔控制动作 $\Delta x$ 和 $\Delta y$ 的函数，同时固定 $\Delta z = 0$。能量函数在接近真实动作时达到最低值，进一步证明模型已学会合理推断动作的效果，而无需高精度传感。同时有趣的是，V-JEPA 2-AC 诱导的能量景观相对平滑且局部凸，这应有助于规划。

灵巧操作。接下来，我们在更具挑战性的灵巧物体操作任务上评估所有模型，即抓取、物体到达和拾取放置。成功率在表 2 和表 3 中列出，并在 10 次试验中取平均，试验间任务的各种排列组合（例如物体位置、起始姿态等）。在抓取和物体到达任务中，模型展示了一个单一的目标图像。对于拾取放置任务，我们向模型展示两个子目标图像以及最终目标图像。第一个目标图像显示被抓取的物体，第二个目标图像显示物体接近目标位置。模型首先在前 4 个时间步内针对第一个子目标优化动作，然后自动切换至第二个子目标进行接下来的 10 个时间步，最后在最后 4 个时间步针对第三个目标进行优化。拾取放置任务的机器人执行实例见图 10。实验室 1 中所有单独任务的起始和目标帧见附录 B.2。抓取任务需要通过视觉反馈进行精确控制，以正确抓取物体。物体到达任务要求模型在保持物体的同时进行导航，这需要对直观物理的基本理解，以避免掉落物体。最后，拾取放置任务测试组合这些基本技能的能力。

尽管所有模型在到达率上都达到了较高的成功率，但在涉及物体交互的任务中，性能差异更加明显。我们观察到，所有模型的成功率都依赖于所操作物体的类型。例如，我们发现杯子主要是通过将手指放置在物体内侧并环绕杯沿握住的，但模型产生的控制动作不够准确，机器人会错过杯沿，无法成功抓取物体。在操控盒子时，有更多可行的抓取配置，但模型需要更精准的手指控制，以确保手指张开得足够宽，以抓住物体。我们发现，对于所有模型，成功率随物体类型的变化是由于次优动作的组合和不同物体操控所面临的独特挑战。然而，我们看到，V-JEPA 2-AC 模型在所有任务中实现了最高的成功率，突显了机器人操控中潜在规划的可行性。

![](images/9.jpg)  
Figure 9 V-JEPA 2-AC Energy Landscape. Energy landscape for single-goal reaching task with respect to end-effector cartesian-control action (sweeping $\Delta x$ and $\Delta y$ while holding $\Delta z = 0$ fixed); ground truth action relating goal image to start frame is located at $( \Delta x , \Delta y ) = ( 0 , - 0 . 1 )$ . We see that the energy function achieves its minimum around $( \Delta x , \Delta y ) \approx ( 0 , - 0 . 0 5 )$ , indicating that the model has learned to reasonably infer the effect of actions without requiring precision sensing.

在表3中，我们比较了使用V-JEPA 2-AC与基于潜在扩散的Cosmos动作条件视频生成模型的规划性能。在两种情况下，我们都利用交叉熵方法（Rubinstein, 1997）来优化动作序列，使用单个NVIDIA RTX 4090 GPU，并通过在模型的潜在空间中编码目标帧来构建能量函数，如公式（5）所示。使用80个样本、10个优化步骤和1的规划时间跨度，Cosmos在每个规划步骤中计算单个动作需要4分钟。虽然在使用Cosmos进行到达任务时我们达到了80%的高成功率，但在物体交互任务上的表现较弱。请注意，在每个动作的规划时间为4分钟的情况下，完整的拾取与放置轨迹需要超过一个小时的机器人执行时间。相比之下，在每个优化步骤中使用$10 \times$更多的样本，V-JEPA 2-AC世界模型每个动作仅需16秒，并在所有考虑的机器人技能上表现更优。我们可以通过在未来的工作中利用额外的计算资源来减少这两种模型的规划时间，从而减少所需的样本数量。

![](images/10.jpg)  
Figure 10 Pick- $\&$ -Place. Closed-loop robot execution of V-JEPA 2-AC for multi-goal pick-&-place tasks. Highlighted frame idicate when the modelchieves a sub-goal and switcheso the next gl. The rst gal age shows the o biaspe the oglhowshejctheiy  thedes can nd thh gl image shows the object placed i the desired positonThe model frst optimize actions with respect t the frst su-goal or 4 timestes beoeutatialy itchinthesecon su-goal o the ext 10 timestes an fnally the thir gl orthe last 4timsteps.Robo actions ar eretro gal-condite planninThe V-JEPA-ACmode is ble peroze-shot pick-place sk twFrank arms iffet as wit u object configurations and cluttered environments.

表2 零样本机器人操作。所有模型在两台Franka机械臂上以零样本方式部署，利用RobotiQ进行目标识别和任务执行，跨试验（例如，物体位置、起始姿态等）完成任务。

<table><tr><td rowspan="2">Method</td><td rowspan="2">Reach</td><td rowspan="2"></td><td colspan="2">Grasp</td><td colspan="2">Reach w/ Obj.</td><td colspan="2">Pick-&amp;-Place</td></tr><tr><td>Cup</td><td>Box</td><td>Cup</td><td>Box</td><td>Cup</td><td>Box</td></tr><tr><td rowspan="3">Octo (Octo Model Team et al., 2024)</td><td>Lab 1</td><td>100%</td><td>20%</td><td>0%</td><td>20%</td><td>70%</td><td>20%</td><td>10%</td></tr><tr><td>Lab 2</td><td>100%</td><td>10%</td><td>0%</td><td>10%</td><td>70%</td><td>10%</td><td>10%</td></tr><tr><td>Avg</td><td>100%</td><td>15%</td><td>0%</td><td>15%</td><td>70%</td><td>15%</td><td>10%</td></tr><tr><td rowspan="3">V-JEPA 2-AC (ours)</td><td>Lab 1</td><td>100%</td><td>70%</td><td>30%</td><td>90%</td><td>80%</td><td>80%</td><td>80%</td></tr><tr><td>Lab 2</td><td>100%</td><td>60%</td><td>20%</td><td>60%</td><td>70%</td><td>80%</td><td>50%</td></tr><tr><td>Avg</td><td>100%</td><td>65%</td><td>25%</td><td>75%</td><td>75%</td><td>80%</td><td>65%</td></tr></table>

在每个时间步骤中使用的样本和精炼步骤，通过在世界模型的想象中训练前馈策略来初始化规划问题，或者在 V-JEPA 2-AC 的案例中可能利用基于梯度的规划。

# 4.3 局限性

相机位置的敏感性。由于V-JEPA 2-AC模型是在没有任何显式相机标定的情况下，基于末端执行器的笛卡尔控制动作来预测下一个视频帧的表示，因此它必须从单目RGB相机输入中隐式推断出动作坐标轴。然而，在许多情况下，机器人基座在相机帧中不可见，因此推断坐标轴的问题并不明确定义，导致世界模型产生错误。在实践中，我们手动尝试了不同的相机位置，最后选择了一个在所有实验中效果良好的位置。我们在附录B.4中对V-JEPA 2-AC世界模型对相机位置的敏感性进行了定量分析。长时间规划。基于世界模型的长时间规划受到多种因素的限制。首先，自回归预测会出现误差累积：表示空间的准确性。表3 规划性能。比较使用MPC与V-JEPA 2-AC世界模型进行闭环机器人操控的表现。在所有情况下，闭环控制方法（Rubisini等）在NVIDIA RTX 90 GPU上进行。为了评估机器人技能，我们的模型在每个规划步骤中计算一个动作，而Cosmos是一个基于动作条件的视频生成模型，整个过程耗时超过一个小时。相比之下，V-JEPA 2-AC世界模型在每个细化步骤中使用$1 0 \times$更多的样本，仅需16秒即可生成一个动作，并在所有考虑的机器人技能上表现出了更高的性能。

<table><tr><td>Lab 2</td><td colspan="5">Planning Details</td><td colspan="2">Grasp</td><td colspan="2">Pick-&amp;-Place</td></tr><tr><td>Method</td><td>#Samples</td><td>Iter.</td><td>Horizon</td><td>Time</td><td>Reach</td><td>Cup</td><td>Box</td><td>Cup</td><td>Box</td></tr><tr><td>Cosmos (Agarwal et al., 2025)</td><td>80</td><td>10</td><td>1</td><td>4 min.</td><td>80%</td><td>0%</td><td>20%</td><td>0%</td><td>0%</td></tr><tr><td>V-JEPA 2-AC (ours)</td><td>800</td><td>10</td><td>1</td><td>16 sec.</td><td>100%</td><td>60%</td><td>20%</td><td>80%</td><td>50%</td></tr></table>

预测随着自回归推演时间的延长而降低，从而使得在长时间范围内可靠规划变得更加困难。其次，长时间范围的规划增加了搜索空间的大小：在规划范围线性增加的情况下，可能的动作轨迹数量呈指数级增加，从而在长时间范围内的规划计算上带来了挑战。另一方面，长时间范围的规划对于解决非贪心预测任务是必要的，例如在没有图像子目标的情况下进行拾取和放置。未来在长时间范围规划中探索世界模型的工作将使得解决许多更复杂和有趣的任务成为可能。图像目标。遵循许多之前关于目标条件的机器人操作的研究（Finn和Levine, 21; Lynch等, 2020; Chebotar等, 2021; Jang等, 2022; Liu等, 2022; Gupta等, 2022），我们当前的优化目标假设我们可以获取视觉目标。然而，在实际部署机器人时，以其他形式表达目标（如使用语言）可能更为自然。未来将潜在动作条件的世界模型与语言模型对齐的工作将朝着通过自然语言进行更通用任务规定的方向迈进。

# 5 理解：基于探针的分类

表示空间世界模型的能力，例如上述讨论的 V-JEPA 2-AC，本质上受到学习到的表示空间中编码的状态信息的限制。在本节及后续章节中，我们将探讨 V-JEPA 2 学到的表示，并将 V-JEPA 2 编码器与其他视觉编码器在视觉分类任务上的表现进行比较。视觉分类任务可以侧重于外观理解或运动理解。虽然外观理解任务通常可以使用输入视频片段中的单帧可见信息来解决（即使分类标签描述的是动作），但运动理解任务则需要多帧才能对视频进行正确分类（Goyal et al, 2017）。为了确保对运动和外观的平衡评估，我们选择了三个运动理解任务，即 Something-Something v2 (SSv2)、Diving-48 和 Jester，这些任务要求模型理解人类手势和动作。对于外观理解，我们选择了 Kinetics400 (K400)、COIN 和 ImageNet (IN1K)，这些任务涉及到对动作、场景和物体的识别。从经验上看，我们展示了 V-JEPA 2 在运动理解任务上优于最先进的视觉编码器，而在外观理解任务上表现出竞争力。注意力探针。我们在冻结的编码器输出之上使用每个任务的训练数据训练了一个 4 层的注意力探针。我们的注意力探针由四个变换器块组成，最后一个块用可学习的查询词元替代了标准自注意力，采用交叉注意力层。按照标准做法，在推理过程中从视频中采样若干个具有固定帧数的片段。然后对这些片段的分类逻辑值进行平均。我们保持与 V-JEPA 2 预训练使用的分辨率相似。我们在附录 C.2 中进行了注意力探针层数的消融实验，并提供了下游任务中使用的片段数量、片段尺寸和其他超参数的详细信息。

表4 动作和对象分类。我们报告了所有模型在分辨率 $256 \times 256$ 下对 64 帧进行预训练的 V-JEPA 2 模型在动作和对象分类上的分类性能，除了 V-JEPA 2 ViT-g384 该模型在分辨率 $384 \times 384$ 下预训练，并将其性能与最先进的图像和视频编码器进行比较。除 V-JEPA 2 ViT-g384 外，所有模型遵循相同的评估协议。我们使用 $256 \times 256$ 的分辨率，SSv2 为 $16 \times 2 \times 3$ 输入（16 帧剪辑，2 个时间裁剪，3 个空间裁剪），K400 为 $16 \times 8 \times 3$，COIN 为 $32 \times 8 \times 3$，而 Diving-48 和 Jester 为 $32 \times 4 \times 3$。V-JEPA 2 ViT-g384 在所有六个任务中使用更高的分辨率 $384 \times 384$，并且对于 SSv2 使用 $64 \times 2 \times 3$ 输入，COIN 使用 $32 \times 8 \times 3$ 输入。我们的 V-JEPA 2 ViT-g 在编码运动理解任务中表现显著优越，并且在外观任务中也具有竞争力。它在各项任务中均表现出色，达到 88.2 的平均性能。$* : \mathrm{P E}_{\mathrm{core}} \mathbf{G}$ 在不同探测架构下在 ImageNet 上的精度达到 $89.8 \%$。

<table><tr><td colspan="4"></td><td colspan="3">Motion Understanding</td><td colspan="3">Appearance Understanding</td></tr><tr><td colspan="2">Method</td><td>Param.</td><td>Avg.</td><td>SSv2</td><td>Diving-48</td><td>3 Jester</td><td>K400</td><td>COIN</td><td>IN1K</td></tr><tr><td colspan="10">Results Reported in the Literature</td></tr><tr><td colspan="10">VideoMAEv2 (Wang et al., 2023) 1B</td></tr><tr><td>InternVideo2-1B(Wang et al., 2024b)</td><td></td><td>1B</td><td></td><td>56.1 67.3</td><td></td><td></td><td>82.8 87.9</td><td></td><td>71.4</td></tr><tr><td>InternVideo2-6B (Wang et al., 2024b)</td><td></td><td>6B</td><td></td><td>67.7</td><td></td><td></td><td>88.8</td><td></td><td></td></tr><tr><td>VideoPrism (Zhao et al., 2024)</td><td></td><td>1B</td><td></td><td>68.5</td><td>71.3</td><td></td><td>87.6</td><td></td><td></td></tr><tr><td colspan="10">Image Encoders Evaluated Using the Same Protocol</td></tr><tr><td colspan="10"></td></tr><tr><td>DINOv2 (Darcet et al., 2024)</td><td></td><td>1.1B 1.9B</td><td>81.1</td><td>50.7</td><td>82.5</td><td>93.4</td><td>83.6</td><td>90.7</td><td>86.1</td></tr><tr><td>PEcoreG (Bolya et al., 2025) SigLIP2 (Tschannen et al., 2025)</td><td></td><td>1.2B</td><td>82.3 81.1</td><td>55.4 49.9</td><td>76.9 75.3</td><td>90.0 91.0</td><td>88.5 87.3</td><td>95.3 95.1</td><td>87.6* 88.0</td></tr><tr><td colspan="10">Video Encoders Evaluated Using the Same Protocol</td></tr><tr><td colspan="10"></td></tr><tr><td>V-JEPA ViT-H (Bardes et al., 2024) InternVideo2s2-1B(Wang et al., 2024b)</td><td>600M 1B</td><td>85.2 87.0</td><td></td><td>74.3</td><td>87.9</td><td>97.7 97.0</td><td>84.5</td><td>87.1</td><td>80.0</td></tr><tr><td></td><td></td><td></td><td></td><td>69.7</td><td>86.4</td><td></td><td>89.4</td><td>93.8</td><td>85.8</td></tr><tr><td>V-JEPA 2 ViT-L</td><td>300M</td><td>86.0 86.4</td><td></td><td>73.7</td><td>89.0</td><td>97.6 97.7</td><td>85.1</td><td>86.8</td><td>83.5</td></tr><tr><td>V-JEPA 2 ViT-H V-JEPA 2 ViT-g</td><td>600M 1B</td><td>87.5</td><td></td><td>74.0</td><td>89.8</td><td>97.7</td><td>85.3</td><td>87.9</td><td>83.8</td></tr><tr><td>V-JEPA 2 ViT-g384</td><td>1B</td><td>88.2</td><td></td><td>75.3</td><td>90.1</td><td>97.8</td><td>86.6</td><td>90.7</td><td>84.6</td></tr><tr><td></td><td></td><td></td><td></td><td>77.3</td><td>90.2</td><td></td><td>87.3</td><td>91.1</td><td>85.1</td></tr></table>

评估协议。我们将 V-JEPA 2 在运动和外观任务上的表现与其他几种视觉编码器进行比较。DNOv2（Darcet 等，2024）是当前图像自监督学习的最先进模型，而 SigLIP2（Tschannen 等，2025）和感知编码器 PE $\mathrm{c o r e}$ G（Bolya 等，2025）是两个最先进的图像文本对比预训练模型。我们还考虑了两个视频编码器：自监督的 JEPA（Bardes 等，2024）和主要依赖于视觉-文本对比预训练的 InternVid2 $s 2$ 1B（Wang 等，2024b）。

我们对每个基准和 V-JEPA 2 使用相同的评估协议，在冻结的编码器上学习一个注意力探头，类似于 Bardes et al. (2024) 的方法。我们按照 Oquab et al. (2023) 使用的程序将基于图像的模型适应于视频，串联每个输入帧的特征。对于 InternVideo2 $s 2$ -1B，我们在 ImageNet 任务中使用其图像位置嵌入，而对于视频任务，我们将其位置嵌入从 4 帧插值到 8 帧，从而生成与 V-JEPA 2 相似的词元计数。尽管使用了共同的评估协议，基准编码器是基于不同的数据（例如，DINOv2 在 LVD-142M 上，PE $\mathbf { c o r e }$ G 在 MetaCLIP 上）进行训练的，因此无法直接比较。因此，我们只能在系统级别比较不同的方法；尽管在训练协议和数据上存在差异，但采用一致的评估协议。我们还包括文献中使用类似冻结协议的现有结果，但注意力头架构可能不同。具体而言，当可用时，我们分享 VideoMAEv2（Wang et al., 2023）、InternVideo-1B 和 6B（Wang et al., 2024b）以及 VideoPrism（Zhang et al., 2024c）在我们考虑的分类任务上的报告结果。我们在附录 C.1 中提供完整的评估和超参数。结果。表 4 报告了 V-JEPA 2、我们评估的其他编码器以及文献中报告的其他显著结果的分类性能。V-JEPA 2 ViT-g（在 256 分辨率下）在运动理解任务上显著优于其他视觉编码器。在 SSv2 上取得了 75.3 的顶级准确率，与表 5 中的预测：人类动作预判相比。在 EK100 行动预期基准测试中与最先进的结果比较，我们报告了 EK100 验证集上的平均类别召回率、名词和动作。V-JEPA 也有优秀表现。

<table><tr><td rowspan="2">Method</td><td rowspan="2">Param.</td><td colspan="3">Action Anticipation</td></tr><tr><td>Verb</td><td>Noun</td><td>Action</td></tr><tr><td>InAViT (Roy et al., 2024)</td><td>160M</td><td>51.9</td><td>52.0</td><td>25.8</td></tr><tr><td>Video-LLaMA (Zhang et al., 2023)</td><td>7B</td><td>52.9</td><td>52.0</td><td>26.0</td></tr><tr><td>PlausiVL (Mittal et al., 2024)</td><td>8B</td><td>55.6</td><td>54.2</td><td>27.6</td></tr><tr><td>Frozen Backbone</td><td></td><td></td><td></td><td></td></tr><tr><td>V-JEPA 2 ViT-L</td><td>300M</td><td>57.8</td><td>53.8</td><td>32.7</td></tr><tr><td>V-JEPA 2 ViT-H</td><td>600M</td><td>59.2</td><td>54.6</td><td>36.5</td></tr><tr><td>V-JEPA 2 ViT-g</td><td>1B</td><td>61.2</td><td>55.7</td><td>38.0</td></tr><tr><td>V-JEPA 2 ViT-g384</td><td>1B</td><td>63.6</td><td>57.1</td><td>39.7</td></tr></table>

InternVideo 为 69.7，PE $C o r e$ G 为 55.4。V-JEPA 2 在外观任务上也表现竞争力，在 ImageNet 上达到了 84.6（比 V-JEPA 提高了 4.6 分）。总体而言，V-JEPA 2 在所有六个任务中取得了最佳平均性能，相较于其他视频和图像编码器。高分辨率、长时长的 V-JEPA 2 ViT-g384 在所有任务上进一步提升，达到了 88.2 的平均性能。

# 6 预测：基于探针的动作预判

动作预测是指根据某个动作之前的上下文视频片段来预测未来的动作。通过使用Epic-Kitchens-100 (EK100)基准（Damen et al, 2022），我们展示了V-JEPA 2在动作预测性能上随着模型规模的增大而持续提升。此外，尽管仅使用了在V-JEPA 2表征基础上进行的注意力探测，我们仍然证明V-JEPA 2显著超越了之前专门为此任务设计的最先进方法。

EK100数据集包含在45个厨房环境中从自我视角记录的100小时烹饪活动。EK100中的每个视频都被注释为动作片段，其中包括开始时间戳、结束时间戳和动作标签。共有3,568个独特的动作标签，每个标签由一个动词和一个名词类别组成，总共有97个动词类别和300个名词类别。EK100动作预测任务涉及从视频片段中预测名词、动词和动作（即联合预测动词和名词），这个视频片段被称为背景，发生在动作片段的开始时间戳之前。背景结束与动作片段开始之间的时间间隔称为预测时间，默认设置为1秒。考虑到从给定背景生成的不同未来动作是可能的，因此采用5类均值召回率作为评估性能的指标（Damen等，2022）。

预期探头。一个注意力探头在冻住的 V-JEPA 2 编码器和预测器上进行训练，以预测未来动作。具体而言，我们采样一个在动作开始前 1 秒结束的视频片段。这个视频上下文被输入到 V-JEPA 2 编码器中。预测器接收编码器表示，以及与未来 1 秒帧对应的掩码词元，预测未来视频帧的表示。预测器和编码器的输出在词元维度上连接，然后输入到与第 5 节中使用的结构相似的注意力探头中，唯一区别在于预期探头的最终交叉注意力层学习三个查询词元（而不是一个），每个查询输出输入到不同的线性分类器中，分别预测动作类别、动词类别和名词类别。对每个分类器独立应用焦点损失（Lin 等，2017），然后求和再通过探头的共享注意力块进行反向传播。我们在附录 D.1 中提供了更多细节和评估超参数。基线。我们将我们的模型与三个专门为动作预期训练的基线进行比较：InAViT（Roy 等，2024）是一种利用显式手物体交互建模的监督方法；Video-LLaMA（Zhang 等，2023）和 PlausiVL（Mittal 等，2024）都是利用大型语言模型的方法，参数量高达 70 亿。

![](images/11.jpg)  
Figure11 Visualization of EK100 prediction. (Left): four selected frames from the context frames. (Middle): m predictions ordered b lkeiod.Right):owrameter hesecoantipationtimeWe sho w examples where the model is successful and one example where the model fails.

结果。表5总结了在EK100动作预测基准上的结果。我们比较了V-JEPA 2与ViT-L、ViT-H和ViT-g编码器，参数数量从3亿递增到10亿。所有三种模型都利用32帧，每秒8帧，分辨率为$256 \times 256$作为视频上下文。我们还报告了使用$384 \times 384$分辨率的ViT-g384的结果。V-JEPA 2在动作预测的召回率达到5时显示出与模型大小之间的线性扩展行为。V-JEPA 2 ViT-L在3亿参数时实现了32.7的召回率。将模型大小增加到10亿参数使召回率提升了$+5.3$点，达到了38.0。此外，V-JEPA 2受益于使用更高分辨率的上下文，分辨率为$384 \times 384$的V-JEPA 2 ViT-g384相比于其他使用$256 \times 256$分辨率的模型，召回率又提升了额外的+1.7点。V-JEPA 2显著超越了之前的最先进模型PlausiVL，即便其参数量为3亿，相较于PlausiVL的80亿参数，仍表现出色。特别是，V-JEPA 2 ViT$\mathrm{8384}$在动作召回率达到5上比PlausiVL提高了$+12.1$点，相当于$44\%$的相对提升。在图11中，我们可视化了V-JEPA 2在EK100验证集中的三个样本预测结果，其中两个模型成功，另一个模型失败。对于两个成功的例子，V-JEPA 2不仅以最高置信度准确地检索到正确动作，还根据给定上下文提出了连贯的前2到5个动作。例如，在顶行中，正确动作是“清洗水槽”，但“打开水龙头”或“清洁墙壁”在有水龙头和墙壁的情况下都是有效的动作。模型还预测了“冲洗海绵”，这也是当前正在执行的动作，可能认为这个动作在1秒后仍然可以继续。对于失败案例，V-JEPA 2仍然提出了诸如“关门”和“放下调料包”等连贯动作，但未能准确识别物体的具体名称：“茶叶包”。局限性。V-JEPA 2和EK100基准存在多个局限性。首先，V-JEPA 2并未完全解决EK100，存在失败案例，其中模型错误理解动词、名词或两者都错。我们在附录D中研究了这些失败的分布。其次，我们在这里重点关注具有1秒预测时间的动作。V-JEPA 2在更长时间范围内进行预测时准确率下降，详见附录D.2。第三，EK100基准仅限于厨房环境，具有封闭且明确的词汇，我们不知道V-JEPA 2在其他环境中的泛化能力。这限制了在EK100上训练模型的效用和适用性。最后，EK100中的动作是从固定的类别集中选择的，这使得无法泛化到训练集中不存在的动作类别。

# 7 理解：视频问答

在本节中，我们探讨 V-JEPA 2 执行开放语言视频问答（VidQA）的能力。为了实现语言能力，我们使用 V-JEPA 2 作为视觉编码器，在非标记早期融合（Wadekar 等，2024）的设置中训练多模态大型语言模型（MLLM），该设置由 LLaVA 系列模型（Li 等，2024b）推广。在这一系列 MLLM 中，视觉编码器与大型语言模型对齐，通过将视觉编码器的输出补丁嵌入投影到大型语言模型的输入嵌入空间。然后，MLLM 可以选择端到端训练，或者保持视觉编码器不变。用于 VidQA 的 MLLM 中，大多数编码器通常是图像编码器，这些编码器在视频输入的每一帧上独立应用（Qwen 团队等，2025；张等，2024b）。此类编码器的流行实例包括 CLIP（Radford 等，2021）、SigLIP（Tschannen 等，2025）和 Perception Encoder（Bolya 等，2025），它们之所以被选择，主要是因为它们与语言的语义对齐，且这些对齐是通过与图像-标题对的预训练获得的。根据我们所知，我们的工作是首次使用未经任何语言监督预训练的视频编码器，来训练一个用于 VidQA 的 MLLM。

MLLM 在下游任务上的表现也高度依赖于对齐数据。在这些实验中，我们使用了一个包含 8850 万对图像和视频文本的数据集，类似于用于训练 PerceptionLM（Cho 等，2025）的数据集。为了展示 V-JEPA 2 编码器的有效性，我们首先在第 7.2 节中将 V-JEPA 2 与其他最先进的视觉编码器进行比较，使用了 1800 万个样本的控制数据集。然后，在相同的控制设置中，我们展示了扩展视觉编码器和输入分辨率大小均能持续提升 VidQA 的性能，见第 7.3 节。最终，我们扩展对齐数据，在第 7.4 节中使用全 8.85 亿个样本测试 V-JEPA 的语言对齐极限。我们的结果表明，在控制数据设置下，V-JEPA 2 在开放式 VidQA 任务上相较于其他视觉编码器获得了具有竞争力的表现。在扩展对齐数据后，V-JEPA 2 在多个 VidQA 基准上达到了最先进的性能。

# 7.1 实验设置

视频问答任务。我们对PerceptionTest（Ptrucean等，2023）进行评估，该测试评估模型在记忆、抽象、物理和语义等不同技能上的表现。此外，我们还对MVP数据集（Krojer等，2024）进行评估，以测试对物理世界的理解，该数据集采用最小视频对评估框架，以减轻文本和外观偏差。我们还在TempCompass、TemporalBench和TOMATO（Liu等，2024c；Cai等，2024；Shangguan等，2024）上进行评估，以调查模型的时间理解和记忆能力。最后，我们报告了在一般理解能力上的结果，采用MVBench（Li等，2024c），该数据集偏向于单帧外观特征（Krojer等，2024；Cores等，2024），以及TVBench（Cores等，2024），该数据集在文献中被提出作为一般理解和时间理解的替代方案，以减轻这些偏差。

视觉指令微调。为了评估 V-JEPA 2 在视觉问答任务上的表示，我们采用了 LLaVA 框架中的视觉指令微调程序将 V-JEPA 2 与 LLM 对齐（Liu et al., 2024a）。该过程涉及使用可学习的投影模块（通常是多层感知器，MLP）将视觉编码器输出（或视觉词元）转换为 LLM 输入。我们通过进阶的三阶段过程训练 MLLM，遵循 Liu et al. (2024b)：阶段 1，我们仅在图像标注数据上训练投影模块；阶段 2，我们在大规模图像问答数据上训练完整模型；阶段 3，我们进一步在大规模视频标注和问答上训练模型。通过这种分阶段训练方法，LLM 逐步提高对视觉词元的理解。视觉编码器可以是固定的，也可以与 MLLM 其余部分一起微调。我们探讨这两种设置，因为固定视觉编码器能够提供关于视觉特征质量的更清晰信号，而微调视觉编码器则可以带来更好的整体性能。视觉指令训练的更多细节在附录 E 中描述。

# 7.2 与图像编码器的比较

为了隔离视觉编码器对多模态大语言模型（MLLM）性能的贡献，并与V-JEPA 2进行比较，我们引入了一种受控的设置：使用相同的大语言模型（LLM）主干和训练设置，训练不同的最先进编码器的单个MLLM。在这个受控设置中，我们使用Qwen2-7B-Instruct（Yang et al., 2024a）并固定视觉编码器。我们使用了1800万对齐的图像和视频-文本样本。我们首先将预训练在分辨率$5 1 2 \times 5 1 2$上的V-JEPA 2与DINOv2（Oquab et al., 2023）、SigLIP-2（Tschannen et al., 2025）和Perception Encoder（Bolya et al., 2025）进行比较。表6展示了在固定编码器设置中，现成图像编码器与V-JEPA 2之间的比较。所有实验均使用相同的LLM主干Qwen2-7B-Instruct数据训练设置。Perception的测试准确性在SFT后于验证集上报告。

<table><tr><td>Method</td><td>Params Enc / LLM</td><td>Avg.</td><td>Preeot 2</td><td>20 MA</td><td>Sadwodess 20</td><td>Teugcn 20</td><td>Mh B</td><td>OOOAAO Ae</td><td>Nc B</td></tr><tr><td colspan="10">Off-the-shelf image encoders</td></tr><tr><td>DINOv2 ViT-g518</td><td>1.1B/7B</td><td>45.7</td><td>67.1</td><td>22.4</td><td>62.3</td><td>26.8</td><td>47.6</td><td>32.0</td><td>61.8</td></tr><tr><td>SigLIP2 ViT-g384</td><td>1.1B/7B</td><td>48.1</td><td>72.4</td><td>26.2</td><td>66.8</td><td>25.7</td><td>48.7</td><td>33.2</td><td>64.0</td></tr><tr><td>PE ViT-G/14448</td><td>1.9B/7B</td><td>49.1</td><td>72.3</td><td>26.7</td><td>67.0</td><td>27.5</td><td>51.6</td><td>34.0</td><td>64.7</td></tr><tr><td>V-JEPA 2 ViT-g512</td><td>1B/7B</td><td>52.3</td><td>72.0</td><td>31.1</td><td>69.2</td><td>33.3</td><td>55.9</td><td>37.0</td><td>67.7</td></tr></table>

表 7 扩展视觉编码器的规模和分辨率。我们将视觉编码器的参数从 3 亿扩展到 10 亿，同时输入分辨率从 256 像素扩展到 512 像素。所有实验使用相同的 LLM 主干（Qwen2-Instno uPTee 验证集在 SFT 后）。增加 V-JEPA 2 编码器的规模和分辨率提高了 VidQA 任务的平均性能。

<table><tr><td>Method</td><td>Params Enc / LLM</td><td>Avg.</td><td>20 1</td><td>ba-cec N0</td><td>Sadwodwss wt-ctce</td><td>20 20</td><td>Mh Ae</td><td>0 </td><td>Mnc </td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>End-to-end Evaluation</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>V-JEPA 2 ViT-L256</td><td>300M/7B</td><td>51.7</td><td>74.6</td><td>32.3</td><td>70.1 70.9</td><td>30.2 29.8</td><td>50.9 54.6</td><td>36.5 35.1</td><td>67.1 68.0</td></tr><tr><td>V-JEPA 2 ViT-H256</td><td>0M/7B</td><td>52.0 52.3</td><td>74.7 75.5</td><td>30.6 31.9</td><td>70.7</td><td>28.3</td><td>54.2</td><td>37.3</td><td>68.3</td></tr><tr><td>V-JEPA 2 ViT-g256</td><td>1B/7B</td><td>54.0</td><td>76.5</td><td>33.0</td><td>71.7</td><td>33.1</td><td>56.5</td><td>39.0</td><td>68.5</td></tr><tr><td>V-JEPA 2 ViT-g384 V-JEPA 2 ViT-g512</td><td>1B/7B 1B/7B</td><td>54.4</td><td>77.7</td><td>33.7</td><td>71.6</td><td>32.3</td><td>57.5</td><td>38.5</td><td>69.5</td></tr></table>

我们观察到 V-JEPA 2 在冷冻设置中表现出竞争力，超越了 DINOv2、SigLIP 和感知编码器（PE）在所有测试基准（表 6）中的表现，唯独在 PerceptionTest 上略逊于 SigLIP 和 PE。特别是在 MVP、TemporalBench 和 TVBench 这几个主要关注时间理解的基准上，性能提升尤为显著。此外，由于我们仅更改了视觉编码器，我们提供了证据表明，在没有语言监督的情况下训练的视频编码器能够超越使用语言监督训练的编码器，这与传统观点相悖（Tong et al., 2024; Li et al., 2024b; Liu et al., 2024d; Yuan et al., 2025）。结果还表明，对于 VidQA 使用视频编码器而非图像编码器可以提高时空理解，这突显了开发更好视频编码器的必要性。

# 7.3 扩大视觉编码器的规模和输入分辨率

之前的研究（Fan et al., 2025）表明，扩展视觉编码器和输入分辨率显著提高了自监督图像编码器在视觉问答（VQA）任务上的表现。因此，我们将 V-JEPA 2 的参数从 3亿扩展至 10亿，并将输入分辨率从 256 像素提高到 512 像素，结果见表 7。当将视觉编码器的容量从 3亿提升到 10亿时，在固定输入分辨率 256 像素下，我们观察到 PerceptionTest 提升了 0.9 分，TVBench 提升了 3.3 分，MVBench 提升了 1.2 分。此外，将输入分辨率提升至 512 像素，在所有下游任务中均有进一步提升，例如 PerceptionTest 提升了 2.2 分，TemporalBench 提升了 4.0 分，TVBench 提升了 3.3 分。这些结果表明，进一步扩展视觉编码器和输入分辨率是一个有前景的方向。我们使用完整的 8850万样本对齐数据集，并采用与 PLM 8B Choe et al.（2025）相同的方法，使用 Llama 3.1 主干网络。我们观察到 V-JEPA 2 在测试集上的 SFT 结果有显著改进；所有其他结果为零-shot。

<table><tr><td>Method</td><td>Params Enc / LLM</td><td>Peeot Avg.</td><td>20 20 NA</td><td>Paedwodess</td><td>Tugec 20</td><td>2</td><td>TOAAAO Ae</td><td>20 A</td><td>20 </td></tr><tr><td colspan="10">≤ 8B Video Language Models Results Reported in the Literature</td></tr><tr><td>InternVL-2.5 (Chen et al., 2024)</td><td>300M/7B</td><td>52.1</td><td>68.9</td><td>39.9</td><td>68.3</td><td>24.3</td><td>29.4</td><td>61.6</td><td>72.6</td></tr><tr><td>Qwen2VL (Wang et al., 2024a)</td><td>675M/7B</td><td>47.0</td><td>66.9</td><td>29.2</td><td>67.9</td><td>20.4</td><td>31.5</td><td>46.0</td><td>67.0</td></tr><tr><td>Qwen2.5VL (Qwen Team et al., 2025)</td><td>1B/7B</td><td>49.7</td><td>70.5</td><td>36.7</td><td>71.7</td><td>24.5</td><td>24.6</td><td>50.5</td><td>69.6</td></tr><tr><td>PLM 8B (Cho et al., 2025)</td><td>1B/8B</td><td>56.7</td><td>82.7</td><td>39.7</td><td>72.7</td><td>28.3</td><td>33.2</td><td>63.5</td><td>77.1</td></tr><tr><td>V-JEPA 2 ViT-g384 LLama 3.1 8B</td><td>1B/8B</td><td>59.5</td><td>84.0</td><td>44.5</td><td>76.9</td><td>36.7</td><td>40.3</td><td>60.6</td><td>73.5</td></tr></table>

提高VidQA性能的方向。

# 7.4 通过扩展数据提高最先进水平

在开发对 V-JEPA 2 在 nMLLM 控制设置下能力的更好理解后，我们研究了增加对齐数据集规模对提升 VidQA 最先进技术的影响。正如 Cho 等人（2025）所观察到的，通过增加训练数据的规模，通常可以实现下游任务性能的阶跃式提升。为此，我们将 MLLM 训练数据的规模从 1800 万增加到全部 8850 万（4.7 倍）。虽然提高模型分辨率有助于下游性能，但这也带来了在 LLM 输入中容纳大量视觉词元的挑战。因此，我们选择 V-JEPA 2 ViT-g384，导致每帧 288 个视觉词元。我们遵循与 Cho 等人（2025）相同的配方来训练 V-JEPA 2 ViT-g384，使用 Llama 3.1 作为主干网络。为了简化训练过程，我们使用一个没有池化的 MLP 投影器。关于规模化训练设置的详细信息在附录 E 中描述。

均匀缩放数据提高了下游基准性能，在多个基准测试（PerceptionTest、MVP、TempCompass、TemporalBench 和 TOMATO）上取得了最先进的结果（表 8）。与当前最先进的 PerceptionLM 8B（Cho 等，2025）相比，我们观察到 PerceptionTest 测试集的准确率提高了 1.3 分，MVP 的成对准确率提高了 4.8 分，TempCompass 的准确率提高了 4.2 分，TemporalBench 的短问答段 Multi-binary 准确率提高了 8.4 分，以及 TOMATO 的准确率提高了 7.1 分。V-JEPA 2 在 TVBench 和 MVBench 上未能超越 PerceptionLM，然而它仍然显著超越了其他相关基线（InternVL 2.5、Qwen2VL 和 Qwen2.5VL）。这些结果强调了为视觉-语言对齐扩展训练数据的必要性，并提供了证据，表明像 V-JEPA 2 这样的无语言监督预训练编码器可以在足够规模的情况下实现最先进的结果。

# 8 相关工作

世界模型与规划。早在 Sutton 和 Barto（1981）以及 Chatila 和 Laumond（1985）的研究中，人工智能研究者就开始尝试构建能够利用内部世界模型的智能体——既建模世界的动态，又映射静态环境——以实现高效的规划和控制。之前的研究已经在模拟任务中探讨了世界模型（Fragkiadaki et al., 2015；Ha 和 Schmidhuber, 201；Hafner et al., 2019b,a；Hansen et al., 2022, 2023；Hafer et al., 2023；Schrittwieser et al 2020；Samsami et al, 2024），以及现实世界的运动与操控任务（Lee et al., 2020；Nagabandi et al., Finn et al., 2016；Ebert et al 2017, 2018；Yen-Chen et al. 2020）。世界模型方法要么直接在像素空间中学习精确的模型（Finn et al., 2016；Ebert et al 2017, 201；Yen-Che et al, 2020），要么在学习的表示空间中进行（Watter et al., 2015；Agrawal et al, 2016；Ha 和 Schmidhuber, 2018；Hafner et al 2019b；Naire et al 202；Wu et al 2023b；Toma et al 2024；Hu et al 2024；Lanster et al 2024），或利用更结构化的表示空间，如关键点表示（Manueli et al., 2020；Das et al., 2020）。先前在机器人任务中展示了实际表现的研究训练了特定任务的世界模型，并依赖于机器人部署环境中的交互数据。评估重点是展示世界建模方法在已探索任务空间中的表现，而非对新环境或未见对象的泛化。在本研究中，我们训练了一个任务无关的世界模型，并展示了其对新环境和对象的泛化能力。一些近期的工作利用互联网规模的视频和交互数据，训练具有一般性（任务无关）动作条件视频生成模型的自主机器人（Bruce et al, 2024；Agarwal et al 202；Russel et al, 2025）。然而，到目前为止，这些方法仅展示了生成在给定机器人动作下视觉上有效的计划的能力，但尚未展示使用这些模型实际控制机器人的能力。

其他研究探讨了生成建模与策略学习的结合（Du et al., 2024；Wu et al., 2023a；Zhao et al., 2025；Zhu et al., 2025；Du et al., 2023；Zheng et al., 2025；Rajasegaran et al., 2025）。与这一研究方向不同，我们的目标是通过模型预测控制利用世界模型，而非策略学习，从而避免需要专家轨迹的模仿学习阶段。这两种方法是正交的，未来工作中可以结合使用。与我们的工作最接近的是，Zhou et al.（2024）和Sobal et al.（2025）展示了可以逐阶段端到端地学习世界模型，并利用它来零-shot解决规划任务。虽然这些先前的工作集中在小规模规划评估上，但我们展示了类似的原理可以扩展并用于解决实际的机器人任务。

用于机器人控制的视觉-语言-动作模型。最近在现实世界机器人控制中的模仿学习方法在学习显示出越来越好的泛化能力的策略方面取得了显著进展。这是通过利用在互联网规模的视频和文本数据上进行预训练的视频-语言模型来实现的，然后通过使用专家示范的行为克隆进行微调（或适应）以预测动作（Driess et al., 2023; Brohan et al, 2023; Black et al., 2024; Kim e al 2024; Bjorck et al., 2025; Black et al., 2025）。尽管这些方法显示出有前景的泛化结果，但尚不清楚它们是否能够学习预测训练数据中未展示的行为，因为它们缺乏明确的世界预测模型，并且不利用推理时计算进行规划。它们需要高质量的大规模远程操作数据，并且只能利用成功的轨迹。相对而言，我们专注于利用任何交互数据，无论是来自成功的还是失败的环境交互。

视觉基础模型。在计算机视觉中，视频基础模型已经显示出，利用由图像和/或视频组成的大规模观察数据集可以学习通用视觉编码器，这些编码器在多种下游任务中表现良好，采用自监督学习方法（Grillet al 2020; Assra e al 2023; Oquabet l 023; Fan et al 205，vid（Bardes e al； Carreira et al., 2024; Wang et al., 2023; Rajasegaran et al., 2025），并辅以弱语言监督（Wang l 202b; Boly al 2025，rcatithere Tscha t al20; Fi l 024）。然而，这些工作往往侧重于通过与大语言模型对齐后，使用基于探测的评估或视觉问答任务来理解评估。尽管这些任务推动了进展，但视觉系统的一个重要目标仍然是使智能体能够与物理世界进行互动（Gibson, 1979）。除了在视觉理解任务上的结果外，我们探讨大规模自监督学习如何使从视频中解决新环境中的规划任务成为可能，并实现零样本学习。

# 9 结论

本研究展示了如何通过联合嵌入预测架构，以自监督的方式从网络规模数据和少量机器人交互数据中学习，从而产生能够理解、预测和规划物理世界的世界模型。V-JEPA 2 在需要运动理解和人类动作预测的动作分类任务中达到了最先进的性能。与大型语言模型对齐时，V-JEPA 2 在视频问答任务上也优于之前的视觉编码器。此外，在后处理阶段，基于 V-JEPA 2 表示的方法训练出的基于动作的世界模型 V-JEPA 2-AC，使得在真实世界的机器人上成功执行零-shot 抓取与放置等先取操控任务。上述发现表明，V-JEPA 2 是向开发能够有效感知并行动的高级 AI 系统迈出的重要一步。 未来工作方面，有几个重要的方向可以针对 V-JEPA 2 的局限性进行深入研究。首先，在本研究中，我们专注于需要预测未来大约 16 秒的任务，这使得从单一目标图像中规划简单的操控任务（如抓取和与物体互动）成为可能。然而，要将此扩展到诸如抓取与放置或更复杂的任务，并且不需要子目标，将需要在建模方面进行进一步的创新。开发能够跨越多个空间和时间尺度进行预测的分层模型的方法，处于不同抽象层次的研究是一个很有前景的方向。 其次，如第 4 节所述，V-JEPA 2-AC 目前依赖于指定为图像目标的任务。尽管这对于某些任务可能很自然，但在其他情况下，基于语言的目标规范可能更为可取。扩展 V-JEPA 2-AC 以接受基于语言的目标，例如，利用一个能够将基于语言的目标嵌入到 V-JEPA 2-AC 表示空间中的模型，是未来工作中的另一个重要方向。在第 7 节中描述的结果，即将 V-JEPA 2 与语言模型对齐，可能作为一个起点。 最后，在本研究中，我们将 V-JEPA 2 模型扩展到适度的 10 亿参数。第 2 节的结果表明，在扩大到这一水平时，性能持续改善。先前的研究探讨了将视觉编码器扩展到多达 200 亿参数的可能性（Zhai et al., 2022; Carreira et al., 2024）。在这一方向上需要进一步的工作，以开发可扩展的预训练方案，持续改进性能。

# 致谢

我们感谢 Rob Fergus, Joelle Pineau, Stephane Kasriel, Naila Murray, Mrinal Kalakrishnan, Jitendra Malik, Randal Balestriero, Julen Urain, Gabriel Synnaeve, Michel Meyer, Pascale Fung, Justine Kao, Florian Bordes, Aaron Foss, Nikhil Gupta, Cody Ohlsen, Kalyan Saladi, Ananya Saxena, Mack Ward, Parth Malani, Shubho Sengupta, Leo Huang, Kamila Benzina, Rachel Kim, Ana Paula Kirschner Mofarrej, Alyssa Newcomb, Nisha Deo, Yael Yungster, Kenny Lehmann, Karla Martucci，以及 PerceptionLM 团队的成员，包括 Christoph Feichtenhofer, Andrea Madotto, Tushar Nagarajan 和 Piotr Dollar，感谢他们对本项目的反馈和支持。

参考文献 Nik Agaral、Arsln Ali、Macie Bla、Ygeh Balajri Barkr、Tiny ai Prihvtatay、oi Chen、Yin Cui、Yifan Ding、Daniel Dworakowski、Jiojiao Fan、Michele Fenzi、Francesco Ferroni、Sanja Fidler、Dieter Fox、Songwei Ge、Yunhao Ge、Jinwei Gu、Siddharth Gururani、Ethan He、Jiahui Huang、Jacob Huffman、Pooya Jannaty、Jingyi Jin、Seung Wook Kim、Gergely Klár、Grace Lam、Shiyi Lan、Laura Leal-Taixe、Anqi Li、Zhaoshuo Li、Chen-Hsuan Lin、Tsung-Yi Lin、Huan Ling、Ming-Yu Liu、Xian Lu、Alice Luo、Qianli Ma、Hanzi Mo、Kaichun Mo、Arsalan Mousavian、Seungjun Nah、Siharsha Niverty、David Page、Despoina Paschalidou、Zeeshan Patel、Lindsey Pavao、Morteza Ramezanali、Fitsum Reda、Xiaowei Ren、Vasanth Rao Naik Sabavat、Ed Schmering、Stela Shi、Bartosz Stefaniak、Shitao Tang、Lyne Tchapmi、Przemek Tredak、Wei-Cheng Tseng、Jibin Varghese、Ha Wang、Haoxiang Wang、Heng Wang、Ting-Chun Wang、Fangyin Wei、Xinyue Wei、Jay Zhangjie Wu、Jiashu Xu、Wei Yang、Lin Yen-Chen、Xiaohui Zeng、Yu Zeng、Jing Zhang、Qinsheng Zhang、Yuxuan Zhang、Qingqing Zhao，以及 Artur Zolkowski。Cosmos 世界基础模型平台用于物理 AI。arXiv 预印本 arXiv:2501.03575，2025。

Pulkit Agrawal, Ashvin V Nair, Pieter Abbeel, Jitendra Malik, 和 Sergey Levine. 通过触碰学习: 直观物理的经验学习. 《神经信息处理系统进展》，29, 2016. Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, OpenAI, Pieter Abbeel, 和 Wojiech Zaremba. 事后经验回放. 《神经信息处理系统进展》，30, 2017. Mahmoud Assran, Randal Balestrino, Quentin Duval, Florian Bordes, Ihan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, 和 Nicolas Ballas. 自监督学习中的隐藏均匀聚类先验. arXiv 预印本 arXiv:2210.07277, 2022. Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yan LeCun, 和 Nicolas Ballas. 基于联合学习的自监督学习框架. 在《IEEE/CVF计算机视觉与模式识别会议论文集》中，页面 15619-15629, 2023. Arin Bardes, Quentin Garrio, Jean Ponce, Xinlei Chen, Michael Rabbat, Yan LeCun, Mahmoud Assran, 和 Nicolas Ballas. 论文标题待定. arXiv 预印本 arXiv:20, 2024. Johan Bjorck, Fernando Castaeda, Nikita Khernidev, Xingye Da, Runyu Ding, Lixi "Jim" Fan, Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, Joel Jang, Zhenyu Jiang, Jan Kautz, Kaushil Kundalia, Lawrence Lao, Zhiqi Li, Zongyu Lin, Kevin Lin, Guilin Liu, Edith Llontop, Loic Magne, Ajay Mandlekar, Avnish Narayan, Soroush Nasiriany, Scott Reed, You Liang Tan, Guanzhi Wang, Zu Wang, Jing Wang, Qi Wang, Jiannan Xiang, Yuqi Xie, Yinze Xu, Zhenjia Xu, Seongyeon Ye, Zhiding Yu, Ao Zhang, Hao Zhang, Yizhou Zhao, Ruijie Zheng, 和 Yuke Zhu. Gr00t n1: 一种用于通用人形机器人的开放基础模型. arXiv 预印本 arXiv:2503.14734, 2025. Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Sergey Levine, Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Lucy Xiaoyang Shi, James Tanner, Quan Vuong, Anna Walling, Haohuan Wang, 和 Ury Zhilinsky. 0: 一种用于通用机器人控制的视觉-语言-动作流模型, 2024. arXiv 预印本 arXiv:2410.24164, 2024. Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Manuel Y. Galliker, Dibya Ghosh, Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Devin LeBlanc, Sergey Levine, Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Allen Z. Ren, Lucy Xiaoyang Shi, Laura Smith, Jost Tobias Springenberg, Kyle Stachowicz, James Tanner, Quan Vuong, Homer Walke, Anna Walling, Haohuan Wang, Lili Yu, 和 Ury Zhilinsky. $\pi _ { 0 . 5 }$: 一种具有开放世界泛化能力的视觉-语言-动作模型. arXiv 预印本 arXiv:2504.16054, 2025. Dnel Bolyao-Yo Hua, eizun, Jang Hyun ho, Andr Mado, Chen Wei Teny a, Jiale Zhi, Jatuha Rajasegaran, Hanoona Rasheed, Junke Wang, Marco Monteiro, Hu Xu, Shiyu Dong, Nikhila Ravi, Daniel Li, Piotr Dolár, 和 Christoph Feichtenhoer 感知编码器: 最佳视觉嵌入不在网络输出处. arXiv 预印本 arXiv:2504.13181, 2025. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, Pete Florence, Chuyuan Fu, Montse Gonzalez Arenas, Keerthana Gopalakrishnan, Kehang Han, Karol Hausman, Alexander Herzog, Jasmine Hsu, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Lisa Lee, Tsang-Wei Edward Lee, Sergey Levine, Yao Lu, Henryk Michalewski, Igor Mordatch, Karl Pertsch, Kanishka Rao, Krista Reymann, Michael Ryoo, Greci Salazar, Pannag Sankei, Perre Sermane, Jaspiar Sing, Anikait Singh, Radu Soricut, Huong Tran, Vincent Vanhoucke, Quan Vuong, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Jialin Wu, Fei Xia, Ted Xiao, Peng Xu, Sicun Xu, Tianhe Yu, 和 Briana Zitkovich. Rt-2: 视觉-语言-动作模型将网络知识转移到机器人控制中. arXiv 预印本 arXiv:2307.15818, 2023. Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, Yusuf Aytar, Sarah Bechtle, Feryal Behbahani, Stephanie Chan, Nicolas Heess, Lucy Gonzalez, Simon Osindero, Sherjil Ozair, Scott Reed, Jingwei Zhang, Konrad Zolna, Jeff Clune, Nando de Freitas, Satinder Singh, 和 Tim Rocktäschel. Genie: 生成性互动环境. 在国际机器学习大会，2024. Muai Reube Tan, Jiai Zhang, Bocheg Zou, Kai Zhang, Fng Yao, Fangi Zhu, Jing G, Yi Zhong, uza Shang, Yao Dou, Jaden Park, Jianfeng Gao, Yong Jae Lee, 和 Jianwei Yang. Temporalbench: 多模态视频模型的细粒度时间理解基准. arXiv 预印本 arXiv:2410.10818, 2024. Joao Carreira, Eric Noland, Andras Banki-Horvath, Chloe Hiller, 和 Andrew Zisserman. 关于 kinetics-600 的简短说明. arXiv 预印本 arXiv:1808.01340, 2018. Jo Carreira, Eric Noland, Chloe Hillier, 和 Andrew Zisserman. 关于 kinetics-700 人类动作数据集的简短说明. arXiv 预印本 arXiv:1907.06987, 2019. Jo Carreira, Dilara Gokay, Michael King, Chuhan Zhang, Iacio Rocco, Aravindh Mahendran, Thomas Abert Keck, Joseph Heyward, Skanda Koppula, Etienne Pot, Goker Erdogan, Yana Hasson, Yi Yang, Klaus Gre, Gullaume Le Moing, Sjerd van Steenkiste, Daniel Zoran, Drew A. Hudson, Pedro Vélez, Luisa Polanía, Luke Friedman, Chris Duvarney, Ross Goroshin, Kelsey Alln, Jacob Walker, Rishabh Kabra, Eric Aboussouan, Jennifer Sun, Thomas Kipf, Carl Doersch, Viorica Ptrucean, Dima Damen, Pauline Luc, Mehdi S. M. Sajjadi, 和 Andrew Zisserman. 扩展 4D 表示. arXiv 预印本 arXiv:2412.15212, 2024. Raja Chatila 和 Jean-Paul Laumond. 移动机器人的位置参考和一致世界建模. 在《IEEE国际机器人与自动化会议论文集》，第 2 卷，页面 138-145, 1985.

叶夫根尼·切博塔尔、卡罗尔·豪斯曼、姚璐、特德·肖、德米特里·卡拉什尼科夫、杰克·瓦利、亚历克斯·伊尔潘、本杰明·艾森巴赫、瑞安·朱利安、切尔西·芬恩和谢尔盖·莱文。可操作模型：无监督离线强化学习机器人技能。arXiv预印本 arXiv:2104.07749，2021年。 陈哲、王伟云、曹悦、刘扬州、高张炜、崔而菲、朱金国、叶胜龙、田浩、刘照阳、顾立欣、王学辉、李青云、任一敏、陈子轩、罗家鹏、王家豪、姜博、何从辉、施博天、张兴城、吕汉、王怡、邵文启、储培、涂中英、何彤、吴志勇、邓辉鹏、葛佳烨、陈凯、张开鹏、王立敏、窦敏、陆乐维、朱熙周、卢彭、林大华、乔宇、戴继锋和王文海。通过模型、数据和测试时间扩展开放源代码多模态模型的性能边界。arXiv预印本 arXiv:2412.05271，2024年。 张贤俊、安德烈亚·马多托、埃弗罗西尼·马夫罗乌迪、特里安塔菲洛斯·阿弗拉斯、图沙尔·纳加拉詹、穆罕默德·马阿兹、耶尔·宋、马腾宇、胡书铭、苏约格·贾因、米格尔·马丁、王慧宇、哈努娜·拉希德、孙佩泽、黄博耀、丹尼尔·博利亚、尼基拉·拉维、沙尚克·贾因、塔米·斯塔克、肖恩·穆恩、巴巴克·达马万迪、维维安·李、安德鲁·维斯特伯里、萨尔曼·汗、菲利普·克雷恩布赫、皮奥特·多拉、洛伦佐·托雷萨尼、克里斯滕·格拉曼和克里斯托夫·费希滕霍夫。感知语言模型：开放获取的数据和模型，用于详细的视觉理解。arXiv预印本 arXiv:2504.13180，2025年。 AnyClarWhateverextprediciv brais suat agets，andhefutur cgitiv scincBehavil brain sciences，36(3)：181204，2013年。 丹尼尔·科雷斯、迈克尔·多肯瓦尔德、曼努埃尔·穆岑提斯、切斯·G·M·斯诺克和雪木·阿萨诺。Tvbench：重新设计视频-语言评估。arXiv预印本 arXiv:2410.07752，2024年。 肯尼斯·詹姆斯·威廉斯·克雷克。《解释的本质》，第445卷。剑桥大学出版社，1967年。 迪玛·达门、海泽尔·道赫提、乔瓦尼·玛利亚·法里内拉、安东尼诺·富纳里、马健、埃万杰洛斯·卡扎科斯、达维德·莫尔蒂桑提、乔纳森·穆罗、托比·佩雷特、威尔·普赖斯和迈克尔·雷。重标定自我中心视觉：对于史诗厨房100的收集、管道和挑战。《计算机视觉国际杂志》（IJCV），130：3355，2022年。https://doi.org/10.1007/s11263-021-01531-2。 蒂莫泰·达塞、马克西姆·奥夸布、朱利安·梅拉尔和皮奥特·博扬诺夫斯基。视觉变换器需要寄存器。第十二届国际学习表示会议，2024年。 内哈·达斯、莎拉·贝希特尔、托多尔·达夫切夫、迪内什·贾亚拉曼、阿克沙拉·赖和弗兰齐斯卡·迈尔。基于模型的逆强化学习从视觉演示中学习。机器人学习会议（CoRL），2020年。 贾登·韦·东、理查德·索彻、李佳、凯·L和李飞飞。Imageet：大规模层次化数据库。《IEEE计算机视觉与模式识别会议论文集》，页面248255，2009年。 亚历克谢·多索维茨基、卢卡斯·贝耶尔、亚历山大·科列斯尼科夫、迪尔克·韦森博恩、夏小华、托马斯·温特希纳、莫斯塔·德赫哈尼、马蒂亚斯·温德勒、乔治·海戈尔、希尔万·吉利、雅各布·乌斯科雷特和尼尔·霍尔斯比。一幅图像值得16x16个词：用于规模图像识别的变换器。arXiv预印本 arXiv:2010.11929，2020年。 丹尼·德里斯、费毅、梅赫迪·S·M·萨贾迪、科里·林奇、阿卡恩克夏·乔达雷、布莱恩·伊赫特、阿扬·瓦希德、乔纳森·汤普森、关·武庸、田和宇、黄文龙、叶夫根尼·切博塔尔、皮埃尔·塞尔曼特、丹尼尔·达克沃斯、谢尔盖·莱文、文森特·范霍克、卡罗尔·豪斯曼、马克·图萨恩、克劳斯·格雷夫、安迪·曾、伊戈尔·莫达奇和皮特·弗洛伦斯。PaLM-E：一个具身多模态语言模型。arXiv预印本 arXiv:2023.03378，2023年。 于伦·杜、雪莉·杨、戴博、韩俊·戴、奥菲尔·纳赫姆、乔什·特能鲍姆、戴尔·舒尔曼斯和彼得·阿贝尔。通过文本引导视频生成学习通用策略。神经信息处理系统进展，36：91569172，2023年。 于伦·杜、雪莉·杨、皮特·弗洛伦斯、费毅、阿扬·瓦希德、布莱恩·伊赫特、皮埃尔·塞尔曼特、田和宇、彼得·阿贝尔、乔舒亚·B·特能鲍姆、莱斯利·帕克·凯尔布林、安迪·曾和乔纳森·汤普森。视频语言规划。ICLR，2024年。 弗雷德里克·埃伯特、切尔西·芬恩、亚历克斯·X·李和谢尔盖·莱文。具有时间跳跃连接的自我监督视觉规划。CoRL，12（16）：23，2017年。 弗雷德里克·埃伯特、切尔西·芬恩、苏迪·达萨兰达、安妮·谢、亚历克斯·李和谢尔盖·莱文。基于视觉的机器人控制的深度强化学习。arXiv预印本 arXiv:1812.00568，2018年。 大卫·安、盛刚·童、贾晨·朱、库斯图·辛哈、庄璐、陈新丽、迈克尔·拉巴特、尼古拉斯·巴拉斯、扬·勒昆、阿米尔·巴尔和赛宁·谢。无语言视觉表征学习的扩展。arXiv预印本 arXiv:2504.01017，2025年。 恩里科·芬尼、穆斯塔法·舒科尔、刘秀军、菲利普·杜夫特、大卫·哈尔迪曼、赛·艾萨拉朱、维克多·吉尔赫尔·图里西·达·科斯塔、路易斯·贝图恩、詹·甘、亚历克萨·T·托谢夫、马尔钦·艾希纳、莫因·纳比、尹飞·燕、乔什·苏斯基安和阿尔迪·努比。多层表征的自我监督学习。arXiv预印本 arXiv:2411.14402，2024年。 切尔西·芬恩和谢尔盖·莱文。深度视觉预测规划机器人运动。在2017年IEEE国际机器人与自动化会议（ICRA）上，页面27862793。IEEE，2017年。 切尔西·芬恩、伊恩·古德费洛和谢尔盖·莱文。通过视频预测进行的物理交互的无监督学习。神经信息处理系统进展，29，2016年。 卡特·阿卡库卡瓦勒尔·列文十、马利索韦尔·皮克西维德为打台球。arXiv预印本 arXiv:1511.07404，2015年。 卡尔·弗里斯顿。自由能原理：统一大脑理论。《自然评论神经科学》，11（2）：12738，2010年。 洛·戈、乔纳森·托、巴伯·阿巴西、斯特尔·比德曼、西德·布莱克、安东尼·迪奥·查尔·福斯特、劳拉·戈德、杰弗里·许、阿兰·勒·诺阿赫、郝南·李、基尔·麦克唐纳、尼克拉斯·穆尼希霍夫、克里斯·奥恰帕、杰森·方和拉里亚·雷诺兹，以及海莉·肖科普夫、阿维亚·斯科隆、林唐·苏塔维卡、埃里克·唐、阿尼许·萨亨、本·王、凯文·王和安迪·周。少样本语言模型评估框架，07 2024。https://zenodo.org/records/1260602。 迪比亚·戈什、阿比谢克·古普塔、阿什温·雷迪、贾斯汀·福、科琳·德文、本杰明·艾森巴赫和谢尔盖·莱文。通过迭代的监督学习学习达成目标。arXiv预印本 arXiv:1912.06088，2019年。 詹姆斯·J·吉布森。《生态视觉知觉法：经典版》。心理学出版社，1979年。 拉贾夫·戈亚尔、萨米拉·艾布拉西米·卡霍、文森特·米哈尔斯基、乔安娜·马特尔兹基、苏珊娜·韦斯特法尔、海纳·金、瓦伦丁·哈内尔、英戈·弗林德、彼得·杨尼罗斯、莫里茨·穆尔尔-弗赖塔、弗洛里安·霍普、克里斯蒂安·图拉乌和英戈·巴克斯，以及罗兰·梅米塞维奇。用于学习和评估视觉常识的“某些某些”视频数据库。在IEEE国际计算机视觉会议论文集上，页面58425850，2017年。 阿隆·格拉特福里、阿比哈维·杜贝、阿比哈维·贾霍里、阿比哈维·潘德、阿比希克·卡迪安、艾哈迈德·阿尔·达赫勒、艾莎·莱特曼、阿基尔·马图尔、阿兰·谢尔滕、亚历克斯·沃恩等。拉马3系列模型。arXiv预印本 arXiv:2407.21783，2024年。 让·巴斯蒂安·格里尔、弗洛里安·斯特鲁布、弗洛朗·阿尔图、科伦丁·塔勒克、皮埃尔·H·里歇蒙、埃琳娜·布哈茨卡亚、卡尔·多尔什、贝尔纳多·阿维拉·皮雷斯、赵汉·丹尼尔·郭、穆罕默德·盖什拉吉·阿扎尔、比拉尔·皮奥特、科雷·卡武库奥格鲁、雷·穆希斯和米哈乌·瓦尔克。自我监督学习的引导潜在新方法。神经信息处理系统进展，33：2127121284，2020年。 阿吉·古普塔、斯蒂芬·田、张云志、吴佳俊、罗贝托·马林-马特因和李飞飞。Maskvit：用于视频预测的掩蔽视觉预训练。arXiv预印本 arXiv:2206.11894，2022年。 大卫·哈阿和白根·施密特。世界模型。arXiv预印本 arXiv:1803.10122，2018年。 达尼贾尔·哈夫纳、蒂莫西·利利克拉普、吉米·巴和穆罕默德·诺鲁兹。控制的梦想：通过潜在的想象学习行为。arXiv预印本 arXiv:1912.01603，2019a。 大卫·哈，洪恺·李和詹姆斯·拉沃尔。拉丁动态为从像素规划。在国际机器学习会议上，页面25552565。PMLR，2019b。 达尼尔·哈夫纳、尤尔基斯·帕苏基斯、吉米·巴和蒂莫西·利利克拉普。超级分母蜂：基于结构的世界模型。在arXiv预印本 arXiv:2301.04104，2023年。 阿尔·哈格尔、埃利·巴库赫、阿特利·科森、鲁阿·本·阿利、莱安德罗·冯·维拉和马尔蒂·纳基。超融合计算最优训练，超越固定训练周期。神经信息处理系统进展，37：7623276264，2024年。 尼卡斯·汉森、肖戈·王和豪特梅尔·罗尔。预期控制器Xi。arXiv预印本 arXiv:2203.04955，2022年。 尼克拉斯·汉森、何苏和肖龙·王。Td-mpc：适用于连续控制的可扩展、稳健的世界模型Xi。arXiv预印本 arXiv:2310.16828，2023年。 乔恩·希尔。用移动相机实时控制机器人。在第九届国际工业机器人研讨会上，页面233245，1979年。 安东尼·胡、洛伊德·拉塞尔·哈德逊·叶奥、扎克·穆雷兹、乔治·费多谢夫、亚历克斯·肯德尔、杰米·肖顿和吉安卢卡·科拉多。盖亚1：用于自动驾驶的生成世界模型。arXiv预印本 arXiv:2309.17080，2023年。 爱德华·S·胡、关俊·安、刘青华、卓然·徐、阿达·兰福德、迪内什·贾亚拉曼、亚历克斯·兰博和约翰·兰戈。通过强化学习实现目标。arXiv预印本 arXiv:2410.23506，224年。 安德鲁·杰格尔、塞巴斯蒂安·博尔戈德、让-巴蒂斯特·阿拉亚克、卡尔·多尔什、卡塔林·约内斯库、大卫·丁、斯坎达·科普拉、丹尼尔·佐兰、安德鲁·布洛克、埃文·谢尔哈默、奥利维耶·赫纳夫、马修·M·博特维尼克、安德鲁·齐瑟曼、奥里奥尔·维尼亚尔斯和朱·卡雷拉。Perceiver io：一种用于结构化输入和输出的通用架构。arXiv预印本 arXiv:2107.14795，2021年。 埃里克·张、亚历克斯·伊尔潘、莫希·哈萨里、丹尼尔·卡普勒、弗雷德里克·埃伯特、科里·林奇、谢尔盖·莱文和切尔西·芬。Bc-z：通过机器人模仿学习的零样本任务泛化。在机器人学习会议上，页面9911002。PMLR，2022年。 威尔·凯、若昂·卡里拉、卡伦·西蒙扬、布莱恩·张、克洛伊·希利尔、苏迪赫德拉·维贾亚纳拉希姆汉、法比奥·维奥拉、蒂姆·格林、特雷弗·巴克、保罗·纳特塞夫、穆斯塔法·斯莱曼和安德鲁·齐瑟曼。Kinetics人类动作视频数据集。arXiv预印本 arXiv:1705.06950，2017年。

亚历山大·哈扎茨基，卡尔·佩尔茨，苏拉吉·奈尔，阿什维·巴拉克里希纳，苏迪普·达萨里，西达特·卡拉梅奇，索鲁什·纳西里亚尼，莫汉·库马尔·斯里拉马，劳伦斯·云良·陈，克尔斯蒂·埃利斯，彼得·大卫·法根，乔伊·赫洛，玛莎·伊特基亚，玛丽昂·勒佩尔特，叶承·杰森·马，帕特里克·特里·米勒，吉米·吴，苏尼尔·贝尔克哈勒，希文·达斯，匡·哈，阿尔汉·贾茨，亚伯拉罕·李，李英焕，玛里乌斯·梅梅尔，朴成宰，伊利贾·拉多萨沃维奇，王凯源，阿尔伯特·詹，凯文·布莱克，郑池，凯尔·贝尔特伦·哈奇，单林，陆静培，简·梅卡特，阿卜杜勒·雷曼，帕纳格·R·桑凯提，阿尔基特·夏尔马，科迪·辛普森，关·武昂，霍默·里奇·沃克，布雷克·乌尔夫，泰德·肖，乔纳森·希永·杨，阿雷费·雅瓦里，赵宗伟，克里斯托弗·阿吉亚，罗汉·拜贾尔，马特奥·瓜曼·卡斯特罗，达芙妮·陈，祁宇·陈，特林蒂·钟，贾伊敏·德雷克，伊桑·保罗·福斯特，詹森·高，维托尔·圭齐林，哈维尔·安东尼奥·埃雷拉，霍敏·许，凯尔·徐，贾恒·胡，穆罕默德·祖拜尔·伊尔沙德，多诺文·杰克逊，夏洛特·乐，李云霜，凯文·林，罗伊·林，马泽汉，阿比拉姆·马杜库里，苏维尔·米尔昌达尼，丹尼尔·莫顿，托尼·阮，阿比盖尔·奥尼尔，罗萨里奥·斯卡利斯，德里克·西尔，维克托·孙，史蒂芬·田，艾米·陈，安德鲁·E·王，吴怡琳，安妮·谢，杨金，帕特里克·尹，严云初·詹，奥斯伯特·巴斯塔尼，格伦·伯斯特，珍妮特·博格，肯·戈德堡，阿比·古普塔，阿比谢克·古普塔，迪内什·贾亚拉曼，约瑟夫·J·林，季滕德拉·马利克，罗伯托·马丁·马丁，苏布拉马尼安·拉马穆尔西，多尔萨·萨迪赫，舒然·宋，吴嘉俊，迈克尔·C·叶，朱宇克，托马斯·科拉尔，谢尔盖·莱维因，切尔西·芬。Droid：一个大规模野外机器人操作数据集。arXiv预印本 arXiv:2403.12945，2024。金无金，卡尔·佩尔茨，西达特·卡拉梅奇，泰德·肖，阿什温·巴拉克里希纳，苏拉吉·奈尔，拉斐尔·拉法伊洛夫，伊桑·福斯特，格蕾丝·林，帕纳格·桑凯提，关·武昂，托马斯·科拉尔，本杰明·布赫费尔，拉斯·泰德雷克，多尔萨·萨迪赫，谢尔盖·莱维因，佩尔西·梁，切尔西·芬。Openvla：一个开源的视觉-语言-动作模型。arXiv预印本 arXiv:2406.09246，2024。本·克罗耶尔，莫基塔·科梅伊尔，坎达斯·罗斯，昆提·加里多，库斯图夫·辛哈，尼古拉斯·巴拉斯，米多·阿斯兰。一个关注捷径的视频问答基准，通过最小视频对进行物理理解。预印本，2024。帕特里克·兰开斯特，尼克拉斯·汉森，阿拉文德·拉杰斯瓦兰，维卡什·库马尔。Modem-v2 视觉-运动世界模型用于现实世界的机器人操作。发表于IEEE国际机器人与自动化会议（ICRA），页7530-7537，2024。扬·勒昆。通往自主机器智能的道路版本0.9.2，2022-06-27。开放评论，62(1)：162 2022。乔·李，金美花，洛雷兹·韦鲁森，瓦德·科尔顿和马克·赫特。学习在具有挑战性的地形上的四轮驱动。科学机器人，5(47)：eabc5986，2020。博·李，佩宇·张，凯琛·张，范怡·普，昕润·杜，宇浩·董，霍天·刘，元涵·张，葛·张，春源·李，子维·刘。Lmms-eval：加速大型多模态模型的发展，2024年3月。https://github.com/EvolvingLMMs-Lab/lmms-eval。博·李，元涵·张，董·郭，任锐·张，冯·李，浩·张，凯琛·张，佩宇·张，艳伟·李，子维·刘，春源·李。Llava-onevision：轻松的视觉任务转移。arXiv预印本 arXiv:2408.03326，2024。李昆昌，王雅莉，何意楠，李怡竹，王毅，刘怡，王尊，徐纪岚，陈果，罗平，王利民，谌宇。Mvbench：一个全面的多模态视频理解基准。发表于IEEE/CVF计算机视觉与模式识别会议，页22195-22206，2024年。李应伟，李怡，努诺·瓦斯科内洛斯。Resound：无代表性偏见的动作识别。发表于欧洲计算机视觉会议（ECCV），页513-528，2018。林素怡，普里·戈亚尔，罗斯·吉尔希克，何凯铭，彼得·美元。Focals rdensebjecdetectin。发表于IEEE国际计算机视觉会议，页2980-2988，2017。范·恩·浩，卢迪·格罗夫和派比·掩蔽可扩展化分析。神经信息处理系统进展，35：12608-12618，2022。霍天·刘，春源·李，宇恒·李，李永爵。改进的基准与视觉指令微调。发表于IEEE/CVF计算机视觉与模式识别会议，页2629-26306，2024年。霍黎，宇恒·李，博·李，元涵·张，沈声，杨杰·李。Llavaex：推理、知识与世界。202b年1月。https://llava-vl.github.io/blog/20240130-avaext/。刘元辛，李世诚，刘逸，王宇翔，任舒怀，李雷，陈思硕，孙旭，侯录。TempCompass：视频LLM真的理解视频吗？arXiv预印本 arXiv:2403.00476，2024。朱志，刘玲，石白芬，张莹，露尤米，尚阳，浩诚·西，诗瑶，余曦，李大成，李秀宇，云浩·方，禹康·陈，谢成宇，黄德安，郑安杰，维什维什·纳斯，胡金怡，薛冬，任央，王晓龙，巴夫罗·莫尔查诺夫，简·考茨，洪谷·尹，孙涵，姚璐。NVILA：高效边界视觉语言模型。arXiv预印本 arXiv:2412.04468，2024。伊利亚·洛希尔洛夫和弗兰克·哈特。SGDR：具有温启动的随机梯度下降。arXiv预印本 arXiv:1608.03983，2016。伊利亚·洛希尔洛夫和弗兰克·哈特。解耦权重衰减正则化。arXiv预印本 arXiv:1711.05101，2017。科里·林奇，穆希·坎萨里，泰德·肖，维卡什·库马尔，乔纳森·汤普森，谢尔盖·莱维因，皮埃尔·塞尔曼。学习潜在计划。发表于机器人学习会议，页1113-1132。PMLR，2020。uu在基于模型的强化学习中。arXiv预印本 arXiv:2009.05085，2020。乔安娜·马特日欣斯卡，纪尧姆·贝尔热，因戈·巴克，和罗兰·梅米塞维奇。小丑数据集：一个大规模的视频数据集，人类手势。发表于IEEE/CVF国际计算机视觉会议，页00，2019。安特·米赫，迪米·朱科夫，尚-巴蒂斯特·阿拉亚克，马克·塔帕斯维，伊万·拉普捷夫和霍塞·西维奇。HowTo10m：通过观看数百万首视频片段学习文本-视频嵌入。发表于IEEE/CVF国际计算机视觉会议，页2630-2640，2019。希曼吉·米塔尔，纳库尔·阿加瓦尔，邵元·洛，权晶俊。没有打破一些蛋就做不成煎蛋卷。使用大型视频-语言模型进行可信的动作预测。发表于IEEE/CVF计算机视觉与模式识别会议，页18580-18590，2024。阿努沙·纳加班迪，库尔特·科诺利杰，谢尔盖·莱维因，维卡什·库马尔。用于学习灵巧操作的深度动态模型。发表于机器人学习会议，页1101-1112。PMLR，2020。苏拉吉·奈尔，阿拉文德·拉杰斯瓦兰，维卡什·库马尔，切尔西·芬，阿比纳夫·古普塔。R3M：机器人操作的通用视觉表示。发表于机器人学习会议（CoRL），2022。诺拉·诺特曼，萨沙·雷考兹克，塞利姆·奥纳特，彼得·科宁，迪尔克·扬克。初级视觉皮层表示过去与现在之间的差异。大脑皮层，25(6)：1427-1440，2015。八爪模型团队，迪比亚·戈什，霍默·沃克，卡尔·佩尔茨，凯文·布莱克，欧尔·梅斯，苏迪普·达萨里，乔伊·赫洛，托比亚斯·克雷曼，查尔斯·许，简兰·罗，尤·梁·谭，劳伦斯·云良·陈，帕纳格·桑凯提，关·武昂，泰德·肖，多尔萨·萨迪赫，切尔西·芬，谢尔盖·莱维因。Octo：一个开源通用机器人策略。arXiv预印本 arXiv:2405.12213，2024。马克西姆·奥夸布，蒂莫西·达尔塞，西奥·穆塔卡尼，霍伊·沃，马克·萨夫兰尼克，瓦西尔·哈利多夫，皮埃尔·费尔南德斯，丹尼尔·哈齐扎，弗朗西斯科·马萨，阿拉埃尔丁·埃尔-诺比，马哈茂德·阿斯兰，尼古拉斯·巴拉斯，乌戈尔·加卢巴，拉塞尔·霍斯，霍耀·黄，尚文·李，伊尚·米斯拉，迈克尔·拉巴特，瓦苏·夏尔玛，加布里埃尔·西纳夫，胡·徐，赫尔维·杰格，朱莉安·马伊拉尔，帕特里克·拉巴图，阿尔芒·朱林，和皮奥特·博贾诺夫斯基。DINOv2：无监督学习鲁棒视觉特征。arXiv预印本 arXiv:2304.07193，2023。维奥里卡·普尔楚安，卢卡斯·斯迈拉，安库什·古普塔，阿德里亚·雷卡森斯·大陆，拉里萨·马基娃，迪伦·巴纳尔斯，斯坎达·科普普拉，约瑟夫·海沃德，马特乌斯·马里诺夫斯基，易·杨，卡尔·多赫，塔季亚娜·马泰约维奇，尤里·苏尔斯基，安托瓦内·米赫，亚历克斯·弗雷切特，汉娜·克里姆查克，拉斐尔·科斯特，简林·张，斯特凡尼·温克勒，尤素夫·艾塔尔，西蒙·奥辛德罗，迪马·达门，安德鲁·齐泽曼，和贾奥·卡雷拉。感知测试：多模态视频模型的诊断基准。神经信息处理系统进展，36：42748-42761，2023。

Qwen团队，帅白，克勤陈，雪晶刘，佳霖王，文斌葛，思博宋，凯唐，鹏王，世界王，俊唐，胡门钟，元志朱，明坤杨，钊海李，建强万，鹏飞王，伟丁，哲仁傅，逸衡徐，佳博叶，希张，天宝谢，泽森程，航张，志博杨，海洋徐，俊扬林，安杨，彬源惠，博文余，晨程，达宜恒刘，范洪，飞黄，佳伟陆，金旭，建宏图，嘉银曾，杰张，金基王，建伟展，景仁周，克欣杨，美丽，明妍，娜妮，瑞门，松涛姜，小冬邓，小明黄，希明周，兴章任，扬帆，怡畅张，逸琪朱，玉琼刘，志方郭。Qwen2.5-vl技术报告，arXiv预印本 arXiv:2502.13923，2025年。Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Aske, Pamela Mishkin, Jack Clark, Gretchen Krueger, 和 Iya Sutskever。 从自然语言监督学习可转移的视觉模型。在国际机器学习会议，页8748-8763，2021年。Jathushan Rajasegaran，Lija Radosavovic，Rahul Ravishankar，Yossi Gandelsman，Christoph Feichtenhofer，和 Jitendra Malik。 自回归预训练视频的实证研究。arXiv预印本 arXiv:2501.05453，2025年。Rajeh PN Rao 和 Dana H Ballard。 视觉皮层中的预测编码：一些超经典感受野效应的功能解释。自然神经科学，2(1)：79-87，1999年。Debaditya Roy，Ramanathan Rajendiran，和 Basura Fernando。 交互区域视觉变换器或视图中心动作预测。在IEEE/CVF计算机视觉应用冬季会议论文集，页6740-6750，2024年。Reuve.Rubinst优化稀有事件的计算模型。《欧洲开放研究杂志》，99：89-112，1997年。Loy Russell Anthony Hu，Lorenzo Bertoni，George Fedoseev，Jamie Shotton，Elahe Arani，和 Gianluca Corrad。Gaia-：一种可控的多视角生成世界模型用于自动驾驶。arXiv预印本 arXiv:2503.20523，2025年。

Mohammad Reza Samsami, Artem Zholus, Janarthanan Rajendran 和 Sarath Chandar. 利用世界模型掌握记忆任务. 发表在国际学习表征会议, 2024. Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Gu, Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy Lillrap 和 David Silver. 通过规划学习模型掌握 Atari、O、国际象棋和将棋. 自然, 588(7839): 604609, 2020. Ziya Shangguan, Chuhan Li, Yuxuan Ding, Yanan Zheng, Yilun Zhao, Tesca Fitzgerald 和 Arman Cohan. Tomato: 评估多模态基础模型的视觉时间推理能力. arXiv 预印本 arXiv:2410.3266, 2024. Vlad Sobal, Wancong Zhang, Kynghyun Cho, Randall Balestriero, Tim GJ Rudner 和 Yann LeCun. 从稀疏奖励数据中学习: 使用潜在动态模型进行规划的案例. arXiv 预印本 arXiv:2502.14819, 2025. Janlin Su, Murtada Ahmed, Yu Lu, Shengfeng Pan, Wen Bo 和 Yunfeng Liu. Roformer: 带旋转位置嵌入的增强型变换器. 神经计算, 568: 127063, 2024. RrBrv l 认知与脑理论, 4(3): 217246, 1981. Richard Sutton 和 Andrew G Barto. 强化学习: 导论, 第 1 卷. MIT出版社, 剑桥, USA, 1998. Yansong Tang, Dajun Ding, Yongming Rao, Yu Zheng, Danyang Zhang, Lili Zhao, Jiwen Lu 和 Jie Zhou. CoIN: 一个用于综合教学视频分析的大规模数据集. 在 IEEE/CVF 计算机视觉与模式识别会议论文集中, 页码 12071216, 2019. Manan Tomar, Philippe Hansen-Estruch, Philip Bachman, Alex Lam, John Langford, Matthew Taylor 和 Sergey Levine. 视频占用模型. arXiv 预印本 arXiv:2407.09533, 2024. Shengbang Tong, Elis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, Ziteng Wang, Rob Fergus, Yann LeCun 和 Saining Xie. Cambrian-1: 完全基于人类感觉的多模态探索. 神经信息处理系统, 37: 8731087356, 2024. Hugo Touvron, Andrea Vedaldi, Matthijs Douze 和 Hervé Jégou. 修正训练与测试分辨率不匹配的问题. 神经信息处理系统进展, 32, 2019. Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, Olivier Héna, Jeremiah Harmsen, Andreas Steiner 和 Xiaohua Zhai. SigLIP: 具有改进语义理解、定位和稠密特征的多语言视觉-语言编码器. arXiv 预印本 arXiv:2502.14786, 2025. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uzkoret, Llion Jones, Aida Gomez, Lukasz Kaiser 和 Ilya Polosukhin. 注意力就是你所需要的一切. 神经信息处理系统进展, 30, 2017. Matur Videau, Badr Youbidriss, Danel Haziza, Luca Wehrstet, Jd Copet, Oliver Teytau 和 Davi Lope Paz. Meta Lingua: 一个最小的 PyTorch LLM 训练库, 2024. https://github.com/facebookresearch/lingua. Shakt Wadekar, Abhishek Chaurasia, Aman Chadha 和 Eugenio Culurciello. 多模态模型架构的发展. arXiv 预印本 arXiv:2405.17927, 2024. Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yinan He, Yi Wang, Yali Wang 和 Yu Qiao. Videomae v2: 通过双重掩码扩展视频掩码自编码器. 在 IEEE/CVF 计算机视觉与模式识别会议论文集中, 页码 1454914560, 2023. Wang Shui Bai Sinaan Sij Wa Zhian Jiz Bai Keqihen Xueu JaWaW Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou 和 Junng Li. Qwen2-v: 提升视觉语言模型对世界的感知能力. arXiv 预印本 arXiv:2409.12191, 2024a. Y Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Chenti Wang, Guo Chen, Baoqi Pei, Ziang Yn, Ronku Zheng, Jilan Xu, Zun Wang, Yanog Shi, Tiaiang Jiang, Songze Li, Hongge Zhang, Yifi Huang, Yu Qiao, Yali Wang 和 Limin Wang. Internvideo: 为多模态视频理解扩展基础模型. 在欧洲计算机视觉会议上, 页码 396416. Springer, 2024b. Manuel Watter, Jost Springenberg, Joschka Boedecker 和 Martin Riedmier. 嵌入以控制: 一个用于原始图像控制的局部线性潜在动态模型. 神经信息处理系统进展, 28, 2015. Daniel M Wolpert 和 Zoubin Ghahramani. 运动神经科学的计算原理. 自然神经科学, 3(11): 12121217, 2000. Hongtao Wu, Ya Jing, Chilam Cheang, Guangzeng Chen, Jiafeng Xu, Xinghang Li, Minghuan Liu, Hang Li 和 Tao Kong. 解放大规模视频生成预训练以进行视觉机器人操作. arXiv 预印本 arXiv:2312.13139, 2023a.

Philpp Wu, Alejandro Escontrela, Daniar Hafner, Pieter Abbeel, 和 Ken Goldberg. Daydreamer: 物理机器人学习的世界模式. 发表在机器人学习会议，页面 2226-2240. PMLR, 2023b. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng L, F HuGu og Ho i Huan, JaltanJia WanJanYa JTu, J Zhan, Jiaxi Ma, Jiani Yang, Jin Xu, Jing Zhou, Jinze Bai, Jinzeg He, Junag Lin, Kai Dang, Kemi Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize G, Runji Lin, Shije Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xidong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yo, Yichang Zhang, u Wan, Yunfei Hu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, 和 Zhihao Fan. Qwen2 技术报告. arXiv 预印本 arXiv:2407.10671, 2024a. Mengo Yang, Yilun Du, Kamyr Ghasemipour, Jonathan Tompson, Dale Schuurmans, 和 Pieter Abbee. 学习互动式真实世界模拟器. 发表在国际学习表征会议，2024b. Yen-Chen MarBuzaan. hil Isolxpebevisal eh.IcRoot L 页面 1015-1024. PMLR, 2020. Lpi Yuan, Jiawei Wang, Haomi Sun, Yuchen Zhang, 和 Yuan Lin. Tarsier: 从详细视频描述到全面视频理解的大型视觉-语言模型的进展. arXiv 预印本 arXiv:2501.07888, 2025. Rowan Zellrs, Jiasen Lu, Ximing Lu, Youngjae Yu, Yanpeng Zhao, Mohammadreza Salehi, Aditya Kusupati, Jack Heel, A Farhadn Yeho Meroerv 神经网络. 发表在 IEEE/CVF 计算机视觉与模式识别会议论文集中，页面 16375-16387, 2022. Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, 和 Lucas Beyer. 扩展视觉变换器. 发表在 IEEE/CVF 计算机视觉与模式识别会议论文集中，页面 12104-12113, 2022. Hang Zhang, Xin Li, 和 Lidong Bing. Video-ama: 一种针对视频理解的指令调优音频-视觉语言模型. arXiv 预印本 arXiv:2306.02858, 2023. Kaichen Zhang, Bo Li, Peiyuan Zhang, Fany Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan Zhang, Jingkang Yang, Chunyuan Li, 和 Ziwei Liu. Lmms-eval: 对大型多模态模型评估的现实检查. arXiv 预印本 arXiv:2407.12772, 2024a. Yuanan Zhang, Bo Li, Haotian Liu, Yongjae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, 和 Chunyuan Li. LLaVA-NeXT: 强大的零-shot视频理解模型，2024年4月。https://llava-vl.github.io/blog/2024-04-30-llava-next-video/. Yuann Zhang, Jinmi Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, 和 Chuyuan Li. 视频指令合成数据. arXiv 预印本 arXiv:2410.02713, 2024c. Lo Zhao, Nitesh B. Gundavarapu, Liange Yuan, Hao Zhou, Shen Yan, Jennir JSun, Luke Friedman, Rui Qan, Tobias Weyand, Yue Zhao, Rachel Hornung, Florian Schrof, Ming-Hsuan Yang, David A. Ross, Huisheng Wang, Hartwig Adam, Mikhail Sirotenko, Ting Liu, 和 Boqing Gong. VideoPrism: 用于视频理解的基础视觉编码器. arXiv 预印本 arXiv:2402.13217, 2024. Qinqig Zhao, Yo Lu, Moo Jin Kim, Zipeng Fu, Zhuoyang Zhang, Yecheng Wu, Zhaoshuo Li, Qianli Ma, Song Han, Chelsea Finn, Ankur Handa, Ming-Yu Liu, Donglai Xiang, Gordon Wetzstein, 和 Tsung-Yi Lin. CoT-VLA: 视觉-语言-动作模型的视觉思维链推理. arXiv 预印本 arXiv:2503.22020, 2025. Rui Zheng, Jing Wang, Scott Ree, Johan Borck, Yu Fang, Fng Hu, Jel Jang, Kauhi Kundala, Zon Lin, Loic Magne, Avnish Narayan, You Liang Tan, Guanzhi Wang, Qi Wang, Jiannan Xiang, Yinzhen Xu, Seonghyeon Ye, Jan Kautz, Furong Huang, Yuke Zhu, 和 Linxi Fan. FLARE: 带隐式世界建模的机器人学习. arXiv 预印本 arXiv:2505.15659, 2025. Ga Zhou, Hengkai Pan, Yann LeCun, 和 Lerrel Pinto. DINO-WM: 基于预训练视觉特征的世界模型启用零-shot规划. arXiv 预印本 arXiv:2411.04983, 2024.

# 附录

# V-JEPA 2 预训练

# A.1 预训练超参数

如第.节所述，我们的训练流程包括两个阶段：恒定学习率阶段和冷却阶段。在第一个阶段，我们对模型进行训练，直到在IN1K、COIN和SSv2任务上观察到性能停滞或下降。此时，我们启动冷却阶段。表9 预训练超参数。大规模计算机视觉模型的预训练常见参数。我们报告这些参数，涵盖主要训练阶段和冷却阶段。

<table><tr><td>Parameter</td><td>Primary Phase</td><td>Cooldown Phase</td></tr><tr><td>Number of frames</td><td>16</td><td>64</td></tr><tr><td>Frames per Second</td><td>4.0</td><td>4.0</td></tr><tr><td>Crop Size</td><td>256</td><td>[256, 384, 512]</td></tr><tr><td>Random Resize Aspect Ratio</td><td>[0.75 1.35]</td><td>[0.75, 1.35]</td></tr><tr><td>Random Resize Scale</td><td>[0.3, 1.0]</td><td>[0.3, 1.0]</td></tr><tr><td>Steps</td><td>Variable</td><td>12000</td></tr><tr><td>Warmup Steps</td><td>12000</td><td>N/A</td></tr><tr><td>Batch Size (global)</td><td>3072</td><td>3072</td></tr><tr><td>Starting Learning Rate</td><td>1e-4</td><td>5.25e-4</td></tr><tr><td>Final Learning Rate</td><td>5.25e-4</td><td>1e-6</td></tr><tr><td>Weight Decay</td><td>0.04</td><td>0.04</td></tr><tr><td>EMA</td><td>0.99925</td><td>0.99925</td></tr><tr><td>Spatial Mask Scale</td><td>[0.15, 0.7]</td><td>[0.15, 0.7]</td></tr><tr><td>Temporal Mask Scale</td><td>[1.0, 1.0]</td><td>[1.0, 1.0]</td></tr><tr><td>Mask Aspect Ratio</td><td>[0.75 1.5]</td><td>[0.75, 1.5]</td></tr><tr><td>Tubelet Size</td><td>2</td><td>2</td></tr><tr><td>Patch Size</td><td>16</td><td>16</td></tr></table>

第一阶段的训练开始时进行了2000步的学习率预热，随后保持恒定的学习率进行其余阶段。我们每60,000步进行一次评估。冷却阶段的学习率为5.25e-4，随后线性降低到最终学习率。在两个阶段中，其他所有超参数保持不变。在冷却阶段，我们增加了每个剪辑的帧数，同时保持每秒帧数不变，因为我们发现向模型输入更多帧有显著的好处（见图5）。此外，我们还在此阶段增加了模型的裁剪尺寸，这对像IN1K这样的任务带来了显著的好处，256裁剪时得分为84.6，而384裁剪时得分提高到了85.1。两个阶段的超参数汇总在表9中。在附录中，我们提到了一种“简化”训练方案，对应于按照Bardes等（2024）的方法进行的90,000步训练。简化方案与常规方案的几个关键区别：首先是学习率：简化方案以学习率预热开始，随后由余弦衰减。其次是权重衰减和指数移动平均（EMA）的调度，这些从起始值线性增加到最终值。最后是总步数，限制为90,000步。我们在多个数据混合的消融实验中使用简化调度，因为这使我们能够在更短的计算预算内探讨数据整理的影响。

# A.2 预训练数据

我们开始对 YT1B 进行整理，采用 PySceneDetect 库进行场景提取，该库在场景切换时将视频切分为多个片段。我们丢弃时长少于 4 秒的场景，保留了 3.16 亿个场景。表 10 简略预训练超参数。针对我们简化的配方，列出了大型计算机视觉模型的常见预训练参数。

<table><tr><td>Parameter</td><td>Abbreviated Recipe</td></tr><tr><td>Number of frames</td><td>16</td></tr><tr><td>Frames per Second</td><td>4.0</td></tr><tr><td>Crop Size</td><td>256</td></tr><tr><td>Random Resize Aspect Ratio</td><td>[0.75 1.35]</td></tr><tr><td>Random Resize Scale</td><td>[0.3, 1.0]</td></tr><tr><td>Steps</td><td>90000</td></tr><tr><td>Warmup Steps</td><td>12000</td></tr><tr><td>Batch Size (global)</td><td>3072</td></tr><tr><td>Starting Learning Rate</td><td>2e-4</td></tr><tr><td>Larning Rate</td><td>6.25e-4</td></tr><tr><td>Final Learning Rate</td><td>1e-6</td></tr><tr><td>Starting Weight Decay</td><td>0.04</td></tr><tr><td>Final Weight Decay</td><td>0.4</td></tr><tr><td>Starting EMA</td><td>0.999</td></tr><tr><td>Final EMA</td><td>1.0</td></tr><tr><td>Spatial Mask Scale</td><td>[0.15, 0.7]</td></tr><tr><td>Temporal Mask Scale</td><td>[1.0, 1.0]</td></tr><tr><td>Mask Aspect Ratio</td><td>[0.75 1.5]</td></tr><tr><td>Tubelet Size</td><td>2</td></tr><tr><td>Patch Size</td><td>16</td></tr></table>

然后将 DINOv2 ViT-L 模型应用于每个片段的中间帧以提取场景嵌入。接着，将 YT1B 嵌入聚类为 150 万个聚类，使用与 Oquab 等人（2023）相同的聚类策略。对于目标分布中的所有视频，也采用相同的方法提取嵌入，然后将其分配给最接近的 YT1B 聚类。我们仅保留那些至少有一个目标视频分配的聚类——从最初的 150 万个聚类中保留约 21 万个聚类。保留的聚类包含 1.15 亿个场景。

基于聚类的检索匹配了支持，但未匹配目标分布的加权。我们使用一种加权样本方案来计算 $w_c = \sum_{d=1}^{D} w_d \times \frac{N_{d,c}}{N_d}$，其中 $w_c$ 是第 $c$ 个目标数据集的加权；$w_d$ 是第 $d$ 个目标数据集（参见表 11）、$N_{d,c}$ 是第 $c$ 个聚类中第 $d$ 个数据集中样本的数量，$N_d$ 是第 $d$ 个数据集中的总样本数，$D$ 是目标数据集的总数。我们根据每个目标数据集检索到的场景数量大致分配检索权重，并为 EpicKitchen 赋予了一些额外的权重。这产生了一个最终策划的数据集，其统计数据更接近于文献中手工制作的数据集。我们发现，在孤立使用策划的 YT1B 代替未策划的对应数据集时，在下游理解任务上取得了更好的结果（见图 4）。表中包含从 YT1B 中提取的最后统一数据集统计。最后一行包括 K710、SSv2、COIN 和 EpicKitchen 检索中的重复项。

<table><tr><td>Retrieval Target</td><td>Cluster Count</td><td>Number of Scenes</td><td>Retrieval Weight</td></tr><tr><td>Uncurated YT1B</td><td>1.5M</td><td>316M</td><td></td></tr><tr><td>K710</td><td>170k</td><td>100M</td><td>0.7</td></tr><tr><td>SSv2</td><td>41k</td><td>19M</td><td>0.125</td></tr><tr><td>COIN</td><td>37k</td><td>21M</td><td>0.125</td></tr><tr><td>EpicKitchen</td><td>4k</td><td>13k</td><td>0.05</td></tr><tr><td>Final Curated (includes duplicates)</td><td>210k</td><td>115M</td><td></td></tr></table>

使用这种策略检索到的聚类和场景的总体统计信息汇总在表11中。整个数据集偏向于使用K710检索的聚类。这一点，加上其检索权重为0，使得整体策划的数据集具有明显的Kinetic权重，正如我们在K400性能中看到的反映（参见附录A.4.1）。如主文中的表1所示，我们将策划的YT1B与SSv2、Kinetics、HowTo100M和ImageNet结合，创建了最终的VM22M数据集。

# A.3 扩大模型规模

模型架构的详细信息如表1所示。所有模型都参数化为视觉变换器（vision transformers）Dosovitskiy等（2020），使用标准的 $16 \times 16$ 补丁大小。在扩展模型规模时，我们将编码器从 ViT-L（3亿参数）增加到 ViT-g（10亿参数），而预测器的大小在所有预训练实验中保持不变。

<table><tr><td>Model</td><td>Params</td><td>Width</td><td>Depth</td><td>Heads</td><td>MLP</td><td>Embedder</td></tr><tr><td>Encoders: Eθ(·)</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>ViT-L</td><td>300M</td><td>1024</td><td>24</td><td>16</td><td>4096</td><td>2 × 16 × 16 strided conv</td></tr><tr><td>ViT-H</td><td>600M</td><td>1280</td><td>32</td><td>16</td><td>5120</td><td>2 × 16 × 16 strided conv</td></tr><tr><td>ViT-g</td><td>1B</td><td>1408</td><td>40</td><td>22</td><td>6144</td><td>2 × 16 × 16 strided conv</td></tr><tr><td>Predictor: Pφ(·)</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>ViT-s</td><td>22M</td><td>384</td><td>12</td><td>12</td><td>1536</td><td>N.A.</td></tr></table>

# A.4 额外结果

# A.4.1 数据整理的影响

表13显示了数据管理在一部分下游分类任务中的结果。在该表中，我们使用原始VJEPA（Bardes等，2024）的简化训练方案训练了ViT-L和ViT-g规模的模型。当训练较小规模的模型（ViT-L）时，在经过整理的YT1B变体上训练模型相比于未整理的变体，能够在各个方面取得提升。然而，当转向混合数据设置（即添加图像和手选视频）时，在使用整理数据的情况下，部分任务的性能实际上会下降，例如在SSv2上，VM22M（混合+整理的YT1B）的性能为72.8，而混合$^+$未整理的YT1B为73.3。在某些情况下，仅使用整理YT1B训练的模型的表现优于混合数据训练的模型，例如在COIN（86.5对86.25）和K400（84.6对83.7）评估任务上。这一结果有些令人惊讶，因为尽管在混合设置中包含了K710训练数据，我们发现这并没有改善K400评估任务的性能。所有规模的模型均使用Bardes等（2024）的简化时间表进行预训练。

<table><tr><td>Training Data</td><td>IN1K</td><td>COIN</td><td>SSv2</td><td>K400</td></tr><tr><td>ViT-L</td><td></td><td></td><td></td><td></td></tr><tr><td>Uncurated YT1B</td><td>80.6</td><td>83.2</td><td>70.9</td><td>82.9</td></tr><tr><td>Curated YT1B</td><td>80.8</td><td>86.5</td><td>73.1</td><td>84.6</td></tr><tr><td>Mixed+Uncurated YT1B</td><td>82.9</td><td>86.25</td><td>73.3</td><td>83.0</td></tr><tr><td>VM22M</td><td>82.9</td><td>86.0</td><td>72.8</td><td>83.7</td></tr><tr><td>ViT-g</td><td></td><td></td><td></td><td></td></tr><tr><td>Uncurated YT1B</td><td>81.8</td><td>86.4</td><td>73.6</td><td>85.1</td></tr><tr><td>Curated YT1B</td><td>81.7</td><td>88.4</td><td>74.8</td><td>86.5</td></tr><tr><td>Mixed+Uncurated YT1B</td><td>83.7</td><td>88.5</td><td>75.5</td><td>85.9</td></tr><tr><td>VM22M</td><td>83.9</td><td>89.2</td><td>75.6</td><td>86.2</td></tr></table>

然而，这一行为在不同尺度上并不一致。在 ViT-g 尺度下，VM22M（混合+策划 YT1B）在所有任务上都优于混合+未经策划 YT1B。

![](images/12.jpg)  
Curated vs.Uncurated (Long Training w/ViT-g)   
Figure 12 Effect of data curation for V-JEPA 2 pre-training. We show model performance averaged across the IN1K, COI, SSv, and K400 tasks as a function of pre-rainin "epochs" (equivalent to 300 optiizatio eps). Models trained with and without uncurated data achieve similar performance until epoch 600, at which point the performance of the model trained with uncurated YT1B beings degrading.

在长时间训练的情况下进行这些任务时，我们继续观察到在ViT-g模型规模下，VM22M与Mixed+Uncurated YT1B之间的差异，如图12所示，该图比较了模型在IN1K、COIN、SSv2和K400图像理解任务上的平均表现。最初，这两个模型的提升速度大致相同，但在第600轮后，它们的表现开始分化，使用未经整理数据的模型未能继续改善。

# A.4.2 长时间训练计划和冷却期的影响

在表14中，我们展示了两阶段训练过程的效果。当与表13中的ViT-g结果进行比较时，我们发现缩短的训练时间表优于在冷却阶段之前的恒定学习率训练时间表。主要的好处是在冷却阶段实现的，该阶段结合使用64帧进行预训练并采用逐步降低的学习率。这使得在所有评估中的成绩提高超过1分。Tau

<table><tr><td>Training Stage</td><td>IN1K</td><td>COIN</td><td>SSv2</td><td>K400</td></tr><tr><td>Phase 1 (epoch 800, no cooldown)</td><td>83.8</td><td>89.1</td><td>75.1</td><td>85.8</td></tr><tr><td>Phase 2 (annealed, 256 × 256 resolution)</td><td>84.6</td><td>90.7</td><td>75.3</td><td>86.6</td></tr><tr><td>Phase 2 (annealed, 384 × 384 resolution)</td><td>85.1</td><td>90.2</td><td>76.5</td><td>87.3</td></tr></table>

# A.4.3 评估中视频长度的影响。

图13考察了输入视频时长在评估过程中如何影响下游任务性能。使用在64帧剪辑上预训练的模型，我们观察到在评估时将视频时长从1帧增加到4帧时，平均提高了9.7个百分点。请注意，由于内存限制，此消融实验使用了一次性评估（即每个视频仅抽样一个剪辑，而不是标准的多剪辑评估）。

![](images/13.jpg)  
Fu Ec  viuration durevaluation.Task peroran urteprov y une longer video clips. All evaluations use ViT-g models that were annealed with 64 frames at resolution $2 5 6 \times 2 5 6$ . Due tmrnueorsa oe processed at inference time boosts average performance by up to $+ 9 . 7$ points.

# B V-JEPA 2-AC 后训练

# B.1 训练后超参数

V-JEPA 2-AC模型使用AdamW优化器（Loshchilov和Hutter，2017）进行训练，采用预热-常量衰减的学习率策略，权重衰减常数为0.04。我们将学习率从$7 . 5 \times 1 0 ^ { - 5 }$线性预热到$4 . 2 5 \times 1 0 ^ { - 4 }$，共计4500次迭代，然后保持常量85500次迭代，最后在4500次迭代中将其衰减至0。我们使用批量大小为256，包含从Droid原始数据集中随机抽样的4秒视频片段，帧率为4fps。我们对Droid的左外部相机视角进行训练——也可以使用右相机视角的视频进行训练，但我们发现同时训练左右相机视角而不额外对相机位置进行条件处理会降低性能。为简单起见，我们丢弃任何少于4秒的视频，使得可用于训练的视频总时长不足62小时。我们对抽样的视频片段应用随机缩放裁剪增强，长宽比在(0.75, 1.35)范围内随机采样。

# B.2 机器人任务定义

图14展示了实验室1中具有杯子的抓取操作任务的起始帧和目标帧示例。对于带物体的抓取与到达任务，模型只展示一个目标图像。对于拾取与放置任务，我们除了最终目标外，还向模型呈现两个子目标图像。第一个目标图像显示了被抓取的物体，第二个目标图像显示了物体在目标位置附近。模型首先针对第一个子目标优化动作4个时间步，之后自动切换到第二个子目标进行接下来的10个时间步，最后在最后的4个时间步中优化第三个目标。在使用V-JEPA 2-AC进行规划时，我们使用800个样本，基于上一轮迭代的前10个样本进行10次细化步骤，以及1的规划时间范围。由于所有考虑的任务相对贪婪，我们发现较短的规划时间范围对我们的设置是足够的。虽然较长的规划时间范围也能合理运作，但它们需要更多的规划时间。

# B.3 可视化世界模型预测

为了可视化模型的预测，我们在Droid数据集上训练了一个帧解码器，将V-JEPA 2表示映射到人类可理解的像素上。具体地，我们使用冻结的V-JEPA 2视频编码器处理4帧剪辑，单独解码每一帧，并使用均方误差（L2）像素重建损失更新解码器的权重。解码器是一个前馈网络（完全确定性的回归模型，内部不使用任何采样），输出维度为$256 \times 256 \times 3$，参数化为ViT-L。我们使用AdamW优化器训练解码器150000步，固定学习率为0.0004。在前2000步内将学习率提升到峰值$5 \times 10^{-4}$，然后按照余弦调度递减。在推理时，我们使用在V-JEPA 2编码器上训练的解码器，并直接将其应用于V-JEPA 2-AC预测器产生的表示。仅使用简单的前馈架构的决定是

![](images/14.jpg)  
Figure 14 Prehensile Manipulation Task Definition. Start and goal frames for prehensile manipulation tasks with a cup in Lab.For the grasp and reach with oject tasks the odel s shown a single goal mage.For the pplacasks  e ulheet he alThe s  T f  ipeeleely ihe sub-goal for the next 10 time-steps, and finally the third goal for the last 4 time-steps.

机器人观测 重建 来自 V-JEPA 2

![](images/15.jpg)

来自 V-JEPA 2-AC 的重建 (a) 比较预测的准确性与真实轨迹。 (顶部行) 真实轨迹 y 的视频帧，即通过前馈框架解码器获取的。 V-JEPA 的重建表示显示，编码器捕捉到了低容量的前馈框架解码器。 (底部行) V-JEPA 2-AC 生成的自回归推演，作为新的框架解码器。 V-JEPA 2-AC 重建显示，基于条件世界模型成功地分析了真实轨迹在最终帧中的表现。 V-JEPA 2-AC 想象：闭合夹持器移动 V-JEPA 2-AC 想象：开放夹持器移动 (b) 开放与闭合夹持器预测的消融。 我们探索 V-JEPA 2-AC 在夹持器开放与闭合时预测的变化。 当使用开放夹持器时，世界模型预测了对象在时间步骤中的位置，推断出合理的直观物理理解（例如，物体恒常性、形状恒常性和重力）。

![](images/16.jpg)

图形解码表示。为了可视化模型的预测，我们在Droid数据集上训练一个框架解码器，它将潜在表示映射为可解释的图像。解码器（完全确定性的会话模型，不使用内部样本）通过最小化重构损失进行训练。通过应用训练好的JEPA 2编码器的框架解码器，我们可以可视化由V-JEPA 2-AC预测器生成的表示，从而展示各种动作序列的世界模型推演。

![](images/17.jpg)  
FSensitiviy to cama position.Rotationeror  the -y plane) the action cordinate xi erd b V-JEPA ACsnction me posin,wi 0de oondng mlcate at herobo se, an90aheo Whee u camera position.

在帧级别解码表示（相对于视频级别）是为了更好地利用解码器作为可解释性工具，分析一组机器人动作序列的 V-JEPA 2-AC 推演。

在图15a中，我们展示了实验室中机器人真实轨迹的视频帧（顶行）、每帧的解码V-JEPA 2编码器表示（中行）以及使用真实动作序列和单一起始帧作为上下文的解码V-JEPA 2-AC世界模型推演（底行）。V-JEPA 2表示的重建（中行）显示编码器捕捉了进行基于视觉控制所需的场景显著部分；模糊的背景生成部分归因于我们前馈帧解码器的低容量。V-JEPA 2-AC推演的重建显示，条件于动作的世界模型成功地使机器人运动，同时保持背景和非互动物体（例如，架子）的不变。我们还看到，在夹具闭合的情况下，模型正确预测了杯子随手臂的移动，表明对直观物理学（如物体恒定性、形状恒定性和重力）的合理理解，但我们确实观察到随着世界模型预测杯子的位置略低于真实轨迹，存在误差累积。在图15b中，我们探讨了在驱动模型时使用相同动作序列，闭合夹具（顶行）和开启夹具（底行）如何改变V-JEPA 2-AC预测。使用开启夹具动作序列时，世界模型预测杯子的位置在不同时间步保持不变。

# B.4 评估对摄像机位置的敏感性

在实际操作中，我们手动尝试了不同的相机位置，最终选择了最适合我们实验的位置；然后相机在所有任务的口头实验中保持在同一位置。在这一部分，我们对 V-JEPA 2-AC 世界模型对相机位置的敏感性进行了定量分析。理想情况下，模型推断的坐标轴应该对相机位置不变，但在这里我们观察到模型推断的坐标轴对相机位置敏感；这是一种问题，因为推断坐标轴的巨大误差可能会降低下游任务的成功率。

我们在机器人基座周围旋转多个相机位置，我们将其描述为围绕桌子中心的顺时针角度位置，其中0度位于机器人基座，90度位于机器人基座的左侧。由于我们从Droid数据集中训练左侧外观相机视图，因此我们在大约35度到85度之间旋转相机位置。接下来，对于每个相机位置，我们收集一个包含201步随机机器人运动的轨迹，在水平x-y平面内。对于这个201步轨迹中每一对相邻帧，我们计算V-JEPA 2-AC推断的最优动作，即在给定1步推演情况下，最小化公式(5)中的能量函数的动作。这使我们能够为每个相机位置构建一个数据集，包括真实动作与推断动作对。我们的分析仅关注$\Delta x$和$\Delta y$的笛卡尔控制动作（动作向量的前两个维度）。令$A \in \mathbb { R } ^ { 200 \times 2 }$表示推断的动作，$B \in \mathbb { R } ^ { 200 \times 2 }$表示真实动作。基于此，我们可以解决一个线性最小二乘问题，以识别将推断动作$A$映射到真实动作$B$的线性变换$W ^ { \star } \in \mathbb R ^ { 2 \times 2 }$。

$$
\begin{array} { r } { W ^ { \star } = \underset { W \in \mathbb { R } ^ { 2 \times 2 } } { \mathrm { a r g m i n } } ~ \| A W - B \| _ { 2 } . } \end{array}
$$

对于所有相机位置，平均绝对预测误差大约为1.6厘米（相比于真实标注数据的位移变化约为5厘米），这表明误差是系统性的。此外，我们还观察到，对于每个相机位置，矩阵 $W ^ { \star }$ 的条件数约为1.5，即在一个固定的标量系数的情况下，$W ^ { \star }$ 大约是一个旋转矩阵，因此我们可以通过使用 $\overline { { W } } ^ { \star } : = U V ^ { \top }$ 来计算推断坐标轴中的旋转误差，其中 $U$ 和 $V$ 分别是 $W ^ { \star }$ 的左奇异向量和右奇异向量。

$$
\begin{array} { r } { W ^ { \star } \approx \overline { { W } } ^ { \star } = \left[ \begin{array} { l l } { \cos \theta } & { - \sin \theta } \\ { \sin \theta } & { \cos \theta } \end{array} \right] , } \end{array}
$$

图16显示了摄像机位置与V-JEPA 2-AC推断坐标轴中的旋转误差之间的关系。我们观察到，在推断坐标轴中的旋转误差几乎是摄像机位置的线性函数。我们可以在图8中清楚地看到推断坐标轴中旋转误差的影响在单目标到达实验中。尽管模型始终能够根据单目RGB摄像机的视觉反馈将手臂移动到目标的4厘米内，但推断坐标轴中的旋转误差导致每个规划步骤中的行动相对不够理想，尽管在每个步骤中与目标的距离以单调的方式减少，但并不是以最大化的方式。有趣的是，由于推断坐标轴中的误差主要基于旋转，因此可以通过简单地将所有推断动作旋转$W^{\star}$来使用这种方法“校准”它们的世界模型，从而引入所需的摄像机位置不变性。这种无监督的校准阶段将涉及机器人执行随机动作，通过比较推断的最优动作与其实际执行的动作来解决线性最小二乘问题，然后在任务执行期间将其推断动作乘以旋转矩阵后发送给控制器。尽管这种方法很有趣，但我们强调在我们的实验中并没有进行这样的校准。

# C 视觉分类

我们将对第5节中描述的分类任务所使用的评估程序进行更详细的说明。

# C.1 超参数

探测器架构。我们在冻结的编码器输出上训练一个注意力探测器，使用来自每个下游任务的训练数据。我们的注意力探测器由四个变换器块组成，每个块在注意力层中使用16个头。前三个块使用标准自注意力；最后一个块使用交叉注意力层，其中查询标记的输出被加回到查询标记中，作为残差连接，然后再应用块的其余部分（层归一化，随后是一个带有单个GeLU激活的MLP）。变换器块之后是一个最终的线性分类器层。

评估设置参数。所有模型遵循相同的评估协议，并使用 $256 \times 256$ 的分辨率，除了我们的 V-JEPA 2 ViT-g384。在视频评估中，我们从每个输入视频中采样多个片段。在验证期间，我们还从每个片段中提取三个空间视图（而不是训练期间的一个视图）。每次评估的片段数量、帧步长参数和全局批大小各不相同；用于每次评估的参数可以在表 15 中找到。默认情况下，我们对 SSv2 使用 $16 \times 2 \times 3$ 的输入（16 帧片段，2 个时间裁剪，3 个空间裁剪），对 K400 使用 $16 \times 8 \times 3$，对 COIN 使用 $32 \times 8 \times 3$，对 Diving-48 和 Jester 使用 $32 \times 4 \times 3$。V-JEPA 2 ViT-g384 对 K400、COIN、Diving-48 和 Jester 使用更高的分辨率 $384 \times 384$，对 ImageNet 使用 $512 \times 512$，对 SSv2 使用 $384 \times 384$ 并带有 $64 \times 2 \times 3$ 的输入。表视觉分类评估参数。默认参数用于视觉分类评估，各评估的非默认值（$^*$ 表示默认值）。所有注意力探测器使用 4 个变换器块和 16 个头。

<table><tr><td>Parameter</td><td>Default (K400)</td><td>ImageNet</td><td>SSv2</td><td>COIN</td><td>Jester/Diving-48</td></tr><tr><td>Number of frames</td><td>16</td><td>16</td><td>16</td><td>32</td><td>32</td></tr><tr><td>Segments / Clip</td><td>8</td><td>1</td><td>2</td><td>8</td><td>4</td></tr><tr><td>Views / Segment</td><td>3</td><td>1</td><td>*</td><td>*</td><td>*</td></tr><tr><td>Frame Step</td><td>4</td><td>n/a</td><td>*</td><td>*</td><td>2</td></tr><tr><td>Epochs</td><td>20</td><td>*</td><td>*</td><td>*</td><td>100</td></tr><tr><td>Batch Size (global)</td><td>256</td><td>1024</td><td>*</td><td>128</td><td>128</td></tr><tr><td>Resolution</td><td>256 × 256</td><td>*</td><td>*</td><td>*</td><td>*</td></tr><tr><td>Classifier Heads</td><td>20 (4x5)</td><td>*</td><td>*</td><td>*</td><td>3 (3x1)</td></tr><tr><td>Classifier Learning Rates</td><td>[5e-3 3e-3 1e-3 3e-4 1e-4]</td><td>*</td><td>*</td><td>*</td><td>[1e-3 3e-4 1e-4]</td></tr><tr><td>Classifier Weight Decay</td><td>[.8 .4 .1 .01]</td><td>*</td><td>*</td><td>*</td><td>[.8]</td></tr></table>

输入层或 Jester/Diving-48。每个编码器的索引，表示在 Jester 和 Diving-48 评估中用作线性分类器输入的编码器层的词元。

<table><tr><td>Encoder</td><td># Layers</td><td>Attended Layers</td></tr><tr><td>ViT-L</td><td>24</td><td>17, 19, 21, 23</td></tr><tr><td>ViT-H</td><td>32</td><td>25, 27, 29, 31</td></tr><tr><td>ViT-g</td><td>40</td><td>24, 29, 34, 39</td></tr></table>

ImageNet 评估。对于 ImageNet，我们将每个输入图像重复生成 16 帧视频剪辑。我们还使用更大的全局批量大小（1024，而不是 256 或 128），且不对每个样本使用多个剪辑或视图。Jester 和 Diving-48 评估。我们的 Jester 和 Diving-48 动作分类评估任务与其他理解评估在多个方面不同，主要体现在我们采用了多层策略。我们提取来自四个编码器层的词元（最后一层和三个中间层），而不仅仅关注编码器的最后一层的词元，并关注所有这些词元。（表 16 显示了我们为每个编码器大小所使用的层。）在这两项评估中，我们还仅使用三个分类头进行探测训练（而不是其他评估中的 20 个），但训练 100 个周期（而不是 20 个），因为这些评估从较长的训练中受益。我们为这两项评估使用全局批量大小为 128。优化。对于每项评估，我们同时使用不同超参数（学习率和权重衰减）训练多个分类头，报告表现最佳的分类器的准确性。在我们的大多数评估中（Kinetics、SSv2、COIN 和 ImageNet），我们训练 20 个周期，使用 20 个头，每个头使用五个学习率值和四个权重衰减值，学习率按余弦计划衰减。我们在表 15 中提供了所有超参数的总结。

# C.2 附加结果

探针规模。由于我们使用了四层注意力探针进行这些评估，我们研究了使用较小探针是否会影响评估性能。我们使用由单个交叉注意力块和16个注意力头组成的更小探针重新运行我们的六个理解评估（针对两种模型规模，ViT-L和ViT-g）。与第5节不同，我们对所有评估使用16帧，包括Diving-48和Jester。见表18的分类性能——我们确认四层探针在所有理解评估中（除了Jester）优于单层注意力探针，ViT-L的准确率平均提升了+1.4个百分点，ViT-g提升了+$1.0$个百分点。编码器多层的影响。我们研究了在评估过程中将来自编码器多个层的token馈送到注意力探针的影响。表17显示，Diving-48和Jester在很大程度上受益于编码器更深层的信息。表“编码器多层消融”显示了我们改变注意力探针的编码器层数的实验结果。我们报告了在$256 \times 256$分辨率下，在V-JEPA 2之上训练的注意力探针的分类性能，使用16帧数据。

<table><tr><td>Model</td><td>Encoder Layers</td><td>Diving-48</td><td>Jester</td></tr><tr><td>ViT-g</td><td>1</td><td>82.9</td><td>96.1</td></tr><tr><td>ViT-g</td><td>4</td><td>86.7</td><td>97.6</td></tr></table>

表1 失效实验。我们变化了注意力探测器的层数。我们报告了在 $256 \times 256$ 分辨率下，基于 V-JEPA 2 训练的注意力探测器的性能。

<table><tr><td></td><td></td><td></td><td colspan="3">Motion Understanding</td><td colspan="3">Appearance Understanding</td></tr><tr><td>Model</td><td>Probe Layers</td><td>Avg.</td><td>SSv2</td><td>Diving-48</td><td>Jester</td><td>K400</td><td>COIN</td><td>IN1K</td></tr><tr><td>ViT-L</td><td>1</td><td>84.0</td><td>72.0</td><td>83.2</td><td>97.7</td><td>83.3</td><td>85.9</td><td>81.8</td></tr><tr><td>ViT-L</td><td>4</td><td>85.6</td><td>73.6</td><td>87.1</td><td>97.7</td><td>85.1</td><td>86.8</td><td>83.5</td></tr><tr><td>ViT-g</td><td>1</td><td>86.0</td><td>74.8</td><td>85.3</td><td>97.8</td><td>85.6</td><td>88.9</td><td>83.5</td></tr><tr><td>ViT-g</td><td>4</td><td>87.0</td><td>75.6</td><td>86.7</td><td>97.6</td><td>86.6</td><td>90.7</td><td>84.6</td></tr></table>

# D 动作预测

我们提供关于第6节Epic-Kitchen 100动作预测评估的附加细节、结果和消融实验。

# D.1 超参数

探测器架构。我们的动作预测探测器架构遵循附录 C.1 中描述的分类探测器架构，包括四个变换器块，最后一个是具有一组可学习查询词元的跨注意力层，后面接着每个查询词元的最终线性分类器层。

评估设置参数。我们在训练探测器时使用焦点损失（Lin et al., 2017），其中 $\alpha = 0.25$ 和 $\gamma = 2.0$；该损失更适合处理长尾分布的类不平衡情况。我们为 V-JEPA 2 ViT-L、ViT-H 和 ViT-g 使用 32 帧的上下文，帧率为每秒 8 帧，分辨率为 $256 \times 256$；而对于 V-JEPA 2 ViT- $\mathrm{8384}$，分辨率为 $384 \times 384$。在探测器训练过程中，我们随机抽取预测时间在 0.25 到 1.75 秒之间，预测点在 0.0 到 0.25 之间。预测点标识一个动作片段中的位置，用于进行预测；即，预测点为 0 表示我们在将数据输入探测器之前，使用 V-JEPA 2 预测器预测动作片段中第一帧的表示，而预测点为 1 则表示我们在将数据输入探测器之前，预测动作片段中最后一帧的表示。验证预测时间设定为 1 秒，验证预测点设定为 0。我们在表 19 中提供了超参数的总结，包括优化参数。

# D.2 附加结果

Impaco架构表20研究了pac，将-JEPA编码器、预测器或两者的输出提供给动作预测探测器。利用编码器输出已经在EK100任务上取得了竞争性性能。添加预测器在动作、动词和物体类别上提供了小而稳定的提升。此外，使用预测器输出也带来了可观的性能提升，但与使用编码器相比仍显著较低，表明EK100任务主要需要强大的语义理解，而不是预测能力。输入分辨率的影响。我们在图17中报告了输入分辨率和帧采样参数的影响。总之，V-JEPA 2受益于更长的上下文、更高的帧率和更高的分辨率，直到性能饱和或略有下降。最佳性能是通过训练32帧的上下文长度、8帧率和$3 8 4 \times 3 8 4$的分辨率获得的。表19 动作预测评估参数。用于EK100动作预测评估的默认参数。

<table><tr><td>Parameter</td><td>EK100</td></tr><tr><td>Train Anticipation time</td><td>0.25s - 1.75s</td></tr><tr><td>Train Anticipation point</td><td>0.0 - 0.25</td></tr><tr><td>Val Anticipation time</td><td>1s</td></tr><tr><td>Val Anticipation point</td><td>0.0</td></tr><tr><td>Number of frames</td><td>32</td></tr><tr><td>Frames per second</td><td>8</td></tr><tr><td>Epochs</td><td>20</td></tr><tr><td>Warmup epochs</td><td>0</td></tr><tr><td>Batch Size (global)</td><td>128</td></tr><tr><td>Classifier Heads</td><td>20 (4x5)</td></tr><tr><td>Classifier Learning Rates Classifier Weight Decay</td><td>[5e-3 3e-3 1e-3 3e-4 1e-4] [1e-4 1e-3 1e-2 1e-1]</td></tr></table>

![](images/18.jpg)  
Table0 EK100:Impact of Anticipation Probe Inputs.Weinvestigate the impac of providing theoutputs the VJPA ncer, predicor, r both, tthe actinantipatin probeUsiencoeroutput already l competitive perormancen the EK100 task.Adding the predicor provide small but consistent mprovementaoss action, verb and object categories.   
Figure17 Protocol ablation for action anticipation on EK100. (Left) Performance with respect to the number xt ames use oracion nticipati.Mid) Perormac with respec the ramerate s)us fo ie; um cnte amefixe (Rih Peornwirepec the pat reoluti and width) of the context frames used for action anticipation.

长期预测。我们在图18（左）中报告了预测较长时间范围的影响，通过改变预期时间（1秒、2秒、4秒、10秒）。针对每个预期时间，我们在多个值（1、5、10、20）下报告了召回率。结果显示，随着预期时间的增加，性能急剧下降，这在预期之中，因为在EK100中预测未来是一个非确定性任务。失败案例分析。我们在图18（右）中报告了EK100验证集中成功与失败预测的分布，分别针对动词、名词和动作的每种成功/失败配置。模型表现非常出色，因此，最常见的配置是动词、名词和动作的完全成功。最常见的失败配置均包含未能找到动作的情况。

![](images/19.jpg)  

图18（左）：长期预测时间的影响。在多个重校值和预测时间下，EK100动作预测的性能。（右）：V-JEPA的成功和失败案例的分布。计算EK1NA的验证意味着动词和名词对的关键类别被MLX指标分类，以表示该组合对应于该动作的有效性。

# E 视频问答

在本节中，我们提供了训练 V-JEPA 2 多模态大语言模型（MLLM）的详细信息。我们遵循 LLaVA 框架（Liu et al., 2024a）来训练 MLLM，其中视觉主干使用 V-JEPA 2，而 LLM 主干可以是任何现成的预训练 LLM，类似于非标记早期融合（Wadekar et al., 2024）设置。MLLM 接收视觉编码器的输出嵌入，这些嵌入通过投影模块映射到 LLM 主干的隐层维度。投影器通常是一个 2 层的多层感知器（MLP）。MLLM 使用图像-文本和视频-文本配对数据的混合进行训练，以一系列进阶训练步骤进行。

为了理解数据规模的影响，我们使用一个包含8850万对图像-文本和视频-文本的 dataset，类似于用于训练 PerceptionLM （Cho et al., 2025）的数据集。如第7节所提到的，我们研究了两种配置： (a) 控制配置，在此配置中我们使用1800万对图像和视频-文本进行训练，并在完全相同的 MLLM 训练环境下评估 V-JEPA 2 和其他编码器，以及 (b) 扩展配置，在此配置中我们采用 V-JEPA 2 VITg384 并使用完整对齐数据集。为进一步测试 V-JEPA 2 的多样性，我们使用 Qwen2-7B-Instruct （Yang et al., 2024a）作为控制实验的语言主干，并使用 Llama 3.1 8B Instruct （Grattafiori et al., 2024）进行扩展实验。我们将在接下来的各节中描述训练细节。

# E.1 处理图像和视频作为输入

由于视频问答使用视频作为输入，而不是图像输入，因此输出的视觉词元数量与图像问答相比显著增加。如果需要，我们可以使用池化方法来减少视觉词元的数量。常用的池化方法包括自适应2x2池化（Cho等，2025），感知器采样（Jaegle等，2021），注意力池化（Bardes等，2024）等。

此外，我们观察到，从图像-文本对中学习对下游基准的高性能至关重要。为了利用图像进行训练，一种简单的方法是将给定的图像重复 $k$ 帧，其中 $k$ 是 V-JEPA 2 支持的最大帧数。然而，在我们的初步实验中，我们发现这种策略并没有有效地提高下游性能，因为它无法使模型提取细粒度信息。因此，我们采用了 Liu et al. (2024d) 提出的修改版动态 $S ^ { 2 }$ 策略，以在训练期间为 V-JEPA 2 提供更高的分辨率粒度。这种方法通过创建一系列最大尺寸的切片（由 V-JEPA 2 支持）自适应地处理原始分辨率的图像，同时不同的长宽比以保持其原始分辨率。在视频的情况下，我们选择用固定数量的帧 $f _ { n }$ 进行训练，通过平衡视觉标记的数量与计算预算。

# E.2 控制设置

训练细节 在受控设置中，我们遵循 LLaVA-NEXT 框架（Liu 等，2024a；Zhang 等，2024b），其中我们使用 Qwen2-7B-Instruct（Yang 等，2024a）作为所有编码器的基础大语言模型。为了减少视觉词元的数量，我们采用了一个注意力池化器，因计算预算和视觉块的数量而有 4-16 的比例。有关更多细节，请参见表 21。我们的训练设置遵循 LLaVA-NeXT 流程（Li 等，2024b），该流程由多个阶段的训练组成。具体而言，各阶段包括：a) 将注意力池化器与图像字幕数据对齐（阶段 1），b) 在高质量图像字幕数据上训练完整模型（阶段 1.5），c) 在大规模图像问答数据上训练完整模型（阶段 2）。我们增加一个额外阶段，以在大规模视频字幕和问答上进行训练（阶段 3）。我们使用 1800 万个图像和视频-文本对齐的数据。经过多阶段训练后，大语言模型逐步改善了对视觉词元的理解，其中在阶段 3 后视频问答任务的改进最大。

我们探索了冻结和微调编码器对齐设置。在这两种设置中，LLM和投影器的所有参数都经过训练，而在后者中，V-JEPA 2 的参数也会解冻。为了减少视觉词元的数量并保持MLLM的上下文长度固定，我们采用了一个注意力池化器作为投影器，将视觉词元的数量减少到原来的四分之一。此控制研究所使用的实现基于Llava-NExT代码库，并使用Pytorch 2.5.1、Transformers 4.46.0、Flash Attention 2和DeepSpeed 0.14.4用于模型实现、更快的训练和多GPU模型分割。我们使用128个H100 GPU进行所有模型的训练，所有阶段的有效批量大小为256。我们使用含有0权重衰减的AdamW进行所有优化。在阶段1和1.5中，我们使用1e-5的学习率和余弦衰减，而在阶段2和3中我们使用固定的学习率5e-6。在所有阶段中，我们对前3%的训练步骤使用线性预热。训练超参数列在表21中。 基线。为了评估V-JEPA 2捕捉VidQA时空细节的能力，我们与领先的现成图像编码器进行了比较。具体而言，我们与DINOv2（Oquab等，2023）、SigLIP2（Tschannen等，2025）和Perception Encoder（Bolya等，2025）进行了比较。DINOv2是一个自监督图像模型，而SigLIP2和Perception Encoder都是使用噪声图像-文本标题进行语言监督训练的。我们在每个视频帧上以其“本地”预训练分辨率应用所有图像编码器，分别为518px、384px和448px。我们保持所有训练细节相同，除了将注意力池化比例增加到16，以保持模型之间图像词元数量相对相似。有关详细信息，请参见表21。 评估。为了评估V-JEPA 2通过视频和语言理解世界的能力，我们选择了旨在测试时空推理能力的流行评估数据集。为了确保可重复的评估，我们利用了Valbray（Li等，2022；Zhan，2024）和Critics，这是一个支持视觉模型的llm-eval-harness（Gao等，2024）分支库，这是一个用于评估LLM在文本任务上的流行评估库。在控制设置中，对于每个模型和数据集，我们通过使用均匀帧采样机制进行评估，在推理期间选择128帧。对于PerceptionTest，我们还在训练集上进一步训练模型5个周期。表21列出了视觉编码器控制比较的超参数。我们使用每个视觉编码器的本地预训练输入分辨率。

<table><tr><td>Model</td><td>Pooling Ratio</td><td>Vision Tokens</td><td>BS</td><td>Stage 1/1.5 LR</td><td>Stage 2/3 LR</td><td>WD</td><td>Frames</td></tr><tr><td>V-JEPA 2 ViT-L256</td><td>4</td><td>4096</td><td>256</td><td rowspan="6">1e-5</td><td rowspan="6">5e-6</td><td rowspan="6"></td><td rowspan="6">128</td></tr><tr><td>V-JEPA 2 ViT-H256</td><td>4</td><td>4096</td><td>256</td></tr><tr><td>V-JEPA 2 ViT-g256</td><td>4</td><td>4096</td><td>256</td></tr><tr><td>V-JEPA 2 ViT-g384</td><td>4 8</td><td>9216</td><td>256</td></tr><tr><td>V-JEPA 2 ViT-g512</td><td></td><td>8192</td><td>256</td></tr><tr><td>DINOv2518</td><td></td><td>10952</td><td>256</td></tr><tr><td>SigLIP2384</td><td>16 16</td><td>5832</td><td>256</td><td rowspan="3">1e-5</td><td rowspan="3">5e-6</td><td rowspan="3">0.0</td><td rowspan="3">128</td></tr><tr><td>T E48</td><td>16</td><td></td><td></td></tr><tr><td></td><td></td><td>8192</td><td>256</td></tr></table>

![](images/20.jpg)  
Fiur Impactof videoduration durinvisual instruction tuninWeinvestigate he efec increasn a V-JEPA2 perormanceinearlyncreasescompared o DINOv2, an SL-base image encoder, howing the potental V-JEPA 2 to scale with more frames.

视频时长的影响 在受控设置中，我们进行分析以理解 V-JEPA 2 在长视频理解方面的能力。我们在 V-JEPA 2 和 DINOv2 上训练 MLLMs，保持编码器冻结，并增加训练和测试中使用的帧数。我们观察到，随着帧数的增加，V-JEPA 2 在下游任务上的性能线性提升，而 DINOv2 的性能则下降并保持平稳（图 19）。这突显了如 V-JEPA 2 这样的视觉编码器在使用自然语言查询理解长视频方面的潜力，通过将 V-JEPA 2 作为视觉编码器来适配一个大型语言模型。

# E.3 数据缩放设置

在规模设置中，我们遵循Cho等人的框架来训练Perion LM 8B。具体而言，我们利用了基于Lingua的发布代码库。我们修改代码以使用V-JEPA 2编码器，并使用Llama 3.1 8B指令作为主干LLM。与Cho等人不同，我们不使用池化，而是使用MLP投影器训练V-JEPA 2 VIT-384，从而实现每帧288个词元。训练设置还包括三个渐进阶段：阶段1）将MLP池化器与图像字幕数据对齐；阶段2）在图像-文本字幕和问答数据上进行训练；阶段3）在视频-文本字幕和问答数据上进行训练。我们将数据规模扩大到8850万样本。我们的设置使用Pytorch 2.5.1和经过V-JEPA 2编码器修改的Perception LM训练代码。我们在512个H100 GPU上进行第二阶段和第三阶段的训练，全球批量大小分别为2048和1024。训练超参数的详细信息见表22。表22 数据扩展训练参数。

<table><tr><td>Parameter</td><td>Values</td></tr><tr><td>Common parameters</td><td></td></tr><tr><td>Crop Size</td><td>384</td></tr><tr><td>Video Frames per Second</td><td>1</td></tr><tr><td>Sampling method</td><td>Uniform</td></tr><tr><td>Seed</td><td>777</td></tr><tr><td>Stage 1</td><td></td></tr><tr><td>Steps</td><td>16000</td></tr><tr><td>Warmup Steps</td><td>96</td></tr><tr><td>Batch Size (global)</td><td>128</td></tr><tr><td>Learning Rate</td><td>1e-4</td></tr><tr><td>Final Learning Rate</td><td>1e-6</td></tr><tr><td>Weight Decay</td><td>0.05</td></tr><tr><td>Max sequence length Stage 2</td><td>1920</td></tr><tr><td>Steps</td><td>35000</td></tr><tr><td></td><td>200</td></tr><tr><td>Warmup Steps</td><td></td></tr><tr><td>Batch Size (global)</td><td>2048</td></tr><tr><td>Learning Rate</td><td>4e-5</td></tr><tr><td>Final Learning Rate</td><td>4e-7</td></tr><tr><td>Weight Decay</td><td>0.05</td></tr><tr><td>Max sequence length</td><td>6400</td></tr><tr><td>Image tiles</td><td>16</td></tr><tr><td>Video frames</td><td>16</td></tr><tr><td>Stage 3</td><td></td></tr><tr><td>Steps</td><td>28000</td></tr><tr><td>Early stopping step</td><td>22000</td></tr><tr><td>Warmup Steps</td><td>168</td></tr><tr><td>Batch Size (global)</td><td>2048</td></tr><tr><td>Learning Rate</td><td>1e-5</td></tr><tr><td>Final Learning Rate</td><td>1e-7</td></tr><tr><td>Weight Decay</td><td>0.05</td></tr><tr><td>Max sequence length</td><td>12800</td></tr><tr><td>Image tiles</td><td>32</td></tr><tr><td>Video frames</td><td>32</td></tr></table>

基准线。我们将我们的扩展运行与 Qwen2VL (Wang et al., 2024a)、Qwen2.5VL (Qwen Team et al 2025)、InternVL-2.5 (Chen et al., 2024) 和 PerceptionLM 8B (Cho et al., 2025) 进行比较。基准线数字直接来源于相关论文，除了 MVP，我们是自行运行的。评估。我们遵循与控制设置中报告的相似评估流程，使用 lmms-eval 库。我们报告在 32 帧上的模型评估结果。