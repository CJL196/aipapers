# 1. 论文基本信息

## 1.1. 标题
Wan: 开放且先进的大规模视频生成模型 (Wan: Open and Advanced Large-Scale Video Generative Models)

## 1.2. 作者
Wan 团队，阿里巴巴集团 (Wan Team, Alibaba Group)

## 1.3. 发表期刊/会议
本文发布于 arXiv，这是一个收录科学论文预印本的在线平台。截至本文档生成时，该论文为预印本状态，尚未在经过同行评审的顶级学术会议或期刊上正式发表。

## 1.4. 发表年份
2025年 (预印本发布于 2025-03-26)

## 1.5. 摘要
该报告介绍了 `Wan`，一套全面且开源的视频基础模型，旨在推动视频生成技术的前沿。`Wan` 基于主流的扩散变换器 (`Diffusion Transformer`) 范式，通过一系列创新（包括新颖的变分自编码器 `VAE`、可扩展的预训练策略、大规模数据处理和自动化评估指标）在生成能力上取得了显著进步。`Wan` 模型主要具备四个特点：
1.  **领先的性能：** 140亿 (`14B`) 参数的模型在数十亿级别的图文和视频数据集上训练，其性能在多个内外部基准测试中持续优于现有的开源模型及顶尖的商业解决方案。
2.  **全面性：** 提供 13亿 (`1.3B`) 和 140亿 (`14B`) 两种参数规模的模型，分别满足效率和性能的需求，并覆盖了图生视频、指令引导的视频编辑、个人化视频生成等多达八项下游任务。
3.  **消费级效率：** `1.3B` 模型仅需 8.19 GB 显存，可在多种消费级 GPU 上运行。
4.  **开放性：** 团队开源了 `Wan` 的完整系列，包括源代码和所有模型，旨在促进视频生成社区的发展。

## 1.6. 原文链接
*   **原文链接:** [https://arxiv.org/abs/2503.20314](https://arxiv.org/abs/2503.20314)
*   **PDF 链接:** [https://arxiv.org/pdf/2503.20314v2.pdf](https://arxiv.org/pdf/2503.20314v2.pdf)
*   **发布状态:** 预印本 (Preprint)

# 2. 整体概括

## 2.1. 研究背景与动机
自 OpenAI 发布 `Sora` 以来，视频生成技术受到了学术界和工业界的极大关注。尽管开源社区在此领域也做出了如 `HunyuanVideo`、`Mochi` 等重要贡献，但与最先进的闭源商业模型之间仍存在显著差距。这篇论文明确指出了当前开源模型面临的三大核心挑战：

1.  <strong>性能欠佳 (Suboptimal Performance):</strong> 开源模型的技术迭代速度远不及商业模型，导致生成质量和能力上存在明显差距。
2.  <strong>能力有限 (Limited Capabilities):</strong> 大多数开源基础模型仅限于通用的文本到视频 (`T2V`) 生成，无法满足视频创作中多样化的需求，如图生视频、视频编辑等。
3.  <strong>效率不足 (Insufficient Efficiency):</strong> 现有的大规模模型通常需要巨大的计算资源，对于计算能力有限的团队或个人而言，实用性很低。

    为了弥合这一差距，推动开源社区的发展，阿里巴巴的 Wan 团队开发并发布了 `Wan` 系列模型。其核心动机是创建一个**性能顶尖、功能全面、运行高效且完全开源**的视频基础模型，为学术研究和工业应用提供一个强大的新基准。

## 2.2. 核心贡献/主要发现
本文的核心贡献在于提出了 `Wan`，一个集大成的视频生成模型套件。其主要贡献和发现可以概括为以下几点：

1.  **提出并开源了高性能的 `Wan` 模型系列：** 包含 140亿 (`14B`) 参数的高性能模型和 13亿 (`1.3B`) 参数的高效率模型，两者在各自的定位上都达到了业界领先水平。
2.  **全方位的技术创新：** 论文详细介绍了一整套自研技术栈，包括：
    *   <strong>新颖的时空变分自编码器 (`Wan-VAE`):</strong> 实现了高效且高质量的视频压缩与重建。
    *   **大规模数据处理流水线：** 建立了一套自动化、多维度的筛选和标注流程，处理了数十亿级别的图像和视频数据，为模型训练提供了高质量的“养料”。
    *   **可扩展的训练与推理优化策略：** 针对超长序列和大规模模型，设计了高效的并行计算和内存优化方案。
    *   <strong>自研的自动化评估基准 (`Wan-Bench`):</strong> 提出了一个与人类感知对齐的综合评估体系，用于更准确地衡量视频生成模型的性能。
3.  **实现了全面的下游应用支持：** 基于强大的基础模型，`Wan` 成功扩展到图生视频 (`I2V`)、视频编辑 (`V2V`)、个性化定制、实时视频流生成和音频生成等多个关键应用场景。
4.  **首次实现中英双语视觉文本生成：** `Wan` 是首个能够在生成的视频中准确渲染中英双语文字的开源模型，极大地增强了其实用价值。
5.  **推动开源生态：** 通过彻底开源模型、代码和技术细节，`Wan` 团队旨在降低高质量视频生成技术的研究和应用门槛，赋能整个社区。

# 3. 预备知识与相关工作

## 3.1. 基础概念
为了更好地理解 `Wan` 模型，以下是一些核心的基础概念：

*   <strong>扩散模型 (Diffusion Models):</strong> 这是一类强大的生成模型。其核心思想分为两个过程：
    1.  <strong>前向过程（加噪）：</strong> 从一张真实的图像或视频帧开始，逐步、多次地向其添加少量高斯噪声，直到它完全变成纯粹的随机噪声。
    2.  <strong>反向过程（去噪）：</strong> 训练一个神经网络模型，让它学习如何“撤销”加噪的每一步。即给定一张加了噪的图片和当前噪声水平，模型需要预测出更“干净”一点的图片。通过反复执行这个去噪步骤，模型最终可以从一个纯粹的随机噪声图像生成一张全新的、逼真的图像。
*   <strong>变分自编码器 (Variational Autoencoder, VAE):</strong> `VAE` 是一种用于数据压缩和生成的神经网络。它包含两个主要部分：
    *   <strong>编码器 (Encoder):</strong> 将高维度的输入数据（如一张 512x512 像素的图片）压缩成一个低维度的<strong>潜在表示 (latent representation)</strong>。这个潜在表示可以看作是原始数据的“精华”摘要。
    *   <strong>解码器 (Decoder):</strong> 将这个低维度的潜在表示重建回原始的高维度数据。
        在扩散模型中，`VAE` 的作用至关重要。直接在像素空间上进行扩散（加噪/去噪）计算成本极高。因此，研究者们先用 `VAE` 的编码器将视频帧压缩到更小的潜在空间，然后让扩散模型在这个小空间里学习生成，最后再用 `VAE` 的解码器将生成的结果还原成高清视频。这大大降低了计算和内存需求。
*   **Transformer:** `Transformer` 是一种最初为自然语言处理设计的神经网络架构，其核心是<strong>自注意力机制 (self-attention mechanism)</strong>。自注意力机制允许模型在处理一个序列（如一句话）时，计算序列中每个元素（如每个单词）与其他所有元素之间的相关性权重。这使得模型能够捕捉长距离的依赖关系。
*   <strong>扩散变换器 (Diffusion Transformer, DiT):</strong> 传统扩散模型通常使用 `U-Net` 架构作为去噪网络。`DiT` 架构（由 `Sora` 和 `Wan` 等模型采用）则用 `Transformer` 替代了 `U-Net`。具体做法是将经过 `VAE` 压缩的潜在表示切分成一个个小块（`patches`），然后将这些块视为序列中的“词元 (`token`)”，输入到 `Transformer` 中进行处理。实验证明，`DiT` 架构具有更好的可扩展性，即模型参数量和数据量越大，性能提升越明显。
*   <strong>流匹配 (Flow Matching):</strong> 这是训练扩散模型的一种替代框架。传统扩散模型需要模拟一个随机过程，而流匹配旨在学习一个确定的<strong>常微分方程 (Ordinary Differential Equation, ODE)</strong>，该方程定义了一个从噪声分布到数据分布的“流动”路径（向量场）。通过直接学习这个路径，流匹配可以实现更稳定、更快速的训练。`Wan` 模型就采用了这一框架。

## 3.2. 前人工作
论文将相关工作分为闭源模型和开源社区贡献两类。

*   <strong>闭源商业模型 (Closed-source models):</strong>
    *   **`Sora` (OpenAI):** 标志着视频生成技术取得巨大飞跃的里程碑模型。
    *   <strong>`Kling` (快手):</strong> 国内领先的视频生成模型，展示了强大的动态和物理模拟能力。
    *   **`Luma` (LUMA AI):** 以高质量和流畅的运镜著称。
    *   **`Gen-3` (Runway):** 在 `Gen-2` 基础上进一步提升了视频创作标准。
    *   其他如 `Vidu` (生数科技)、`Hailuo Video` (MiniMax)、`Pika` (Pika Labs) 和 `Veo` (Google) 等，都在不断推动商业视频生成技术的发展。

*   <strong>开源社区贡献 (Contributions from open-source community):</strong>
    *   **基础架构:** 大多数视频生成模型建立在 `Stable Diffusion` 的潜在扩散模型 (`Latent Diffusion Model`) 框架之上，包含自编码器、文本编码器和去噪网络三个核心模块。
    *   **去噪网络结构:**
        *   **`U-Net`:** 早期图像和视频生成模型广泛使用的网络结构，通过引入时间维度模块来处理视频。
        *   **`Diffusion Transformers (DiT)`:** 后来被证明在视觉生成任务上优于 `U-Net`，成为 `Sora`、`Wan` 等新一代模型的主流选择。其变体包括使用<strong>交叉注意力 (cross-attention)</strong> 嵌入文本条件的原始 `DiT`，以及将文本嵌入与视觉嵌入拼接后处理的 `MM-DiT`。
    *   **自编码器:**
        *   **`VAE`:** `Stable Diffusion` 使用的标准 `VAE`。
        *   `VQ-VAE` / `VQGAN`: 改进的自编码器，提升了重建质量和压缩率。
    *   **文本编码器:**
        *   **`T5` 系列:** 强大的文本编码器，被广泛应用于视频生成模型中。
        *   **`CLIP`:** 常与 `T5` 结合使用，增强文图对齐能力。
        *   <strong>多模态大语言模型 (Multimodal Large Language Model, MLLM):</strong> 如 `HunyuanVideo` 所示，使用 `MLLM` 作为文本编码器可以实现更强的文本-视觉对齐。

## 3.3. 技术演进
视频生成技术的技术脉络经历了从像素空间到潜在空间，从 `U-Net` 架构到 `Transformer` 架构的演进。
1.  **早期模型**直接在像素级别操作，计算成本极高，生成视频的分辨率和时长受限。
2.  **`Stable Diffusion`** 引入了 `VAE`，将扩散过程转移到计算量更小的潜在空间，实现了高清图像生成，并被后续视频模型借鉴。
3.  在去噪网络方面，**`U-Net`** 因其在图像分割等任务中的成功而被广泛用于早期扩散模型。为了处理视频，研究者们通过增加时间维度的卷积或注意力模块来改造 $2D U-Net$。
4.  **`DiT` 的出现**是一个转折点。它证明了 `Transformer` 架构在视觉生成任务上同样强大且更具扩展性。这启发了 `Sora`、`Wan` 等模型采用 `DiT` 作为核心，通过堆叠 `Transformer` 模块来构建一个能够处理时空信息的强大网络，从而实现了在模型和数据规模扩大时性能的持续提升。

## 3.4. 差异化分析
相较于上述相关工作，`Wan` 的核心差异化和创新点在于：
*   **系统性的整体方案:** `Wan` 不仅仅是一个新模型，而是一套涵盖**数据、模型、训练、推理、评估**全链路的综合解决方案。论文详细披露了其数据处理流水线、`Wan-VAE` 的设计、大规模训练的并行策略等工程细节，这是许多论文所缺乏的。
*   **规模与开放的结合:** `Wan` 将模型规模推向了开源社区前所未有的 140亿 (`14B`) 参数，同时保持了完全的开放性。这使得学术界和中小型企业能够接触并使用到与顶级商业模型性能相近的基础模型。
*   **效率与性能的兼顾:** 同时推出 `1.3B` 的消费级模型，解决了开源社区普遍存在的“模型虽好，但跑不动”的痛点，极大地降低了使用门槛。
*   **功能全面性:** `Wan` 不仅在 `T2V` 任务上表现出色，还系统性地扩展到了编辑、个性化、实时流等多个实用场景，并解决了中英文视觉文本生成这一具体但重要的需求，展现了其作为“基础模型”的强大潜力。

# 4. 方法论
`Wan` 的方法论是一个系统性工程，涵盖了从数据处理到模型设计，再到训练、推理优化的全过程。

## 4.1. 数据处理流水线 (Data Processing Pipeline)
高质量、大规模、高多样性的数据是训练强大生成模型的基石。`Wan` 设计了一套精密的自动化数据处理流水线。

### 4.1.1. 预训练数据筛选
`Wan` 的团队从内部版权数据和公开数据中整理出一个数十亿规模的候选数据集，并通过一个四阶段流程进行清洗：

1.  **基础维度筛选:**
    *   **文本检测:** 剔除文字过多的视频/图像。
    *   **美学评估:** 使用 `LAION-5B` 的美学分类器过滤低质量内容。
    *   **NSFW 评分:** 过滤不适宜内容。
    *   **水印/Logo 检测与裁剪:** 移除或裁剪带商业标识的区域。
    *   **黑边检测与裁剪:** 自动裁剪无用黑边。
    *   **过曝检测:** 过滤色调异常的数据。
    *   **合成图像检测:** 训练分类器以剔除由其他 AI 模型生成的“污染”数据。
    *   **模糊检测:** 移除模糊不清的内容。
    *   **时长与分辨率:** 筛选时长超过4秒且分辨率达标的视频。
        经过此轮筛选，约 50% 的初始数据被淘汰。

2.  **视觉质量筛选:**
    *   <strong>聚类 (Clustering):</strong> 将数据分为100个簇，确保在后续筛选中不会因长尾分布而丢失小众但重要的数据类别。
    *   <strong>打分 (Scoring):</strong> 在每个簇中人工标注一批样本的质量分数（1-5分），然后训练一个“专家评估模型”对整个数据集进行打分。

3.  **运动质量评估:**
    将视频按运动质量分为六个等级，并根据等级调整其在训练中的采样率。例如，**优质运动**（运动显著、平滑）的视频会被优先采样，而**静态视频**或**相机抖动**的视频则会被降权或剔除。

4.  **视觉文本数据构建:**
    为了让模型学会生成文字，`Wan` 采用了两种数据源：
    *   **合成数据:** 在白色背景上渲染数亿张包含中文字符的图像。
    *   **真实数据:** 收集现实世界中包含文字的图像/视频，使用多个 `OCR` 模型识别中英文本，然后将识别出的文本内容输入多模态语言模型 `Qwen2-VL`，生成包含这些文本的自然语言描述。

        下图（原文 Figure 3）展示了在不同训练阶段，模型对不同类型（运动、质量、类别）数据的动态配比策略。

        ![Figure 3: Data provisioning across different training phases. For each stage, we dynamically adjust the proportions of data related to motion, quality, and category based on data throughput.](images/3.jpg)
        *该图像是数据在不同训练阶段的配置示意图。每个阶段根据数据处理量动态调整与运动、质量和类别相关的数据比例，以优化视频生成模型的训练效率。*

### 4.1.2. 精调数据筛选 (Post-training Data)
在模型预训练完成后，使用更高质量的数据进行微调（精调）。
*   **图像数据:** 从高分图像中，通过专家模型和人工挑选的方式，选出数百万张在质量、构图、细节上都顶尖的图像。
*   **视频数据:** 同样地，筛选出视觉质量和运动质量（分为简单运动和复杂运动）俱佳的数百万个视频片段，并确保覆盖科技、动物、艺术、人物等12个主要类别。

### 4.1.3. 密集视频字幕生成 (Dense Video Caption)
原始数据自带的文本描述通常过于简单。受 `DALL-E 3` 启发，`Wan` 团队训练了一个内部的<strong>字幕模型 (caption model)</strong>，为数据集中的每个图像和视频生成详细、描述性强的字幕。该字幕模型的训练数据来源广泛，包括：
*   **开源数据集:** 包含图像/视频字幕、视觉问答等数据集。
*   <strong>自建数据集 (In-house Dataset):</strong> 针对性地构建了多个特定能力的数据集，如名人/地标识别、物体计数、`OCR`、相机角度/运动描述、细粒度类别识别等。
*   **模型架构:** 字幕模型采用了 `LLaVA` 风格的架构，使用 `ViT` 提取视觉特征，并输入到 `Qwen` 大语言模型中生成文本。

## 4.2. 模型设计与加速 (Model Design and Acceleration)

### 4.2.1. 时空变分自编码器 (Spatio-temporal VAE, `Wan-VAE`)
`Wan-VAE` 是 `Wan` 模型中的关键组件，负责将视频数据在时空维度上进行高效压缩。

*   **核心思想与架构:**
    `Wan-VAE` 是一个<strong>3D 因果 VAE (3D causal VAE)</strong>，专门为视频设计。它能将输入视频在时间维度上压缩4倍，在空间维度（高和宽）上各压缩8倍，即总体压缩率为 $4 \times 8 \times 8$。
    *   <strong>因果性 (Causality):</strong> 模型在处理某一帧时，不会看到未来的帧，这对于生成连贯、符合物理逻辑的视频至关重要。这是通过将网络中的 `GroupNorm` 层替换为 `RMSNorm` 层来实现的。
    *   **高效推理:** 设计了<strong>特征缓存机制 (feature cache mechanism)</strong>，允许模型以“块 (`chunk`)”为单位流式处理任意长度的视频，而不会耗尽内存。前一个视频块计算出的特征会被缓存下来，用于下一个块的计算，从而保证视频的连续性。

        下图（原文 Figure 5）展示了 `Wan-VAE` 的整体框架。

        ![Figure 5: Our Wan-VAE Framework. Wan-VAE can compress the spatio-temporal dimension of a video by $4 \\times 8 \\times 8$ times. The orange rectangles represent $2 \\times$ spatio-temporal compression, and the green rectangles represent $2 \\times$ spatial compression.](images/5.jpg)
        *该图像是示意图，展示了Wan-VAE框架。Wan-VAE可以通过 `4 imes 8 imes 8` 倍的压缩比例有效降低视频的时空维度，其中橙色矩形表示 `2 imes` 的时空压缩，绿色矩形表示 `2 imes` 的空间压缩。*

    下图（原文 Figure 6）详细解释了特征缓存机制在普通因果卷积和时间下采样中的工作方式。

    ![Figure 6: Our feature cache mechanism. (a) and (b) show how we use this mechanism in regular causal convolution and temporal downsampling, respectively.](images/6.jpg)
    *该图像是示意图，展示了我们特征缓存机制的应用。图（a）展示了在默认设置下的特征缓存机制，而图（b）展示了在 2 倍时间下采样中的应用。机制中使用了零填充和缓存填充来优化特征提取过程。*

*   **训练过程:** 采用三阶段训练法：
    1.  先在图像数据上训练一个 `2D` 版本的 `VAE`。
    2.  将训练好的 `2D VAE` 的权重“膨胀”成 `3D`，作为 `Wan-VAE` 的初始化，这比从零开始训练视频 `VAE` 快得多。此阶段使用低分辨率、短时长的视频加速收敛。
    3.  最后，在高质量、不同分辨率和时长的视频上进行微调，并引入 `GAN` 损失进一步提升重建质量。

### 4.2.2. 视频扩散变换器 (Video Diffusion Transformer)
这是 `Wan` 模型的主体部分，负责在 `VAE` 压缩的潜在空间中学习视频的生成过程。

*   **整体架构:**
    `Wan` 的主体是一个 `DiT` 架构。`Wan-VAE` 将视频编码为潜在表示 $x$，然后 $x$ 被切分成小块（`patch`），送入一系列 `Transformer` 模块进行处理。

    下图（原文 Figure 9）展示了 `Wan` 的整体架构。

    ![Figure 9: Architecture of the Wan.](images/9.jpg)
    *该图像是Wan的体系结构示意图，展示了在扩散过程中如何通过Wan编码器和Wan解码器处理数据。图中包含了一个时间步$t$、NiT块及交叉注意力机制，用于提高视频生成的效果。*

*   **`Transformer` 模块设计:**
    `Wan` 的 `Transformer` 模块（如下图所示）经过了精心设计，以平衡性能和效率。
    *   **条件注入:**
        *   **文本条件:** 通过<strong>交叉注意力 (cross-attention)</strong> 机制将 `umT5` 编码器输出的文本嵌入注入到模型中。
        *   **时间步条件:** 将扩散过程中的时间步 $t$ 编码后，输入到一个<strong>共享的 MLP (多层感知机)</strong> 中，该 MLP 为所有 `Transformer` 模块预测一组调制参数（如缩放和偏移），但每个模块会学习自己独特的偏置。这种共享设计在减少约 25% 参数的同时，提升了模型性能。

            下图（原文 Figure 10）展示了 `Wan` 的 `Transformer` 模块内部结构。

            ![Figure 10: Transformer block of Wan.](images/10.jpg)
            *该图像是Wan的Transformer块示意图。图中展示了V-Tokens和T-Tokens的处理流程，包括层归一化、自注意力、交叉注意力和前馈网络等模块结构，体现了模型的计算逻辑和架构层次。*

*   **文本编码器:** 选择了 `umT5` 作为文本编码器，因为它具备强大的多语言（中英）编码能力、在组合性上表现更优，并且收敛速度更快。

### 4.2.3. 训练方法 (Training Objective)
`Wan` 采用<strong>流匹配 (Flow Matching)</strong> 框架进行训练，具体使用的是<strong>矫正流 (Rectified Flows, RFs)</strong>。

*   **核心原理:**
    给定一个真实的潜在表示 $x_1$（来自 `VAE` 编码器）和一个纯噪声 $x_0$，模型的目标是学习一个从 $x_0$ 到 $x_1$ 的“直线”路径。在任意时间点 $t \in [0, 1]$，路径上的点 $x_t$ 可以通过线性插值得到：
    $x_t = t x_1 + (1 - t) x_0$
    这个路径上的“速度”向量 $v_t$ 是恒定的，即从起点到终点的方向：
    $$
    v_t = \frac{d x_t}{d t} = x_1 - x_0
    $$
    模型的任务就是预测这个速度向量 $v_t$。

*   **损失函数:**
    训练的损失函数是模型预测的速度 $u(x_t, c_{txt}, t; \theta)$ 与真实速度 $v_t$ 之间的均方误差 (`MSE`)。
    $$
    \mathcal{L} = \mathbb{E}_{x_0, x_1, c_{txt}, t} || u(x_t, c_{txt}, t; \theta) - v_t ||^2
    $$
    **符号解释:**
    *   $x_1$: 经过 `VAE` 编码的真实视频的潜在表示。
    *   $x_0$: 从标准正态分布 $\mathcal{N}(0, I)$ 采样的随机噪声。
    *   $t$: 从 `[0, 1]` 区间内随机采样的时间步。
    *   $x_t$: $x_0$ 和 $x_1$ 之间的线性插值点，作为模型的输入。
    *   $c_{txt}$: `umT5` 文本编码器输出的文本条件嵌入。
    *   $\theta$: 扩散变换器模型的参数。
    *   $u(x_t, c_{txt}, t; \theta)$: 模型在给定 $x_t$、文本条件 $c_{txt}$ 和时间步 $t$ 的情况下，预测出的速度向量。
    *   $v_t$: 训练的目标，即真实的“去噪”方向 $x_1 - x_0$。

*   **多阶段训练课程:**
    为了稳定高效地训练 140亿 (`14B`) 的大模型，`Wan` 采用了一个渐进式的训练策略：
    1.  **图像预训练:** 首先在低分辨率 (256px) 的图像上进行训练，让模型学会基本的文图对齐和几何结构。
    2.  <strong>图文联合训练 (阶段一):</strong> 引入视频数据，与 256px 的图像联合训练，视频分辨率为 392px。
    3.  <strong>图文联合训练 (阶段二):</strong> 将图像和视频的分辨率提升至 480px。
    4.  <strong>图文联合训练 (阶段三):</strong> 进一步将分辨率提升至 720px。
    5.  <strong>精调 (Post-training):</strong> 使用在 4.1.2 节中筛选出的高质量数据，在 480px 和 720px 分辨率上对模型进行微调。

# 5. 实验设置

## 5.1. 数据集
`Wan` 的训练未使用公开的标准数据集，而是基于团队从内部和公开渠道整理并深度清洗的**私有大规模数据集**。该数据集包含**数十亿级别的图像和视频**，总计处理了万亿 (`trillion`) 级别的词元 (`token`)。其特点如下：

*   **规模巨大:** 保证了模型能够学习到丰富多样的视觉概念和动态模式。
*   **质量极高:** 通过第 4.1 节详述的多维度、多阶段数据清洗流水线，过滤了大量低质量、不相关或有害的内容。
*   **多样性丰富:** 覆盖了广泛的主题、风格和运动类型，并通过聚类等方法确保了长尾数据的覆盖。
*   **高质量标注:** 通过自研的 `caption` 模型为数据生成了详细的文本描述，极大地提升了模型的指令跟随能力。

## 5.2. 评估指标
论文的评估体系结合了自研的综合基准和公认的第三方基准。

### 5.2.1. `Wan-Bench`
这是 `Wan` 团队自研的一个自动化、全面且与人类感知对齐的视频生成评估基准。它包含三大维度和 14 个细分指标。

下图（原文 Figure 13）展示了 `Wan-Bench` 覆盖的评估维度。

![Figure 13: The dimensions covered in WanBench.](images/13.jpg)
*该图像是图示，展示了 WanBench 的不同维度，主要分为动态质量、图像质量和指令跟随三个部分。动态质量包括大规模运动生成、人类伪影等；图像质量涵盖综合图像质量、场景生成质量和风格化能力；指令跟随则包括摄像机控制和多个对象的动作指令跟随等。*

*   <strong>动态质量 (Dynamic quality):</strong>
    *   `大幅度运动生成`: 使用光流法评估视频中运动的剧烈程度。
    *   `人体伪影`: 训练 `YOLOv3` 模型检测 AI 生成视频中常见的人体部位错误（如多余的手指）。
    *   `物理合理性与平滑度`: 使用多模态大模型 `Qwen2-VL` 判断视频是否违反物理定律（如物体穿模、不合常理的碰撞），以及运动是否流畅无卡顿。
    *   `像素级稳定性`: 检查视频静态区域是否存在不应有的闪烁或噪声。
    *   `ID 一致性`: 评估视频中同一对象（人、动物、物体）的外观是否在时间上保持一致。
*   <strong>图像质量 (Image quality):</strong>
    *   `综合图像质量`: 结合 `MANIQA` (保真度)、`LAION aesthetic predictor` (美学) 和 `MUSIQ` (美学) 三个模型的分数。
    *   `场景生成质量`: 评估场景的帧间一致性和文图对齐度。
    *   `风格化能力`: 使用 `Qwen2-VL` 判断模型是否准确复现了指定的艺术风格。
*   <strong>指令跟随 (Instruction following):</strong>
    *   `物体与空间关系`: 使用 `Qwen2-VL` 评估生成的视频是否准确包含了指定数量、类别和空间位置的物体。
    *   `相机控制`: 评估模型对平移、升降、缩放、航拍、跟拍等相机运动指令的执行准确度。
    *   `动作指令跟随`: 评估模型对人、动物、物体动作指令的理解和生成能力。

### 5.2.2. `VBench`
这是一个公开的、被广泛用于评估视频生成模型的基准测试套件。它将视频质量分解为16个与人类偏好对齐的维度，如美学质量、运动平滑度、语义一致性等。

### 5.2.3. 弗雷歇初始距离 (Fréchet Inception Distance, FID)
`FID` 是一个广泛用于评估生成模型（尤其是图像生成）性能的指标。它衡量了生成图像的特征分布与真实图像特征分布之间的距离。
1.  **概念定义:** `FID` 的核心思想是，高质量的生成模型应该能产出与真实数据在“语义特征”上非常相似的图像。它使用一个预训练的图像分类网络（通常是 Inception-v3）来提取图像的深层特征。然后，它计算两组特征（一组来自真实图像，一组来自生成图像）的均值和协方差，并通过一个公式来量化这两组统计数据之间的“距离”。`FID` 分数越低，表示生成图像的质量和多样性越接近真实图像。
2.  **数学公式:**
    $$
    \text{FID}(x, g) = ||\mu_x - \mu_g||^2_2 + \text{Tr}(\Sigma_x + \Sigma_g - 2(\Sigma_x \Sigma_g)^{1/2})
    $$
3.  **符号解释:**
    *   $x$ 和 $g$ 分别代表真实图像分布和生成图像分布。
    *   $\mu_x$ 和 $\mu_g$ 是真实图像和生成图像在 Inception 网络特征空间中的均值向量。
    *   $\Sigma_x$ 和 $\Sigma_g$ 是对应的协方差矩阵。
    *   $||\cdot||^2_2$ 表示向量的 L2 范数的平方。
    *   $\text{Tr}(\cdot)$ 表示矩阵的迹（主对角线元素之和）。

## 5.3. 对比基线
论文将 `Wan` 与市场上主流的商业模型和开源模型进行了比较。
*   <strong>商业模型 (Commercial Models):</strong>
    *   `Sora` (OpenAI)
    *   `Kling` (快手)
    *   `Hailuo` (MiniMax)
    *   `Runway` (Runway)
    *   `Vidu` (生数科技)
    *   以及一些内部代号为 `CN-TopA`, `CN-TopB`, `CN-TopC` 的国内顶尖商业模型。
*   <strong>开源模型 (Open-source Models):</strong>
    *   `Mochi` (Genmo)
    *   `CogVideoX` (清华大学)
    *   `Hunyuan` (腾讯)

# 6. 实验结果与分析

## 6.1. 核心结果分析
`Wan` 在多个维度的评测中均展现出业界领先的性能。

### 6.1.1. `Wan-Bench` 自动评估结果
在自研的 `Wan-Bench` 上，`Wan 14B` 模型的综合加权得分最高，超越了所有参与比较的商业和开源模型。

以下是原文 Table 2 的结果，展示了各模型在 `Wan-Bench` 上的详细得分：

<table>
<thead>
<tr>
<th>Wan-Bench Dimension</th>
<th>CNTopB</th>
<th>Hunyuan</th>
<th>Mochi</th>
<th>CNTopA</th>
<th>Sora</th>
<th>Wan 1.3B</th>
<th>Wan 14B</th>
</tr>
</thead>
<tbody>
<tr>
<td>Large Motion Generation</td>
<td>0.405</td>
<td>0.413</td>
<td>0.420</td>
<td>0.284</td>
<td>0.482</td>
<td>0.468</td>
<td>0.415</td>
</tr>
<tr>
<td>Human Artifacts</td>
<td>0.712</td>
<td>0.734</td>
<td>0.622</td>
<td>0.833</td>
<td>0.786</td>
<td>0.707</td>
<td>0.691</td>
</tr>
<tr>
<td>Pixel-level Stability</td>
<td>0.977</td>
<td>0.983</td>
<td>0.981</td>
<td>0.974</td>
<td>0.952</td>
<td>0.976</td>
<td>0.972</td>
</tr>
<tr>
<td>ID Consistency</td>
<td>0.940</td>
<td>0.935</td>
<td>0.930</td>
<td>0.936</td>
<td>0.925</td>
<td>0.938</td>
<td>0.946</td>
</tr>
<tr>
<td>Physical Plausibility</td>
<td>0.836</td>
<td>0.898</td>
<td>0.728</td>
<td>0.759</td>
<td>0.933</td>
<td>0.912</td>
<td>0.939</td>
</tr>
<tr>
<td>Smoothness</td>
<td>0.765</td>
<td>0.890</td>
<td>0.530</td>
<td>0.880</td>
<td>0.930</td>
<td>0.790</td>
<td>0.910</td>
</tr>
<tr>
<td>Comprehensive Image Quality</td>
<td>0.621</td>
<td>0.605</td>
<td>0.530</td>
<td>0.668</td>
<td>0.665</td>
<td>0.596</td>
<td>0.640</td>
</tr>
<tr>
<td>Scene Generation Quality</td>
<td>0.369</td>
<td>0.373</td>
<td>0.368</td>
<td>0.386</td>
<td>0.388</td>
<td>0.385</td>
<td>0.386</td>
</tr>
<tr>
<td>Stylization Ability</td>
<td>0.623</td>
<td>0.386</td>
<td>0.403</td>
<td>0.346</td>
<td>0.606</td>
<td>0.430</td>
<td>0.328</td>
</tr>
<tr>
<td>Single Object Accuracy</td>
<td>0.987</td>
<td>0.912</td>
<td>0.949</td>
<td>0.942</td>
<td>0.932</td>
<td>0.930</td>
<td>0.952</td>
</tr>
<tr>
<td>Multiple Object Accuracy</td>
<td>0.840</td>
<td>0.850</td>
<td>0.693</td>
<td>0.880</td>
<td>0.882</td>
<td>0.859</td>
<td>0.860</td>
</tr>
<tr>
<td>Spatial Position Accuracy</td>
<td>0.518</td>
<td>0.464</td>
<td>0.512</td>
<td>0.434</td>
<td>0.458</td>
<td>0.476</td>
<td>0.590</td>
</tr>
<tr>
<td>Camera Control</td>
<td>0.465</td>
<td>0.406</td>
<td>0.605</td>
<td>0.529</td>
<td>0.380</td>
<td>0.483</td>
<td>0.527</td>
</tr>
<tr>
<td>Action Instruction Following</td>
<td>0.917</td>
<td>0.735</td>
<td>0.907</td>
<td>0.783</td>
<td>0.721</td>
<td>0.844</td>
<td>0.860</td>
</tr>
<tr>
<td><strong>Weighted Score</strong></td>
<td><strong>0.690</strong></td>
<td><strong>0.673</strong></td>
<td><strong>0.639</strong></td>
<td><strong>0.693</strong></td>
<td><strong>0.700</strong></td>
<td><strong>0.689</strong></td>
<td><strong>0.724</strong></td>
</tr>
</tbody>
</table>

**分析:**
*   `Wan 14B` 的<strong>加权总分 (Weighted Score) 达到 0.724</strong>，在所有模型中排名第一，超过了 `Sora` (0.700) 和其他顶尖商业模型。
*   `Wan 14B` 在<strong>物理合理性 (Physical Plausibility)</strong>、<strong>ID 一致性 (ID Consistency)</strong> 和<strong>空间位置准确性 (Spatial Position Accuracy)</strong> 等关键指标上表现尤为突出，显示了其对世界模型的深刻理解。
*   值得注意的是，高效的 `Wan 1.3B` 模型总分也达到了 0.689，与一些商业模型相当，展示了其卓越的能效比。

### 6.1.2. `VBench` 公开排行榜结果
在公认的 `VBench` 排行榜上，`Wan` 同样取得了领先地位。

以下是原文 Table 4 的结果：

<table>
<thead>
<tr>
<th>Model Name</th>
<th>Quality Score</th>
<th>Semantic Score</th>
<th>Total Score</th>
</tr>
</thead>
<tbody>
<tr>
<td>MiniMax-Video-01 (MiniMax, 2024.09)</td>
<td>84.85%</td>
<td>77.65%</td>
<td>83.41%</td>
</tr>
<tr>
<td>Hunyuan (Open-Source Version) (Kong et al., 2024)</td>
<td>85.09%</td>
<td>75.82%</td>
<td>83.24%</td>
</tr>
<tr>
<td>Gen-3 (2024-07) (Runway, 2024.06)</td>
<td>84.11%</td>
<td>75.17%</td>
<td>82.32%</td>
</tr>
<tr>
<td>CogVideoX1.5-5B (5s SAT prompt-optimized) (Yang et al., 2025b)</td>
<td>82.78%</td>
<td>79.76%</td>
<td>82.17%</td>
</tr>
<tr>
<td>Kling (2024-07 high-performance mode) (Kuaishou, 2024.06)</td>
<td>83.39%</td>
<td>75.68%</td>
<td>81.85%</td>
</tr>
<tr>
<td>Sora (OpenAI, 2024)</td>
<td>85.51%</td>
<td>79.35%</td>
<td>84.28%</td>
</tr>
<tr>
<td>Wan 1.3B</td>
<td>84.92%</td>
<td>80.10%</td>
<td>83.96%</td>
</tr>
<tr>
<td><strong>Wan 14B (2025-02-24)</strong></td>
<td><strong>86.67%</strong></td>
<td><strong>84.44%</strong></td>
<td><strong>86.22%</strong></td>
</tr>
</tbody>
</table>

**分析:**
*   `Wan 14B` 的总分高达 **86.22%**，显著超过了包括 `Sora` (84.28%) 在内的所有其他模型，登顶排行榜。
*   `Wan 1.3B` 的表现同样出色，得分 83.96%，超过了 `Hunyuan` 开源版和 `Kling 1.0` 等强大的竞争对手。

### 6.1.3. 人工评估结果
在超过700个任务的人工盲评中，`Wan 14B` 相比其他商业模型在**视觉质量、运动质量、匹配度和综合排名**上均获得了更高的胜率。

以下是原文 Table 3 的结果，展示了 `Wan 14B` 与其他模型进行成对比较时的胜率：

<table>
<thead>
<tr>
<th></th>
<th>CN-TopA</th>
<th>CN-TopB</th>
<th>CN-TopC</th>
<th>Runway</th>
<th>All Rounds</th>
</tr>
</thead>
<tbody>
<tr>
<td>Visual Quality</td>
<td>30.6%</td>
<td>15.9%</td>
<td>27.8%</td>
<td>48.1%</td>
<td>5710</td>
</tr>
<tr>
<td>Motion Quality</td>
<td>16.1%</td>
<td>9.7%</td>
<td>14.9%</td>
<td>40.3%</td>
<td>5785</td>
</tr>
<tr>
<td>Matching</td>
<td>46.0%</td>
<td>57.9%</td>
<td>56.7%</td>
<td>69.1%</td>
<td>5578</td>
</tr>
<tr>
<td>Overall Ranking</td>
<td>44.0%</td>
<td>44.0%</td>
<td>48.9%</td>
<td>67.6%</td>
<td>5560</td>
</tr>
</tbody>
</table>

**分析:** 表中数值代表 `Wan 14B` 在与对应模型比较时，被评为更优的比例。例如，在综合排名 (`Overall Ranking`) 中，`Wan 14B` 对 `Runway` 的胜率高达 67.6%，显示出明显的人类偏好优势。

## 6.2. 消融实验/参数分析
论文对几个关键设计选择进行了消融实验，以验证其有效性。

*   <strong>自适应归一化层 (`adaLN`) 的设计:</strong>
    实验比较了四种 `adaLN` 层的配置：全共享参数（`Wan` 的方案）、半共享、不共享以及增加网络深度。
    
    下图（原文 Figure 16）展示了不同配置下的训练损失曲线。

    ![Figure 16: Training loss curve with different configurations.](images/23.jpg)
    *该图像是图表，展示了不同配置下的训练损失曲线。横轴为训练步数，纵轴为训练损失，各曲线代表不同的模型配置，包括 Full-shared-adaLN-1.3B、Full-shared-adaLN-1.5B、Half-shared-adaLN-1.5B 及 Non-shared-adaLN-1.7B。*
    
    **结论:** 实验发现，将参数更多地用于**增加网络深度**（`Full-shared-adaLN-1.5B`），而不是用于 `adaLN` 层本身（`Half-shared-adaLN-1.5B` 和 `Non-shared-AdaLN-1.7B`），能带来更低的训练损失和更好的性能。这验证了 `Wan` 采用的**完全共享 `adaLN` 参数**的设计是高效且有效的。

*   **文本编码器的选择:**
    实验比较了 `umT5`、`Qwen2.5-7B` 和 `GLM-4-9B` 三种双语文本编码器。
    
    下图（原文 Figure 17）展示了不同编码器下的训练损失曲线。

    ![Figure 17: Training loss curve with different text encoders.](images/24.jpg)
    *该图像是一个训练损失曲线图，展示了不同文本编码器（umT5、GLM-4-9B 和 Qwen2.5-7B-Instruct）在训练步骤上的损失变化。曲线随着训练步骤的增加而下降，反映出模型性能的提升。*
    
    **结论:** `umT5` 展现出最快的收敛速度和最低的训练损失。论文分析认为，`umT5` 的双向注意力机制比 `decoder-only` 模型的因果注意力更适合扩散模型。这证明了选择 `umT5` 作为文本编码器的正确性。

*   **自编码器的选择:**
    实验比较了 `Wan-VAE`（使用 L1 重建损失）和一个变体 `VAE-D`（使用扩散损失）。

    以下是原文 Table 5 的结果，展示了两种 `VAE` 在文生图任务上的 `FID` 分数：

    <table>
    <thead>
    <tr>
    <th>Model</th>
    <th>VAE</th>
    <th>VAE-D</th>
    </tr>
    </thead>
    <tbody>
    <tr>
    <td>10k steps</td>
    <td>42.60</td>
    <td>44.21</td>
    </tr>
    <tr>
    <td>15k steps</td>
    <td>40.55</td>
    <td>41.16</td>
    </tr>
    </tbody>
    </table>

    **结论:** `Wan-VAE` 始终取得了更低的 `FID` 分数，表明其重建质量更高，更有利于后续扩散模型的训练。

# 7. 总结与思考

## 7.1. 结论总结
这篇报告成功地介绍并发布了 `Wan`，一个在视频生成领域树立了新标杆的开源基础模型。论文全面阐述了 `Wan` 的架构设计、训练策略、数据处理流程和评估方法。通过系统性的创新，`Wan` 在性能上不仅超越了所有现存的开源模型，甚至在多个维度上优于顶尖的商业模型。

`Wan` 的主要贡献在于：
1.  **发布了性能领先的 `14B` 模型和高效的 `1.3B` 模型**，兼顾了前沿研究和大众应用的需求。
2.  **展示并开源了一整套成熟的技术方案**，包括新颖的 `Wan-VAE`、精细的数据流水线和高效的训练/推理优化，为社区提供了宝贵的实践经验。
3.  **验证了其在多个下游任务中的强大泛化能力**，如视频编辑、个性化生成等，证明了其作为“基础模型”的价值。
4.  **通过完全开源，极大地推动了视频生成技术的民主化**，为学术界和产业界的研究者和开发者提供了强大的新工具。

## 7.2. 局限性与未来工作
论文作者坦诚地指出了当前 `Wan` 模型存在的局限性，并规划了未来的研究方向：

*   **局限性:**
    1.  **大幅度运动下的细节保持:** 在生成如体育、舞蹈等剧烈运动场景时，保持精细的细节仍然是一个挑战，这也是整个视频生成领域的共同难题。
    2.  **高昂的计算成本:** 即使经过优化，`14B` 模型的推理成本依然很高，在单张高端 GPU 上生成一段视频需要约30分钟，这限制了其广泛应用。
    3.  **缺乏领域特化知识:** 作为通用视频模型，`Wan` 在特定专业领域（如教育、医疗）的生成能力可能不足。

*   **未来工作:**
    1.  **持续扩展模型与数据规模:** 团队计划继续扩大模型和数据量，以攻克视频生成领域最前沿的挑战。
    2.  **提升效率与可访问性:** 进一步研究模型压缩、加速算法，降低使用门槛。
    3.  **赋能社区共同发展:** 希望通过开源，激励社区在 `Wan` 的基础上开发出适用于各种垂直领域的专用模型。

## 7.3. 个人启发与批判
这篇论文给我带来了深刻的启发，同时也引发了一些思考。

*   **启发:**
    1.  **系统工程的胜利:** `Wan` 的成功再次证明，构建一个顶级的 AI 模型远不止是提出一个新颖的算法。它是一个庞大的系统工程，需要对数据、模型、训练、推理和评估等每一个环节都进行极致的优化和创新。论文中对数据处理流水线的详尽描述，其价值不亚于模型本身。
    2.  **开源的真正力量:** `Wan` 团队不仅开源了模型权重，更重要的是分享了其背后的“方法论”和“实践经验”。这种深度的开放，能够真正赋能整个社区，加速技术的迭代和创新。
    3.  **平衡的艺术:** 同时推出 `14B` 和 `1.3B` 两个版本的模型，体现了团队对学术前沿探索和现实世界应用落地之间平衡的深刻理解。这为其他大型模型的研究提供了很好的范例。

*   **批判性思考:**
    1.  **评估基准的独立性:** 论文的核心评估大量依赖于自研的 `Wan-Bench`。尽管其设计看起来非常全面，但评估体系的开发和使用都由同一团队完成，其客观性和公正性可能会受到质疑。特别是其中使用 `Qwen2-VL` 进行的复杂语义评估（如物理合理性），其评估结果的可靠性需要更多独立的第三方验证。
    2.  **与闭源模型的比较:** 论文中与 `Sora` 等闭源模型的比较是基于它们公开发布的视频样本。这些样本可能经过精心挑选，无法完全代表模型的平均水平或全部能力。因此，这种比较的公平性存在一定的局限。
    3.  <strong>“开源”</strong>的实际门槛: 虽然模型是开源的，但要复现或微调 `14B` 规模的模型，仍然需要巨大的计算资源，这对于大多数学术研究机构和独立开发者来说依旧是一个难以逾越的障碍。真正的“民主化”之路，或许还需要在模型效率上有更革命性的突破。