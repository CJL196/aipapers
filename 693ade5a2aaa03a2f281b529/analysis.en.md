# 1. Bibliographic Information

## 1.1. Title
Diffusion Policy: Visuomotor Policy Learning via Action Diffusion

## 1.2. Authors
The authors are Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchfiel, Russ Tedrake, and Shuran Song. Their affiliations include:
*   ¹ Columbia University
*   ² Toyota Research Institute (TRI)
*   ³ Massachusetts Institute of Technology (MIT)
*   ⁴ NVIDIA

    This team comprises prominent researchers in robotics, machine learning, and computer vision. Shuran Song is a well-known professor at Columbia leading the Columbia Artificial Intelligence and Robotics (CAIR) lab. Russ Tedrake is a leading figure in robotics and control theory at MIT. The collaboration between academia (Columbia, MIT) and an industrial research lab (TRI) highlights the work's focus on both foundational research and practical application.

## 1.3. Journal/Conference
The paper was published at **Robotics: Science and Systems (RSS) 2023**. RSS is a premier, highly selective international conference for robotics research. It is known for its rigorous peer-review process and for publishing high-impact work that bridges theory and practice in robotics. Publication at RSS signifies that the work is considered a significant contribution to the field.

## 1.4. Publication Year
2023. The initial preprint was made available on arXiv in March 2023.

## 1.5. Abstract
The paper introduces **Diffusion Policy**, a novel approach for learning robot visuomotor policies from demonstrations. This method represents the policy as a conditional denoising diffusion process, where actions are generated by iteratively refining random noise based on visual and state observations. The authors benchmark Diffusion Policy across 12 tasks from 4 different manipulation benchmarks, demonstrating an average performance improvement of 46.9% over existing state-of-the-art methods. The core mechanism involves learning the gradient of the action-distribution score function and using it for optimization during inference via Langevin dynamics. The paper highlights several advantages of this formulation: it gracefully handles multimodal action distributions, scales to high-dimensional action spaces (like action sequences), and exhibits remarkable training stability. To make this approach practical for physical robots, the authors present key technical contributions, including the use of receding horizon control, an efficient visual conditioning scheme, and a time-series diffusion transformer.

## 1.6. Original Source Link
The paper is available as a preprint on arXiv.
*   **Original Source Link:** https://arxiv.org/abs/2303.04137
*   **PDF Link:** https://arxiv.org/pdf/2303.04137v5.pdf
*   **Publication Status:** The paper was officially published at the RSS 2023 conference. The linked version is the extended preprint.

# 2. Executive Summary

## 2.1. Background & Motivation
The central problem addressed is **visuomotor policy learning from demonstrations**, a subfield of robotics also known as **Behavior Cloning (BC)**. The goal is to train a robot to mimic a task demonstrated by a human expert by learning a mapping from observations (e.g., camera images, robot joint angles) to actions (motor commands).

While conceptually simple, this supervised learning approach faces unique challenges in robotics:
1.  **Multimodality:** For a given situation, there can be multiple, equally valid actions or sequences of actions. For example, to push an object, a robot could approach it from the left or the right. Simple regression models tend to average these modes, resulting in an invalid, "in-between" action.
2.  **Temporal Correlation:** Robot actions are not independent events. They form a continuous, time-correlated sequence. A policy must generate smooth and consistent actions over time.
3.  **High Precision:** Many manipulation tasks require highly precise movements, which can be difficult for a learned policy to replicate.

    Prior research has attempted to solve these issues with methods like Mixture Density Networks (which model a limited number of modes), action discretization (which struggles with high-dimensional action spaces), or implicit energy-based models (which are powerful but notoriously unstable to train). These methods often represent a bottleneck, limiting the performance that can be extracted from a given dataset of demonstrations.

The paper's innovative entry point is to reframe policy learning as a **generative modeling problem** and leverage the power of **Denoising Diffusion Models**. These models have shown extraordinary success in generating complex, high-dimensional data like images. The authors' core idea is that the same mechanism used to "denoise" a random image into a coherent picture can be used to "denoise" random noise into a coherent, expert-like sequence of robot actions, conditioned on the robot's current observations.

## 2.2. Main Contributions / Findings
The paper's primary contributions are both a novel framework and the practical engineering required to make it work on real robots.

1.  **A New Policy Representation:** The paper introduces `Diffusion Policy`, a visuomotor policy that learns to generate actions through a conditional denoising diffusion process. This is a fundamental shift from prior explicit (direct mapping) and implicit (energy optimization) policy structures.

2.  **Inherent Advantages of the Diffusion Formulation:** The findings show that this representation naturally provides powerful benefits:
    *   **Expressiveness:** It can model complex, continuous, and multimodal action distributions without pre-specifying the number of modes.
    *   **Scalability:** It scales effectively to high-dimensional outputs, allowing it to predict an entire *sequence* of future actions, which promotes temporal consistency and avoids myopic behavior.
    *   **Training Stability:** Unlike prior energy-based models (`IBC`) that rely on unstable negative sampling, Diffusion Policy training is stable because it directly learns the score function (gradient of the log-probability), bypassing the need to estimate a difficult normalization constant.

3.  **Key Technical Innovations for Real-World Robotics:**
    *   **Receding Horizon Control:** The policy predicts a future sequence of actions, but only executes a small portion of it before re-planning. This achieves a balance between long-term planning (from the sequence) and responsiveness to new observations (from re-planning).
    *   **Efficient Visual Conditioning:** The diffusion process is conditioned on visual features that are extracted only once per inference cycle, drastically reducing computation and enabling real-time control.
    *   **Time-series Diffusion Transformer:** A novel transformer-based architecture for the denoising network that is better suited for tasks requiring high-frequency action changes, mitigating the over-smoothing bias of typical CNNs.

4.  **Comprehensive Empirical Validation:** The authors conduct an extensive evaluation on 15 different tasks across simulation and the real world (including complex bimanual manipulation). Diffusion Policy is shown to **outperform state-of-the-art baselines by an average of 46.9%**, providing strong evidence that the policy representation itself is a critical factor in the success of behavior cloning.

# 3. Prerequisite Knowledge & Related Work

## 3.1. Foundational Concepts

### 3.1.1. Behavior Cloning (BC)
Behavior Cloning is a form of **Imitation Learning** where the goal is to learn a policy for an agent (e.g., a robot) by observing an expert (e.g., a human teleoperator). In its simplest form, BC is treated as a standard supervised learning problem. An expert provides a dataset of observation-action pairs $(\mathbf{o}, \mathbf{a})$. The learner then trains a model, the **policy** $\pi_\theta(\mathbf{o}) \rightarrow \mathbf{a}$, to predict the expert's action given an observation. While simple, BC can be very effective, but suffers from issues like distributional shift (the policy encounters states not seen in the training data and doesn't know how to recover) and the multimodality problem described earlier.

### 3.1.2. Visuomotor Policy
A visuomotor policy is a specific type of policy where the observations primarily consist of visual data (from cameras), often combined with the robot's own state information (proprioception), such as joint angles or end-effector pose. The policy's output is a motor command to control the robot. This paper focuses on learning such policies end-to-end, from raw pixels to robot actions.

### 3.1.3. Denoising Diffusion Probabilistic Models (DDPMs)
DDPMs are a class of powerful generative models that learn to create data samples by reversing a gradual noising process. They consist of two main parts:

*   **Forward Process (Fixed):** This process takes a clean data sample $\mathbf{x}_0$ (e.g., an image) and gradually adds a small amount of Gaussian noise over $K$ discrete timesteps. At each step $k$, the new sample $\mathbf{x}_k$ is generated from the previous one $\mathbf{x}_{k-1}$. After $K$ steps, $\mathbf{x}_K$ is indistinguishable from pure Gaussian noise.

*   **Reverse Process (Learned):** The goal is to learn a neural network, $\varepsilon_\theta(\mathbf{x}_k, k)$, that can reverse this process. Starting from a random noise sample $\mathbf{x}_K \sim \mathcal{N}(0, \mathbf{I})$, the model iteratively denoises it, step by step, to produce a clean sample $\mathbf{x}_0$. At each step $k$, the network predicts the noise $\varepsilon$ that was added to create $\mathbf{x}_k$. By subtracting a portion of this predicted noise, the model generates a slightly cleaner sample $\mathbf{x}_{k-1}$. This iterative refinement allows the model to construct highly complex and detailed data structures.

    The training objective is typically a simple Mean Squared Error (MSE) loss, where the network is trained to predict the noise that was added to a clean sample at a random timestep $k$.

### 3.1.4. Energy-Based Models (EBMs)
EBMs define a probability distribution over a data space by assigning a scalar "energy" value $E_\theta(\mathbf{x})$ to each data point $\mathbf{x}$. The probability is defined as:
\$
p_\theta(\mathbf{x}) = \frac{e^{-E_\theta(\mathbf{x})}}{Z(\theta)}
\$
where $Z(\theta) = \int e^{-E_\theta(\mathbf{x})} d\mathbf{x}$ is the normalization constant, known as the partition function. Data points that are "good" or "likely" are assigned low energy, while unlikely points are assigned high energy. A major challenge in training EBMs is that $Z(\theta)$ is almost always intractable to compute. Training often relies on contrastive methods (like `InfoNCE`) that push down the energy of real data ("positive samples") and push up the energy of randomly generated data ("negative samples"), which can lead to training instability.

## 3.2. Previous Works

The paper positions itself relative to several categories of behavior cloning methods:

*   **Explicit Policies:** These policies directly map observations to actions.
    *   **Simple Regression:** Using models like Multi-Layer Perceptrons (MLPs) or Convolutional Neural Networks (CNNs) to predict a single action. Fails on multimodal tasks.
    *   **Mixture Density Networks (MDNs):** These models, often combined with LSTMs for temporal modeling (`LSTM-GMM`), output the parameters of a Gaussian Mixture Model (GMM). This allows them to represent a fixed number of action modes, but they can be difficult to tune and may not capture the full complexity of the action distribution. `LSTM-GMM` is a key baseline.
    *   **Action Discretization:** Methods like `Transporter Networks` or `BET` (Behavior Transformers) turn the continuous action space into a discrete one. The policy then becomes a classifier that predicts which action "bin" or "token" to select. This avoids the averaging problem but suffers from the curse of dimensionality as the action space grows. `BET` is another key baseline.

*   **Implicit Policies:** These policies define the action distribution implicitly via an energy function.
    *   **Implicit Behavioral Cloning (IBC):** This is the most relevant prior work in this category and a major baseline. `IBC` models the conditional policy $p(\mathbf{a} | \mathbf{o})$ as an EBM. To generate an action, it performs optimization at inference time to find a low-energy (high-probability) action $\mathbf{a}$. Training uses an `InfoNCE`-style loss, which requires generating "negative" action samples and is a known source of training instability.

*   **Diffusion Models for Planning:**
    *   **Diffuser (Janner et al., 2022a):** This work uses diffusion models for offline reinforcement learning. It learns to generate entire trajectories (sequences of states and actions) to plan a path to a goal. Diffusion Policy differs by learning a *conditional policy* $p(\mathbf{A}_t | \mathbf{O}_t)$ for closed-loop control, rather than a joint distribution for open-loop planning. This makes it more suitable for a reactive policy.

## 3.3. Technological Evolution
The field of behavior cloning has evolved from simple regression models to increasingly sophisticated representations to tackle the multimodality and precision challenges:
1.  **Direct Regression (MLP/CNN):** Simple but limited.
2.  **Explicit Multimodal Models (MDN/GMM):** An improvement, but still constrained by the parametric form of the mixture model.
3.  **Discretization/Classification:** A different paradigm that trades quantization error for easier multimodal modeling, but scales poorly.
4.  **Implicit Energy-Based Models (IBC):** A major leap in expressiveness, able to represent arbitrary distributions, but plagued by training instability.
5.  **Diffusion Models (Diffusion Policy):** The work in this paper represents the next step. It retains the high expressiveness of implicit models while achieving stable training by reformulating the problem from energy learning to score matching (noise prediction).

## 3.4. Differentiation Analysis
*   **vs. Explicit Policies (`LSTM-GMM`, `BET`):** `Diffusion Policy` is far more expressive. It can model continuous, arbitrarily complex action distributions without being confined to a fixed number of Gaussian modes or discrete action bins.
*   **vs. Implicit Policies (`IBC`):** The core innovation is in the **training objective**. `IBC` trains an energy function $E_\theta(\mathbf{o}, \mathbf{a})$ using a contrastive loss that requires negative samples, leading to instability. `Diffusion Policy` trains a noise prediction network $\varepsilon_\theta(\mathbf{o}, \mathbf{a}, k)$, which is equivalent to learning the score function $\nabla_{\mathbf{a}} \log p(\mathbf{a} | \mathbf{o})$. This objective does not require negative sampling or estimating the intractable partition function $Z(\theta)$, resulting in significantly more stable training.
*   **vs. `Diffuser`:** `Diffusion Policy` is designed as a **closed-loop policy** for real-time control, whereas `Diffuser` is an **open-loop planner**. Diffusion Policy conditions on current observations to produce a near-future action sequence, making it computationally cheaper and more reactive.

# 4. Methodology

## 4.1. Principles
The central principle of `Diffusion Policy` is to model the robot's policy $\pi(\mathbf{A}_t | \mathbf{O}_t)$—the probability of an action sequence $\mathbf{A}_t$ given an observation sequence $\mathbf{O}_t$—as a conditional denoising diffusion process.

Instead of directly outputting an action, the policy learns the **gradient field** of the action distribution. During inference, it starts with a randomly generated noisy action sequence and iteratively "descends" this gradient field (or more accurately, performs Langevin dynamics) to find a high-probability, expert-like action sequence. This process allows it to navigate complex, multi-modal energy landscapes and converge to one of the valid modes.

## 4.2. Core Methodology In-depth

The methodology builds upon standard Denoising Diffusion Probabilistic Models (DDPMs) and adapts them for visuomotor control.

### 4.2.1. Denoising Diffusion Probabilistic Models (DDPMs)
A DDPM is defined by a forward noising process and a reverse denoising process.

The **reverse (generative) process** starts with a sample from a standard Gaussian distribution, $\mathbf{x}^K \sim \mathcal{N}(0, \mathbf{I})$, and iteratively refines it over $K$ steps to produce a clean sample $\mathbf{x}^0$. Each step is defined by the following equation:

\$
\mathbf { x } ^ { k - 1 } = \alpha ( \mathbf { x } ^ { k } - \gamma \varepsilon _ { \theta } ( \mathbf { x } ^ { k } , k ) + \mathcal { N } \big ( 0 , \sigma ^ { 2 } I \big ) ) ,
\$
(Equation 1 from the paper)

where:
*   $\mathbf{x}^k$: The (noisy) data sample at denoising step $k$.
*   $\varepsilon_{\theta}(\mathbf{x}^k, k)$: A neural network with parameters $\theta$ that takes the noisy sample $\mathbf{x}^k$ and the current step index $k$ as input, and predicts the noise that was added to the original clean sample to obtain $\mathbf{x}^k$.
*   $\alpha, \gamma, \sigma$: These are functions of the step $k$ and are known as the **noise schedule**. They control the step size and amount of stochasticity in the denoising process.
*   $\mathcal{N}(0, \sigma^2 \mathbf{I})$: A small amount of Gaussian noise added at each step, which corresponds to Stochastic Langevin Dynamics.

    This process can be interpreted as performing noisy gradient descent on an energy function $E(\mathbf{x})$, where the noise prediction network $\varepsilon_\theta$ is implicitly learning the gradient field $\nabla E(\mathbf{x})$.

The **training process** for the network $\varepsilon_\theta$ is remarkably simple. We take a clean data sample $\mathbf{x}^0$ from the training set, pick a random noise level (timestep) $k$, add a corresponding amount of random Gaussian noise $\varepsilon^k$, and train the network to predict this noise. The loss function is a Mean Squared Error (MSE):

\$
\mathcal { L } = MSE ( \varepsilon ^ { k } , \varepsilon _ { \theta } ( \mathbf { x } ^ { 0 } + \varepsilon ^ { k } , k ) )
\$
(Equation 3 from the paper)

where:
*   $\mathbf{x}^0$: A ground-truth sample from the dataset.
*   $\varepsilon^k$: A random noise vector sampled from a Gaussian distribution with variance corresponding to step $k$.
*   $\mathbf{x}^0 + \varepsilon^k$: The noisy input given to the network.
*   $\varepsilon_{\theta}(\cdot)$: The noise prediction network.

### 4.2.2. Adapting DDPM for Visuomotor Policy
The paper makes two crucial modifications to apply DDPMs as a robot policy.

1.  **Action Sequence Prediction with Receding Horizon Control:**
    The policy does not predict a single action but a sequence of future actions.
    *   **Input:** At time $t$, the policy takes the latest $T_o$ steps of observations, $\mathbf{O}_t = \{o_{t-T_o+1}, ..., o_t\}$.
    *   **Output:** The diffusion process generates a sequence of $T_p$ future actions, $\mathbf{A}_t^0 = \{a_t, ..., a_{t+T_p-1}\}$.
    *   **Execution:** For robustness and reactivity, only the first $T_a$ actions of the predicted sequence are executed on the robot ($T_a \le T_p$). After $T_a$ steps, the robot observes the new state of the world, and the policy re-plans by generating a new action sequence. This is known as **receding horizon control**. This approach provides temporal consistency (from the sequence prediction) while allowing for closed-loop corrections.

2.  **Visual Observation Conditioning:**
    The diffusion process must be conditioned on the robot's observations $\mathbf{O}_t$. The model learns the conditional distribution $p(\mathbf{A}_t | \mathbf{O}_t)$. This is achieved by modifying the denoising step and the training loss to include $\mathbf{O}_t$ as a conditioning input.

    The **conditional denoising step** becomes:
    \$
    \mathbf { A } _ { t } ^ { k - 1 } = \alpha ( \mathbf { A } _ { t } ^ { k } - \gamma \varepsilon _ { \theta } ( \mathbf { O } _ { t } , \mathbf { A } _ { t } ^ { k } , k ) + \mathcal { N } \big ( 0 , \sigma ^ { 2 } I \big ) )
    \$
    (Equation 4 from the paper)

    The **conditional training loss** becomes:
    \$
    \mathcal { L } = MSE ( \boldsymbol { \varepsilon } ^ { k } , \boldsymbol { \varepsilon } _ { \theta } ( \mathbf { O } _ { t } , \mathbf { A } _ { t } ^ { 0 } + \boldsymbol { \varepsilon } ^ { k } , k ) )
    \$
    (Equation 5 from the paper)

    A critical design choice is that the observation features $\mathbf{O}_t$ are only used as a **conditioning input** to the noise prediction network $\varepsilon_\theta$. They are not part of the data being denoised. This means the visual encoder that processes $\mathbf{O}_t$ only needs to run once per inference cycle, not at every denoising step $k$. This makes inference significantly faster and suitable for real-time control.

The overall architecture is depicted in Figure 2 from the paper.

![Figure 2. Diffusion Policy Overview a) General formulation. At time step $t$ , the policy takes the latest `T _ { o }` steps of observation data `O _ { t }` as input and outputs `T _ { a }` steps of actions `A _ { t }` . b) In the CNN-based Diffusion Policy, FiLM (Feature-wise Linear Modulation) Per l.8conditioning f he servation fe `O _ { t }` isapplied to every convolution layer, channel-wise. Starting from $\\mathbf { A } _ { t } ^ { K }$ drawn from Gaussian noise, the output of noise-prediction network $\\varepsilon _ { \\theta }$ is subtracted, repeating $K$ times to get ${ \\bf A } _ { t } ^ { 0 }$ , the denoised a $\\mathbf { O } _ { t }$ is passed itself and previous action embeddings (causal attention) using the attention mask illustrated.](images/2.jpg)
*该图像是一个示意图，展示了扩散策略的框架。上方部分说明了输入的图像观测序列和预测的动作序列之间的关系，同时突出了扩散政策的核心过程，使用 $ abla E(A_t) $ 来优化动作输出。下方部分则分别介绍了基于CNN和基于Transformer的扩散政策架构，显示了FiLM条件调节和交叉注意力机制的应用。*

### 4.2.3. Key Design Decisions

The paper details several key implementation choices that are crucial for performance.

*   **Network Architecture for $\varepsilon_\theta$:**
    *   **CNN-based:** This architecture uses a 1D temporal U-Net-style CNN. The conditioning observation $\mathbf{O}_t$ is incorporated into the network using **Feature-wise Linear Modulation (FiLM)** layers. A FiLM layer takes features from the conditioning input (the observation) and uses them to compute a scale and shift, which are then applied element-wise to the feature maps of the main network. This allows the observations to modulate the behavior of the denoising process at every level of the network.
    *   **Time-series Diffusion Transformer:** To combat the over-smoothing tendency of CNNs, the paper proposes a transformer-based architecture. Here, the noisy action sequence $\mathbf{A}_t^k$ is treated as a sequence of tokens. The observation $\mathbf{O}_t$ is encoded and fed into the cross-attention layers of the transformer decoder, conditioning the prediction of the noise for each action token.

*   **Visual Encoder:** A `ResNet-18` network is used to process image observations. Key modifications include replacing `BatchNorm` with `GroupNorm` for more stable training with diffusion models and using a `spatial softmax` layer instead of global average pooling to retain spatial information from the images.

*   **Inference Acceleration:** To achieve real-time performance, the authors use **Denoising Diffusion Implicit Models (DDIM)** for inference. DDIM is a variant of the diffusion process that allows for deterministic generation and, more importantly, decouples the number of training and inference steps. This means a model can be trained for, say, 100 diffusion steps, but can perform inference in as few as 10 steps, providing a significant speed-up.

*   **Action Space:** The paper finds that using **position control** (predicting a sequence of target end-effector poses) works significantly better for Diffusion Policy than **velocity control**. This is contrary to many recent works in BC. The authors hypothesize this is because multimodality is more pronounced in position control, and Diffusion Policy is uniquely capable of handling it. Furthermore, position control is less susceptible to the compounding errors that can plague velocity control over a sequence.

# 5. Experimental Setup

## 5.1. Datasets
The authors use a comprehensive suite of 4 benchmarks, covering a wide range of manipulation challenges.

*   **Robomimic:** A large-scale benchmark for imitation learning. The paper uses 5 tasks:
    *   `Lift`: Lifting a cube.
    *   `Can`: Picking and placing a can.
    *   `Square`: Picking up a square nut and placing it on a peg.
    *   `Transport`: A bimanual task involving picking, handing over, and placing objects.
    *   `ToolHang`: Using a hook tool to hang an object.
        For most tasks, both proficient human (`PH`) and mixed-quality human (`MH`) datasets are used.

*   **Push-T:** A simulated task requiring pushing a T-shaped block to a target location. It is known for requiring precise, contact-rich manipulation and exhibiting multimodality.

*   **Multimodal Block Pushing:** A task designed to test long-horizon multimodality, where two blocks must be pushed into two goals in any order.

*   **Franka Kitchen:** A long-horizon, multi-task environment where a robot must interact with various kitchen objects in arbitrary sequences.

*   **Real-World Tasks:** To demonstrate real-world viability, the method is tested on several physical robot setups:
    *   **Push-T:** A real-world version of the simulated task.
    *   **6DoF Mug Flipping:** A complex 3D reorientation task.
    *   **Sauce Pouring and Spreading:** Tasks involving non-rigid objects (liquids).
    *   **Bimanual Tasks:** `Egg Beater`, `Mat Unrolling`, and `Shirt Folding`, demonstrating coordination between two robot arms.

## 5.2. Evaluation Metrics

*   **Success Rate:** This is the primary metric for most tasks. It measures the fraction of trials where the robot successfully completes the task according to pre-defined criteria.
    1.  **Conceptual Definition:** It simply answers: "What percentage of the time did the robot succeed?" It is the most direct measure of task performance.
    2.  **Mathematical Formula:**
        \$
        \text{Success Rate} = \frac{\text{Number of Successful Trials}}{\text{Total Number of Trials}}
        \$
    3.  **Symbol Explanation:**
        *   `Number of Successful Trials`: The count of runs where the task goal was achieved.
        *   `Total Number of Trials`: The total number of evaluation episodes run.

*   **Intersection over Union (IoU):** Used for tasks where the goal is to cover a target area, such as `Push-T` and `Sauce Pouring`.
    1.  **Conceptual Definition:** IoU measures the overlap between the area covered by the robot's action (e.g., the final position of the T-block) and the target goal area. A value of 1 means a perfect match, and 0 means no overlap. It quantifies the accuracy of placement tasks.
    2.  **Mathematical Formula:**
        \$
        \text{IoU}(A, B) = \frac{\text{Area}(A \cap B)}{\text{Area}(A \cup B)}
        \$
    3.  **Symbol Explanation:**
        *   $A$: The predicted region (e.g., the mask of the moved object).
        *   $B$: The ground-truth target region.
        *   $A \cap B$: The intersection of the two regions (the area of overlap).
        *   $A \cup B$: The union of the two regions (the total area covered by both).

## 5.3. Baselines
The paper compares `Diffusion Policy` against three state-of-the-art behavior cloning methods:

*   **`LSTM-GMM`:** Represents the class of explicit policies that use Mixture Density Networks. It combines an LSTM to model temporal dependencies with a GMM to capture a limited form of multimodality. This is also referred to as `BC-RNN` in some contexts.
*   **`IBC` (Implicit Behavioral Cloning):** The leading example of an implicit, energy-based policy. It is highly expressive but suffers from training instability. It serves as a crucial point of comparison for expressiveness and stability.
*   **`BET` (Behavior Transformers):** Represents methods that use transformers and discretize the action space. It models actions as a sequence of discrete "tokens" learned via clustering.

    These baselines are representative of the major competing paradigms in modern behavior cloning research.

# 6. Results & Analysis

The paper's experimental results provide strong evidence for the superiority of `Diffusion Policy`.

## 6.1. Core Results Analysis
Across all simulated benchmarks, `Diffusion Policy` (in both its CNN and Transformer variants) consistently and significantly outperforms the baselines.

*   **Overall Performance:** The central claim of a **46.9% average improvement** is supported by the data in the tables. This improvement is most dramatic on the more complex tasks like `Transport` (bimanual), `ToolHang` (tool use), and `Kitchen` (long-horizon), where modeling complex, long-term behavior is critical.

*   **Handling Multimodality:**
    *   **Short-Horizon (Push-T):** Figure 3 shows a scenario where the robot can push the block from the left or the right. `Diffusion Policy` learns both modes and commits to one consistent trajectory within a rollout. In contrast, `LSTM-GMM` and `IBC` show a bias to one mode, and `BET` jitters between modes, failing to commit.
    *   **Long-Horizon (Block Push, Kitchen):** On tasks where the order of sub-goals is arbitrary, `Diffusion Policy` shows massive gains. For instance, on the Kitchen task, it achieves a 96% success rate for completing 4 sub-tasks ($p4$ metric), whereas the next best baseline (`BET`) only achieves 44%. This indicates its ability to model high-level, long-term choices.

*   **Training Stability:** Figure 6 contrasts the training process of `IBC` and `Diffusion Policy`. While `IBC`'s evaluation performance oscillates wildly, making it hard to select a good model, `Diffusion Policy`'s performance is stable and consistently improves, simplifying the training workflow.

*   **Real-World Performance:** The real-world experiments are perhaps the most compelling.
    *   On the real `Push-T` task, `Diffusion Policy` achieves a 95% success rate, near human-level performance, while the best baselines (`LSTM-GMM`) only reach 20%.
    *   On complex 6-DoF tasks like `Mug Flipping` and liquid manipulation (`Sauce Pouring/Spreading`), `Diffusion Policy` again achieves high success rates, whereas baselines fail completely.
    *   The model demonstrates robustness to real-world perturbations (Figure 8), such as having its view blocked or the object being moved mid-task. The policy reacts intelligently and re-plans, even generating novel corrective behaviors not seen in the demonstrations.
    *   Success on challenging bimanual tasks like `Shirt Folding` and `Egg Beater` further underscores its capability.

## 6.2. Data Presentation (Tables)
The following are the key results tables from the paper.

The following are the results from Table 1 of the original paper (State-based observations):

<table>
<thead>
<tr>
<th rowspan="2"></th>
<th colspan="2">Lift</th>
<th colspan="2">Can</th>
<th colspan="2">Square</th>
<th colspan="2">Transport</th>
<th rowspan="2">ToolHang ph</th>
<th rowspan="2">Push-T ph</th>
</tr>
<tr>
<th>ph</th>
<th>mh</th>
<th>ph</th>
<th>mh</th>
<th>ph</th>
<th>mh</th>
<th>ph</th>
<th>mh</th>
</tr>
</thead>
<tbody>
<tr>
<td>LSTM-GMM</td>
<td>1.00/0.96</td>
<td>1.00/0.93</td>
<td>1.00/0.91</td>
<td>1.00/0.81</td>
<td>0.95/0.73</td>
<td>0.86/0.59</td>
<td>0.76/0.47</td>
<td>0.62/0.20</td>
<td>0.67/0.31</td>
<td>0.67/0.61</td>
</tr>
<tr>
<td>IBC</td>
<td>0.79/0.41</td>
<td>0.15/0.02</td>
<td>0.00/0.00</td>
<td>0.01/0.01</td>
<td>0.00/0.00</td>
<td>0.00/0.00</td>
<td>0.00/0.00</td>
<td>0.00/0.00</td>
<td>0.00/0.00</td>
<td>0.90/0.84</td>
</tr>
<tr>
<td>BET</td>
<td>1.00/0.96</td>
<td>1.00/0.99</td>
<td>1.00/0.89</td>
<td>1.00/0.90</td>
<td>0.76/0.52</td>
<td>0.68/0.43</td>
<td>0.38/0.14</td>
<td>0.21/0.06</td>
<td>0.58/0.20</td>
<td>0.79/0.70</td>
</tr>
<tr>
<td>DiffusionPolicy-C</td>
<td>1.00/0.98</td>
<td>1.00/0.97</td>
<td>1.00/0.96</td>
<td>1.00/0.96</td>
<td>1.00/0.93</td>
<td>0.97/0.82</td>
<td>0.94/0.82</td>
<td>0.68/0.46</td>
<td>0.50/0.30</td>
<td>0.95/0.91</td>
</tr>
<tr>
<td>DiffusionPolicy-T</td>
<td>1.00/1.00</td>
<td>1.00/1.00</td>
<td>1.00/1.00</td>
<td>1.00/0.94</td>
<td>1.00/0.89</td>
<td>0.95/0.81</td>
<td>1.00/0.84</td>
<td>0.62/0.35</td>
<td>1.00/0.87</td>
<td>0.95/0.79</td>
</tr>
</tbody>
</table>

*Analysis: With state-based observations, Diffusion Policy (both CNN and Transformer versions) achieves near-perfect scores on simpler tasks (Lift, Can) and substantially outperforms baselines on harder tasks like `Transport` and `ToolHang`, where `IBC` completely fails and `BET` struggles.*

The following are the results from Table 2 of the original paper (Image-based observations):

<table>
<thead>
<tr>
<th rowspan="2"></th>
<th colspan="2">Lift</th>
<th colspan="2">Can</th>
<th colspan="2">Square</th>
<th colspan="2">Transport</th>
<th rowspan="2">ToolHang ph</th>
<th rowspan="2">Push-T ph</th>
</tr>
<tr>
<th>ph</th>
<th>mh</th>
<th>ph</th>
<th>mh</th>
<th>ph</th>
<th>mh</th>
<th>ph</th>
<th>mh</th>
</tr>
</thead>
<tbody>
<tr>
<td>LSTM-GMM</td>
<td>1.00/0.96</td>
<td>1.00/0.95</td>
<td>1.00/0.88</td>
<td>0.98/0.90</td>
<td>0.82/0.59</td>
<td>0.64/0.38</td>
<td>0.88/0.62</td>
<td>0.44/0.24</td>
<td>0.68/0.49</td>
<td>0.69/0.54</td>
</tr>
<tr>
<td>IBC</td>
<td>0.94/0.73</td>
<td>0.39/0.05</td>
<td>0.08/0.01</td>
<td>0.00/0.00</td>
<td>0.03/0.00</td>
<td>0.00/0.00</td>
<td>0.00/0.00</td>
<td>0.00/0.00</td>
<td>0.00/0.00</td>
<td>0.75/0.64</td>
</tr>
<tr>
<td>DiffusionPolicy-C</td>
<td>1.00/1.00</td>
<td>1.00/1.00</td>
<td>1.00/0.97</td>
<td>1.00/0.96</td>
<td>0.98/0.92</td>
<td>0.98/0.84</td>
<td>1.00/0.93</td>
<td>0.89/0.69</td>
<td>0.95/0.73</td>
<td>0.91/0.84</td>
</tr>
<tr>
<td>DiffusionPolicy-T</td>
<td>1.00/1.00</td>
<td>1.00/0.99</td>
<td>1.00/0.98</td>
<td>1.00/0.98</td>
<td>1.00/0.90</td>
<td>0.94/0.80</td>
<td>0.98/0.81</td>
<td>0.73/0.50</td>
<td>0.76/0.47</td>
<td>0.78/0.66</td>
</tr>
</tbody>
</table>

*Analysis: The trend continues with image-based observations. `DiffusionPolicy-C` (the CNN variant) is particularly strong, achieving perfect or near-perfect scores on most tasks and dramatically outperforming all baselines on `Transport (ph)` (1.00 vs 0.88) and `ToolHang (ph)` (0.95 vs 0.68).*

## 6.3. Ablation Studies / Parameter Analysis

*   **Position vs. Velocity Control:** Figure 4 shows that switching from velocity control to position control hurts the performance of `BCRNN` and `BET`, but *improves* the performance of `Diffusion Policy`. This key finding highlights that the choice of action space and policy representation are deeply intertwined.

*   **Action Horizon and Latency:** Figure 5 (left) shows the trade-off in the action execution horizon ($T_a$). A very short horizon ($T_a=1$) leads to myopic behavior, while a very long horizon makes the policy slow to react. An intermediate value (around 8) is found to be optimal. Figure 5 (right) shows that `Diffusion Policy` with position control is remarkably robust to system latency, maintaining peak performance even with a delay of up to 4 steps.

*   **Vision Encoder:** Table 5 investigates different vision backbones and pre-training strategies.
    The following are the results from Table 5 of the original paper:

    <table>
    <thead>
    <tr>
    <th>Architecture & Pretrain Dataset</th>
    <th>From Scratch</th>
    <th>Pretrained frozen</th>
    <th>Pretrained finetuning</th>
    </tr>
    </thead>
    <tbody>
    <tr>
    <td>ResNet18 (in21k)</td>
    <td>0.94</td>
    <td>0.40</td>
    <td>0.92</td>
    </tr>
    <tr>
    <td>ResNet34 (in21k)</td>
    <td>0.92</td>
    <td>0.58</td>
    <td>0.94</td>
    </tr>
    <tr>
    <td>ViT-base (clip)</td>
    <td>0.22</td>
    <td>0.70</td>
    <td>0.98</td>
    </tr>
    </tbody>
    </table>

    *Analysis: This study reveals that simply using a frozen pre-trained encoder gives poor results. The best performance is achieved by **fine-tuning** a pre-trained encoder, especially a `ViT` model pre-trained with `CLIP`, which reached a 98% success rate. This suggests that while pre-training provides a useful initialization, the visual features must be adapted to the specific manipulation task.*

# 7. Conclusion & Reflections

## 7.1. Conclusion Summary
This paper introduces `Diffusion Policy`, a new paradigm for visuomotor policy learning that models the policy as a conditional denoising diffusion process. Through extensive experiments on 15 simulated and real-world tasks, the authors demonstrate that this approach consistently and significantly outperforms existing state-of-the-art methods. The key to its success lies in the inherent advantages of the diffusion formulation: its ability to gracefully model complex, multimodal action distributions, its scalability to high-dimensional action sequences, and its exceptional training stability. The paper also highlights critical design decisions for practical robotics, such as using receding horizon position control and efficient visual conditioning, which are crucial for unlocking the full potential of diffusion models for real-time control. The work makes a strong case that the policy architecture itself has been a major performance bottleneck in behavior cloning, and that moving to more powerful generative models can yield substantial gains.

## 7.2. Limitations & Future Work
The authors acknowledge several limitations and avenues for future research:

*   **Dependence on Demonstration Quality:** As a behavior cloning method, `Diffusion Policy` inherits the limitation that its performance is bounded by the quality and coverage of the expert demonstration data. The authors suggest that applying diffusion policies within other learning paradigms, such as **Reinforcement Learning (RL)**, could help overcome this by allowing the policy to learn from its own experience, including suboptimal and negative data.
*   **Computational Cost and Latency:** The iterative nature of diffusion model inference makes it more computationally expensive and introduces higher latency compared to single-pass explicit policies. While the paper mitigates this with action sequence prediction and `DDIM`, it may still be too slow for tasks requiring very high control frequencies. Future work could explore recent advancements in fast diffusion sampling, such as consistency models or improved solvers, to further reduce inference time.

## 7.3. Personal Insights & Critique
This paper is a landmark contribution to robot learning, marking a significant shift in how policies can be represented.

*   **Inspiration:** The work is a powerful demonstration of the benefits of cross-pollination between different fields of AI. By successfully adapting a state-of-the-art technique from generative image modeling to the domain of robotics, the authors have opened up a new and promising research direction. The clarity and rigor of the experimental validation are exemplary.

*   **Key Insights:**
    1.  The analysis of **why Diffusion Policy is more stable than IBC** (by learning the score function directly and avoiding the estimation of the partition function $Z$) is a crucial theoretical insight that explains the practical performance gains.
    2.  The counter-intuitive finding that **position control outperforms velocity control** is a valuable contribution. It challenges a common convention in the field and provides a well-reasoned explanation tied to the properties of sequence prediction and the policy's ability to handle multimodality.
    3.  The real-world experiments, particularly the robustness tests and the complex bimanual tasks, convincingly show that this is not just a "simulation trick" but a practical method for physical robots.

*   **Potential Issues & Areas for Improvement:**
    *   The paper is heavily empirical. While the connection to LQR for a linear system is a nice sanity check, a more in-depth theoretical analysis of the closed-loop stability of a system controlled by a diffusion policy would be a valuable future contribution.
    *   The hyperparameter tuning for the Transformer variant is noted to be more sensitive. While the paper provides recommendations, a deeper investigation into why this is the case and how to make transformer training more robust in this context would be beneficial.
    *   While the receding horizon approach mitigates latency, the fundamental trade-off between the number of diffusion steps (quality of action) and inference speed remains. For highly dynamic environments, even the accelerated inference might not be fast enough. This remains an open challenge for applying diffusion models in reactive control settings.

        Overall, "Diffusion Policy" is a well-executed and highly impactful paper that not only presents a superior method but also provides deep insights into the underlying challenges of behavior cloning, setting a new and exciting standard for the field.