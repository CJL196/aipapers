# VideoAgent: Long-form Video Understanding with Large Language Model as Agent

Xiaohan Wang $\star$ , Yuhui Zhang $\star$ , Orr Zohar, and Serena Yeung-Levy Stanford University {xhanwang,yuhuiz,orrzohar,syyeung}@stanford.edu Abstract. Long-form video understanding represents a significant challenge within computer vision, demanding a model capable of reasoning over long multi-modal sequences. Motivated by the human cognitive process for long-form video understanding, we emphasize interactive reasoning and planning over the ability to process lengthy visual inputs. We introduce a novel agent-based system, VideoAgent, that employs a large language model as a central agent to iteratively identify and compile crucial information to answer a question, with vision-language foundation models serving as tools to translate and retrieve visual information. Evaluated on the challenging EgoSchema and NExT-QA benchmarks, VideoAgent achieves $5 4 . 1 \%$ and $7 1 . 3 \%$ zero-shot accuracy with only 8.4 and 8.2 frames used on average. These results demonstrate superior effectiveness and efficiency of our method over the current state-of-the-art methods, highlighting the potential of agent-based approaches in advancing long-form video understanding. Keywords: Long-form Video Understanding · Large Language Model Agent · Vision-Language Foundation Models

# 1 Introduction

Understanding long-form videos, which range from minutes to hours, poses a significant challenge in the field of computer vision. This task demands a model capable of processing multi-modal information, handling exceedingly long sequences, and reasoning over these sequences effectively. Despite numerous attempts [10, 12, 14, 27, 30, 42, 44, 50, 53, 59] to address this challenge by enhancing these capabilities, existing models struggle to excel in all three areas simultaneously. Current large language models (LLMs) excel in reasoning and handling long contexts [13, 48, 52, 62], yet they lack the capability to process visual information. Conversely, visual language models (VLMs) struggle to model lengthy visual inputs [9, 17, 18, 21, 23]. Early efforts have been made to enable VLMs' long context modeling capability, but these adaptations underperform in video understanding benchmarks and are inefficient in dealing with long-form video content [22].

![](images/1.jpg)  
Fig. 1: Overview of VideoAgent. Given a long-form video, VideoAgent iteratively searches and aggregates key information to answer the question. The process is controlled by a large language model (LLM) as the agent, with the visual language model (VLM) and contrastive language-image model (CLIP) serving as tools.

Do we really need to feed the entire long-form video directly into the model? This diverges significantly from how humans achieve the long-form video understanding task. When tasked with understanding a long video, humans typically rely on the following interactive process to formulate an answer: The process begins with a quick overview of the video to understand its context. Subsequently, guided by the specific question at hand, humans iteratively select new frames to gather relevant information. Upon acquiring sufficient information to answer the question, the iterative process is concluded, and the answer is provided. Throughout this process, the reasoning capability to control this iterative process is more critical than the capacity to directly process lengthy visual inputs.

Drawing inspiration from how humans understand long-form videos, we presen VideoAgent, a system that simulates this process through an agent-based system. We formulate the video understanding process as a sequence of states, actions, and observations, with an LLM serving as the agent controlling this process (Figure 1). Initially, the LLM familiarizes itself with the video context by glancing at a set of uniformly sampled frames from the video. During each iteration, the LLM assesses whether the current information (state) is sufficient to answer the question; if not, it identifies what additional information is required (action). Subsequently, it utilizes CLIP [36] to retrieve new frames containing this information (observation) and VLM to caption these new frames into textual descriptions, updating the current state. This design emphasizes the reasoning capability and iterative processes over the direct processing of long visual inputs, where the VLM and CLIP serve as instrumental tools to enable the LLM to have visual understanding and long-context retrieval capabilities. Our work differs from previous works in two aspects. Compared to the works that uniformly sample frames or select frames in a single iteration [16,56,66], our method selects frames in a multi-round fashion, which ensures the information gathered to be more accurate based on the current need. Compared to the works that retrieve frames using the original question as the query [56,66], we rewrite the query to enable more accurate and fine-grained frame retrieval. Our rigorous evaluation of two well-established long-form video understanding benchmarks, EgoSchema [28] and NExT-QA [55], demonstrates VideoAgent's exceptional effectiveness and efficiency compared to existing methods. VideoAgent achieves $5 4 . 1 \%$ and $7 1 . 3 \%$ accuracy on these two benchmarks, respectively, outperforming concurrent state-of-the-art method LLoVi [67] by $3 . 8 \%$ and $3 . 6 \%$ . Notably, VideoAgent only utilizes 8.4 frames on average to achieve such performance, which is 20x fewer compared to LLoVi. Our ablation studies highlight the significance of the iterative frame selection process, which adaptively searches and aggregates relevant information based on the complexity of the videos. Additionally, our case studies demonstrate that VideoAgent generalizes to arbitrarily long videos, including those extending to an hour or more. In summary, VideoAgent represents a significant stride for long-form video understanding, which embraces the agent-based system to mimic human cognitive process and underscores the primacy of reasoning over long-context visual information modeling. We hope our work not only sets a new benchmark in long-form video understanding but also sheds light on future research in this direction.

# 2 Related Work

# 2.1 Long-form Video Understanding

Long-form video understanding is a particularly challenging domain in computer vision due to the inherent complexity and high dimensionality of spatio-temporal inputs, which leads to significant computational demands. Long-form video understanding methods need to balance computational efficiency and performance, and can broadly be categorized into selective or compressive sparsity strategies.

Compressive sparsity methods $[ 1 0 , 1 2 , 1 4 , 3 0 , 4 2 , 4 4 , 5 0 , 5 3 , 5 9 , 6 5 ]$ , attempt to compress the video into meaningful embeddings/representations with the minimum possible dimensionality. For example, MovieChat [42] employs a memory consolidation mechanism that merges similar adjacent frame tokens based on cosine similarity, effectively reducing token redundancy in long video sequences. Chat-UniVi [14] utilized kNN clustering to spatio-temporally compress video tokens. However, the compression need not happen on the embeddings themselves, and can be compressed into space-time graphs [10,50,59] or even text [19,38,67]. For example, Zhang et. al. [67] introduced LLoVi, and have shown that simply captioning videos before and prompting an LLM with these captions can serve as a strong baseline. Meanwhile, selective-compressive methodologies attempt to sub-sample the video into more meaningful frames, utilizing the input question/text as a guide, and in effect attempt to only sample frames relevant to the question at hand [7, 20, 37, 56, 66]. For instance, methods such as R-VLM and R2A [8, 33, 56] utilize a CLIP model to retrieve relevant frames given a text prompt, and while

![](images/2.jpg)  
Fig. 2: Detailed view of VideoAgent's iterative process. Each round starts with the state, which includes previously viewed video frames. The large language model then determines subsequent actions by answering prediction and self-reflection. If additional information is needed, new observations are acquired in the form of video frames.

Q-ViD [38] utilize the question to selectively caption the video. Unlike previous works, we allow the LLM to direct the video frames to be sampled by the captioner.

# 2.2 LLM Agents

An agent is defined as an entity that makes decisions and takes actions in a dynamic, real-time environment to achieve some specific goals. Advances in large language models (LLMs), especially their emerging reasoning and planning capabilities [52, 62, 70], has inspired recent research in natural language processing to leverage them as agents in real-world scenarios [35, 63]. These models have demonstrated great success across various scenarios, such as online search, card game playing, and database management [25, 26, 61]. Their effectiveness is further amplified with methods such as chain-of-thought reasoning or selfreflection [41, 52]. Simultaneously, the computer vision community has begun to explore LLMas-agent-based approach in diverse visual contexts, such as GUI understanding and robot navigation [3, 5, 9, 45]. In the area of long-form video understanding, several studies have made an initial attempt with an agent-like approach, which utilize LLMs to interact with external tools or to incorporate additional functionalities [6, 45, 60]. Unlike these approaches, our work reformulates video understanding as a decision-making process, which is inspired by how humans solve video interpretation methods. We view the video as an environment where decisions involve either seeking more information or concluding the interaction. This perspective has guided the creation of VideoAgent, a novel framework that significantly diverges from existing methodologies by emphasizing the decisionmaking aspects inherent in video understanding.

# 3 Method

In this section, we introduce the method of VideoAgent. VideoAgent is inspired by the human cognitive process of understanding long-form videos. Given a video with the question, a human will first glance at several frames to understand its context, then iteratively search additional frames to obtain enough information to answer the question, and finally aggregate all the information and make the prediction. We formulate the process into a sequence of states, actions, and observations $\{ ( s _ { t } , a _ { t } , o _ { t } ) | 1 \leq t \leq T \}$ , where the state is the existing information of all the seen frames, action is whether to answer the question or continue to search new frames, observation is the new frames seen in the current iteration, and $T$ is the maximum number of iterations. We leverage large language model (LLM) GPT-4 [31] as an agent to perform the above process (Figure 1). LLMs have been demonstrated to have memory, reasoning and planning, and tool-use capability [39,46,52,70], which can be used to model states, actions, and observations, respectively.

# 3.1 Obtaining the Initial State

To start the iterative process, we first familiarize the LLM with the context of the video, which can be achieved by glancing at $N$ frames uniformly sampled from the video. Since the LLM has no capability for visual understanding, we leverage vision-language models (VLMs) to convert the visual content to language descriptions. Specifically, we caption these $N$ frames with the prompt "describe the image in detail" and feed the captions to the LLM. This initial state $s _ { 1 }$ records a sketch of the content and semantics of the video.

# 3.2 Determining the Next Action

Given the current state $s _ { t }$ that stores the information of all the seen frames, there are two possible options for the next action $a _ { t }$ : Action 1: answer the question. If the information in state $s _ { t }$ is enough to answer the question, we should answer the questions and exit the iterative process. Action 2: search new information. If the current information in $s _ { t }$ is insufficient, we should decide what further information is required to answer the question and continue searching for it. To decide between actions 1 and 2, we need the LLM to reason over the question and existing information. This is achieved by a three-step process. First, we force the LLM to make a prediction based on the current state and question via chain-of-thought prompting. Second, we ask the LLM to self-reflect and generate a confidence score based on the state, question, prediction and its reasoning process generated by step 1. The confidence score has three levels: 1 (insufficient information), 2 (partial information), and 3 (sufficient information). Finally, we choose action 1 or 2 based on the confidence score. This process is illustrated in Figure 2. We propose to use a three-step process over a single-step process that directly chooses action as direct prediction always decides to search for new information (Action 2). This self-reflection process is motivated by [41], which has demonstrated superior effectiveness in natural language processing.

# 3.3 Gathering a New Observation

Suppose the LLM determines insufficient information to answer the question and chooses to search for new information. In that case, we further ask the LLM to decide what extra information is needed so that we can leverage tools to retrieve them (Figure 2). Since some piece of information could occur multiple times within a video, we perform segment-level retrieval instead of video-level retrieval to enhance the temporal reasoning capability. For example, suppose the question is "What is the toy left on the sofa after the boy leaves the room?" and that we have seen the boy leave the room at frame $_ i$ . If we retrieve with the query "a frame showing the toy on the sofa," there may be frames before frame $_ i$ containing "toy on the sofa", but they are irrelevant to answering the question. To perform segment-level retrieval, we first split the video into different segments based on the seen frame indices, and ask the LLM to predict what segments to retrieve with the query texts. For example, if we have seen frames $i$ , $j$ , and $k$ of a video, one valid prediction is segment 2 (frame $i$ to $j$ ) with the query "a frame showing the toy on the sofa". We leverage CLIP [36] to obtain this additional information given the output by the LLM. Specifically, given each query and segment, we return the image frame with the highest cosine similarity with the text query in that segment. These retrieved frames are served as observations to update the state. The use of CLIP in the retrieval step is computationally efficient and negligible compared to using an LLM or VLM for several reasons. Firstly, CLIP's feature computation involves just a single feed-forward process. Secondly, CLIP employs an image-text late interaction architecture, enabling the caching and reusing of image frame features across different text queries. Lastly, our segmentlevel retrieval design only requires computing features within specific segments, further enhancing efficiency. Empirically, our experiments show that CLIP computations are less than 1% of that of a VLM and LLM.

# 3.4 Updating the Current State

Finally, given the new observations (i.e., retrieved frames), we leverage VLMs to generate captions for each frame, and then simply sort and concatenate the new captions with old frame captions based on frame index, and ask the LLM to generate next-round predictions. A question one may posit is why we leverage the multi-round process, as some existing works use all or uniformly sampled frames as the state in a single step [16, 67]. There are many advantages of our approach over these baselines. First, using too many frames introduces extensive information and noise, which leads to performance degradation because LLMs suffer from long contexts and can be easily distracted [24,40]. Furthermore, it is computationally inefficient and hard to scale up to hour-long videos due to the LLM context length limit [31]. On the opposite, using too few frames may not capture relevant information. Our adaptive selection strategy finds the most relevant information and requires the lowest cost to answer questions at different difficulty levels. We summarize the VideoAgent as Algorithm 1.

# Algorithm 1 VideoAgent

Require: Video $v$ , question $q$ , LLM $F _ { l }$ , VLM $F _ { v }$ , CLIP $F _ { c }$ , max iteration $T$ , confidence   
threshold $C$

Ensure: Prediction $\hat { y }$ , state-action-observation sequence $\{ s _ { t } , a _ { t } , o _ { t } | 1 \leq t \leq T \}$   
1: $s _ { 1 } \gets$ GenerateCaptions( $F _ { v }$ ,UniformSample(ν))   
2:for $t = 1$ to $T$ do   
3: $\hat { y } \gets$ PredictAnswer $( F _ { l } , s _ { t } , q )$   
4: C← SelfReflect $( F _ { l } , s _ { t } , q , \hat { y } )$   
5: if $a _ { t } \gets \mathbb { 1 } _ { [ c \geq C ] }$ then   
6: break   
7: else   
9: 8: $\begin{array} { r l } & { h \gets \mathtt { F i n d M i s s i n g I n f o } ( F _ { l } , s _ { t } , q ) } \\ & { o _ { t } \gets \mathtt { R e t r i e v e F r a m e s } ( F _ { c } , v , h ) } \\ & { s _ { t + 1 } \gets \mathtt { M e r g e } ( s _ { t } , \mathtt { G e n e r a t e C a p t i o n s } ( F _ { v } , o _ { t } ) ) } \end{array}$   
10:   
11: end if   
12: end for   
13: return $\hat { y }$ , $\{ s _ { t } , a _ { t } , o _ { t } | 1 \leq t \leq T \}$

# 4 Experiments

In this section, we first introduce the datasets and implementation details, and then we present the results, analyses, ablations, and case studies of VideoAgent.

# 4.1 Datasets and Metrics

In our experiments, we use two distinct well-established datasets to benchmark our model's performance, with a particular focus on zero-shot understanding capabilities. EgoSchema [28]. EgoSchema is a benchmark for long-form video understanding, featuring 5,000 multiple-choice questions derived from 5,000 egocentric videos. These videos provide an egocentric viewpoint of humans engaged in a wide range of activities. A distinctive feature of this dataset is the length of its videos, each lasting 3 minutes. EgoSchema comprises only a test set, with a subset of 500 questions having publicly available labels. The full set of questions is evaluated exclusively on the official leaderboard. NExT-QA [55]. The NExT-QA dataset includes 5,440 natural videos that feature object interactions in daily life, accompanied by 48,000 multiple-choice questions. The average length of video is 44 seconds. These questions fall into three categories: Temporal, Causal, and Descriptive, providing a comprehensive evaluation for video understanding models. In line with standard practices, our zero-shot evaluation focused on the validation set, which contains 570 videos and 5,000 multiple-choice questions. We additionally follow [4] to report performance on the ATP-hard subset of the NExT-QA validation set. This subset keeps the hardest QA pairs that can not be solved with one frame, focusing more on long-term temporal reasoning.

Table 1: Results on EgoSchema compared to public models. Full-set results are obtained from the official leaderboard.   

<table><tr><td colspan="2">Method</td><td rowspan="2">Frames Subset</td><td rowspan="2"></td><td rowspan="2">Full</td></tr><tr><td>FrozenBiLM [58]</td><td>[NeurIPS2022]</td></tr><tr><td>InternVideo [51]</td><td></td><td>90 90</td><td>-</td><td>26.9 32.1</td></tr><tr><td>ImageViT [34]</td><td>[arXiv2022.12]</td><td>16</td><td>- 40.8</td><td>30.9</td></tr><tr><td>ShortViViTloc [34]</td><td>[arXiv2023.12] [arXiv2023.12]</td><td>32</td><td>49.6</td><td>31.3</td></tr><tr><td>LongViViT [34]</td><td>[arXiv2023.12]</td><td>256</td><td>56.8</td><td>33.3</td></tr><tr><td>SeViLA [66]</td><td>[NeurIPS2023]</td><td>32</td><td>25.7</td><td>22.7</td></tr><tr><td>Vamos [49]</td><td>[arXiv2023.11]</td><td>-</td><td>.</td><td>48.3</td></tr><tr><td>LLoVi [67]</td><td>[arXiv2024.2]</td><td>180</td><td>57.6</td><td>50.3</td></tr><tr><td>MC-ViT-L [2]</td><td>[arXiv2024.2]</td><td>128+</td><td>62.6</td><td>44.4</td></tr><tr><td>VideoAgent (ours)</td><td></td><td>8.4</td><td>60.2</td><td>54.1</td></tr></table>

Table 2: Results on EgoSchema compared to large-scale proprietary models.   

<table><tr><td>Model</td><td>Subset</td><td>Full</td></tr><tr><td>Random Chance</td><td>20.0</td><td>20.0</td></tr><tr><td>Bard only (blind) [2]</td><td>[2023.3] 27.0</td><td>33.2</td></tr><tr><td>Bard + ImageViT [34]</td><td>35.0</td><td>35.0</td></tr><tr><td>Bard + ShortViViT [34]</td><td>[2023.3] 42.0</td><td>36.2</td></tr><tr><td>Bard + PALI [34]</td><td>[2023.3] 44.8</td><td>39.2</td></tr><tr><td>GPT-4 Turbo (blind) [2]</td><td>[2023.4]</td><td>31.0 30.8</td></tr><tr><td>GPT-4V [2]</td><td>[2023.9] 63.5</td><td>55.6</td></tr><tr><td>Gemini 1.0 Pro [47]</td><td>[2023.12]</td><td>55.7</td></tr><tr><td>VideoAgent</td><td>(ours)</td><td>60.2 54.1</td></tr></table>

Since each dataset features multiple-choice questions, we utilized accuracy as our evaluation metric.

# 4.2 Implementation Details

We decode all the videos in our experiments at 1 fps and use EVA-CLIP-8Bplus [43] to retrieve the most relevant frames based on the cosine similarity between the generated visual descriptions and the frame features. For the experiments on EgoSchema, we utilize LaViLa [68] as the captioner, which is a clip-based captioning model. Following [67], to ensure zero-shot evaluation, we utilize the LaViLa model retrained on the ego4D data, filtering out the overlapped videos with EgoSchema. We sample the video clip for captioning based on the frame index returned by the CLIP retrieval module. For NExT-QA, we utilize CogAgent [9] as the captioner. We use GPT-4 [31] as the LLM for all experiments, the version of GPT is fixed to gpt-4-1106-preview to ensure reproducibility.

# 4.3 Comparison with State-of-the-arts

VideoAgent sets new benchmarks, achieving state-of-the-art (SOTA) results on the EgoSchema and NExT-QA datasets, surpassing previous methods significantly while requiring only a minimal number of frames for analysis. EgoSchema. As shown in Tables 1 and 2, VideoAgent achieves an accuracy of $5 4 . 1 \%$ on the EgoSchema full dataset and $6 0 . 2 \%$ on a 500-question subset. The full dataset's accuracy was verified by uploading our model's predictions to the official leaderboard, as ground-truth labels are not publicly available. These results not only significantly outperforms the previous SOTA method LLoVi [67] by $3 . 8 \%$ , but also achieves comparable performance to advanced proprietary models like Gemini-1.0 [47]. Notably, our method requires an average of only 8.4 frames per video — significantly fewer by 2x to 30x compared to existing approaches.

<table><tr><td rowspan="2" colspan="2">Methods</td><td colspan="4">Val</td><td colspan="3">ATP-hard subset</td></tr><tr><td>Acc@C Acc@T</td><td>Acc@D</td><td></td><td>Acc@All</td><td>Acc@C</td><td>−Acc@T</td><td>Acc@All</td></tr><tr><td colspan="10">Supervised 63.2</td></tr><tr><td colspan="2">VFC [57]</td><td>49.6</td><td colspan="2">51.5</td><td>52.3</td><td></td><td>36.5</td><td></td></tr><tr><td colspan="2">ATP</td><td>[ICCV2021] [CVPR2022]</td><td>53.1</td><td>50.2</td><td>66.8 66.9</td><td>54.3 57.2</td><td>38.4</td><td>38.8</td></tr><tr><td colspan="2">MIST GF</td><td>[CVPR2023]</td><td>54.6</td><td>56.6</td><td></td><td></td><td></td><td></td></tr><tr><td colspan="2"></td><td>[NeurIPS2023]</td><td>56.9</td><td>57.1</td><td>70.5 58.8 69.9</td><td>48.7</td><td>50.3</td><td>49.3</td></tr><tr><td colspan="2">CoVGT</td><td>[TPAMI2023]</td><td>59.7</td><td>58.0</td><td>60.7</td><td>-</td><td>-</td><td>-</td></tr><tr><td colspan="2">SeViT</td><td>[arXiv2023.1]</td><td>54.0</td><td>54.1</td><td>56.7 63.1</td><td>43.3</td><td>46.5</td><td>-</td></tr><tr><td colspan="2">HiTeA</td><td>[ICCV2023]</td><td>62.4</td><td>58.3</td><td>71.3 75.6</td><td>47.8</td><td>48.6</td><td>-</td></tr><tr><td colspan="8">Zero-shot</td></tr><tr><td colspan="2">VFC [29]</td><td>[ICCV2023]</td><td>51.6</td><td>45.4 48.0</td><td>64.1</td><td>51.5</td><td>32.2</td><td></td></tr><tr><td>InternVideo</td><td>[51]</td><td>[arXiv2022.12]</td><td>43.4</td><td>65.1</td><td>49.1</td><td>-</td><td>30.0 -</td><td>31.4 -</td></tr><tr><td>AssistGPT</td><td>[6]</td><td>[arXiv2023.6]</td><td>60.0</td><td>51.4 67.3</td><td>58.4</td><td>-</td><td>-</td><td></td></tr><tr><td>ViperGPT</td><td>[45]</td><td>[ICCV2023]</td><td></td><td></td><td>60.0</td><td>-</td><td>-</td><td></td></tr><tr><td>SeViLA</td><td>[66]</td><td>[NeurIPS2023]</td><td>61.3</td><td>61.5</td><td>75.6 63.6</td><td></td><td></td><td></td></tr><tr><td>LLoVi</td><td>[67]</td><td>[arXiv2024.2]</td><td>69.5</td><td>61.0</td><td>75.6</td><td>67.7</td><td></td><td></td></tr><tr><td colspan="2">VideoAgent</td><td>(ours)</td><td>72.7</td><td>64.5</td><td>81.1</td><td>71.3 57.8</td><td>58.8</td><td>58.4</td></tr></table>

Table 3: Results on NExT-QA compared to the state of the art. C, T, and D are causal, temporal, and descriptive subsets, respectively.

NExT-QA. In Table 3, we show that VideoAgent achieves a 71.3% accuracy on the NExT-QA full validation set, surpassing the former SOTA, LLoVi [67], by $3 . 6 \%$ . With an average of merely 8.2 frames used per video for zero-shot evaluation, VideoAgent consistently outperforms previous supervised and zero-shot methods across all subsets by a large margin, including those testing the model's causal, temporal, and descriptive abilities. Importantly, VideoAgent achieves remarkable performance improvements on the more challenging subsets, ATPhard [4], demonstrating its adeptness at addressing complex long-form video queries. These results underscore VideoAgent's exceptional effectiveness and efficiency in processing and understanding complex questions from long-form videos.

# 4.4 Analysis of Iterative Frame Selection

One of the key components of VideoAgent is its iterative frame selection, which dynamically searches for and aggregates more information until it is sufficient to answer the question, mimicking the human process of understanding videos. We conducted comprehensive analyses and ablation studies to understand this process better.

Frame efficiency. Our first analysis focused on whether frame selection effectively identifies the informative frames needed to answer a question. This can be measured by frame efficiency: given a fixed number of frames, what model accuracy can be achieved. The hypothesis is that the more informative frames it identifies, the higher the frame efficiency should be. In Figure 3 (left), we plot the accuracy of our method compared to uniform sampling baselines and other previous methods on the EgoSchema 500-question subset. Our method significantly outperforms uniform selection and other baselines at the same number of frames, demonstrating its superiority in frame efficiency. Notably, our method, which uses only 8.4 frames to achieve $6 0 . 2 \%$ accuracy, surpasses the baseline that uniformly samples 180 frames to achieve $5 9 . 6 \%$ accuracy. This underscores the effectiveness of our method in finding informative frames and reveals that more frames do not always lead to better performance, as irrelevant and noisy information can overwhelm the language model with long contexts and distractions [24, 40].

![](images/3.jpg)  
Fig. 3: (Left) Frame efficiency compared to uniform sampling and previous methods. Xaxis is in log scale. Our method achieves exceptional frame efficiency for long-form video understanding. (Right) Number of frames for different types of NExT-QA questions. Min, mean, max, distribution are plotted. VideoAgent selects more frames on questions related to temporal reasoning than causal reasoning and descriptive questions.

Number of rounds. We also analyzed how the number of iterative rounds affects model performance. In the same Figure 3 (left), we plot the performance across 1-4 rounds and the number of selected frames, achieving accuracies of $5 3 . 8 \%$ , $5 8 . 6 \%$ , $6 0 . 2 \%$ , and $5 9 . 8 \%$ with 5, 7.5, 8.4, and 9.9 frames, respectively. The performance improves with additional rounds but saturates at three rounds on the EgoSchema 500-question subset. This indicates that our approach can efficiently find the information needed to answer the question, and beyond a certain point, additional information does not further help in answering the question. Different question types. Given that our frame selection process is dynamic, with the language model agent determining whether the information is sufficient, we hypothesized that different question types might require varying amounts of information due to their differing levels of difficulty. We tested this hypothesis on the NExT-QA dataset, which provides annotations for each question type: descriptive tasks, causal reasoning, or temporal reasoning. In Figure 3 (right), we plot the distribution of the number of frames for each type of question. We observed that the average number of frames used ranks as follows: descriptive (5.9 frames), causal (7.1 frames), and temporal (7.8 frames) questions. This aligns with human intuition that descriptive tasks often require fewer frames as initial uniform sampling is usually sufficient, whereas reasoning tasks, especially temporal reasoning, require viewing more frames to accurately answer the question.

<table><tr><td rowspan="2">Uniform</td><td rowspan="2">Uni-7 54.6</td><td rowspan="2">Uni-9 54.8</td><td rowspan="2">Uni-11</td></tr><tr><td>55.8</td></tr><tr><td>Ours</td><td>3→6.4 58.4</td><td>5→8.4 60.2</td><td>8→11.0 57.4</td></tr></table>

Table 4: Ablation of initial number of uniformly sampled frames.

<table><tr><td>Method</td><td>Frames</td><td>Acc</td></tr><tr><td>Ours w/o Seg. Selection</td><td>7.5</td><td>56.6</td></tr><tr><td>Ours w/o Self-Evaluation</td><td>11.8</td><td>59.6</td></tr><tr><td>Ours</td><td>8.4</td><td>60.2</td></tr></table>

Table 5: Ablation of segment selection and self-evaluation.

Initial Number of Frames. Before initiating the iterative frame selection process, we uniformly sample $N$ frames to acquaint the language model with the video context. To explore how the number of initially sampled frames influences model performance and the average number of frames utilized, we conduct an ablation study. Specifically, we sample 3, 5, and 8 frames initially on the EgoSchema 500-question subset and report the findings in Table 4. The results indicate accuracies of $5 8 . 4 \%$ , $6 0 . 2 \%$ , and $5 7 . 4 \%$ with an average of 6.4, 8.4, and 11.0 frames used, respectively. Starting with 5 frames leads to the highest performance. Furthermore, when comparing our method against uniform sampling with a similar or slightly higher number of frames, we observe accuracies of 54.6%, $5 4 . 8 \%$ , and $5 5 . 8 \%$ for 7, 9, and 11 frames, respectively. This comparison again highlights the superior efficiency of our frame selection method. Self-evaluation. During the iterative selection process, we perform a self-evaluatior to ascertain whether the available information suffices to answer the query. If sufficient, the iteration terminates at this stage. We benchmark this against a baseline method without self-evaluation, where every question is processed through three rounds of iteration. As detailed in Table 5, we observe an increase in the average number of frames from 8.4 to 11.8 and a decrease in accuracy from $6 0 . 2 \%$ to $5 9 . 6 \%$ . These results underscore the efficacy of self-evaluation in determining the adequacy of information, thereby curtailing unnecessary iterations. Notably, gathering more information through additional rounds does not lead to performance improvement but rather results in a marginal decline. Segment selection. When it is determined that additional information is required, the input videos are divided into segments. The language model then generates queries specifically tailored to retrieve information within those segments. This approach is contrasted with an alternative strategy that involves generating queries without specifying segments. In Table 5, we observe a $3 . 6 \%$ accuracy degradation when segment selection is disabled. Segment selection improves the model's temporal reasoning capabilities and mitigates the risk of conflating information from disparate segments. This is particularly beneficial for queries such as "what happens after..?", where retrieval is only desired from subsequent segments.

<table><tr><td>LLM</td><td>Model Size</td><td>Acc. (%)</td></tr><tr><td>Mistral-8x7B</td><td>70B</td><td>37.8</td></tr><tr><td>Llama2-70B</td><td>70B</td><td>45.4</td></tr><tr><td>GPT-3.5</td><td>N/A</td><td>48.8</td></tr><tr><td>GPT-4</td><td>N/A</td><td>60.2</td></tr></table>

Table 6: LLM ablation.

<table><tr><td>Captioner</td><td>Type</td><td># Words</td><td>Acc. (%)</td></tr><tr><td>BLIP-2</td><td>Frame-based</td><td>8.5</td><td>52.4</td></tr><tr><td>LaViLa</td><td>Clip-based</td><td>7.2</td><td>60.2</td></tr><tr><td>CogAgent</td><td>Frame-based</td><td>74.2</td><td>60.8</td></tr></table>

Table 7: VLM ablation.

<table><tr><td>CLIP</td><td>Model Size</td><td>Resolution</td><td>Acc. (%)</td></tr><tr><td>OpenCLIP ViT-G</td><td>1B</td><td>224</td><td>59.2</td></tr><tr><td>EVA-CLIP-8B</td><td>8B</td><td>224</td><td>59.4</td></tr><tr><td>EVA-CLIP-8B-plus</td><td>8B</td><td>448</td><td>60.2</td></tr></table>

Table 8: CLIP ablation.

# 4.5 Ablation of Foundation Models

Given that VideoAgent integrates three foundational model types — large language model (LLM), visual language model (VLM), and contrastive languageimage model (CLIP) — we conduct a series of ablation studies to evaluate the impact of each component's design on the overall performance of the model. LLM. We initiated our study by evaluating how different LLMs influence the performance of our model, given the pivotal role of LLMs in our methodology, where they function as agents orchestrating the entire process. In Table 6, we compare several state-of-the-art public and proprietary LLMs, including LLaMA-2-70B [48], Mixtral-8x7B [13], GPT-3.5 [32], and GPT-4 [31]. Our findings indicate that GPT-4 significantly outperforms its counterparts. However, it is primarily due to its capability in structured prediction. The iterative process employs JSON for output, where accurate JSON parsing is crucial. GPT-4 demonstrates robust performance in generating correct JSON formats, a feat not as consistently achieved by other models, which remains an active research area in LLM [69]. VLM. Leveraging GPT-4, a text-only model without visual capabilities, we translate image frames into descriptive captions through VLMs, subsequently feeding these captions to GPT-4. To assess the impact of caption quality produced by various VLMs on performance, we examined three state-of-the-art VLMs: frame-based BLIP-2 [18] and CogAgent [9], along with clip-based LaViLa [68] as presented in Table 7. Our analysis revealed that captions from CogAgent and LaViLa yield similar performances, even though their generated captions have significantly different lengths, while BLIP-2 generated captions are much worse. CLIP. CLIP excels in retrieval tasks due to the late-interaction design for image and text features, eliminating the need for recomputing image embeddings for varying queries. We evaluated three versions of CLIP: OpenCLIP ViT-G [11], EVA-CLIP-8B [43], and EVA-CLIP-8B-plus [43], with the results shown in Table 8. Our findings suggest comparable performance across different CLIP models, indicating that retrieval does not constitute a bottleneck for our methodology. It's important to note that the main contribution of our research is the introduction of a framework emulating the human process of understanding long-form videos, rather than the employment of any specific model. With the rapid developments of foundational models such as LLMs, VLMs, and CLIPs, our approach can be further improved with the integration of better models, or by adopting a

# Question: Why did the man in black sweater hold up a cup of water while talking to friends?

ake a piure hand the drink to someon l

![](images/4.jpg)

![](images/5.jpg)

![](images/6.jpg)

Caption Frame 1 The image depicts there's a man sitting on..

![](images/7.jpg)  
Frame 52

![](images/8.jpg)

Frame 18 There's a person's arm...a man seated on a chair... ...a young woman sitting on a couch... a piece of paper... ... a man seated.….seems to men .. in a conversation e in motion..gesturing.....and bottles and a glass Predict Answer: Given the following descriptions of five sampled frames in the videCaptlonPlease answer the olowingQuestonPlease think ste-ystep and write the best answer index in JSON format. Output: The provided descriptions do not explicitly mention a man holding up a cup of water while talking to friends... {"final answer":null} Self Reflect: Please assess the confidence level in the decision-making process. vaui oal o (Level-2); Sufficient Information (Level-3). Output: The answer indicates a lack of information to make a proper choice... {"conf idence": "1" caption-free methodology by replacing GPT-4 with GPT-4V. We hope our work sheds light on future work in this direction.

![](images/9.jpg)  
Fig. 4: Case study on NExT-QA. VideoAgent accurately identifies missing information in the first round, bridges the information gap in the second round, and thereby makes the correct prediction.

# 4.6 Case Studies

We present several case studies to demonstrate the capability of VideoAgent in understanding long-form videos. Questions from NExT-QA [55]. In Figure 4, we illustrate an instance from NExT-QA solved in two iterations. The question is asking why the man holds up a cup of water when talking to friends. VideoAgent accurately identify missing information (although the cup is visible in frame 69, it does not reveal the man is holding it). It then determines what additional information is required (frame of the man in the black sweater holding a cup of water). Finally, it utilizes CLIP to retrieve this detail (the man is holding a glass and is in the act of drinking from it) and feel confident about its answer.

![](images/10.jpg)  
Fig. 5: Case study on hour-long videos. VideoAgent accurately identifies the key frame during the second iteration, subsequently making an accurate prediction. Conversely, GPT-4V, when relying on 48 uniformly sampled frames up to its maximum context length, does not get successful prediction. However, by integrating the frame pinpointed by VideoAgent, GPT-4V is able to correctly answer the question.

Hour-long videos. Given that both NExT-QA and EgoSchema videos span only a few minutes, Figure 5 shows how VideoAgent can accurately solve hour-long videos from YouTube†. The question is about figuring out the color of the stairs surrounding by green plants, which only occupy a small portion of the video. VideoAgent efficiently identifies the necessary information and answers questions within only two iterations and seven frames, outperforming state-of-the-art models like GPT-4V. Notably, GPT-4V struggles with uniform sampling across its maximum context length of 48 images. However, when GPT-4V is provided with the frame pinpointed by VideoAgent, it can successfully answer the question. This underscores the potential of enhancing GPT-4V's capabilities in video understanding by integrating our approach. In conclusion, VideoAgent is ready to tackle real-world video understanding challenges, surpassing traditional methods reliant on one-round sparse or dense sampling.

# 5 Conclusion

In this work, we introduce VideoAgent, a system that employs a large language model as an agent to mirror the human cognitive process for understanding longform videos. VideoAgent effectively searches and aggregates information through a multi-round iterative process. It demonstrates exceptional effectiveness and efficiency in long-form video understanding, as evidenced by both quantitative and qualitative studies on various datasets.