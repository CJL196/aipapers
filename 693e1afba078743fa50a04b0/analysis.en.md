# 1. Bibliographic Information

## 1.1. Title
One-step Diffusion with Distribution Matching Distillation

## 1.2. Authors
The authors are Tianwei Yin, Michaël Gharbi, Richard Zhang, Eli Shechtman, Frédo Durand, William T. Freeman, and Taesung Park. Their affiliations are with the Massachusetts Institute of Technology (MIT) and Adobe Research. This collaboration brings together top-tier academic researchers (MIT) and leading industry experts in computer graphics and AI (Adobe Research), indicating a strong foundation in both theoretical principles and practical application.

## 1.3. Journal/Conference
The paper was published as a preprint on arXiv. As of its publication date, it had not yet been peer-reviewed for a major conference. However, its significant results and the authors' backgrounds suggest it would be a strong candidate for top-tier computer vision or machine learning conferences like CVPR, ICCV, NeurIPS, or ICML.

## 1.4. Publication Year
The paper was first submitted to arXiv on November 30, 2023.

## 1.5. Abstract
Diffusion models are known for generating high-quality images but suffer from slow inference speeds due to their iterative nature, requiring dozens of forward passes. This paper introduces **Distribution Matching Distillation (DMD)**, a method to convert a pre-trained diffusion model into a one-step image generator while preserving high image quality. The core idea is to force the one-step generator's output distribution to match the target (real) data distribution. This is achieved by minimizing an approximate Kullback-Leibler (KL) divergence between the two distributions. The gradient of this KL divergence is ingeniously expressed as the difference between two score functions: one for the target distribution and one for the synthetic distribution from the generator. These score functions are parameterized by two separate diffusion models. The DMD method is further stabilized by a simple regression loss that matches the large-scale structure of images generated by the original multi-step diffusion model. The authors report that DMD outperforms all existing few-step diffusion methods, achieving a state-of-the-art Fréchet Inception Distance (FID) of 2.62 on ImageNet 64x64 and 11.49 on zero-shot COCO-30k. This performance is comparable to the original Stable Diffusion model but is orders of magnitude faster, capable of generating images at 20 frames per second (FPS) on modern hardware.

## 1.6. Original Source Link
-   **Original Source Link:** [https://arxiv.org/abs/2311.18828](https://arxiv.org/abs/2311.18828)
-   **PDF Link:** [https://arxiv.org/pdf/2311.18828v4.pdf](https://arxiv.org/pdf/2311.18828v4.pdf)
-   **Publication Status:** This is an arXiv preprint.

    ---

# 2. Executive Summary

## 2.1. Background & Motivation
The central problem this paper addresses is the **prohibitive computational cost** of diffusion models. While these models represent the state-of-the-art in image generation quality, their iterative sampling process, which involves 10 to 100s of neural network evaluations, makes them too slow for real-time or interactive applications.

Existing solutions fall into two categories:
1.  **Fast Samplers:** These methods reduce the number of required sampling steps but often see a catastrophic drop in quality when the step count becomes very small (e.g., <10).
2.  **Model Distillation:** These techniques train a smaller "student" model to mimic the output of a multi-step "teacher" diffusion model in a single pass. However, training a student to perfectly replicate the complex, high-dimensional mapping from a random noise vector to a final image is extremely challenging. Prior methods often struggled to close the quality gap with their multi-step teachers.

    The paper's key insight is to **reframe the distillation problem**. Instead of enforcing a strict point-to-point correspondence between the student's and teacher's outputs ($student(z) = teacher(z)$), the authors propose to enforce a match at the **distribution level**. The goal is simply to make the *set* of images produced by the student statistically indistinguishable from the *set* of images produced by the teacher. This approach is inspired by Generative Adversarial Networks (GANs) and recent work in score-based modeling like Variational Score Distillation (VSD).

## 2.2. Main Contributions / Findings
The paper makes several key contributions:

1.  **Distribution Matching Distillation (DMD):** A novel framework for distilling a multi-step diffusion model into a one-step generator. The core of DMD is a distribution matching loss based on KL divergence.

2.  **Score-Based Gradient Formulation:** The gradient for the distribution matching loss is elegantly formulated as the difference between two score functions:
    *   `s_real`: Pushes the generated image to be "more realistic." This is estimated using the pre-trained teacher diffusion model.
    *   `s_fake`: Pushes the generated image to be "less fake" (i.e., increases diversity). This is estimated using a *second* diffusion model that is dynamically trained on the generator's own outputs.

3.  **Hybrid Loss Function for Stability:** The distribution matching loss is complemented by a simple **regression loss** (LPIPS) on a pre-computed set of noise-image pairs from the teacher model. This acts as a powerful regularizer, preventing mode collapse (where the generator only produces a few types of images) and ensuring the student's outputs are structurally aligned with the teacher's.

4.  **State-of-the-Art Performance:** DMD significantly outperforms all published few-step and one-step diffusion methods on standard benchmarks. On ImageNet 64x64, it achieves an FID of **2.62**, getting remarkably close to the 512-step teacher model's FID of 2.32. For text-to-image generation, it achieves an FID of **11.49** on COCO, comparable to the 50-step Stable Diffusion v1.5 teacher (8.78 FID) while being ~30x faster.

    ---

# 3. Prerequisite Knowledge & Related Work

## 3.1. Foundational Concepts

### 3.1.1. Diffusion Models
Diffusion models are a class of generative models that learn to create data by reversing a gradual noising process. The process has two parts:
*   **Forward Process (Noising):** This is a fixed process where Gaussian noise is incrementally added to a real image $x_0$ over a series of timesteps $T$. At each timestep $t$, the image becomes slightly noisier, until at the final timestep $T$, the image is indistinguishable from pure Gaussian noise. The image at timestep $t$ is denoted $x_t$.
*   **Reverse Process (Denoising):** This is the generative part. The model, typically a U-Net architecture, learns to reverse the noising process. Starting with a pure noise sample $x_T$, the model iteratively predicts a slightly less noisy version of the image, $x_{t-1}$, given the current noisy image $x_t$ and the timestep $t$. After $T$ steps, it produces a clean image $x_0$. The model can be trained to predict the original clean image $x_0$ (data prediction) or the noise that was added ($\epsilon$-prediction). The two are mathematically equivalent.

### 3.1.2. Score-Based Generative Models and Score Matching
A **score function** of a probability distribution `p(x)` is defined as the gradient of the log-probability with respect to the data $x$:
\$
s(x) = \nabla_x \log p(x)
\$
Intuitively, the score function at any point $x$ provides a vector that points in the direction of the steepest ascent in probability density. Following this vector field allows one to find the modes (high-probability regions) of the distribution.

Score-based models train a neural network to estimate this score function. A key insight is that a pre-trained diffusion denoiser $\mu(x_t, t)$ implicitly learns the score of the noised data distribution $p(x_t)$. The relationship, known as Tweedie's formula, is given in the paper (Equation 4) and allows the score to be computed directly from the denoiser's output:
\$
s(x_t, t) = -\frac{x_t - \alpha_t \mu(x_t, t)}{\sigma_t^2}
\$
where $\alpha_t$ and $\sigma_t$ are parameters from the noise schedule. This connection is fundamental to DMD.

### 3.1.3. Kullback-Leibler (KL) Divergence
The KL divergence is a statistical measure of how one probability distribution, $P$, diverges from a second, expected probability distribution, $Q$. For discrete distributions, it is defined as:
\$
D_{KL}(P \parallel Q) = \sum_{x \in \mathcal{X}} P(x) \log\left(\frac{P(x)}{Q(x)}\right)
\$
For continuous distributions, the sum is replaced by an integral. The KL divergence is always non-negative ($D_{KL} \geq 0$) and is zero if and only if $P$ and $Q$ are identical. It is asymmetric, meaning $D_{KL}(P \parallel Q) \neq D_{KL}(Q \parallel P)$. In this paper, the goal is to minimize $D_{KL}(p_{fake} \parallel p_{real})$, which encourages the generator (`p_fake`) to cover the modes of the real data distribution (`p_real`).

### 3.1.4. Knowledge Distillation
Knowledge distillation is a machine learning technique where a compact "student" model is trained to reproduce the behavior of a larger, more complex "teacher" model. The goal is to transfer the knowledge from the cumbersome teacher to the efficient student, allowing for faster inference with minimal loss in performance. In the context of this paper, the multi-step diffusion model is the teacher, and the one-step generator is the student.

## 3.2. Previous Works

*   **Progressive Distillation (PD):** A foundational distillation method that trains a student model to perform two steps of the teacher in a single pass. This process is repeated, with the student becoming the new teacher, effectively halving the number of required sampling steps at each stage until a one-step model is achieved.
*   **Consistency Models (CM):** This approach trains a model to map any point on a diffusion trajectory to the trajectory's origin (the clean image). The key "consistency" property is that outputs from different timesteps along the same trajectory should be identical. This allows for one-step generation by simply mapping a noise sample at the final timestep directly to a clean image.
*   **Rectified Flow / InstaFlow:** These methods aim to learn a "straighter" path from noise to data in the latent space. A straighter trajectory is easier to approximate with a single large step, enabling high-quality one-step generation.
*   **Variational Score Distillation (VSD):** A highly influential work (`ProlificDreamer`) that serves as a major inspiration for DMD. VSD uses a pre-trained 2D text-to-image diffusion model as a powerful loss function to optimize a 3D scene representation (like a NeRF). Its gradient is also formulated as the difference between a "real" score (from the pre-trained model) and a "fake" score (from a model trained on rendered views of the 3D scene). DMD adapts this core idea from test-time optimization of a single 3D object to training an entire generative image model.

## 3.3. Technological Evolution
The field of fast diffusion sampling has evolved rapidly:
1.  **Initial Models:** Early models like DDPM required thousands of steps.
2.  **Faster Samplers:** Methods like DDIM and DPM-Solver reduced the step count to ~20-50 while maintaining quality.
3.  **Early Distillation:** Initial distillation approaches directly regressed the student's output to the teacher's, but faced challenges in quality.
4.  **Advanced Distillation:** Methods like Progressive Distillation and Consistency Models introduced more sophisticated objectives, significantly improving one-step quality.
5.  **Distribution Matching:** DMD represents the next step, moving away from direct output replication to a more flexible distribution-level matching objective, borrowing powerful ideas from score-based modeling and GANs.

## 3.4. Differentiation Analysis
Compared to previous distillation methods, DMD's key innovations are:

*   **Objective Function:** Instead of a direct regression loss (`L2` or `LPIPS`) or a consistency loss, DMD uses a **distribution matching loss** derived from KL divergence. This is a fundamentally different and more flexible objective.
*   **Dynamic "Discriminator":** Unlike GANs which use a fixed discriminator architecture, DMD employs a dynamic "fake score" model ($μ_fake$) that evolves with the generator. This can be seen as a highly sophisticated, diffusion-based discriminator that helps shape the generator's output distribution.
*   **Hybrid Approach:** DMD is not a pure distribution matching method. It cleverly combines the novel score-based loss with a simple, traditional **regression loss**. This hybrid approach gets the best of both worlds: the power of distribution matching for high fidelity and diversity, and the stability and structural guidance of regression to prevent training issues like mode collapse.

    ---

# 4. Methodology

## 4.1. Principles
The core principle of Distribution Matching Distillation (DMD) is to train a one-step generator $G_θ$ such that the distribution of its generated images, `p_fake`, becomes indistinguishable from the distribution of real data, `p_real`. The method achieves this by minimizing the Kullback-Leibler (KL) divergence from `p_fake` to `p_real`, denoted as $D_{KL}(p_{fake} \parallel p_{real})$.

Directly computing this KL divergence is intractable. However, the authors show that its *gradient* with respect to the generator's parameters $θ$ can be approximated. This gradient provides a direction to update the generator to make its outputs "more real" and "less fake." This update signal is constructed using the difference between two score functions, which are themselves estimated using diffusion models.

The overall method is a two-part learning process:
1.  A **distribution matching loss** that pushes the generator's output distribution towards the real data distribution.
2.  A **regression loss** that regularizes the training, ensuring the generator's outputs remain structurally similar to those of a multi-step teacher model, preventing mode collapse and improving stability.

    The following figure from the paper provides an overview of the method.

    ![Figure 2. Method overview. We train one-step generator $G _ { \\theta }$ to map random noise $z$ into a realistic image. To match the multi-step distribution matching gradient $\\nabla _ { \\boldsymbol { \\theta } } \\overline { { \\boldsymbol { D } _ { K L } } }$ to the fake image to enhance realism. We inject a random amount of noise to the fake image and the one-step generator.](images/2.jpg)
    *该图像是示意图，展示了一步生成器 $G_{θ}$ 的结构及其与配对数据集之间的关系。图中包含分布匹配梯度计算过程，重点在于如何通过 $∇_{θ}D_{KL}$ 来增强生成图像的真实感，并引入随机噪声以优化假图像与真实图像之间的分布匹配。*

## 4.2. Core Methodology In-depth (Layer by Layer)

### 4.2.1. Model Setup
*   **Base Model ($μ_base$):** A pre-trained, multi-step diffusion denoiser model that serves as the "teacher." This model is kept frozen during training. It takes a noisy image $x_t$ and a timestep $t$ as input and predicts the clean image.
*   **One-Step Generator ($G_θ$):** The "student" model we want to train. It has the same architecture as $μ_base$ but does not take a time input. It directly maps a random noise vector $z$ (sampled from a standard Gaussian distribution $\mathcal{N}(0, \mathbf{I})$) to an image $x = G_{\theta}(z)$. It is initialized with the weights of the base model.

### 4.2.2. The Distribution Matching Loss

The training objective starts with the KL divergence between the fake (generator) and real distributions:
\$
D_{KL}(p_{fake} \parallel p_{real}) = \underset{x \sim p_{fake}}{\mathbb{E}} \left( \log \left( \frac{p_{fake}(x)}{p_{real}(x)} \right) \right)
\$
To train the generator $G_{\theta}$ via gradient descent, we need the gradient of this loss with respect to its parameters $\theta$. Using the definition of the score function $s(x) = \nabla_x \log p(x)$, the gradient can be expressed as:
$$
\nabla_{\theta} D_{KL} = \underset{z \sim \mathcal{N}(0; \mathbf{I})}{\mathbb{E}} \left[ - \left( s_{real}(x) - s_{fake}(x) \right) \frac{dG}{d\theta} \right],
$$
where $x = G_{\theta}(z)$, $s_{real}(x) = \nabla_x \log p_{real}(x)$ and $s_{fake}(x) = \nabla_x \log p_{fake}(x)$.

**Intuition:**
*   $-s_{real}(x)$ points *away* from regions of high real data density, but since it's applied to a fake sample $x$, the update term $-(s_{real}(x))$ effectively pushes the generator's output $x$ *towards* being more real.
*   $-(-s_{fake}(x)) = +s_{fake}(x)$ points *towards* regions of high fake data density. This term acts like a diversity-promoting force, preventing the generator from collapsing all its outputs to a single mode.

    However, computing these scores is difficult because the distributions $p_{real}$ and $p_{fake}$ may not overlap, and their densities can be zero, causing the log-probabilities to diverge.

**Solution: Perturbation and Score Approximation**

To solve this, the authors perturb the generated image $x$ with Gaussian noise, creating a diffused sample $x_t$. This ensures the perturbed real and fake distributions are fully supported and overlap.
$$
q_t(x_t | x) \sim \mathcal{N}(\alpha_t x; \sigma_t^2 \mathbf{I})
$$
where $\alpha_t$ and $\sigma_t$ come from the diffusion noise schedule.

Now, the score functions can be estimated using diffusion denoisers:

1.  **Real Score (`s_real`):** The score of the perturbed *real* data distribution is estimated using the fixed, pre-trained teacher model $μ_base$. Based on Tweedie's formula, the score is:
    $$
    s_{real}(x_t, t) = - \frac{x_t - \alpha_t \mu_{base}(x_t, t)}{\sigma_t^2}
    $$
    *   **$x_t$**: The generated image after adding noise.
    *   **$t$**: The diffusion timestep, determining the amount of noise.
    *   **$\mu_{base}(x_t, t)$**: The pre-trained teacher model's prediction of the clean image from the noisy input $x_t$.
    *   **$\alpha_t, \sigma_t^2$**: Noise schedule parameters.

2.  **Fake Score (`s_fake`):** The score of the perturbed *fake* data distribution (the generator's outputs) is estimated using a second denoiser model, $μ_fake^φ$. This model has the same architecture as $μ_base$ but its weights $φ$ are updated dynamically during training. The score is:
    $$
    s_{fake}(x_t, t) = - \frac{x_t - \alpha_t \mu_{fake}^{\phi}(x_t, t)}{\sigma_t^2}
    $$
    This "fake" denoiser $μ_fake^φ$ is trained concurrently with the generator $G_θ$ on a standard denoising objective, using the generator's most recent outputs as its training data:
    $$
    \mathcal{L}_{denoise}^{\phi} = ||\mu_{fake}^{\phi}(x_t, t) - x_0||_2^2
    $$
    where $x_0 = G_{\theta}(z)$ is the clean image produced by the generator (with gradients stopped).

**Final Distribution Matching Gradient**

By substituting the approximate scores back into the gradient formula and taking the expectation over random timesteps $t$, we get the final update rule for the generator from the distribution matching loss:
$$
\nabla_{\theta} D_{KL} \simeq \underset{z, t, x, x_t}{\mathbb{E}} \left[ w_t \alpha_t \left( s_{fake}(x_t, t) - s_{real}(x_t, t) \right) \frac{dG}{d\theta} \right]
$$
*   **$z \sim \mathcal{N}(0; \mathbf{I})$**: A random noise vector.
*   **$x = G_{\theta}(z)$**: The image generated by the student.
*   **$t \sim \mathcal{U}(T_{min}, T_{max})$**: A randomly sampled timestep.
*   **$x_t$**: The noised version of $x$.
*   **$w_t$**: A time-dependent weight to stabilize training. The authors propose a novel weighting scheme that normalizes the gradient's magnitude across different noise levels:
    $$
    w_t = \frac{\sigma_t^2}{\alpha_t} \frac{CS}{||\mu_{base}(x_t, t) - x||_1}
    $$
    where $C$ is the number of channels and $S$ is the number of spatial locations. This weight is designed to counteract the varying scales of the score difference at different timesteps $t$.

### 4.2.3. The Regression Loss
The distribution matching loss alone can be unstable and prone to mode collapse (where the generator produces only a limited variety of outputs). To counteract this, the authors add a simple but effective regression loss.

First, a static, paired dataset $\mathcal{D} = \{z, y\}$ is created offline.
*   $z$: A batch of random Gaussian noise vectors.
*   $y$: The corresponding images generated by running the full, multi-step teacher model $μ_base$ on the noise vectors $z$.

    The regression loss then encourages the one-step generator $G_θ$ to produce an image that is perceptually similar to the teacher's output $y$ for the same input noise $z$.
$$
\mathcal{L}_{reg} = \underset{(z, y) \sim \mathcal{D}}{\mathbb{E}} \ell(G_{\theta}(z), y)
$$
*   **$\ell$**: The distance function, chosen to be the **Learned Perceptual Image Patch Similarity (LPIPS)**, which is known to align better with human perception of image similarity than simple `L1` or `L2` loss.

    As shown in Figure 3 from the paper, this regression loss is crucial for recovering all modes of the target distribution and preventing collapse.

    ![Figure 3. Optimizing various objectives starting from the same configuration (left) leads to different outcomes. (a) Maximizing the real score only, the fake samples all collapse to the closest mode of the real distribution. (b) With our distribution matching objective but not regression loss, the generated fake data covers more of the real distribution, but only recovers the closest mode, missing the second mode entirely. (c) Our full objective, with the regression loss, recovers both modes of the target distribution.](images/3.jpg)
    *该图像是一个示意图，展示了不同优化目标下生成的结果。左侧为初始状态，(a) 仅优化真实分数，假样本塌缩至真实分布的最近模式；(b) 结合真实与假样本分数，生成数据更广泛，但仅恢复最近的模式；(c) 完整目标结合回归损失，成功恢复目标分布的两个模式。*

### 4.2.4. Final Training Procedure
The generator $G_θ$ and the fake score model $μ_fake^φ$ are trained simultaneously. The final objective for the generator $G_θ$ is a weighted sum of the two losses:
\$
\mathcal{L}_G = D_{KL} + \lambda_{reg} \mathcal{L}_{reg}
\$
The paper uses $\lambda_{reg} = 0.25$. The overall training process is outlined in Algorithm 1:

**Algorithm 1: DMD Training procedure**
1.  **Initialize:** Initialize the generator $G$ and the fake score model $μ_fake$ with the weights of the pre-trained real diffusion model $μ_real$.
2.  **Loop:** In each training iteration:
    a.  **Sample Data:** Sample a batch of random noise $z$ for the distribution matching loss, and a batch of paired data `(z_ref, y_ref)` from the pre-computed dataset $\mathcal{D}$ for the regression loss.
    b.  **Generate Images:**
        *   `x = G(z)`
        *   $x_{ref} = G(z_{ref})$
            c.  **Update Generator $G$:**
        *   Calculate the distribution matching loss gradient $\nabla D_{KL}$ using the generated images $x$, the real score model $μ_real$, and the fake score model $μ_fake$ as described in Section 4.2.2.
        *   Calculate the regression loss $\mathcal{L}_{reg} = \text{LPIPS}(x_{ref}, y_{ref})$.
        *   Combine the losses: $\mathcal{L}_G = \mathcal{L}_{KL} + \lambda_{reg} \mathcal{L}_{reg}$. (Note: The paper uses $\mathcal{L}_{KL}$ to denote the loss whose gradient is $\nabla D_{KL}$).
        *   Update the generator $G$ using the gradient of $\mathcal{L}_G$.
            d.  **Update Fake Score Model $μ_fake$:**
        *   Take the generated images $x$ (with gradients detached).
        *   Add noise to get $x_t$.
        *   Calculate the denoising loss $\mathcal{L}_{denoise}$ for the fake model: $\mathcal{L}_{denoise} = \text{denoisingLoss}(\mu_{fake}(x_t, t), x)$.
        *   Update the fake score model $μ_fake$ using the gradient of $\mathcal{L}_{denoise}$.

            ---

# 5. Experimental Setup

## 5.1. Datasets
The authors validate their method on several standard generative modeling benchmarks:

*   **CIFAR-10:** A dataset of 60,000 32x32 color images in 10 classes. It's a common benchmark for initial validation and ablation studies.
*   **ImageNet:** A large-scale dataset of over a million high-resolution images across 1,000 classes. The experiments use a 64x64 resolution version for class-conditional image generation, a challenging and standard benchmark.
*   **LAION-Aesthetics 6.25+ and 6+:** Subsets of the large-scale LAION dataset containing images with high aesthetic scores. These datasets contain millions of image-text pairs and are used to train the text-to-image models by distilling Stable Diffusion.
*   **MS COCO:** A dataset with images and corresponding captions. The 2014 validation set (with 30,000 prompts) is used for **zero-shot** evaluation of the text-to-image models, meaning the model is evaluated on prompts it has not seen during training.

## 5.2. Evaluation Metrics
The primary metrics used to evaluate the quality and text-alignment of generated images are:

*   **Fréchet Inception Distance (FID):** This is the most common metric for evaluating the quality of generative models. It measures the difference between the distribution of generated images and the distribution of real images.
    1.  **Conceptual Definition:** FID calculates the "distance" between two sets of images by comparing the statistics of their features as extracted by a pre-trained InceptionV3 network. A lower FID score indicates that the generated images are statistically more similar to real images in terms of features, which corresponds to higher quality and diversity.
    2.  **Mathematical Formula:**
        \$
        \text{FID}(x, g) = ||\mu_x - \mu_g||^2_2 + \text{Tr}(\Sigma_x + \Sigma_g - 2(\Sigma_x \Sigma_g)^{1/2})
        \$
    3.  **Symbol Explanation:**
        *   $x$ and $g$ represent the populations of real and generated images, respectively.
        *   $\mu_x$ and $\mu_g$ are the means of the InceptionV3 feature vectors for the real and generated images.
        *   $\Sigma_x$ and $\Sigma_g$ are the covariance matrices of the InceptionV3 feature vectors.
        *   $||\cdot||^2_2$ is the squared L2 norm.
        *   $\text{Tr}(\cdot)$ denotes the trace of a matrix.

*   **CLIP Score:** This metric evaluates how well a generated image aligns with its text prompt.
    1.  **Conceptual Definition:** It uses the CLIP (Contrastive Language-Image Pre-Training) model, which can embed both images and text into a shared latent space. The CLIP score is the cosine similarity between the CLIP embedding of the generated image and the CLIP embedding of the corresponding text prompt. A higher score means the image is a better representation of the text description.
    2.  **Mathematical Formula:**
        \$
        \text{CLIP Score} = 100 \times \cos(\text{Emb}_I(I_{gen}), \text{Emb}_T(T_{prompt}))
        \$
    3.  **Symbol Explanation:**
        *   $I_{gen}$ is the generated image.
        *   $T_{prompt}$ is the text prompt.
        *   $\text{Emb}_I$ and $\text{Emb}_T$ are the image and text encoders of the CLIP model.
        *   $\cos(\cdot, \cdot)$ is the cosine similarity function.

## 5.3. Baselines
The paper compares DMD against a comprehensive set of state-of-the-art methods, including:
*   **Original Teacher Models:** The multi-step diffusion models that are being distilled, such as EDM and Stable Diffusion v1.5. These represent the quality upper bound.
*   **GANs:** Powerful one-step generators like BigGAN and StyleGAN-T.
*   **Fast Diffusion Samplers:** Methods like DPM-Solver++ and UniPC that accelerate sampling without retraining the model. These are evaluated at a few steps (e.g., 4 steps).
*   **Other Distillation Methods:** A wide range of competing distillation techniques, including:
    *   Progressive Distillation (PD)
    *   Consistency Models (CM)
    *   TRACT
    *   InstaFlow
    *   Latent Consistency Models (LCM)
    *   UFOGen

        ---

# 6. Results & Analysis

## 6.1. Core Results Analysis

### 6.1.1. Class-Conditional Generation (ImageNet)
The results on ImageNet 64x64 are a standout achievement. DMD is compared against other one-step distillation methods and traditional generative models.

The following are the results from Table 1 of the original paper:

| Method | # Fwd Pass (↓) | FID (↓) |
| :--- | :--- | :--- |
| BigGAN-deep [4] | 1 | 4.06 |
| ADM [9] | 250 | 2.07 |
| Progressive Distillation [65] | 1 | 15.39 |
| DFNO [92] | 1 | 7.83 |
| BOOT [16] | 1 | 16.30 |
| TRACT [3] | 1 | 7.43 |
| Meng et al. [51] | 1 | 7.54 |
| Diff-Instruct [50] | 1 | 5.57 |
| Consistency Model [75] | 1 | 6.20 |
| **DMD (Ours)** | **1** | **2.62** |
| EDM (Teacher) [31] | 512 | 2.32 |

**Analysis:**
*   **Superiority over Distillation Methods:** DMD achieves an FID of **2.62**, which is a dramatic improvement over the next best one-step distillation method, Diff-Instruct (5.57). This is a greater than 2x improvement in FID score, demonstrating the effectiveness of the distribution matching approach.
*   **Closing the Gap with the Teacher:** DMD's FID (2.62) is remarkably close to the 512-step teacher model (2.32). It nearly matches the quality of the slow, iterative model in a **single forward pass**, a 512-fold speedup.
*   **Beating GANs:** DMD also outperforms strong GAN models like BigGAN-deep (4.06 FID), a significant accomplishment as GANs were traditionally the kings of fast, high-quality generation.

### 6.1.2. Text-to-Image Generation (MS COCO)
The paper evaluates a version of DMD that distills Stable Diffusion v1.5 for text-to-image synthesis. The results are benchmarked on the zero-shot MS COCO 30k set.

The following are the results from Table 3 of the original paper:

<table>
<thead>
<tr>
<th>Family</th>
<th>Method</th>
<th>Resolution (↑)</th>
<th>Latency (↓)</th>
<th>FID (↓)</th>
</tr>
</thead>
<tbody>
<tr>
<td rowspan="9">Original, unaccelerated</td>
<td>DALL·E [60]</td>
<td>256</td>
<td></td>
<td>27.5</td>
</tr>
<tr>
<td>DALL·E 2 [61]</td>
<td>256</td>
<td>-</td>
<td>10.39</td>
</tr>
<tr>
<td>Parti-750M [87]</td>
<td>256</td>
<td>-</td>
<td>10.71</td>
</tr>
<tr>
<td>Parti-3B [87]</td>
<td>256</td>
<td>6.4s</td>
<td>8.10</td>
</tr>
<tr>
<td>Make-A-Scene [13]</td>
<td>256</td>
<td>25.0s</td>
<td>11.84</td>
</tr>
<tr>
<td>GLIDE [52]</td>
<td>256</td>
<td>15.0s</td>
<td>12.24</td>
</tr>
<tr>
<td>LDM [63]</td>
<td>256</td>
<td>3.7s</td>
<td>12.63</td>
</tr>
<tr>
<td>Imagen [64]</td>
<td>256</td>
<td>9.1s</td>
<td>7.27</td>
</tr>
<tr>
<td>eDiff-I [2]</td>
<td>256</td>
<td>32.0s</td>
<td>6.95</td>
</tr>
<tr>
<td rowspan="3">GANs</td>
<td>LAFITE [94]</td>
<td>256</td>
<td>0.02s</td>
<td>26.94</td>
</tr>
<tr>
<td>StyleGAN-T [67]</td>
<td>512</td>
<td>0.10s</td>
<td>13.90</td>
</tr>
<tr>
<td>GigaGAN [26]</td>
<td>512</td>
<td>0.13s</td>
<td>9.09</td>
</tr>
<tr>
<td rowspan="6">Accelerated diffusion</td>
<td>DPM++ (4 step) [46]†</td>
<td>512</td>
<td>0.26s</td>
<td>22.36</td>
</tr>
<tr>
<td>UniPC (4 step) [91]†</td>
<td>512</td>
<td>0.26s</td>
<td>19.57</td>
</tr>
<tr>
<td>LCM-LoRA (4 step)[49]†</td>
<td>512</td>
<td>0.19s</td>
<td>23.62</td>
</tr>
<tr>
<td>InstaFlow-0.9B [43]</td>
<td>512</td>
<td>0.09s</td>
<td>13.10</td>
</tr>
<tr>
<td>UFOGen [84]</td>
<td>512</td>
<td>0.09s</td>
<td>12.78</td>
</tr>
<tr>
<td><strong>DMD (Ours)</strong></td>
<td><strong>512</strong></td>
<td><strong>0.09s</strong></td>
<td><strong>11.49</strong></td>
</tr>
<tr>
<td>Teacher</td>
<td>SDv1.5† [63]</td>
<td>512</td>
<td>2.59s</td>
<td>8.78</td>
</tr>
</tbody>
</table>

**Analysis:**
*   **Dominance in Accelerated Diffusion:** With an FID of **11.49**, DMD significantly outperforms other accelerated diffusion methods. For instance, it is much better than InstaFlow (13.10) and fast samplers like DPM++ (22.36 at 4 steps).
*   **Comparable to Teacher:** The quality gap to the 50-step Stable Diffusion teacher (8.78 FID) is small, especially considering DMD is ~30x faster (0.09s vs 2.59s). This makes high-quality text-to-image generation feasible for real-time applications.
*   **Competitive with SOTA:** DMD's performance is on par with or better than many original, slow text-to-image models like GLIDE (12.24) and LDM (12.63), and it surpasses the fast GAN-based `StyleGAN-T` (13.90).

## 6.2. Ablation Studies / Parameter Analysis
The authors conduct crucial ablation studies to validate the contribution of each component of their proposed method.

The following are the results from Table 2 of the original paper:

<table>
<thead>
<tr>
<th colspan="3">CIFAR ImageNet</th>
<th colspan="2">Sample weighting CIFAR</th>
</tr>
<tr>
<th>Training loss</th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>w/o Dist. Matching</td>
<td>3.82</td>
<td>9.21</td>
<td>σt/αt [58]</td>
<td>3.60</td>
</tr>
<tr>
<td>w/o Regress. Loss</td>
<td>5.58</td>
<td>5.61</td>
<td>σ/αt [58, 80]</td>
<td>3.71</td>
</tr>
<tr>
<td><strong>DMD (Ours)</strong></td>
<td><strong>2.66</strong></td>
<td><strong>2.62</strong></td>
<td><strong>Eq. 8 (Ours)</strong></td>
<td><strong>2.66</strong></td>
</tr>
</tbody>
</table>

**Analysis:**
1.  **Importance of Both Losses:**
    *   **Without Distribution Matching:** When the core `D_KL` loss is removed and only the regression loss is used, the FID score on ImageNet skyrockets from 2.62 to **9.21**. This confirms that simple regression is insufficient and the distribution matching objective is the primary driver of high fidelity. The qualitative results in Figure 5(a) show that without it, images lack realism and structural integrity.
    *   **Without Regression Loss:** When the `L_reg` loss is removed, the FID score also worsens significantly to **5.61**. The authors note this leads to training instability and mode collapse. Figure 5(b) qualitatively demonstrates this, showing a lack of diversity (multiple grey cars) compared to the full model. This proves the regression loss is a critical regularizer.
2.  **Effectiveness of Weighting Strategy:**
    *   The right side of the table shows that the proposed weighting scheme for the distribution matching loss (Eq. 8) achieves the best FID of **2.66** on CIFAR-10. It outperforms other popular weighting strategies used in prior work like DreamFusion and ProlificDreamer, which yield FIDs of 3.60 and 3.71 respectively. This validates the authors' specific design choice for the weight $w_t$.

        The qualitative results from the paper's Figure 5 clearly illustrate these points.
*   Figure 5(a) compares the full model (left) to one without distribution matching (right). The right-side images are blurry and malformed.

    ![该图像是插图，展示了通过Distribution Matching Distillation生成的多种图像样例，包括羊、汽车、狗和兔子。每个图像呈现了不同的视觉特征，表明该方法能生成多样化的高质量图像。](images/6.jpg)
    *该图像是插图，展示了通过Distribution Matching Distillation生成的多种图像样例，包括羊、汽车、狗和兔子。每个图像呈现了不同的视觉特征，表明该方法能生成多样化的高质量图像。*

*   Figure 5(b) compares the full model (left) to one without the regression loss (right). The right side shows mode collapse, with multiple images of the same grey car.

    ![该图像是示意图，展示了使用 DMD 方法生成的汽车图像（左）与未使用回归损失生成的汽车图像（右）之间的对比。可以看到，通过 DMD 方法生成的图像在视觉质量上更为出色。](images/7.jpg)
    *该图像是示意图，展示了使用 DMD 方法生成的汽车图像（左）与未使用回归损失生成的汽车图像（右）之间的对比。可以看到，通过 DMD 方法生成的图像在视觉质量上更为出色。*

---

# 7. Conclusion & Reflections

## 7.1. Conclusion Summary
The paper successfully introduces **Distribution Matching Distillation (DMD)**, a highly effective method for transforming slow, multi-step diffusion models into extremely fast, one-step generators. The core innovation is to reframe model distillation as a distribution matching problem, solved by minimizing an approximate KL divergence whose gradient is the difference between two diffusion-based score functions. By combining this novel loss with a simple perceptual regression loss for regularization, DMD achieves a new state-of-the-art in one-step image generation. The method significantly outperforms prior distillation and acceleration techniques, producing images with quality nearly identical to the original slow teacher models but at speeds suitable for interactive applications (e.g., 20 FPS for 512x512 images).

## 7.2. Limitations & Future Work
The authors acknowledge two primary limitations:

1.  **Quality Gap:** While significantly reduced, a minor quality discrepancy still exists between the one-step DMD model and the teacher model when run for a very large number of steps (e.g., 100-1000). The very last bits of fidelity are still lost in the distillation process.
2.  **Memory Usage:** The training framework is memory-intensive. It requires holding three large models in memory: the generator $G_θ$, the dynamic fake score model $μ_fake^φ$, and the fixed real score model $μ_base$. This can be a significant barrier for researchers with limited computational resources.

    As for future work, the authors suggest that memory-saving techniques like **Low-Rank Adaptation (LoRA)** could be applied to fine-tune the generator and fake score model, potentially alleviating the high memory footprint during training.

## 7.3. Personal Insights & Critique
**Personal Insights:**
*   The paper's core idea is exceptionally clever. It combines strengths from three major paradigms in generative modeling: the high quality of **diffusion models**, the distribution-matching objective of **GANs**, and the gradient-via-score-functions concept from **score-based modeling**.
*   The hybrid loss function is a key practical contribution. The theoretical elegance of the distribution matching loss is grounded by the pragmatic stability of the regression loss. This demonstrates a deep understanding of the practical challenges of training generative models.
*   The concept of a dynamically trained "fake score" model is a powerful generalization of the GAN discriminator. It provides a flexible, data-driven way to enforce diversity and prevent mode collapse, learning to identify the "tells" of its own generator as it trains.

**Critique & Potential Issues:**
*   **Training Complexity:** The primary drawback is the complexity and resource requirement of the training process. Managing two optimizers for two large networks ($G_θ$ and $μ_fake^φ$) simultaneously, along with the specific loss calculations, makes this a non-trivial method to implement and tune compared to simpler distillation schemes. The authors' own training log for the high-guidance model (Table 5) reveals a complex, multi-stage tuning process, suggesting that finding the optimal hyperparameters might be difficult.
*   **Reliance on Pre-computed Data:** The regression loss depends on a large, pre-computed dataset of teacher outputs. This adds a significant offline computation and storage cost before training can even begin. For very large datasets, generating and storing millions of paired samples could be a bottleneck.
*   **Generalization to Other Modalities:** While the paper focuses on images, the DMD framework is theoretically general. It would be interesting to see if it can be successfully applied to distill models for other data types like audio, video, or 3D shapes, and what challenges might arise in those domains.