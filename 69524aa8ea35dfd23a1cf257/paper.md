# MemFlow: Flowing Adaptive Memory for Consistent and Efficient Long Video Narratives

Sihui Ji1, † Xi Chen1 Shuai Yang3 Xin Tao2 Pengfei Wan2 Hengshuang Zhao1 1HKU 2Kling Team, Kuaishou Technology 3HKUST(GZ) https://github.com/KlingTeam/MemFlow

![](images/1.jpg)  
long-term consistency, enabling narrative coherence even if new subjects appear or scenario switches.

# Abstract

The core challenge for streaming video generation is maintaining content consistency over long context, which poses high requirement for the memory design. Most existing solutions maintain the memory by compressing historical frames with predefined strategies. However, different to-generate video chunks should refer to different historical cues, which is hard to satisfy with fixed strategies. In this work, we propose MEMFLoW to address this problem. Specifically, before generating the coming chunk, we dynamically update the memory bank by retrieving the most relevant historical frames with the text prompt of this chunk. This design not only accurately sources the context needed to maintain visual consistency, but also ensures semantic coherence even as new events unfold or scenes transition. In addition, during generation, we only activate the most relevant tokens in the memory bank for each query in the attention layers, which effectively guarantees the generation efficiency. In this way, MEMFLow achieves outstanding long-context consistency with negligible computation burden ( $7 . 9 \%$ speed reduction compared with the memory-free baseline) and keeps the compatibility with any streaming video generation model with KV cache.

# 1. Introduction

Video generation has attained remarkable quality [20, 21, 24, 29, 37], making its extension to long durations critical for advancing creative and cinematic applications. While Diffusion Transformer (DiT) models [26] leverage bidirectional attention to capture complex spatiotemporal dependencies, their inherent computational costs and GPU memory limits constrain them to short video generation. Autoregressive (AR) diffusion models [3, 15, 28, 39] offer a promising alternative by decomposing long videos into sequential clips, which alleviates the computation bottleneck through a reduced attention window. Interactive video generation has emerged as a crucial application for enabling users to guide narratives with streaming prompt inputs. Most existing works conduct chunklevel autoregressive generation, where new video segments are streamingly generated based on previously generated content and newly-provided text prompts. This interactive paradigm with dynamic prompt transitions allows the introduction of new elements and scene switches across extended temporal horizons. However, it also poses difficulties in effectively preserving memory for long-range content consistency due to complex inter-clip dependencies. First, since different to-generate video chunks should refer to different historical cues, the memory is required to adaptively provide relevant context according to streaming prompts; Second, the capacity of stored memory must be highly constrained, a necessity dictated by both the hardware limits of GPU memory and the demands of generation efficiency. While the necessity of such adaptive and efficient memory module is evident, many existing approaches have been overly simplistic, failing to fully address the dual challenges outlined above. They preserve memory in predefined paradigms, some only employ the first video chunk as memory sink [35], some attempt to store more historical frames through fixed compression schemes [8, 32, 42], some try to bake context implicitly with trainable memory modules [6, 12, 18]. However, those rigid strategies struggle to dynamically provide historical content corresponding to different prompt inputs, especially for new element emergence or scenario switches in prompt transitions. Thus, we innovatively design Narrative Adaptive Memory (NAM), a memory mechanism that adaptively retrieves relevant historical content for interactive streaming video generation. Specifically, we introduce a memory bank aggregating historical visual token (KV cache) from streamingly generated chunks. During the sequential generation of each chunk, we first retrieve the context which aligns with the current prompt most, by calculating the attention score between textual token of the prompt and visual token from memory. The context frame with higher score is considered to be semantically relevant with current chunk generation, and will be integrated to update the memory along with a condensed representation of the immediately preceding chunk. This design enables the current chunk to utilize historical cues, which have truly relevant content with the new prompt. Our NAM is effective in preserving narrative coherence even if new event happens or scenario switches, which is hard to satisfy with fixed memory strategies. However, the introduction of memory inevitably brings an extra computation burden, which hinders real-time generation. Thus, we propose Sparse Memory Activation (SMA), which strategically activates only the most relevant tokens in attention layers according to the attention scores calculated from query (current chunk) and key (context in memory) by top- $k$ selection. Subsequent attention is then applied within these selected tokens, which effectively accelerates inference by reducing computation cost while preserving quality. In this way, our MEMFLow effectively maintains contextual consistency over long durations and adeptly balances the memoryefficiency trade-off. It achieves stateof-the-art quality for interactive video generation with only $7 . 9 \%$ speed reduction compared with memory-free baseline. Our framework sustains 18.7 FPS on a single NVIDIA H100, demonstrating a clear advantage in producing narrative-coherent, long-term consistent videos with complex character and scene switching.

# 2. Related Works

Long Video Generation. Prior efforts to extend video generation to longer durations can be broadly categorized into three approaches. Autoregressive-diffusion hybrid approaches generate long videos by iteratively predicting frames [3, 15, 28, 35, 39, 42]. Diffusion-Forcing [2] mitigates error propagation by adjusting denoising schedules. CausVid [39] distills bidirectional models into efficient fewstep causal models, with Self Forcing [15] further addressing the traintest gap. MAGI-1 [28] and SkyReels-V2 [3] successfully scale up this AR-diffusion paradigm. Multistage methods [13, 31, 32, 47] decompose a long video into multiple clips to be generated separately. They either first synthesize a sequence of coherent keyframes followed by video infilling for each clip [14, 32, 46], or draft sequential prompts and use a T2V model to synthesize individual segments [23, 45]. A fundamental limitation of these approaches is the isolated nature of clip generation, which often leads to a lack of temporal coherence over long horizons. The third category applies efficient architectures to manage computational costs. TTTVideo [6] and LaCT [43] learn context using neural networks with linear attention. TokensGen [25] represents video clips with condensed tokens. Mixture of Contexts [1] dynamically selects relevant context for attention computation. These methods often sacrifice visual fidelity for efficiency.

![](images/2.jpg)  
attention computation, improving the generation efficiency without sacrificing visual quality. (Sec 3.3).

Memory Mechanisms in Video Generation. Effective memory mechanisms are crucial for maintaining consistency in long video generation. Action-guided video generation often relies on geometric and spatial dependencies [5, 33, 41]. Worldmem [33] and Context as memory [40] conduct memory retrieval based on Field of View (FOV) overlap between conditioned camera poses. VMem [22] introduces Surfel-Indexed View Memory for efficient retrieval by indexing past views with 3D surface elements. These methods, however, are highly dependent on spatial priors, thus lack generalizability. General video generation primarily maintains memory through context compression [8, 12, 42]. FramePack [42] compresses input frames into a fixed-size context to manage memory and efficiency. FAR [8] and StreamingT2V [11] combine short- and long-term memory via multiscale compression and learnable modules, respectively. These methods often maintain memory without adaptive retrieval, making it challenging to build dynamic connections between relevant context and the currently generated clip.

# 3. Method

MEMFLow enhances long-video narrative consistency by incorporating a novel dynamic memory bank into a streaming video generation framework (Sec 3.1). To dynamically recall relevant historical context according to current prompt, we first adopt (1) Narrative Adaptive Memory (NAM) mechanism for memory retrieval and updating (Sec 3.2); then (2) Sparse Memory Activation (SMA) is employed for memory selection to address memoryefficiency trade-off (Sec 3.3). The refreshed memory is subsequently utilized by AR-diffusion model to synthesize the current video chunk, after which this process continues to roll out over extended temporal horizon. The entire framework is trained end-to-end using a streaming long-tuning strategy, enabling the model to learn how to effectively manage its memory during long-duration rollouts. Figure 2 provides a high-level illustration of our framework.

# 3.1. Overall Framework

Baseline. Our work builds upon a hybrid autoregressivediffusion framework that integrates autoregressive chunkwise video generation with denoising diffusion [15, 28, 39]. At each generation iteration, the model produces a chunk of $T$ frames, conditioned on the $n$ immediately preceding frames. This autoregressive process naturally produces Key-Value (KV) cache from previous iterations, which is leveraged as the foundational structure for our memory bank. This design allows us to store historical context efficiently without incurring extra computational overhead. In a standard setup, the autoregressive attention mechanism operates over $( n + T )$ local frames (the last $n$ preceding frames and current $T$ generated frames). By integrating our memory bank containing $B$ frames, the attention operation is extended to cover $( n + B + T )$ frames, seamlessly blending short-term dependencies and long-term memory.

Training Mechanism. We train our memory-augmented AR-diffusion model using a distillation-based approach, specifically adopting the Self-Forcing [15] paradigm. Specifically, we adopt Distribution Matching Distillation (DMD) loss [38] that minimizes the gap between the student and teacher generator's output distribution, to distill a pretrained bidirectional model into a few-step causal model. To equip the model with long-context capabilities, we employ a streaming long-tuning strategy [35]. During this phase, the generator samples a short video clip (e.g., 5s) in each round conditioned on previous clips, and the teacher provides reliable supervision on this newly generated short clip via DMD. We can repeat this rolling extension for generating long sequences until the video reaches a preset maximum length, with supervision applied throughout the entire rollout. Crucially, we integrate our memory mechanism (NAM and SMA) into this tuning process: employing NAM in the streaming tuning allows the model to learn how to retrieve relevant history from self-generated frames during training, aligning training with inference and improving long-range consistency; while SMA mitigates the computational overhead introduced by memory, incurring only $7 . 9 \%$ efficiency loss compared to the memory-free baseline.

# 3.2. Narrative Adaptive Memory (NAM)

We first formulate the components of our memory bank in NAM. At each iteration, a new chunk is generated autoregressively, during which it is processed by the DiT to proal $\{ K _ { m } ^ { l } , V _ { m } ^ { l } \} _ { l = 1 } ^ { L }$ at each transformer layer $l$ , where $m$ denotes the index of chunk generation iteration. At the beginning of next iteration, the memory is updated as $\{ K _ { m } ^ { l ^ { \prime } } , \bar { V } _ { m } ^ { l ^ { \prime } } \} _ { l = 1 } ^ { L }$ for subsequent computation. Our memory mechanism aims to provide contentaligned context for incoming generation, which necessitates its ability to retrieve history relevant to incoming prompt and incorporate the most recently generated content for updating. To avoid excessive expansion of the memory bank as generation proceeds, we introduce two synergistic techniques: (i) Semantic Retrieval, which retrieve most informative context based on the cross-attention relevance between textual queries and visual keys, and (ii) Redundant Removal, which leverages temporal redundancy to select the KV feature of first latent frame as prototype for the entire local chunk. Semantic Retrieval. During generation, each transformer layer l produces key-value representations for the current chunk while attending across the present sequence, the KV cache in local window, and the global memory bank. The retrieval criterion is derived from cross-attention scores between the textual tokens as query and visual tokens from KV cache as key, which has proven effective in prior works [4, 30, 36] of large vision-language models. In our design, the textual tokens are computed from the prompt of chunk to be generated, thus the visual tokens with higher scores are semantically-aligned with this chunk. By retrieving those KV cache in the memory bank, we expect the model to attend to content-relevant visual features.

Let $Q _ { \mathrm { t e x t } } ^ { l } \in \mathbb { R } ^ { d }$ be the textual query of the current text prompt at layer $l$ . For each of the $b$ frames stored in the ${ \cal K } _ { m , i } ^ { l } \in$ score, $\mathbb { R } ^ { n \times d }$ where $S _ { m , i } ^ { l }$ $i = 1 , \ldots , b$ , we compute a semantic relevance where the $\operatorname { S o f t m a x } ( \cdot )$ computes attention weights, and Aggregate $( \cdot )$ is mean pooling here to produce a scalar score $S _ { m , i } ^ { l } \in \mathbb { R }$ .Then we can identify the top- $k$ most semantically aligned frames to be retained.

$$
\mathcal { S } _ { m , i } ^ { l } = \mathrm { A g g r e g a t e } \left( \mathrm { S o f t m a x } \left( \frac { Q _ { \mathrm { t e x t } } ^ { l } ( K _ { m , i } ^ { l } ) ^ { \top } } { \sqrt { d } } \right) \right) ,
$$

Redundant Removal. Following Semantic Retrieval, the immediately preceding chunk is consolidated into a representative prototype before being integrated into the memory. Instead of adopting computationally intensive contextmerging techniques [10, 36, 44] that rely on importance weighting, we propose a highly efficient heuristic. We leverage the high temporal redundancy inherent in short video chunks, where visual information exhibits strong similarity across consecutive frames. We posit that a single frame is sufficient to encapsulate the core visual content of the entire chunk. Therefore, we simply select the KV pair of the first frame from the preceding chunk to serve as its compact prototype. The updated memory bank $\{ K _ { m } ^ { l \prime } , V _ { m } ^ { l \prime } \}$ is then formed by concatenating the selected historical frames with the newly consolidated local prototype. The two strategies ensure that the memory is semantically relevant and real-time updated, enabling the model to build long-term and short-term dependencies crucial for narrative coherence.

# 3.3. Sparse Memory Activation (SMA)

Directly extending local context window to incorporate a memory bank introduces computational burden, as attention complexity scales with context size. While rigidly compressing the context can improve efficiency, it often compromises memory quality, as critical historical cues may be discarded indiscriminately. To address this memory—efficiency trade-off, we introduce Sparse Memory Activation, a relevance-gated memory selection technique for dynamic memory pruning before attention computation.

Our approach operates on the principle of selective attention, where query token from the current video chunk attends only to a subset of the most relevant historical frames in memory. Formally, we first partition key $( { \pmb K } _ { m } ^ { l } )$ and value $( V _ { m } ^ { l } )$ of the memory bank into $b$ frames. We then compute a compact descriptor for both the query $( Q _ { \mathrm { v i s } } ^ { l } )$ from current chunk and key of each frame using mean pooling over the token dimension, which is highly sufficient and expressive for generation tasks and has demonstrated by prior works [1]. This yields a single query descriptor, $\bar { q } _ { \mathrm { v i s } } \in \mathbb { R } ^ { 1 \times d }$ , and a set of frame-wise key descriptors, $\{ \bar { \pmb { k } } _ { j } \} _ { j = 1 } ^ { b } \in \mathbb { R } ^ { 1 \times d }$ the chunk index $m$ is omitted here for simplicity. The relevance between the current query and frame-wise key in memory is then determined by the inner product of their respective descriptors: Tauo R o  who  exc r  e hi ual   e   

<table><tr><td rowspan="2">Method</td><td rowspan="2">Quality Score ↑</td><td rowspan="2">Consistency Score ↑</td><td rowspan="2">Aesthetic Score ↑</td><td colspan="6">CLIP Score ↑</td></tr><tr><td>0-10 s</td><td>1020 s</td><td>2030 s</td><td>3040 s</td><td>4050 s</td><td>5060 s</td></tr><tr><td>SkyReels-V2 [3]</td><td>81.55</td><td>94.72</td><td>56.83</td><td>25.31</td><td>23.40</td><td>22.50</td><td>21.62</td><td>21.67</td><td>20.91</td></tr><tr><td>Self Forcing [15]</td><td>83.94</td><td>95.74</td><td>58.45</td><td>26.24</td><td>24.87</td><td>23.46</td><td>21.92</td><td>22.05</td><td>21.07</td></tr><tr><td>LongLive [35]</td><td>84.28</td><td>96.05</td><td>59.89</td><td>26.63</td><td>25.77</td><td>24.65</td><td>23.99</td><td>24.52</td><td>24.11</td></tr><tr><td>FramePack [15]</td><td>84.40</td><td>96.77</td><td>59.44</td><td>26.51</td><td>22.60</td><td>22.18</td><td>21.53</td><td>21.98</td><td>21.62</td></tr><tr><td>MeMFLOW</td><td>85.02</td><td>96.60</td><td>61.07</td><td>6.31</td><td>24.70</td><td>23.94</td><td>24.13</td><td>24.90</td><td>24.22</td></tr></table>

$$
s _ { j } = \bar { q } _ { \mathrm { v i s } } ^ { \top } \bar { k } _ { j } , \quad \mathrm { f o r } \quad j = 1 , \ldots , b
$$

Based on these relevance scores, we identify the set of indices $\mathcal { T } _ { k }$ corresponding to the top- $k$ most relevant frames:

$$
\mathcal { T } _ { k } = \underset { I \subseteq \{ 1 , \dots , b \} , | I | = k } { \arg \operatorname* { m a x } } \sum _ { j \in I } s _ { j }
$$

This formulation selects the subset of indices $I$ of size $k$ that maximizes the sum of relevance scores. Finally, the attention computation for query $Q _ { \mathrm { v i s } } ^ { l }$ is restricted to the keyvalue pairs belonging to the selected top- $k$ chunks:

$$
\mathrm { A t t n } ( Q _ { \mathrm { v i s } } ^ { l } , K _ { m } ^ { l } , V _ { m } ^ { l } ) \approx \mathrm { A t t n } ( Q _ { \mathrm { v i s } } ^ { l } , K _ { m , \mathcal { T } _ { k } } ^ { l } , V _ { m , \mathcal { T } _ { k } } ^ { l } )
$$

where ${ \cal K } _ { m ,  { \mathcal { T } } _ { k } } ^ { l }$ and Vl $V _ { m ,  { \mathcal { T } } _ { k } } ^ { l }$ are the concatenated key and value tensors from the chunks indexed by the set $\mathcal { T } _ { k }$ . By activating part of the memory bank, SMA reduces computational latency while retaining the most pertinent historical information. This strategy enables the model to selectively recall the right context at the right time, thereby preserving long-range dependencies and narrative coherence. Moreover, by implicitly filtering out less relevant or potentially erroneous information from the history, our approach mitigates error accumulation. This allows MEMFLow to achieve both robust memorization and computational efficiency, ensuring the generation of coherent long videos without the degradation of visual quality over time.

# 4. Experiment

# 4.1. Implementation Details

We build MEMFLOw on Wan2.1-T2V-1.3B [29], following the training and inference pipeline from LongLive [35], while enabling our memory bank and sparse activation. We perform Self Forcing [15] DMD pipeline with streaming long tuning on a 60s sequence by using the switchprompt dataset from LongLive constructed by Qwen2-72BInstruct [34]. We conduct streaming long tuning equipped with the memory bank for 3000 steps. During training, each iteration continues the model's own rollout by generating the next 5s video clip until a maximum length of 60s is reached.

# 4.2. Comparisons for Multi-prompt Generation

Since our memory mechanism is designed for interactive long-form videos with multiple prompts, we first compare MEMFLow's abilities with representative long video generation models. For fair comparison, we adapt existing methods - including SkyReels-V2 [3], FramePack [42], and Self Forcing [15] - for multi-prompt video generation by switching prompts during the autoregressive synthesis. Note that LongLive [35] inherently supports generating videos with interactive instructions. Following LongLive [35], we customize 100 groups of narrative scripts, with each consisting of 6 successive 10-second prompts for a total of 100 videos lasting for 60 seconds. We use metrics from VBench-Long [17] for assessing visual quality of all generated videos, among which the two dimensions of consistency and aesthetic are highlighted for comparison of long-range consistency in subjects, background and visual aesthetics. Table 1 shows that MEMFLOw achieves the best quality score among all methods, verifying its comprehensive competitiveness in perceptual quality. In terms of consistency score, our method outperforms all other models except Framepack [42], with Framepack tending to synthesize videos with reduced interframe dynamics, which definitely shows superiority in "consistency". Thus the result can still demonstrate our superior performance in global consistency with the specially designed memory. Our advantage in aesthetic score also highlights the effectiveness of our method in mitigating error accumulation during long rollouts.

For text alignment with interactive prompts, videos are segmented according to prompt boundaries for evaluating clip-wise semantic adherence. The CLIP score [27] between each video clip and its corresponding text is calculated at 10-second intervals. The results demonstrate outstanding prompt adherence and narrative coherence, particularly when the video is extrapolated to longer durations, owing to the model's ability to establish long-term contextual associations. Such superiority is further corroborated by the qualitative results in Figure 3. Our Narrative Adaptive Memory successfully links description in the prompt, a woman in a casual sweater", with the exact person in previous frames, thus maintaining the subject consistency. While our baseline, LongLive [35], fails to make connections between visual cues and semantic instructions, therefore continuously introduces new characters after prompt switching, exhibiting less temporal coherence and prompt compliance. Other approaches exhibit more severe error accumulation, with subject inconsistency in SkyReels-V2 [3] and color drifting in FramePack. Self Forcing [15] also suffers from similar problems with LongLive, showing misalignment between prompt scripts and narrative progression across clips as characters are repeatedly introduced into the ongoing scene. Additionally, We conducted a user study with 20 participants to compare our method against the aforementioned models, and report the result in supplementary material. It includes human preference rates in visual quality, instruction following, and global consistency, further supporting the effectiveness of our approach. In terms of speed, MEMFLOW is more than $3 8 \times$ faster than SkyReels-v2 [3], slightly faster than Self Forcing [15], while slightly slower than LongLive [35] due to memory updating and activation. Tai in semantic score attributed to textual retrieval-based memory.  denotes the scores reproduced by us.   

<table><tr><td rowspan="2">Model</td><td rowspan="2">#Params</td><td rowspan="2">Resolution</td><td rowspan="2">Throughput (FPS) ↑</td><td colspan="3">Evaluation scores ↑</td></tr><tr><td>Total</td><td>Quality</td><td>Semantic</td></tr><tr><td>Diffusion models</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>LTX-Video [9]</td><td>1.9B</td><td>768×512</td><td>8.98</td><td>80.00</td><td>82.30</td><td>70.79</td></tr><tr><td>Wan2.1 [29]</td><td>1.3B</td><td>832×480</td><td>0.78</td><td>84.26</td><td>85.30</td><td>80.09</td></tr><tr><td>Autoregressive models</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>SkyReels-V2 [3]</td><td>1.3B</td><td>960×540</td><td>0.49</td><td>82.67</td><td>84.70</td><td>74.53</td></tr><tr><td>MAGI-1 [28]</td><td>4.5B</td><td>832×480</td><td>0.19</td><td>79.18</td><td>82.04</td><td>67.74</td></tr><tr><td>CausVid [39]</td><td>1.3B</td><td>832×480</td><td>17.0</td><td>81.20</td><td>84.05</td><td>69.80</td></tr><tr><td>NOVA [7]</td><td>0.6B</td><td>768×480</td><td>0.88</td><td>80.12</td><td>80.39</td><td>79.05</td></tr><tr><td>Pyramid Flow [19]</td><td>2B</td><td>640×384</td><td>6.7</td><td>81.72</td><td>84.74</td><td>69.62</td></tr><tr><td>Self Forcing, chunk-wise [15]</td><td>1.3B</td><td>832×480</td><td>17.0</td><td>84.31</td><td>85.07</td><td>81.28</td></tr><tr><td>Self Forcing, frame-wise [15]</td><td>1.3B</td><td>832×480</td><td>8.9</td><td>84.26</td><td>85.25</td><td>80.30</td></tr><tr><td>LongLive [35]</td><td>1.3B</td><td>832×480</td><td>20.3†</td><td>84.87</td><td>86.97</td><td>76.47</td></tr><tr><td>MeMFLOW</td><td>1.3B</td><td>832×480</td><td>18.7</td><td>85.14</td><td>85.95</td><td>81.90</td></tr></table>

![](images/3.jpg)  
lv

![](images/4.jpg)  
aliv nalyfe eo e po whole memory bank without filtering, and "NAM $^ +$ SMA" is our full model which compresses memory by relevance-gated selection.

# 4.3. Comparisons for Single-prompt Generation

Although not specifically trained for single-prompt generation, our model shows superior performance compared to state-of-the-art models for durations of 5s and 30s on VBench [16] official prompt set.

Short Video Generation. We evaluate MEMFLOw 's short-video generation and compare it with relevant open-source models of similar scale, including LTXVideo [9], Wan2.1 [29], SkyReels-V2 [3], MAGI-1 [28], CausVid [39], NOVA [7], Pyramid Flow [19], Self Forcing [15], and LongLive [15]. For 5-second videos, MEMFLOw achieves strong overall quality with the highest total score, compared with state-of-the-art models as in Table 2. By retrieving relevant context from Narrative Adaptive Memory via prompt query, our model surpasses all other models in semantic score. Due to computation cost in memory updating and activation, our MEMFLoW sacrifices inference speed by $8 . 6 \%$ compared with LongLive, but still outperforms other methods with 18.7 FPS for real-time inference. The results also show that our framework does not degrade short-clip generation capability. Long video generation. The superiority of our method becomes more pronounced in long-horizon single prompt generation for 30-second videos. We observe consistent improvements across quality and semantic metrics in Table 4, leading to an outstanding overall performance than SkyReels-V2 [3], FramePack [42], Self Forcing [15], and LongLive [35]. It verifies that our Narrative Adaptive Memory provides more semantic-consistent context for video generation over long duration compared with using local context window only (SkyReels-V2, Self Forcing), context compression (FramePack) or retaining the first chunk as memory (LongLive). Moreover, the retrieval-based memory updating strategy interrupts the error propagation in memorization implicitly-only context with the highest level of semantic adherence is included for attention computation-thus alleviating the degradation of visual quality due to error accumulation over time. MEMFLOW maintains an advantage in long video generation quality with comparable performance on efficiency. auttivnalyy u whole memory bank without filtering, and $\mathrm { \partial ^ { \cdot } N A M + S M A ^ { \prime } }$ is our full model which compresses memory by relevance-gated selection.   

<table><tr><td rowspan="2">Memory Mechanism</td><td rowspan="2">Subject Consistency ↑</td><td rowspan="2">Background Consistency ↑</td><td rowspan="2">Throughput (FPS) ↑</td><td colspan="6">CLIP Score ↑</td></tr><tr><td>0-10 s</td><td>1020 s</td><td>2030 s</td><td>3040 s</td><td>4050 s</td><td>5060 s</td></tr><tr><td>w/o Memory</td><td>94.41</td><td>95.15</td><td>23.5</td><td>26.74</td><td>25.10</td><td>24.60</td><td>23.61</td><td>24.23</td><td>24.14</td></tr><tr><td>Frame Sink [35]</td><td>97.66</td><td>96.20</td><td>20.3</td><td>26.63</td><td>25.77</td><td>24.65</td><td>23.99</td><td>24.52</td><td>24.11</td></tr><tr><td>NAM+SMA</td><td>98.01</td><td>96.70</td><td>18.7</td><td>26.31</td><td>24.70</td><td>23.94</td><td>24.13</td><td>24.90</td><td>24.22</td></tr><tr><td>NAM</td><td>98.05</td><td>96.57</td><td>17.6</td><td>26.50</td><td>25.30</td><td>24.42</td><td>24.23</td><td>24.96</td><td>24.28</td></tr></table>

Table 4. Quantitative comparisons for single-prompt 30- second setting with representative long video generation models, showing more pronounced superiority than 5-second setting on all metrics with efficiency comparable to state-of-the-art models.   

<table><tr><td>Model</td><td>Total Score ↑</td><td>Quality Score ↑</td><td>Semantic Score ↑</td><td>Throughput (FPS) ↑</td></tr><tr><td>SkyReels-V2 [3]</td><td>75.29</td><td>80.77</td><td>53.37</td><td>0.49</td></tr><tr><td>FramePack [42]</td><td>81.95</td><td>83.61</td><td>75.32</td><td>0.92</td></tr><tr><td>Self Forcing [15]</td><td>81.59</td><td>83.82</td><td>72.70</td><td>17.0</td></tr><tr><td>LongLive [35]</td><td>83.52</td><td>85.44</td><td>75.82</td><td>20.3</td></tr><tr><td>MEMFLOW</td><td>84.51</td><td>85.92</td><td>78.87</td><td>18.7</td></tr></table>

![](images/5.jpg)  
Figure 5. Quantitative analysis of different memory capacity under multi-prompt 60-second setting. "w/o Memory" means only attending to the local attention window, "Frame Sink" refers to keeping KV cache from the first chunk as memory [35], "NAM" adopts the whole memory bank including $^ { b }$ latent frames.

# 4.4. Ablation Studies

We perform an ablation study on the core designs of our framework, Narrative Adaptive Memory and Sparse Memory Activation, in a 60-second interactive multi-prompt video generation setting with five prompt switches. Memory Mechanism. In Table 3, we ablate different memory mechanism by comparing (i) w/o memory: conditioned only on local context window; (ii) Frame Sink: addtionally retain the KV cache from the first chunk; (ii) Narrative Adaptive Memory (NAM): maintain our dynamic memory bank; and (iii) Narrative Adaptive Memory and Sparse Memory Activation $\begin{array} { r } { ( \mathrm { N A M + S M A } ) } \end{array}$ : our full model. Frame Sink is utilized by LongLive [35], allowing direct comparison with its model. The configuration of w/o memory is also implemented on the baseline of LongLive by removing the sink latent frames. Table 3 highlights the effectiveness of our NAM, which consistently outperforms others in maintaining temporal consistency and semantic coherence. The retrieval-based memory establishes intrinsic dependencies across contexts, enabling stable narrative transitions even under subject insertion or switching. With SMA, inference efficiency improves from 17.6 to 18.7 frames per second with minimal quality degradation. As shown in Figure 4, removing memory causes abrupt scene transitions, while Frame Sink preserves continuity only for initial subjects but collapses on later ones. In contrast, our model captures the relations between existing and emerging subjects, achieving superior semantic alignment on switched prompts, especially as the video extends beyond 30 seconds.

Memory Capacity. In Figure 5, we ablate the impact of our two key components under the 60-second setting: the capacity of origin memory in NAM and activated memory after SMA. The left panel analyzes NAM with varying capacities $b = \{ 3 , 6 , 9 \}$ against two baselines "w/o Memory" and "Frame Sink". The results reveal that a larger memory capacity does not guarantee better performance. Notably, NAM $b = 6$ ) consistently underperforms the baseline, while NAM $\mathrm { \Delta } \mathrm { b } = 9$ ) exhibits significant performance instability. We attribute this to an imbalance within the attention's receptive field: as memory capacity $b$ increases, the proportion of global context from memory significantly outweighs that of the local window. This over-reliance on global context can disrupt short-term narrative flow, leading to the observed fluctuations in CLIP score. Therefore, we select NAM $\mathrm { b } = 3$ ) , a capacity equivalent to half the size of our local context window, as it provides the most stable balance between local and global context and effective enhancement in semantic coherence over baseline.

# 5. Conclusion

In this work, we introduce MEMFLoW, a memory mechanism for equipping interactive long video generation with long-range consistency without severe efficiency degradation. For maintain long-term memory for narrative coherence, we design Narrative Adaptive Memory for dynamic retrieval of semantic-aligned context via textual query. We also introduce Sparse Memory Activation for balancing the memory-efficiency trade-off by relevance-gated memory filtering. Our model achieves 18.7 FPS inference on a single NVIDIA H100 GPU, and supports interactive video generation while maintaining consistency, visual quality and narrative coherence even under complex narrative transitions and character switching.

# References

[1] Shengqu Cai, Ceyuan Yang, Lvmin Zhang, Yuwei Guo, Junfei Xiao, Ziyan Yang, Yinghao Xu, Zhenheng Yang, Alan Yuille, Leonidas Guibas, eal.Mixture f contexts r video generation. arXiv preprint arXiv:2508.21058, 2025. 3, 4   
[2] Boyuan Chen, Diego Martí Monsó, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion forcing: Next-token prediction meets full-sequence diffusion. In NeurIPS, 2025. 2   
[3] Guibin Chen, Dixuan Lin, Jiangping Yang, Chunze Lin, Junchen Zhu, Mingyuan Fan, Hao Zhang, Sheng Chen, Zheng Chen, Chengcheng Ma, Weiming Xiong, Wei Wang, Nuo Pang, Kang Kang, Zhiheng Xu, Yuzhe Jin, Yupeng Liang, Yubing Song, Peng Zhao, Boyuan Xu, Di Qiu, Debang Li, Zhengcong Fei, Yang Li, and Yahui Zhou. Skyreels-v2: Infinite-length film generative model. CoRR, abs/2504.13074, 2025. 2, 5, 6, 7, 8   
[4] Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and Baobao Chang. An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large vision-language models. In European Conference on Computer Vision, pages 1935. Springer, 2024. 4   
[5] Taiye Chen, Xun Hu, Zihan Ding, and Chi Jin. Learning world models for interactive video generation. arXiv preprint arXiv:2505.21996, 2025. 3   
[6] Karan Dalal, Daniel Koceja, Jiarui Xu, Yue Zhao, Shihao Han, Ka Chun Cheung, Jan Kautz, Yejin Choi, Yu Sun, and Xiaolong Wang. One-minute video generation with test-time training. In CVPR, pages 1770217711, 2025. 2, 3   
[7] Haoge Deng, Ting Pan, Haiwen Diao, Zhengxiong Luo, Yufeng Cui, Huchuan Lu, Shiguang Shan, Yonggang Qi, and Xinlong Wang. Autoregressive video generation without vector quantization. In ICLR, 2025. 6, 7   
[8] Yuchao Gu, Weijia Mao, and Mike Zheng Shou. Longcontext autoregressive video modeling with next-frame prediction. CoRR, abs/2503.19325, 2025. 2, 3   
[9] Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, Poriya Panet, Sapir Weissbuch, Victor Kulikov, Yaki Bitterman, Zeev Melumian, and Ofir Bibi. Ltx-video: Realtime video latent diffusion. CoRR, abs/2501.00103, 2025. 6, 7   
10] Bo He, Hengduo Li, Young Kyun Jang, Menglin Jia, Xuefei Cao, Ashish Shah, Abhinav Shrivastava, and Ser-Nam Lim. Ma-lmm: Memory-augmented large multimodal model for long-term video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1350413514, 2024. 4   
[11] Roberto Henschel, Levon Khachatryan, Hayk Poghosyan, Daniil Hayrapetyan, Vahram Tadevosyan, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Streamingt2v: Consistent, dynamic, and extendable long video generation from text. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 25682577, 2025. 3   
[12] Yining Hong, Beide Liu, Maxine Wu, Yuanhao Zhai, KaiWei Chang, Linjie Li, Kevin Lin, Chung-Ching Lin, Jianfeng Wang, Zhengyuan Yang, et al. Slowfast-vgen: Slowfast learning for action-driven long video generation. arXiv preprint arXiv:2410.23277, 2024. 2, 3   
[13] Kaiyi Huang, Yukun Huang, Xintao Wang, Zinan Lin, Xuefei Ning, Pengfei Wan, Di Zhang, Yu Wang, and Xihui Liu. Filmaster: Bridging cinematic principles and generative ai for automated film generation. arXiv preprint arXiv:2506.18899, 2025. 2   
[14] Lianghua Huang, Wei Wang, Zhi-Fan Wu, Yupeng Shi, Huanzhang Dou, Chen Liang, Yutong Feng, Yu Liu, and Jingren Zhou. In-context lora for diffusion transformers. arXiv preprint arXiv:2410.23775, 2024. 2   
[15] Xun Huang, Zhengqi Li, Guande He, Mingyuan Zhou, and Eli Shechtman. Self forcing: Bridging the train-test gap in autoregressive video diffusion. CoRR, abs/2506.08009, 2025. 2, 3, 5, 6, 7, 8   
[16] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. VBench: Comprehensive benchmark suite for video generative models. In CVPR, 2024. 7   
[17] Ziqi Huang, Fan Zhang, Xiaojie Xu, Yinan He, Jiashuo Yu, Ziyue Dong, Qianli Ma, Nattapol Chanpaisit, Chenyang Si, Yuming Jiang, Yohui Wang, Xinyuan Chen, Yng-Cong Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. Vbench $^ { + + }$ :Comprehensive and versatile benchmark suite for video generative models. In arXiv, 2024. 5   
[18] Jiaxiu Jiang, Wenbo Li, Jingjing Ren, Yuping Qiu, Yong Guo, Xiaogang Xu, Han Wu, and Wangmeng Zuo. Lovic: Efficient long video generation with context compression. arXiv preprint arXiv:2507.12952, 2025. 2   
[19] Yang Jin, Zhicheng Sun, Ningyuan Li, Kun Xu, Hao Jiang, Nan Zhuang, Quzhe Huang, Yang Song, Yadong Mu, and Zhouchen Lin. Pyramidal flow matching for efficient video generative modeling. In ICLR, 2025. 6, 7   
[20] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: A systematic framework for large video generative models. arXiv:2412.03603, 2024. 2   
[21] Kuaishou. Kling video model. https : / / kling. kuaishou.com/en,2024.2   
[22] Runjia Li, Philip Torr, Andrea Vedaldi, and Tomas Jakab. Vmem: Consistent interactive video scene generation with surfel-indexed view memory. arXiv preprint "V:.2506 10002, 20252   
[∠5] ruclel Long, Lllaval iu, 1ig 1au, anu lav vel. Videostudio: Generating consistent-content and multi-scene videos. In European Conference on Computer Vision, pages 468485. Springer, 2024. 2   
[24] OpenAI. Sora: Creating video from text, 2024. 2   
[25] Wenqi Ouyang, Zeqi Xiao, Danni Yang, Yifan Zhou, Shuai Yang, Lei Yang, Jianlou Si, and Xingang Pan. Tokensgen: Harnessing condensed tokens for long video generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1819718206, 2025. 3   
[26] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, pages 41724182, 2023. 2   
[27] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, pages 87488763, 2021. 5   
[28] Hansi Teng, Hongyu Jia, Lei Sun, Lingzhi Li, Maolin Li, Mingqiu Tang, Shuai Han, Tiannig Zhang, W. Q. Zhang, Weifeng Luo, Xiaoyang Kang, Yuchen Sun, Yue Cao, Yunpeng Huang, Yutong Lin, Yuxin Fang, Zewei Tao, Zheng Zhang, Zhongshu Wang, Zixun Liu, Dai Shi, Guoli Su, Hanwen Sun, Hong Pan, Jie Wang, Jiexin Sheng, Min Cui, Min Hu, Ming Yan, Shucheng Yin, Siran Zhang, Tingting Liu, Xianping Yin, Xiaoyu Yang, Xin Song, Xuan Hu, Yankai Zhang, and Yuqiao Li. MAGI-1: autoregressive video generation at scale. CoRR, abs/2505.13211, 2025. 2, 3, 6, 7   
[29] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 2, 5, 6,7   
[30] Xiao Wang, Qingyi Si, Shiyu Zhu, Jianlong Wu, Li Cao, and Liqiang Nie. Adaretake: Adaptive redundancy reduction to perceive longer for video-language understanding. In Findings of the Association for Computational Linguistics: ACL 2025, pages 54175432, 2025. 4   
[31] Xunzhi Xiang, Yabo Chen, Guiyu Zhang, Zhongyu Wang, Zhe Gao, Quanming Xiang, Gonghu Shang, Junqi Liu, Haibin Huang, Yang Gao, et al. Macro-from-micro planning for high-quality and parallelized autoregressive long video generation. arXiv preprint arXiv:2508.03334, 2025. 2   
[32] Junfei Xiao, Ceyuan Yang, Lvmin Zhang, Shengqu Cai, Yang Zhao, Yuwei Guo, Gordon Wetzstein, Maneesh Agrawala, Alan Yuille, and Lu Jiang. Captain cinema: Towards short movie generation. arXiv preprint arXiv:2507.18634, 2025. 2   
[33] Zeqi Xiao, Yushi Lan, Yifan Zhou, Wenqi Ouyang, Shuai Yang, Yanhong Zeng, and Xingang Pan. Worldmem: Longterm consistent world simulation with memory. arXiv preprint arXiv:2504.12369, 2025. 3   
[34] An Yang, Jinze Bai, et al. Qwen2 technical report. arXiv, 2024. 5   
[35] Shuai Yang, Wei Huang, Ruihang Chu, Yicheng Xiao, Yuyang Zhao, Xianbang Wang, Muyang Li, Enze Xie, Yingcong Chen, Yao Lu, et al. Longlive: Real-time interactive long video generation. arXiv preprint arXiv:2509.22622, 2025. 1, 2, 4, 5, 6, 7, 8   
[36] Yanlai Yang, Zhuokai Zhao, Satya Narayan Shukla, Aashu Singh, Shlok Kumar Mishra, Lizhu Zhang, and Mengye Ren. Streammem: Query-agnostic kv cache memory for streaming video understanding. arXiv preprint arXiv:2508.15717, 2025. 4   
[37] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv:2408.06072, 2024. 2   
[38] Tianwei Yin, Michaël Gharbi, Richard Zhang, Eli Shechtman, Frédo Durand, William T. Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In CVPR, pages 66136623, 2024. 3   
[39] Tianwei Yin, Qiang Zhang, Richard Zhang, William T Freeman, Fredo Durand, Eli Shechtman, and Xun Huang. From slow bidirectional to fast autoregressive video diffusion models. In CVPR, 2025. 2, 3, 6, 7   
[40] Jiwen Yu, Jianhong Bai, Yiran Qin, Quande Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Xihui Liu. Context as memory: Scene-consistent interactive long video generation with memory retrieval. arXiv preprint arXiv:2506.03141, 2025. 3   
[41] Shangjin Zhai, Zhichao Ye, Jialin Liu, Weijian Xie, Jiaqi Hu, Zhen Peng, Hua Xue, Danpeng Chen, Xiaomeng Wang, Lei Yang, et al. Stargen: A spatiotemporal autoregression framework with video diffusion model for scalable and controllable scene generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 26822- 26833, 2025. 3   
[42] Lvmin Zhang and Maneesh Agrawala. Packing input frame context in next-frame prediction models for video generation. CoRR, abs/2504.12626, 2025. 2, 3, 5, 7, 8   
[43] Tianyuan Zhang, Sai Bi, Yicong Hong, Kai Zhang, Fujun Luan, Songlin Yang, Kalyan Sunkavalli, William T Freeman, and Hao Tan. Test-time training done right. In arXiv, 2025. 3   
[44] Yiming Zhang, Zhuokai Zhao, Zhaorun Chen, Zenghui Ding, Xianjun Yang, and Yining Sun. Beyond training: Dynamic token merging for zero-shot video understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2204622055, 2025. 4   
[45] Canyu Zhao, Mingyu Liu, Wen Wang, Weihua Chen, Fan Wang, Hao Chen, Bo Zhang, and Chunhua Shen. Moviedreamer: Hierarchical generation for coherent long visual sequences. In ICLR, 2025. 2   
[46] Yupeng Zhou, Daquan Zhou, Ming-Ming Cheng, Jiashi Feng, and Qibin Hou. Storydiffusion: Consistent selfattention for long-range image and video generation. Advances in Neural Information Processing Systems, 37: 110315110340, 2024. 2   
[47] Shaobin Zhuang, Kunchang Li, Xinyuan Chen, Yaohui Wang, Ziwei Liu, Yu Qiao, and Yali Wang. Vlogger: Make your dream a vlog. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 88068817, 2024. 2