# One-Minute Video Generation with Test-Time Training

Karan Dalal\*4 Daniel Koceja\*2 Gashon Hussein\*2 Jiarui ${ \mathrm { X u } } ^ { * 1 , 3 }$ Yue Zhao†5 Youjin Song‡2 Shihao Han1 Ka Chun Cheung1 Jan Kautz1 Carlos Guestrin2 Tatsunori Hashimoto2 Sanmi Koyejo2 Yejin Choi1 Yu Sun1,2 Xiaolong Wang1,3 1NVIDIA 2Stanford University 3UCSD 4UC Berkeley 5UT Austin

![](images/1.jpg)

![](images/2.jpg)  
Jerrhapp Toic poloy. Je oriv Tomaet e hee andhe tw eo oeth, henh .

J

# Abstract

Transformers today still struggle to generate one-minute videos because self-attention layers are inefficient for long context. Alternatives such as Mamba layers struggle with complex multi-scene stories because their hidden states are less expressive. We experiment with Test-Time Training (TTT) layers, whose hidden states themselves can be neural networks, therefore more expressive. Adding TTT layers into a pre-trained Transformer enables it to generate one-minute videos from text storyboards. For proof of concept, we curate a dataset based on Tom and Jerry cartoons. Compared to baselines such as Mamba 2, Gated DeltaNet, and sliding-window attention layers, TTT layers generate much more coherent videos that tell complex stories, leading by 34 Elo points in a human evaluation of 100 videos per method. Although promising, results still contain artifacts, likely due to the limited capability of the pre-trained 5B model. The efficiency of our implementation can also be improved. We have only experimented with one-minute videos due to resource constraints, but the approach can be extended to longer videos and more complex stories. Sample videos, code and annotations are available at: https://test-time-training.github.io/video-dit

# 1. Introduction

Despite the remarkable progress in visual and physical realism, state-of-the-art video Transformers are still generating mostly short clips of single scenes without complex stories. At the time of writing (March 2025), the maximum length of public APIs for video generation is 20 seconds for Sora (OpenAI), 16 seconds for MovieGen (Meta), 10 for Ray 2 (Luma), and 8 for Veo 2 (Google). None of these APIs can autonomously generate complex multi-scene stories.

![](images/3.jpg)  
the hidden state itself a model $f$ with weights $W$ $\ell$ Therefore, updating the hidden state on a test sequence is equivalent to training the model $f$ at test time. This process, known as Test-Time Training (TTT), is programmed into TTT layers. Figure and caption taken from [43].

A fundamental challenge behind these technical limitations is long context, because the cost of self-attention layers in Transformers increases quadratically with context length. This challenge is especially acute for video generation with dynamic motion, whose context cannot be easily compressed by a tokenizer. Using a standard tokenizer, each of our one-minute videos requires over $3 0 0 \mathrm { k }$ tokens in context. With self-attention, generating a one-minute video would have taken $1 1 \times$ longer than generating 20 videos of 3 seconds each, and training would have taken $1 2 \times$ longer. To address this challenge, recent work on video generation has investigated RNN layers as an efficient alternative to self-attention, because their cost increases linearly with context length [47]. Modern RNN layers, especially variants of linear attention [23, 37] such as Mamba [8, 12] and DeltaNet [35, 53], have shown impressive results for natural language tasks. However, we have yet to see long videos with complex stories or dynamic motion generated by RNNs. Videos (link) in [47] are high resolution and one-minute long, but contain only single scenes and slow motion, let alone complex stories. We believe that these RNN layers generate less complex videos because their hidden states are less expressive. RNN layers can only store past tokens into a hidden state of fixed size, which is only a matrix for linear attention variants such as Mamba and DeltaNet. It is inherently challenging to compress hundreds of thousands of vectors into a matrix with only thousands in rank. As a consequence, these RNN layers struggle to remember the deep relationships between distant tokens. We experiment with an alternative class of RNN layers whose hidden states themselves can be neural networks. Specifically, we use two-layer MLPs with $2 \times$ more hidden cells and richer nonlinearities than the linear (matrix) hidden states in linear attention variants. Since the neural network hidden states are updated by training even on test sequences, these new layers are called Test-Time Training (TTT) layers [43]. We start from a pre-trained Diffusion Transformer (CogVideo-X 5B [19]) that could only generate 3-second short clips at 16 fps (or 6 seconds at 8 fps). Then, we add TTT layers initialized from scratch and fine-tune this model to generate one-minute videos from text storyboards. We limit the self-attention layers to 3-second segments so their cost stays manageable. With only preliminary systems optimization, our training run takes the equivalent of 50 hours on $2 5 6 \mathrm { H } 1 0 0 \mathrm { s }$ . We curate a text-to-video dataset based on $\approx 7$ hours of Tom and Jerry cartoons with human-annotated storyboards. We intentionally limit our scope to this specific domain for fast research iteration. As a proof-of-concept, our dataset emphasizes complex, multi-scene, and long-range stories with dynamic motion, where progress is still needed; it has less emphasis on visual and physical realism, where remarkable progress has already been made. We believe that improvements in long-context capabilities for this specific domain will transfer to general-purpose video generation. Compared to strong baselines such as Mamba 2 [8], Gated DeltaNet [53], and sliding-window attention layers, TTT layers generate much more coherent videos that tell complex stories with dynamic motion, leading by 34 Elo points in a human evaluation of 100 videos per method. For context, GPT-4o scores 29 Elo points over GPT-4 Turbo in LMSys Chatbot Arena [6]. Sample videos, code and annotations are available at: https://test-time-training.github.io/video-dit

# 2. Test-Time Training Layers

Following standard practice [44, 54], each video is preprocessed into a sequence of $T$ tokens, where $T$ is determined by its duration and resolution. This section reviews Test-Time Training (TTT) layers for general sequence modeling, using some of the exposition in Section 2 of [43]. We first discuss how to process general input sequences in a causal manner (chronological order). Section 3 discusses how to use RNN layers in a non-causal backbone by invoking them in opposite directions.

# 2.1. TTT as Updating a Hidden State

All RNN layers compress historical context in a hidden state of fixed size. This compression has two consequences. On one hand, mapping an input token $x _ { t }$ to output token $z _ { t }$ is efficient, because both the update rule and output rule take constant time per token. On the other hand, an RNN layer's ability to remember long context is limited by the amount of information its hidden state can store. The goal of [43] is to design RNN layers with expressive hidden states that can compress massive context. As an inspiration, they observe that self-supervised learning can compress a massive training set into the weights of a machine learning model. The key idea in [43] is to use self-supervised learning to compress the historical context $x _ { 1 } , \ldots , x _ { t }$ into a hidden state $W _ { t }$ , by making the context an unlabeled dataset and the hidden state the weights of a machine learning model $f$ . The update rule, illustrated in Figure 2, is a step of gradient descent on some self-supervised loss $\ell$ :

$$
W _ { t } = W _ { t - 1 } - \eta \nabla \ell ( W _ { t - 1 } ; x _ { t } ) ,
$$

with learning rate $\eta$ .Intuitively, the output token is just the prediction on $x _ { t }$ , made by $f$ with the updated weights $W _ { t }$ :

$$
z _ { t } = f ( x _ { t } ; W _ { t } ) .
$$

One choice of $\ell$ is reconstructing $x _ { t }$ itself. To make the learning problem nontrivial, one can first process $x _ { t }$ into a corrupted input $\tilde { x } _ { t }$ (see Subsection 2.2), then optimize:

$$
\ell ( W ; x _ { t } ) = \| f ( \tilde { x } _ { t } ; W ) - x _ { t } \| ^ { 2 } .
$$

Similar to denoising autoencoders [46], $f$ needs to discover the correlations between dimensions of $x _ { t }$ in order to reconstruct it from partial information $\tilde { x } _ { t }$ . As with other RNN layers and self-attention, this algorithm that maps an input sequence $x _ { 1 } , \ldots , x _ { T }$ to output sequence $z _ { 1 } , \dots , z _ { T }$ can be programmed into the forward pass of a sequence modeling layer. Even at test time, the layer still trains a different sequence of weights $W _ { 1 } , \dots , W _ { T }$ for every input sequence. Therefore, it is called Test-Time Training (TTT) layer. Conceptually, calling backward on $\nabla \ell$ means taking gradients of gradients  a well-explored technique in metalearning. TTT layers have the same interface as RNN layers and self-attention, therefore can be replaced in any larger network architecture. [43] refers to training the larger network as the outer loop, and training $W$ within each TTT layer as the inner loop.

# 2.2. Learning a Self-Supervised Task for TTT

Arguably, the most important part of TTT is the selfsupervised task specified by $\ell$ Instead of handcrafting a self-supervised task from human priors, [43] takes a more end-to-end approach, learning it as part of the outer loop. Starting from the naive reconstruction task in Equation 3, they use a low-rank projection $\tilde { x } _ { t } = \theta _ { K } x _ { t }$ , where $\theta _ { K }$ is a matrix that is learnable in the outer loop. Moreover, perhaps not all the information in $x _ { t }$ is worth remembering, so the reconstruction label can also be a lowrank projection $\theta _ { V } x _ { t }$ instead of $x _ { t }$ . In summary, the selfsupervised loss in [43] is:

$$
\ell ( W ; x _ { t } ) = \| f ( \theta _ { K } x _ { t } ; W ) - \theta _ { V } x _ { t } \| ^ { 2 } .
$$

Lastly, since $\theta _ { K } x _ { t }$ has fewer dimensions than $x _ { t }$ , [43] can no longer use the output rule in Equation 2. So they make another projection $\theta _ { Q } { x } _ { t }$ , and change the output rule to:

$$
z _ { t } = f \left( \theta _ { Q } x _ { t } ; W _ { t } \right) .
$$

Note that in the inner loop, only $W$ is optimized, therefore written as an argument of $\ell$ ; the $\theta \mathrm { s }$ are "hyper-parameters" of this inner-loop loss function. $\theta _ { K } , \theta _ { V } , \theta _ { Q }$ are optimized in the outer loop, analogous to the Query, Key, and Value parameters of self-attention.

# 2.3. TTT-MLP Instantiation

Following [43], we instantiate the inner-loop model $f$ as a wrapper around $f _ { \mathsf { M L P } }$ : a two-layer MLP similar to those in Transformers. Specifically, the hidden dimension is $4 \times$ the input dimension, followed by a GELU activation [16]. For better stability during TTT, $f$ always contains a Layer Norm and residual connection. That is,

$$
f ( x ) = x + \mathsf { L N } ( f _ { \mathsf { M L P } } ( x ) ) .
$$

A TTT layer with this $f$ is called TTT-MLP, which is the default instantiation throughout this paper. In Section 4 we also instantiate TTT-Linear (the $f$ above wrapping around a linear model) as a baseline.

# 3. Approach

At a high level, our approach simply adds TTT layers to a pre-trained Diffusion Transformer and fine-tunes it on long videos with text annotations. At a practical level, making this approach work involves many design choices.

# 3.1. Architecture

Pre-trained Diffusion Transformer. Our approach of adding TTT layers then fine-tuning can, in principle, work with any backbone architecture. We choose Diffusion Transformers [32] for our initial demonstration because it is the most popular architecture for video generation. Since the cost of pre-training a Diffusion Transformer on videos is prohibitive, we start from a pre-trained checkpoint called CogVideo-X 5B [19].

![](images/4.jpg)  
selattention layers locally over segments and TTT layers gobally over the entire sequence.See Subsection 3..

Gating. Given an input sequence $X = ( x _ { 1 } , \dots , x _ { T } )$ where each token $x _ { t } \in \mathbb { R } ^ { d }$ , a TTT layer produces an output sequence $Z = ( z _ { 1 } , . . . , z _ { T } ) = \mathsf { T T T } ( X )$ . Each $z _ { t } \in \mathbb { R } ^ { d }$ follows the recurrence described by Equations 1, 4 and 5 in Section 2. Naively inserting TTT layers into a pre-trained network would dramatically worsen its predictions at the beginning of fine-tuning, when the TTT layers are randomly initialized. To avoid this degradation, we gate TTT with a learned vector $\boldsymbol { \alpha } \in \mathbb { R } ^ { d }$ following standard practice [1]:

$$
\mathtt { g a t e } ( \mathsf { T T T } , X ; \alpha ) = \operatorname { t a n h } ( \alpha ) \otimes \mathsf { T T T } ( X ) + X ,
$$

where $\operatorname { t a n h } ( \alpha ) \in ( - 1 , 1 ) ^ { d }$ is multiplied element-wise with each $z _ { t }$ in $Z = { \mathsf { T T T } } ( X )$ . We initialize all values in $\alpha$ to 0.1, so the values in $\operatorname { t a n h } ( \alpha )$ are close to 0 $( \approx 0 . 1 )$ at the beginning of fine-tuning. This initialization of $\alpha$ allows TTT to still contribute to $\mathtt { g a t e ( T T T , } X ; \alpha )$ without significantly overwriting $X$ . Bi-direction. Diffusion models, including CogVideo-X, are non-causal, meaning that an output token $z _ { t }$ can condition on all of $x _ { 1 } , \ldots , x _ { T }$ instead of only the past tokens $x _ { 1 } , \ldots , x _ { t }$ . To use TTT layers in a non-causal manner, we apply a standard trick called bi-direction [30]. Given an operator $\mathsf { r e v } ( X ) \ = \ ( x _ { T } , . . . , x _ { 1 } )$ that reverses $\boldsymbol { X } \ =$ $( x _ { 1 } , \dots , x _ { T } )$ in time, we define

$$
{ \mathsf { T T T } } ^ { \prime } ( X ) = { \mathsf { r e v } } ( { \mathsf { T T T } } ( { \mathsf { r e v } } ( X ) ) ) .
$$

Since rev is applied twice, $\mathsf { T T T } ^ { \prime } ( X )$ is still in chronological order. But the TTT layer inside it now scans through $X$ in reverse-chronological order. Modified architecture. Standard Transformers, including CogVideo-X, contain interleaving sequence modeling blocks and MLP blocks. Specifically, a standard sequence modeling block takes an input sequence $X$ and produces where LN is Layer $\mathrm { { N o r m } ^ { 1 } }$ and $X ^ { \prime } + X$ forms a residual connection. We only modify the sequence modeling blocks, leaving everything else in the architecture unchanged. Each modified block, illustrated in the left panel of Figure 3, continues from the $X ^ { \prime }$ in Equation 8 and produces

$$
\begin{array} { c } { { X ^ { \prime } = \mathsf { s e l f } _ { - } \mathsf { a t t n } ( \mathsf { L N } ( X ) ) } } \\ { { Y = X ^ { \prime } + X , } } \end{array}
$$

$$
\begin{array} { r c l } { { } } & { { } } & { { Z = \tt g a t e ( T I T , } X ^ { \prime } ; \alpha ) , }  \\ { { } } & { { } } & { { Z ^ { \prime } = \tt g a t e ( T T T ^ { \prime } , } Z ; \beta ) , }  \\ { { } } & { { } } & { { Y = Z ^ { \prime } + X . } } \end{array}
$$

Note that $\mathsf { T } \mathsf { T } \mathsf { T } ^ { \prime }$ only makes another call to TTT, so they share the same underlying parameters $\theta _ { K } , \theta _ { V } , \theta _ { Q }$ .But for gating, Equation 10 and 11 use different parameters $\alpha$ and $\beta$ .

# 3.2. Overall Pipeline

In this subsection, we discuss how to create the input sequence of tokens to our architecture and how each sequence is processed in segments. Except for the first two text formats in the upcoming discussion, everything applies to both fine-tuning and inference. Our pipeline is illustrated in the right panel of Figure 3. Scenes and segments. We structure our videos to contain multiple scenes,2 and each scene contains one or more 3- second segments. We use a 3-second segment as the atomic unit of text-to-video pairing for three reasons: •The maximum length of generation for the original pretrained CogVideo-X is 3 seconds. •The length of most scenes in the Tom and Jerry episodes is at least 3 seconds. •Building a dataset with multiple stages (Subsection 3.3) is most convenient given 3-second segments. Formats of text prompts. At inference time, a user can write the text prompt for a long video in any of the three formats listed below in the order of increasing detail. See Figure 8 in Appendix for examples of each format. •Format 1: A short summary of the plot in 5-8 sentences. Some of the examples are shown in Figure 1. •Format 2: A more detailed plot in roughly 20 sentences, with each sentence roughly corresponding to a 3-second segment. Sentences can be labeled as belonging to certain scenes or groups of scenes, but these labels will be treated only as suggestions. • Format 3: A storyboard. Each 3-second segment is described by a paragraph of 3-5 sentences, containing details such as background colors and camera movements. Groups of one or more paragraphs are strictly enforced as belonging to certain scenes with the keywords <scene start> and <scene end>. The actual input to our text tokenizer is always in Format 3 during both fine-tuning and inference. Conversion between the formats is performed by Claude 3.7 Sonnet in the order of $1  2  3$ 3 For fine-tuning, our human annotations are already in Format 3, as discussed in Subsection 3.3. From text to sequences. After the original CogVideo-X tokenizes the input text for each video, it concatenates the text tokens with noisy video tokens to form the input sequence to the Transformer. To generate a long video, we apply the same procedure independently for each 3-second segment. Specifically, given a storyboard in Format 3 with $n$ paragraphs, we first produce n sequence segments, each containing text tokens extracted from the corresponding paragraph followed by video tokens. Then we concatenate all $n$ sequence segments together to form the input sequence, which now has interleaved text and video tokens. Local attention, global TTT. CogVideo-X uses selfattention layers to process the entire input sequence globally for each video of maximum length 3 seconds, but global attention becomes inefficient for long videos. To avoid increasing the context length of self-attention layers, we make them local to each 3-second segment, attending to each of the $n$ sequence segments independently.4 The TTT layers process the entire input sequence globally because they are efficient in long context.

# 3.3. Fine-Tuning Recipe and Dataset

Multi-stage context extension. Following standard practice for LLMs [51], we extend the context length of our modified architecture to one minute in five stages. First, we fine-tune the entire pre-trained model on 3-second segments of Tom and Jerry to adapt it to this domain. New parameters (specifically those in TTT layers and gates) are assigned a higher learning rate during this stage. Over the next four stages, we fine-tune on videos of 9, 18, 30, and eventually 63 seconds. To avoid forgetting too much of the world knowledge from pre-training, we only fine-tune the TTT layers, gates, and self-attention layers, using a lower learning rate during these four stages. See Appendix A for the detailed recipe. Super-resolution on original videos. We start with 81 episodes of Tom and Jerry released between 1940 and 1948. Each episode is about 5 minutes, adding up to about 7 hours for all episodes. The original videos vary in resolution, which is uniformly poor by modern standards. We run a video super-resolution model [49] on the original videos, producing visually enhanced videos with shared resolution of $7 2 0 \times 4 8 0$ for our dataset. Multi-stage dataset. Following the structure discussed in Subsection 3.2, we first have human annotators break down each episode into scenes, then extract 3-second segments from each scene. Next we have human annotators write a detailed paragraph for each 3-second segment.5 Stage 1 fine-tunes directly on these segments. To create data for the last four stages, we concatenate contiguous 3-second segments into videos of 9, 18, 30 and 63 seconds together with their text annotations. Scene boundaries are marked by the same keywords in Subsection 3.2. As a result, annotations for all training videos are in Format 3.

# 3.4. Parallelization for Non-Causal Sequences

The update rule discussed in Section 2 cannot be naively parallelized across tokens in a sequence, since computing $W _ { t }$ requires $\nabla \ell ( W _ { t - 1 } ; x _ { t } )$ , which in turn requires $W _ { t - 1 }$ . To enable parallelization, we update $W$ on $b$ tokens at a time, which [43] calls an inner-loop mini-batch. Throughout this paper, we set $b = 6 4$ . Concretely, for mini-batch $i = 1 , \dots , T / b$ (assuming $T$ is an integer multiple of $b$ ),

$$
{ \cal W } _ { i b } = { \cal W } _ { ( i - 1 ) b } - \frac { \eta } { b } \sum _ { t = ( i - 1 ) b + 1 } ^ { i b } \nabla \ell \bigl ( W _ { ( i - 1 ) b } ; x _ { t } \bigr ) .
$$

Because the sequence is non-causal, we then use $W _ { i b }$ to produce the output tokens for all timesteps in mini-batch $i$ :

$$
z _ { t } = f ( W _ { i b } ; x _ { t } ) , \qquad \mathrm { f o r } \ t = ( i - 1 ) b + 1 , \dots , i b .
$$

Note that $W _ { ( i - 1 ) b + 1 } , \dots , W _ { i b - 1 }$ are no longer needed After this modification, $f$ can process an (inner-loop) mini-batch of tokens in parallel, similar to how a regular MLP processes an (outer-loop) mini-batch of training data. As a side benefit, we observe that averaging gradients across tokens reduces variance and stabilizes each update to $W$ .

![](images/5.jpg)  
the hidden state $\mathbf { \hat { W } } ^ { ( 1 ) }$ and $W ^ { ( 2 ) }$ across SMs, transferring them between HBM and SMEM only during initial loading and final output. intermediate activations among SMs.

# 3.5. On-Chip Tensor Parallel

# 4. Evaluation

Implementing TTT-MLP efficiently for GPUs requires special designs to take advantage of their memory hierarchy. A chip on a GPU is called a Streaming Multiprocessor (SM), analogous to a core on a CPU. All SMs on a GPU share a relatively slow but large global memory called HBM, then each SM has a fast but small on-chip memory called SMEM. Frequent data transfers between the SMEMs and HBM on a GPU can significantly hurt overall efficiency. Efficient implementations of Mamba and self-attention layers (Flash Attention [9]) use kernel fusion to minimize this kind of transfer. The high-level idea of these implementations is to load inputs and initial states into each SMEM, perform computations entirely on-chip, and write only the final outputs back to HBM. However, the hidden state for TTT-MLP, namely the weights $W ^ { ( 1 ) }$ and $W ^ { ( 2 ) }$ of the twolayer MLP $f$ , is too large to be stored in the SMEM of a single SM (when combined with inputs and activations). To reduce the memory required on each SM, we use Tensor Parallelism [39] to shard $\mathbf { \bar { \boldsymbol { W } } } ^ { ( 1 ) }$ and $W ^ { ( 2 ) }$ across SMs, as shown in Figure 4. Similar to how large MLP layers can be sharded and trained across the HBMs of multiple GPUs, we apply the same idea now across the SMEMs of multiple SMs, treating each SM as the analogy of a GPU. We use the DSMEM feature on the NVIDIA Hopper GPU architecture to implement AllReduce among SMs. More details of our kernel are discussed in Appendix B. Our implementation significantly improves efficiency, since hidden states and activations are now read from and written to HBMs only during initial loading and final output. As a general principle, if a model architecture $f$ can be sharded with standard Tensor Parallelism across GPUs, then the same sharding strategy can be applied across SMs when $f$ is used as the hidden state. We perform human evaluation on a multi-axis benchmark for TTT-MLP and five baselines, all with linear complexity: local attention, TTT-Linear, Mamba 2, Gated DeltaNet, and sliding window attention layers.

# 4.1. Baselines

Except for local attention, all baselines are added to the same pre-trained CogVideo-X 5B using the approach in Subection 3.1; their modified architectures all have 7.2B parameters. All baselines use the same fine-tuning recipe in Subsection 3.3 and Appendix A. Next we discuss the baselines in detail.

• Local attention: No modification to the original architecture, which performs self-attention on each 3-second segment independently. •TTT-Linear [43]: A TTT layer that instantiates $f ( x ) =$ $x + \mathsf { L N } ( f _ { \mathsf { L i n e a r } } ( x ) )$ , where $f _ { \mathsf { L i n e a r } }$ is a linear model. • Mamba 2 [8]: A modern RNN layer with a matrix hidden state, which is $\approx 4 \times$ larger than the hidden state in TTTLinear but $\approx 2 \times$ smaller than that in TTT-MLP. •Gated DeltaNet [53]: An extension of DeltaNet [52] and Mamba 2 with an improved update rule. •Sliding-window attention [3]: Self-attention with a fixed window of 8192 tokens (about 1.5 seconds of video).

# 4.2. Evaluation Axes and Protocol

From the six evaluation axes in MovieGen [44], we adopt the four relevant to our domain for human evaluation. 6 T

![](images/6.jpg)

ITT-ML el ha  g uoh-ql.

![](images/7.jpg)

GatD

![](images/8.jpg)

Fi a

<table><tr><td></td><td>Text following</td><td>Motion naturalness</td><td>Aesthetics</td><td>Temporal consistency</td><td>Average</td></tr><tr><td>Mamba 2</td><td>985</td><td>976</td><td>963</td><td>988</td><td>978</td></tr><tr><td>Gated DeltaNet</td><td>983</td><td>984</td><td>993</td><td>1004</td><td>991</td></tr><tr><td>Sliding window</td><td>1016</td><td>1000</td><td>1006</td><td>975</td><td>999</td></tr><tr><td>TTT-MLP</td><td>1014</td><td>1039</td><td>1037</td><td>1042</td><td>1033</td></tr></table>

Axes with the most improvements are scene consistency $( + 3 8 )$ and motion smoothness $( + 3 9 )$ . For context, GPT-4 scores 46 Elo points over GPT-3.5 Turbo, and GPT-4o scores 29 over GPT-4 Turbo in Chatbot Arena [6].

![](images/9.jpg)  
Figure 6. For 63-second videos, inference with full attention (over 300k tokens) would have taken $1 1 \times$ longer than local attention, and training $1 2 \times$ longer, as discussed in Section 1. TTT-MLP takes $2 . 5 \times$ and $3 . 8 \times$ respectively  significantly more efficient than full attention, but still less efficient than, for example, Gated DeltaNet, which takes $1 . 8 \times$ longer than local attention in both inference and training.

Text following: "aligment with the provided prompt. • Motion naturalness: "natural limb movements, facial expressions, and adherence to physical laws. Motion that appears unnatural or uncanny will be penalized." Aesthetics:interesting and compelling content, lighting, color, and camera effects." •Temporal consistency: both inside and across scenes. The quoted descriptions are from MovieGen [44]. Our evaluation is based on pairwise preferences in blind comparisons, because directly rating long videos or ranking many of them at once is challenging. Specifically, an evaluator is given a random axis from the four above and a random pair of videos sharing the same plot, then asked to indicate the better video for that axis. To collect the pool of videos, we first sample 100 plots using Claude 3.7 Sonnet (in Format $1  2  3$ as discussed in Subsection 3.2), then generate one video per method per plot. The methods generating the videos are always unknown to the evaluators. Our evaluators were recruited on prolific . com with the filters: living in the U.S., English as a first language, aged 18 to 35 years, with at least 100 previous submissions and an approval rate of at least $98 \%$ . The demographics of our evaluators, disclosed on the website, are as follows. Gender: $5 0 . 7 8 \%$ male, $4 7 . 6 6 \%$ female, $1 . 5 6 \%$ other. Ethnicity: $5 7 . 0 3 \%$ White, $2 3 . 4 4 \%$ Black, $1 0 . 9 4 \%$ Mixed, $5 . 4 7 \%$ Asian, and $3 . 1 2 \%$ other. Based on this information, we believe that our evaluators constitute a representative sample of the U.S. population.

# 4.3. Results

We aggregate the pairwise preferences using the Elo system in LMSys Chatbot Arena [6]. The Elo scores are shown in Table 1.

TTT-MLP improves over the second-best method by 34 Elo points on average. For context, GPT-4 scores 46 Elo points over GPT-3.5 Turbo (1163 vs. 1117), and GPT-40 scores 29 over GPT-4 Turbo (1285 vs. 1256) in LMSys Chatbot Arena [6], so our improvement by 34 is practically meaningful.7 Figure 5 compares frames of sample videos generated by TTT-MLP and the baselines. The videos illustrated in Figure 5 can be accessed on the project website: https://test-time-training.github.io/video-dit 18-second elimination round. Note that local attention and TTT-Linear do not appear in Table 1. To avoid the much higher cost of evaluating longer videos on every method, we first conducted an elimination round using 18-second videos following the same procedure discussed in Subsection 4.2. This round eliminated local attention, which performed worst, and also TTT-Linear, which performed worse than TTT-MLP. Results of the elimination round are shown in Table 3 in the Appendix.

![](images/10.jpg)  

Temporal consistency: The boxes morph between 3-second segments of the same scene.

![](images/11.jpg)

Motion naturalness: The cheese hovers in mid-air rather than falling naturally to the ground.

![](images/12.jpg)

Aesthetics: The lighting in the kitchen becomes dramatically brighter as Tom turns around. F pl

# 4.4. Limitations

Short context. For the 18-second elimination round discussed above, Gated DeltaNet performs the best on average, leading Mamba 2 by 27 Elo points and TTT-MLP by 28 (see Table 3 in the Appendix). For 18-second videos, the context length is roughly $1 0 0 \mathrm { k }$ tokens. This evaluation shows the scenario where RNN layers with linear (matrix) hidden states, such as Gated DeltaNet and Mamba 2, are still the most effective. Moreover, evaluation results for both 18 and 63-second videos indicate that Gated DeltaNet improves meaningfully on Mamba 2. Video artifacts. The generated 63-second videos demonstrate clear potential as a proof of concept, but still contain notable artifacts, especially in motion naturalness and aesthetics. Figure 7 illustrates examples of artifacts corresponding to three of our evaluation axes. We observe that videos with these kinds of artifacts are not particular to TTT-MLP, but common among all methods. The artifacts might have been a consequence of the limited capability of the pre-trained CogVideo-X 5B model. For example, videos (link) generated by the original CogVideo-X also seem to have limited motion naturalness and aesthetics. Wall-clock time. Even after applying our improvements in Subsection 3.4 and 3.5, the efficiency of TTT-MLP is still worse than Gated DeltaNet and Mamba 2. This limitation is highlighted in Figure 6, where inference and training with TTT-MLP are $1 . 4 \times$ and $2 . 1 \times$ slower than with Gated DeltaNet, for example. Section 6 discusses two potential improvements of our TTT-MLP kernel for better efficiency. Note that training efficiency is not a significant concern in our application because the RNN layers are integrated after pre-training, which constitutes most of the overall training budget. Training efficiency of the RNN layers is only relevant during fine-tuning, which is a small part of the budget to begin with. In contrast, inference efficiency is much more meaningful.

# 5. Related Work

Modern RNN layers, especially linear attention variants [23, 37], such as Mamba [8, 12] and DeltaNet [35, 52], have demonstrated impressive performance in natural language tasks. Inspired by their success and ideas from Fast Weight Programmers [7, 21, 24, 36], [43] proposes scalable and practical ways to make the hidden states large and nonlinear, therefore more expressive. Recent work [2] develops even larger and more nonlinear hidden states, and updates them with more sophisticated optimization techniques. The related work section in [43] contains a detailed discussion of inspirations for TTT layers. [48] gives a good overview of recent developments in RNN layers. Long video modeling. Some early work [40] generates long videos by training GAN [11, 22] to predict the next frame based on the current frame and the motion vector. Generation quality has improved significantly due to recent progress in auto-regression (AR) and diffusion-based approaches [13, 25, 44, 54]. TATS [10] proposes the sliding window attention on the Transformer to generate videos longer than the training length. Phenaki [45] works in a similar auto-regressive way, but each frame is generated by MaskGIT [4]. Pre-trained diffusion models can be extended to generate longer videos by using cascade [15, 50, 55], streaming [17], and adding transitions [5]. Story synthesis methods such as [20, 26, 28, 29, 31, 33] generate sequences of images or videos corresponding to individual sentences in a text story. For example, Craft [14] generates videos of complex scenes through retrieval, and StoryDiffusion [56] uses diffusion to improve the smoothness of transitions between frames. While related to textto-video generation, story synthesis methods usually need additional components in their pipeline to maintain coherence across scenes, which are not processed end-to-end.

# 6. Future Work

We outline several promising directions for future work. Faster implementation. Our current TTT-MLP kernel is bottlenecked by register spills and suboptimal ordering of asynchronous instructions. Efficiency could probably be further improved by minimizing register pressure and developing a more compiler-aware implementation of asynchronous operations. Better integration. Using bi-direction and learned gates is only one possible strategy for integrating TTT layers into a pre-trained model. Better strategies should further improve generation quality and accelerate fine-tuning. Other video generation backbones, such as autoregressive models, might require different integration strategies. Longer videos with larger hidden states. Our approach can potentially be extended to generate much longer videos with linear complexity. The key to achieving that goal, we believe, is to instantiate the hidden states as much larger neural networks than our two-layer MLP. For example, $f$ itself can be a Transformer. Acknowledgements. We thank Hyperbolic Labs for compute support, Yuntian Deng for help with running experiments, and Aaryan Singhal, Arjun Vikram, and Ben Spector for help with systems questions. Yue Zhao would like to thank Philipp Krähenbühl for discussion and feedback. Yu Sun would like to thank his PhD advisor Alyosha Efros for the insightful advice of looking at the pixels when working on machine learning. Note on authorship. Gashon Hussein and Youjin Song joined the team after an initial version of this project was submitted to CVPR, and have made major contributions to the final version. Because CVPR does not allow us to add authors after submission, their names could not appear on OpenReview and the conference webpage. However, we all agree that the official author list should include their names, as presented in our released PDFs. This project would not be possible without their work.

# References

[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. NeurIPS, 2022. 4   
[2] Ali Behrouz, Peilin Zhong, and Vahab Mirrokni. Titans: Learning to memorize at test time. arXiv preprint arXiv:2501.00663, 2024. 9   
[3] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. 6   
[4] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. Maskgit: Masked generative image transformer. In CVPR, 2022. 10   
[5] Xinyuan Chen, Yaohui Wang, Lingjun Zhang, Shaobin Zhuang, Xin Ma, Jiashuo Yu, Yali Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. Seine: Short-to-long video diffusion model for generative transition and prediction. In ICLR, 2023. 10   
[6] Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael Jordan, Joseph E Gonzalez, et al. Chatbot arena: An open platform for evaluating llms by human preference. In ICML, 2024. 2, 8   
[7] Kevin Clark, Kelvin Guu, Ming-Wei Chang, Panupong Pasupat, Geoffrey Hinton, and Mohammad Norouzi. Metalearning fast weight language models. EMNLP, 2022. 9   
[8] Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality. In ICML, 2024. 2, 6, 9   
[9] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness. In NeurIPS, 2022. 6   
10] Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan Pang, David Jacobs, Jia-Bin Huang, and Devi Parikh. Long video generation with time-agnostic vqgan and timesensitive transformer. In ECCV, 2022. 10   
11] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 2020. 10   
12] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. In COLM, 2024. 2, 9   
13] Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Fei-Fei Li, Irfan Essa, Lu Jiang, and José Lezama. r lltalsue vutu gellalo wiu ullusiol oui. II ECCV, 2024. 10   
[14] Tanmay Gupta, Dustin Schwenk, Ali Farhadi, Derek Hoiem, and Aniruddha Kembhavi. Imagine this! scripts to compositions to videos. In ECCV, 2018. 10   
[15] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity long video generation. arXiv preprint arXiv:2211.13221, 2022. 10   
[16] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016. 3   
[17] Roberto Henschel, Levon Khachatryan, Daniil Hayrapetyan, Hayk Poghosyan, Vahram Tadevosyan, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Streamingt2v: Consistent, dynamic, and extendable long video generation from text. arXiv preprint arXiv:2403.14773, 2024. 10   
[18] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 1   
[19] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. In ICLR, 2023. 2, 3   
[20] Ting-Hao Huang, Francis Ferraro, Nasrin Mostafazadeh, Ishan Misra, Aishwarya Agrawal, Jacob Devlin, Ross Girshick, Xiaodong He, Pushmeet Kohli, Dhruv Batra, et al. Visual storytelling. In NAACL, 2016. 10   
[21] Kazuki Irie, Imanol Schlag, Róbert Csordás, and Jürgen Schmidhuber. Going beyond linear transformers with recurrent fast weight programmers. NeurIPS, 2021. 9   
[22] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. In CVPR, 2020. 10   
[23] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In ICML, 2020. 2, 9   
[24] Louis Kirsch and Jürgen Schmidhuber. Meta learning backpropagation and improving it. NeurIPS, 34:1412214134, 2021. 9   
[25] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jn Zhou, Jiag Xiog, XinL Bo Wu, Jiai a Kathrina Wu, Qin Lin, Junkun Yuan, Yanxin Long, Aladdin Wang, Andong Wang, Changlin Li, Duojun Huang, Fang Yang, Hao Tan, Hongmei Wang, Jacob Song, Jiawang Bai, Jianbing Wu, Jinbao Xue, Joey Wang, Kai Wang, Mengyang Liu, Pengyu Li, Shuai Li, Weiyan Wang, Wenqing Yu, Xinchi Deng, Yang Li, Yi Chen, Yutao Cui, Yuan ng Zhentao Yu, Zhiyu He, Zhiyong Xu, Zixiang Zhou, Zunnan Xu, Yangyu Tao, Qinglin Lu, Songtao Liu, Dax Zhou, Hongfa Wang, Yong Yang, Di Wang, Yuhong Liu, Jie Jiang, and Caesar Zhong. Hunyuanvideo: A systematic framework for large video generative models. arXiv preprint arXiv 2412.03603, 2025. 10   
[26] Yitong Li, Zhe Gan, Yelong Shen, Jingjing Liu, Yu Cheng, Yuexin Wu, Lawrence Carin, David Carlson, and Jianfeng Gao. Storygan: A sequential conditional gan for story visualization. In CVPR, 2019. 10   
[27] Shanchuan Lin, Bingchen Liu, Jiashi Li, and Xiao Yang. Common diffusion noise schedules and sample steps are flawed. In WACV, 2024. 1   
[28] Chang Liu, Haoning Wu, Yujie Zhong, Xiaoyun Zhang, Yanfeng Wang, and Weidi Xie. Intelligent grimm-open-ended visual storytelling via latent diffusion models. In CVPR, 2024. 10   
[29] Adyasha Maharana, Darryl Hannan, and Mohit Bansal. Storydall-e: Adapting pretrained text-to-image transformers for story continuation. In ECCV, 2022. 10   
[30] Shentong Mo and Yapeng Tian. Scaling diffusion mamba with bidirectional ssms for efficient image and video generation. arXiv preprint arXiv:2405.15881, 2024. 4   
[31] Xichen Pan, Pengda Qin, Yuhong Li, Hui Xue, and Wenhu Chen. Synthesizing coherent story with auto-regressive latent diffusion models. In WACV, 2024. 10   
[32] William Peebles and Saining Xie. Scalable diffusion models with transformers. In CVPR, 2023. 3, 4   
[33] Tanzila Rahman, Hsin-Ying Lee, Jian Ren, Sergey Tulyakov, Shweta Mahajan, and Leonid Sigal. Make-a-story: Visual memory conditioned consistent story generation. In CVPR, 2023. 10   
[34] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In ICLR, 2022. 1   
[35] Imanol Schlag, Kazuki Irie, and Jürgen Schmidhuber. Linear transformers are secretly fast weight programmers. In ICML, 2021. 2, 9   
[36] Jürgen Schmidhuber. Learning to control fast-weight memories: An alternative to dynamic recurrent networks. Neural Computation, 4(1):131139, 1992. 9   
[37] Jürgen Schmidhuber. Learning to control fast-weight memories: An alternative to dynamic recurrent networks. Neural Computation, 4(1):131139, 1992. 2, 9   
[38] Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, and Tri Dao. Flashattention-3: Fast and accurate attention with asynchrony and low-precision, 2024. 1   
[39] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatronlm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. 6   
[40] Ivan Skorokhodov, Sergey Tulyakov, and Mohamed Elhoseiny. Stylegan-v: A continuous video generator with the price, image quality and perks of stylegan2. In CVPR, 2022. 10   
[41] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR, 2021. 1   
[42] Benjamin F Spector, Simran Arora, Aaryan Singhal, Daniel Y Fu, and Christopher Ré. Thunderkittens: Simple, fast, and adorable ai kernels. In ICLR, 2025. 1   
[43] Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong Wang, Sanmi Koyejo, Tatsunori Hashimoto, and Carlos Guestrin. Learning to (learn at test time): Rnns with expressive hidden states. arXiv preprint arXiv:2407.04620, 2024. 2, 3, 5, 6, 9, 1   
[44] The Movie Gen team. Movie gen: A cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. 2, 6, 8, 10   
[43] Kuen viegas, monammau Bavaerzauen, rieter-Jan Kndermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable length video generation from open domain textual description. In ICLR, 2023. 10   
[46] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and composing robust features with denoising autoencoders. In ICML, 2008. 3   
[47] Hongjie Wang, Chih-Yao Ma, Yen-Cheng Liu, Ji Hou, Tao Xu, Jialiang Wang, Felix Juefei-Xu, Yaqiao Luo, Peizhao Zhang, Tingbo Hou, Peter Vajda, Niraj K. Jha, and Xiaoliang Dai. Lingen: Towards high-resolution minute-length text-to-video generation with linear computational complexity, 2024. 2   
[48] Ke Alexander Wang, Jiaxin Shi, and Emily B Fox. Testtime regression: a unifying framework for designing sequence models with associative memory. arXiv preprint arXiv:2501.12352, 2025. 9   
[49] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan. Real-esrgan: Training real-world blind super-resolution with pure synthetic data. In ICCVW, 2021. 5   
[50] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, et al. Lavie: High-quality video generation with cascaded latent diffusion models. IJCV, 2024. 10   
[51] Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, et al. Effective long-context scaling of foundation models. In NAACL, 2024. 5   
[52] Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, and Yoon Kim. Parallelizing linear transformers with the delta rule over sequence length. In NeurIPS, 2024. 6, 9   
[53] Songlin Yang, Jan Kautz, and Ali Hatamizadeh. Gated delta networks: Improving mamba2 with delta rule. In ICLR, 2025. 2, 6   
[54] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. In ICLR, 2025. 2, 10, 1   
[55] Shengming Yin, Chenfei Wu, Huan Yang, Jianfeng Wang, Xiaodong Wang, Minheng Ni, Zhengyuan Yang, Linjie Li, Shuguang Liu, Fan Yang, et al. Nuwa-xl: Diffusion over diffusion for extremely long video generation. arXiv preprint arXiv:2303.12346, 2023. 10   
[56] Yupeng Zhou, Daquan Zhou, Ming-Ming Cheng, Jiashi Feng, and Qibin Hou. Storydiffusion: Consistent selfattention for long-range image and video generation. In NeurIPS, 2024. 10 T J parameters are fine-tuned at reduced learning rates.   

<table><tr><td>Video len.</td><td>Ctx. len</td><td>Trainable parameters</td><td>Learning rate</td><td>Schedule</td><td>Steps</td></tr><tr><td>3 sec</td><td>18048</td><td>TTT / Pre-trained Params</td><td>1 × 10−4 / 1 × 10−5</td><td>Cosine / Constant</td><td>5000</td></tr><tr><td>9 sec</td><td>51456</td><td>TTT + Local Attn (QKVO)</td><td>1 × 10−5</td><td>Constant</td><td>5000</td></tr><tr><td>18 sec</td><td>99894</td><td>TTT + Local Attn (QKVO)</td><td>1 × 10-5</td><td>Constant</td><td>1000</td></tr><tr><td>30 sec</td><td>168320</td><td>TTT + Local Attn (QKVO)</td><td>1 × 10−5</td><td>Constant</td><td>500</td></tr><tr><td>63 sec</td><td>341550</td><td>TTT + Local Attn (QKVO)</td><td>1 × 10-5</td><td>Constant</td><td>250</td></tr></table>

Table 3. Human evaluation results for 18-second videos, discussed in Subsection 4.3 and 4.4.   

<table><tr><td></td><td>Text following</td><td>Motion naturalness</td><td>Aesthetics</td><td>Temporal consistency</td><td>Average</td></tr><tr><td>Local Attention</td><td>965</td><td>972</td><td>969</td><td>944</td><td>962</td></tr><tr><td>TTT-Linear</td><td>1003</td><td>995</td><td>1007</td><td>1001</td><td>1001</td></tr><tr><td>Mamba 2</td><td>1023</td><td>987</td><td>1008</td><td>1004</td><td>1005</td></tr><tr><td>Gated DeltaNet</td><td>1020</td><td>1039</td><td>1044</td><td>1026</td><td>1032</td></tr><tr><td>SWA</td><td>995</td><td>1004</td><td>993</td><td>980</td><td>993</td></tr><tr><td>TTT-MLP</td><td>994</td><td>1002</td><td>1002</td><td>1019</td><td>1004</td></tr></table>

# A. Experiment Details

Diffusion schedule. Following CogVideoX [54], we finetune our model using v-prediction [34], which includes a diffusion noise schedule with 1000 steps and ZeroSNR [27] enforced at the final step. Training configurations. We use the following hyperparameters for all stages of training: Optimizer: AdamW with $( \beta _ { 1 } , \beta _ { 2 } ) = ( 0 . 9 , 0 . 9 5 )$ Learning Rate: Linear warmup over $2 \%$ of training steps Batch Size: 64 • Gradient Clipping: 0.1 Weight Decay: $1 0 ^ { - 4 }$ applied to all params except biases and normalization layers VAE Scale Factor: 1.0 Dropout: Zero-out text prompt with probability 0.1 •Precision: Mixed Precision with PyTorch FSDP2 TTT configurations. A key hyperparameter for TTT layers is the inner-loop learning rate $\eta$ ,which we set $\eta = 1 . 0$ for TTT-Linear and $\eta = 0 . 1$ for TTT-MLP. Sampling schedule. We follow the DDIM sampler [41] with 50 steps, applying dynamic classifier-free guidance (CFG) [18] that increases CFG magnitude from 1 to 4 and utilizing negative prompts to further enhance video quality.

# B. On-Chip Tensor Parallel Details

We use ThunderKittens [42] to implement the TTT-MLP kernel, described in Subsection 3.5. Hidden state sharding. We follow the standard strategy for Tensor Parallel, sharding the first layer column-wise and the second layer row-wise. As the GeLU non-linearity is elementwise, the forward pass of the TTT-layer requires a single reduction for computing the inner loss used to update the hidden state. Further latency optimizations. We incorporate several techniques from FlashAttention-3 [38] to further reduce I/O latency on NVIDIA Hopper GPUs. In particular, we implement a multi-stage pipelining scheme that asynchronously prefetches future mini-batches from HBM, overlapping data transfers with computation on the current mini-batch. This approach, known as producer-consumer asynchrony, involves dedicating specialized warpgroups to either data loading (producer) or computation (consumer). Gradient checkpointing. We integrate gradient checkpointing along the sequence dimension [43] directly into our fused kernel. To reduce I/O-induced stalls and CUDA thread workloads, we use the Tensor Memory Accelerator (TMA) to perform asynchronous memory stores.

# Format 1

T tn Tom is about to catch Jerry, Jerry makes it through the mouse hole and Tom slams into the wall.

# Format 2

Seent 1-: Tom walks into the kitchen carrying an apple pi. He sits at the table and begins eating. ST watches Tom eating the pie, and eagerly rubs his tummy. He then darts off-screen to the right. S  u J  o s     qu. The story continues..

# Format 3 <arThenas   all his, nd wit-n-hie le T a c camera smoothly follows Tom from left to right, clearly showing each of his movements.

Thea H os coT   b <arThenas   walls whbi n  wind-whiecku lettT vhshi  l ak behind the alt shaker.The cmer captures Jerry s he merges rom behind the salt shaker and stands o the countertop. Thea v remains in position slightly to the side of Jerry, capturing his hungry expression. Thea t  r o c The story continues... tions of the segments, and (3) a detailed storyboard.