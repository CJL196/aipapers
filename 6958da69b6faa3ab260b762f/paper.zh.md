# VBench: 视频生成模型的综合基准测试套件

黄自琦1\* 姜逸南 $\mathrm { H e ^ { 2 * } }$ 许家硕 $\mathrm { { Y u ^ { 2 * } } }$ 张帆2\* 孙晨阳 $S \mathrm { i } ^ { 1 }$ 姜宇明1 张元汉1 田星 ${ \mathbf { W } } { \mathbf { u } } ^ { 1 }$ 晋青阳1 查纳坡·查帕西特1 王耀辉2 陈鑫元2 王丽敏4,2 林达华2,3 乔宇2 刘子威1 1南洋理工大学S-Lab 2上海人工智能实验室 3香港中文大学 南京大学 https://vchitect.github.io/VBench-project/

![](images/1.jpg)  
aligned with human perceptions. VBench can provide valuable insights from multiple perspectives.

# 摘要

视频生成技术已经取得了显著进展，但评估这些模型依然面临挑战。一套全面的视频生成评估基准是不可或缺的，原因有二：1) 现有指标并未完全符合人类的感知；2) 理想的评估系统应能提供洞见，以指导视频生成的未来发展。为此，我们提出了VBench，一个全面的基准套件，将“视频生成质量”细分为具体的、层次分明的、相互独立的维度，每个维度都配有量身定制的提示和评估方法。VBench具有三个显著特性：1) 综合维度：VBench包含了16个视频生成维度（如主体身份不一致、运动平滑性、时间闪烁、空间关系等）。细粒度的评估指标揭示了各个模型的优缺点。2) 人类对齐：我们还提供了人类偏好注释的数据集，以验证我们基准在每个评估维度上与人类感知的一致性。3) 有价值的洞见：我们检视了当前模型在各种评估维度和不同内容类型上的表现，并探讨了视频生成模型与图像生成模型之间的差距。我们将开源VBench，包含所有提示、评估方法、生成的视频及人类偏好注释，并将更多视频生成模型纳入VBench，以推动视频生成领域的发展。

![](images/2.jpg)  
Figure 2. VBench Evaluation Results of Video Generative Models. We visualize the evaluation results of four video generation models in 16 VBench dimensions. We normalize the results per dimension for clearer comparisons. For comprehensive numerical results, please refer to Table 1.

# 1. 引言

图像生成模型在过去几年中取得了快速进展，例如变分自编码器（VAEs）、生成对抗网络（GANs）、基于向量量化（VQ）的方法以及扩散模型。这推动了最近在视频生成领域的探索，这超越了静态图像，建模现实场景的动态和运动学。随着视频生成模型的发展，迫切需要有效的评估方法。评估应该能够准确反映人类对生成视频的感知，提供可靠的模型性能衡量标准。此外，它还应反映每个模型的特定优缺点，提供有益的见解，以指导未来视频生成模型的数据、训练和架构选择。然而，现有的用于视频生成的指标，如启发分数（IS）、Fréchet 启发距离（FID）、Fréchet 视频距离（FVD）以及 CLIPSIM 等，与人类判断不一致。同时，主要为真实视频设计的视频质量评估（VQA）方法，忽视了生成模型所带来的独特挑战，如合成视频中的伪影。因此，迫切需要一个与人类感知紧密对齐、专门为视频生成模型特征设计的评估框架。为此，我们引入了 VBench，一个全面的基准套件，用于评估视频生成模型的性能。VBench 具有三个显著特点：1）全面的评估维度，2）与人类感知的一致性，3）有价值的见解。

首先，我们的框架包含一个评估维度套件，采用分层和解耦的方法对“视频生成质量”进行分解。该套件系统性地将评估分为两个主要维度：视频质量和视频条件一致性。每个维度又进一步细分为更具体的标准。这种分层划分确保每个维度独立评估视频质量的单一方面，不会受到其他变量的干扰，如图1所示。鉴于视频生成面临的独特挑战，我们为其特点量身定制了评估维度。例如，在视频质量方面，保持生成视频中的主体身份一致（例如，一个泰迪熊）至关重要，而这是在现实视频中少见的问题。此外，视频条件一致性对条件视频生成任务至关重要，因此需要专门的评估标准。对于每个评估维度，我们精心准备了大约100个文本提示作为文本到视频（T2V）生成的测试用例，并为每个维度设计了专门的评估方法。除了多维度评估外，我们还评估了不同内容类别下的T2V模型。我们为八种不同类型（如动物、建筑、人类和风景）组织了提示套件，使每个类别内能够进行单独评估。这项探索揭示了不同内容类型在T2V生成中的能力差异，突出了擅长领域和需要进一步增强的领域。其次，我们系统地展示了我们的评估方法套件在每个细粒度评估维度上与人类感知的紧密关联。我们收集了每个维度的人类偏好标注。具体而言，我们使用不同的T2V模型从我们的提示套件中抽取视频。然后，在从同一提示中抽取的两个视频中，我们要求人类标注者根据每个VBench维度指示偏好。我们表明VBench评估与人类偏好高度相关。此外，人类偏好标注可用于多种目的，例如微调生成或评估模型，以增强与人类感知的一致性。例如，我们利用标注在视觉语言模型（VLM）中实施指令微调，提高其T2V评估与人类偏好的对齐。第三，VBench的多维度和多类别方法能为视频生成社区提供有价值的见解。我们的多维度系统能够对视频生成模型在各个能力方面的优缺点提供详细反馈。这种方法不仅确保对现有模型的全面评估，还为高级视频生成模型的训练提供了宝贵的见解，指导架构和数据选择以改善视频生成结果。此外，VBench还可以轻松应用于图像生成模型的评估，因此我们研究了视频和图像生成模型之间的差异。在第5节中，我们详细讨论从VBench评估中得出的各种观察和见解。我们将开源VBench，包括其评估维度套件、评估方法套件、提示套件、生成视频以及人类偏好标注的数据集。我们还鼓励更多视频生成模型参与VBench挑战。

# 2. 相关工作

视频生成模型。最近，扩散模型在图像合成方面取得了显著进展，并推动了一系列视频生成的研究方向。许多近期基于扩散的工作都是文本到视频（T2V）模型。此外，还有其他引导模态可用，包括图像到视频、视频到视频，以及各种控制图，如姿态、深度和草图。视频生成模型的蓬勃发展需要一个全面的评估系统，以了解其当前能力并指导未来的发展，VBench积极提供了一个全面的基准测试套件，用于细致且符合人类评判标准的评估。

视觉生成模型的评估。现有的视频生成模型通常使用诸如Inception Score (IS) [87]、Fréchet inception distance (FID) [37]、Fréchet Video Distance (FVD) [95] 和CLIPSIM [83]等指标进行评估。UCF-101 [92] 数据集的类别标签通常用作IS、FID和FVD的文本提示，而MSR-VTT [120]中的人工标注视频标题则用于CLIPSIM。尽管涵盖了各种真实场景，这些提示缺乏多样性和特异性，限制了对视频生成的准确和细致评估。对于文本到图像（T2I）模型，已有多个基准 [6, 7, 44, 61, 86, 99] 被提出，以评估其组合性 [44] 和编辑能力 [7, 99] 等各种能力。然而，视频生成模型仍然缺乏全面的评估基准，以获取详细和与人类对齐的反馈。我们的工作与同时期研究 [70, 71] 有三个关键区别：1) 我们创建了16个独特的评估维度，每个维度都有专门的提示，用于精准评估；2) 我们经过实证验证每个维度与人类感知密切相关；3) 我们的多维度、多类别评估为视频生成提供了有价值的全面见解。

# 3. VBench 套件

在本节中，我们介绍 VBench 的主要组成部分。在第 3.1 节中，我们阐述设计 16 个评估维度的理由，以及每个维度的定义和评估方法。然后在第 3.2 节中详细说明我们使用的提示套件。为了验证 VBench 与人类感知的一致性，我们对每个维度进行人类偏好标注（见第 3.3 节）。关于 VBench 的实验及其得出的洞见将在第 4 节和第 5 节中详细介绍。

# 3.1. 评估维度套件

我们首先介绍我们的评估维度及其对应的评估方法。现有的评估指标，例如FVD [95]，通常将视频生成模型的性能总结为一个单一的数字。这种做法过于简化评估，存在多种风险。首先，单一的数字可能会掩盖一个模型的优缺点，无法提供关于模型在特定领域的优劣表现的洞察。这使得基于单值指标难以为未来的架构与训练设计提供有价值的见解。其次，“高质量视频生成”的概念复杂且多面，不同个体在意图应用中优先考虑的视频属性各不相同。例如，有些人可能优先关注时间闪烁的缺失，而另一些人则可能认为对文本提示的忠实度是最重要的，而对闪烁的关注程度则较少。因此，与其对视频生成质量进行单值评估，我们建议采取解构的方法，通过将“视频生成性能”的广义概念分解为多个离散维度进行细粒度评估。具体而言，我们将“视频生成质量”从上到下分解为16个解构维度，每个评估维度评估视频生成质量的一个方面。在最上层，我们从两个广泛的角度评估T2V性能：1）视频质量——“不考虑与文本提示的对齐，视频本身看起来好吗？”该评估侧重于合成视频的感知质量，并不考虑输入条件（例如，文本提示）；2）视频-条件一致性——“视频是否与用户希望生成的内容一致？”该评估侧重于合成视频是否与用户提供的指导条件（例如，T2V生成的文本提示）一致。在“视频质量”和“视频-条件一致性”这两个维度下，我们进一步将粗粒度维度细分为更细粒度的维度，如图1所示。

# 3.1.1 视频质量

我们将“视频质量”拆分为两个独立的方面：“时间质量”和“逐帧质量”，前者仅考虑跨帧的一致性和动态，后者仅关注每个单独帧的质量，而不考虑时间质量。对于“时间质量”，我们进一步设计了五个不同的评估维度，每个维度聚焦于时间质量的不同方面。我们在此简要介绍每个维度。详细的定义和评估方法请参考补充文件。 时间质量 - 主题一致性。对于视频中的一个主题（例如，一个人、一辆车或一只猫），我们评估其外观在整个视频中是否保持一致。为此，我们计算跨帧的DINO特征相似性。 时间质量 - 背景一致性。我们通过计算跨帧的CLIP特征相似性来评估背景场景的时间一致性。 时间质量 - 时间闪烁。生成的视频在局部和高频细节上可能表现出不完美的时间一致性。我们提取静态帧并计算跨帧的平均绝对差异。 时间质量 - 动作流畅性。主题一致性和背景一致性侧重于“视觉”的时间一致性，而不是“运动和动态”的流畅性。我们认为评估生成视频中的运动是否流畅、是否遵循现实世界的物理规律是重要的。我们利用视频帧插值模型中的运动先验评估生成动作的流畅性（详细方法见补充文件）。 时间质量 - 动态程度。由于完全静态的视频在上述时间质量维度中可能得分较高，因此评估每个模型生成的动态程度（即它是否包含大幅动作）也是重要的。我们使用RAFT来估计合成视频中的动态程度。 逐帧质量 - 美学质量。我们使用LAION美学预测器评估人类对每个视频帧所感知的艺术性和美感值。它可以反映美学方面，比如布局、色彩的丰富性和和谐、照片真实感、自然性和视频帧的艺术质量。 逐帧质量 - 成像质量。成像质量是指生成帧中出现的失真（例如过曝、噪声、模糊），我们使用在SPAQ数据集上训练的MUSIQ图像质量预测器来评估。

# 3.1.2 视频条件一致性

我们主要将“视频条件一致性”分解为“语义”（即实体类型及其属性）和“风格”（即生成的视频是否与用户请求的风格一致），并将每个部分细分为更细粒度的维度。语义 - 对象类别。我们使用 GRiT [115] 来检测生成特定类别对象的成功率，该对象在文本提示中被描述。语义 - 多个对象。除了生成特定类别的单个对象外，能够在同一帧中组合来自不同类别的多个对象也是视频生成中的一项重要能力。我们检测每个视频帧中生成所有在文本提示中指定的对象的成功率。语义 - 人类动作。人类动作是以人为中心的视频生成中的一个重要方面。我们应用 UMT [65] 来评估生成视频中的人类主体是否能够准确执行文本提示中提到的特定动作。语义 - 颜色。为了评估合成的对象颜色是否与文本提示一致，我们使用 GRiT [115] 提供颜色标注，并与预期颜色进行比较。语义 - 空间关系。除了合成对象的类别和属性外，我们还评估它们的空间关系是否遵循文本提示中所指定的内容。我们关注四种主要空间关系，并进行类似于 [44] 的基于规则的评估。语义 - 场景。我们需要评估合成视频是否与文本提示中描述的预期场景一致。例如，当提示为“海洋”时，生成的视频应为“海洋”而不是“河流”。我们使用 Tag2Text [45] 对生成的场景进行标注，然后检查其与文本提示中场景描述的对应关系。风格 - 外观风格。除了与文本提示的语义一致性之外，视频条件一致性的另一个重要支柱是风格。有许多风格会改变合成视频帧的外观、颜色和纹理，如“油画风格”、“黑白风格”、“水彩画风格”、“赛博朋克风格”、“黑白”等。我们计算合成帧与这些风格描述之间的 CLIP [83] 特征相似度。风格 - 时间风格。除了外观风格之外，视频还有时间风格，如各种摄像机运动。我们使用 ViCLIP [105] 计算视频特征与时间风格描述特征的相似度，以反映时间风格的一致性。整体一致性。我们进一步使用 ViCLIP [105] 在通用文本提示上计算的整体视频-文本一致性作为辅助指标，以反映语义和风格的一致性。

![](images/3.jpg)  
Figure 3. Prompt Suite Statistics. The two graphs provide an overview of our prompt suites. Left: the word cloud to visualize word distribution of our prompt suites. Right: the number of prompts across different evaluation dimensions and different content categories.

![](images/4.jpg)  
Figure 4. Interface for Human Preference Annotation. Top: prompt and question. Right: choices that annotators can make. Bottom left: control for stop and playback.

对于每个维度，请参考补充文件以获取：1) 定义的详细信息，2) 每个维度的正面和负面示例（即合成视频），3) 详细的评估方法和流程实现。

# 3.2. 提示套件

当前基于扩散的视频生成模型的采样过程耗时较长（例如，LaVie每个视频耗时90秒，CogVideo每个视频耗时超过2分钟）。因此，我们需要控制测试用例的数量以实现高效评估。同时，我们需要保持提示套件的多样性和全面性，因此我们设计了紧凑而具有代表性的提示，既涵盖评估维度，也包含内容类别。我们在图3中可视化了我们的提示套件分布。 每个维度的提示套件。在每个VBench评估维度中，我们精心设计了一组约100个提示作为测试用例。提示套件经过精心策划，以测试与所测试维度相应的特定能力。例如，在“主体一致性”维度中，该维度旨在评估视频中主体外观的一致性，我们确保每个提示都有一个可移动的主体（例如动物或交通工具），执行非静态动作，其一致性可能由于移动或变化位置引入的不一致性而受到影响。在“物体类别”维度中，我们确保每个提示都包含特定类别的物体。对于“人类动作”维度，每个测试提示包含一个人类主体执行来自Kinetics-400数据集的明确定义的动作，其中选择了100个具有最小语义重叠的代表性动作。有关每个16个维度的提示套件设计依据，请参阅补充文件。 按类别的提示套件。在为每个维度设计提示时，重点是展示模型在该特定维度的能力。我们进一步结合了八个内容类别的提示套件，以提供对不同内容类型表现的见解。为此，我们从互联网上准备了一系列人类策划的提示，并根据YouTube的分类将其划分为8个不同的类别。然后，我们将类别标签和提示输入到一个大型语言模型（LLM）中（更多实施细节请参见补充文件），为每个标题获得多标签输出。我们选择了800个提示并手动清理其标签，以作为按类别的提示套件。最后，我们为这八个类别中的每一个获得了100个提示：动物、建筑、食物、人类、生活方式、植物、风景和交通工具。

# 3.3. 人类偏好标注

我们对大量生成的视频进行人类偏好标注。主要目标是验证 VBench 评估在 16 个评估维度上与人类感知的对齐程度，验证结果将在第 4.2 节中详细说明。我们还展示了我们的人类偏好注释在未来微调生成和评估模型的任务中可以帮助增强与人类感知的对齐。

数据准备。给定文本提示 $p _ { i }$ ，以及四个待评估的视频生成模型 $\{ A , B , C , D \}$ ，我们使用每个模型生成一个视频，形成一个视频“组” $\begin{array} { r c l } { G _ { i , j } } & { = } & { \{ V _ { i , A , j } , V _ { i , B , j } , V _ { i , C , j } , V _ { i , D , j } \} } \end{array}$ 对于每个提示 $p _ { i }$ ，我们抽样五组此类视频 $\{ G _ { i , 0 } , G _ { i , 1 } , G _ { i , 2 } , G _ { i , 3 } , G _ { i , 4 } \}$ 。对于每组，我们将视频配对，形成六个成对组合：$( V _ { A } , V _ { B } ) , ( V _ { A } , V _ { C } ) , ( V _ { A } , V _ { D } ) , ( V _ { B } , V _ { C } ) , ( V _ { B } , V _ { D } ) , ( V _ { C } , V _ { D } )$ ，并请人类注释者指出他们偏好的视频。 在 VBench 评估框架内，一个由 $N$ 个提示组成的提示集会产生 $N \times 5 \times 6$ 个成对视频比较。每对中的视频顺序是随机的，以确保标注的无偏性。人类标注规则。具体而言，人类注释者被要求仅考虑特定的评估维度，并选择偏好的视频。例如，在图 4 中，对于外观风格维度，问题是“这个视频的风格是梵高风格吗？”，人类注释者被指示仅关注生成视频的风格是否属于梵高风格，而不应考虑生成视频的其他质量方面，例如潜在的时间闪烁程度等问题。在此图示的示例中，视频 A 更像梵高风格相比于视频 B，因此注释者被期望选择“A更好”。对于每个维度，我们仔细准备了说明，并对人类注释者进行培训，使其理解维度的定义，并通过预标注试验和两轮后标注检查执行多个质量保证协议（详细信息见补充文件）。

<table><tr><td>Models</td><td>Subject Consistency</td><td>Background Consistency</td><td>Temporal Flickering</td><td>Motion Smoothness</td><td>Dynamic Deree</td><td>Aesthetic Quality</td><td>Imaging Quality</td><td>Object Class</td></tr><tr><td>LaVie [104]</td><td>91.41%</td><td>97.47%</td><td>98.30%</td><td>96.38%</td><td>49.72%</td><td>54.94%</td><td>61.90%</td><td>91.82%</td></tr><tr><td>ModelScope [72, 98]</td><td>89.87%</td><td>95.29%</td><td>98.28%</td><td>95.79%</td><td>66.39%</td><td>52.06%</td><td>58.57%</td><td>82.25%</td></tr><tr><td>VideoCrafter [35]</td><td>86.24%</td><td>92.88%</td><td>97.60%</td><td>91.79%</td><td>89.72%</td><td>44.41%</td><td>57.22%</td><td>87.34%</td></tr><tr><td>CogVideo [41]</td><td>92.19%</td><td>95.42%</td><td>97.64%</td><td>96.47%</td><td>42.22%</td><td>38.18%</td><td>41.03%</td><td>73.40%</td></tr><tr><td>Empirical Min Empirical Max</td><td>14.62%</td><td>26.15%</td><td>62.93%</td><td>70.60%</td><td>0.00%</td><td>0.00%</td><td>0.00%</td><td>0.00%</td></tr><tr><td>Models</td><td>100.00% Multiple</td><td>100.00% Human</td><td>100.00%</td><td>99.75% Spatial</td><td>100.00%</td><td>100.00% Appearance</td><td>100.00% Temporal</td><td>100.00% Overall</td></tr><tr><td></td><td>Objects</td><td>Action</td><td>Color</td><td>Relationship</td><td>Scene</td><td>Style</td><td>Style</td><td>Consistency</td></tr><tr><td>LaVie [104]</td><td>33.32%</td><td>96.80%</td><td>86.39%</td><td>34.09%</td><td>52.69%</td><td>23.56%</td><td>25.93%</td><td>26.41%</td></tr><tr><td>ModelScope [72, 98]</td><td>38.98%</td><td>92.40%</td><td>81.72%</td><td>33.68%</td><td>39.26%</td><td>23.39%</td><td>25.37%</td><td>25.67%</td></tr><tr><td>VideoCrafter [35]</td><td>25.93%</td><td>93.00%</td><td>78.84%</td><td>36.74%</td><td>43.36%</td><td>21.57%</td><td>25.42%</td><td>25.21%</td></tr><tr><td>CogVideo [41]</td><td>18.11%</td><td>78.20%</td><td>79.57%</td><td>18.24%</td><td>28.24%</td><td>22.01%</td><td>7.80%</td><td>7.70%</td></tr><tr><td>Empirical Min</td><td>0.00%</td><td>0.00%</td><td>0.00%</td><td>0.00%</td><td>0.00%</td><td>0.09%</td><td>0.00%</td><td>0.00%</td></tr><tr><td>Empirical Max</td><td>100.00%</td><td>100.00%</td><td>100.00%</td><td>100.00%</td><td>82.22%</td><td>28.55%</td><td>36.40%</td><td>36.40%</td></tr></table>

VLM 调优的注释。我们将来自不同维度的 VBench 评估得分映射到 0-10 的范围，并结合人类偏好注释形成指令数据，这些数据随后用于对预训练的 VideoChat 模型进行微调，以展示改进的评估能力。有关实施细节和调优结果，请参阅补充文件。

# 4. 实验

我们采用视频生成模型 LaVie [104]、ModelScope [72, 98]、VideoCrafter [35] 和 CogVideo [41] 进行 VBench 评估，未来会随着开源的推出添加更多模型。模型和采样程序的详细信息请参见补充文件。

# 4.1. 每维度评估

对于每个维度，我们使用第3.1节中描述的评估方法套件计算VBench分数，并通过图2和表1展示结果。此外，我们还设计了三个参考基线，即经验最大值、经验最小值和WebVid-Avg。前两个基线近似视频可能达到的最大/最小分数，而WebVid-Avg反映了WebVid-10M [5] 数据集在每个VBench维度上的质量。经验最大值。对于大多数维度，为了近似可达到的最大值，我们首先根据我们的提示套件检索WebVid-10M [5] 视频。我们使用CLIP [83] 提取WebVid-10M的标题和我们的提示的文本特征。对于每个提示，我们根据文本特征与给定提示的相似度检索前5个WebVid-10M视频。考虑到生成的视频通常长度为2秒，我们随机从每个检索到的视频中选择一个2秒的片段，并以每秒8帧（FPS）的速度抽取帧。对于每个维度，我们使用根据其提示套件检索到的视频，并报告得分最高的视频结果作为经验最大值。经验最小值。为了近似可达到的最小值，我们使用随机生成的2秒高斯噪声片段来计算“视频条件一致性”维度的结果。对于大多数“视频质量”维度，我们选择真实视频中的帧，并为每个维度设计帧拼接，以近似每个VBench维度可以达到的最低分数。WebVid-Avg。与经验最大值类似，我们计算检索到的WebVid-10M [5] 视频在每个维度上的平均值。这个基线可以反映常用视频生成训练数据集WebVid-10M的每个维度的平均质量，并为模型性能提供参考。与WebVid-Avg和经验最大值的比较在图6(b)中进行了可视化。

# 4.2. 验证 VBench 的人类对齐性

为了验证我们的评估方法能够真实反映人类感知，我们对每个维度进行了大规模的人类标注，如第3.3节所述。我们在图5中展示了VBench评估结果与人类偏好标注之间的相关性。胜率。在给定人类标签的情况下，我们计算每个模型的胜率。在成对比较中，如果某个模型的视频被选为更好，则该模型得分为1，另一个模型得分为0。如果平局，则两个模型的得分均为0.5。对于每个模型，胜率计算为总得分除以参与的成对比较总数。

![](images/5.jpg)  
F cc linearlyft astraight line to visualize the corelation and calculate the Spearman' correlation coeficnt $( \rho )$ for each dimension.

![](images/6.jpg)  
Figure 6. More Comparisons of Video Generation Models with Other Models and Baselines. We use VBench to evaluate other models and baselines for further comparative analysis of T2V models. (a) Comparison with text-to-image (T2I) generation models. (b) Comparison with WebVid-Avg and Empirical Max baselines. See the Supplementary File for comprehensive numerical results and details on normalization methods.

按维度评估。对于每个评估维度，我们分别基于 (1) VBench 评估结果和 (2) 人工标注结果计算模型胜率，并计算它们之间的相关性，如图 5 所示。我们观察到，VBench 的按维度评估结果与人工偏好标注高度相关。

# 4.3. 按类别评估

我们在八个不同的内容类别中评估T2V模型，通过根据第3.2节描述的每个类别的提示套件生成视频，然后计算它们在不同评估维度上的表现。图7可视化了每个模型在八个内容类别中的评估结果。

# 4.4. 视频生成与图像生成

我们对文本到视频（T2V）模型和文本到图像（T2I）模型的逐帧生成能力进行了比较分析，主要有两个目标：首先，评估T2V模型在多大程度上成功继承了T2I模型的逐帧生成能力；其次，研究现有T2I和T2V模型之间的逐帧生成能力差距。作为对这一问题的初步探索，我们将视频生成模型与三种图像生成模型进行了比较，即Stable Diffusion (SD) 1.4、SD2.1和SDXL。我们选择了10个能够涵盖逐帧生成能力的VBench维度，并根据第3.2节中描述的“每个评估维度的提示套件”从所有图像和视频生成模型中抽样帧。图6（a）可视化了T2V与T2I模型的评估结果。

# 5. 见解与讨论

在本节中，我们讨论了从全面评估实验中得出的观察结果和见解。 能力维度之间的权衡。我们注意到视频生成模型在1) 时间一致性（主体一致性、背景一致性、时间闪烁、运动平滑性）和2) 动态程度之间存在权衡。强时间一致性的模型通常具有较低的动态程度，因为这两个方面在某种程度上是互补的（见图2和表1）。例如，LaVie在背景一致性和时间闪烁方面表现优异，但动态程度较低，可能是因为生成相对静态的场景可以通过“作弊”来获得较高的时间一致性分数。相反，VideoCrafter的动态程度较高，但在所有时间一致性维度上表现不佳。这一趋势突显了当前模型在实现大型动作的动态内容时，保持时间一致性的挑战。未来的研究应集中于同时增强这两个方面，因为单独改善一个可能意味着牺牲另一个。 揭示T2V模型在特定内容类别中的潜在能力。我们的分析显示，某些模型在不同内容类型中的能力差异显著。例如，在美学质量上，CogVideo在食物类别得分较高（见图7最右侧图表），而在动物和车辆等其他类别则表现不佳。不同提示的平均结果可能表明整体“美学质量”较低（如图2所示），但CogVideo在至少食物类别中显示出相对较强的美学表现。这表明，通过量身定制的训练数据和策略，CogVideo有潜力在美学方面通过提升其他内容类型的能力来与其他模型相匹配。因此，我们建议在评估视频生成模型时，不仅要基于能力维度，还应考虑特定内容类别，以揭示其隐藏的潜力。

![](images/7.jpg)  
F

• 时间复杂类别的瓶颈影响空间和时间表现。对于空间复杂类别（例如，动物、生活方式、人类、车辆），模型的表现普遍较差，主要体现在美学质量上（如图7所示）。这可能是由于在复杂元素中合成和谐的色彩方案、结构清晰的构造和吸引人的布局的挑战。另一方面，对于涉及复杂和强烈运动的类别，如人类和车辆（见补充文件中的动态程度），所有维度的表现都相对较差。这表明运动复杂性和动态强度显著阻碍合成，影响空间和时间维度，可能是因为差的时间建模导致了失真和模糊的图像。这突显了在视频生成模型中需改善对动态运动的处理。 • 处理复杂类别（如人类）中数据数量的挑战。WebVid-10M数据集[5]将$26 \%$的内容分配给人类类别，这是八个类别中占比最大的（见补充文件中的统计数据）。然而，人类类别在八个类别中表现出最差的结果之一（见图7）。这表明，仅仅增加数据量可能无法显著提升像人类这样的复杂类别的性能。一种潜在的方法可能涉及整合与人类相关的先验或控制，如骨骼，以更好地捕捉人类外观和运动的关节特性。 • 在大规模数据集中优先考虑数据质量而非数量。对于美学质量，图7显示食品类别几乎总是倾向于在所有类别中获得最高分。这一点在WebVid-10M数据集中得到了证实，其中食品在美学质量上根据VBench评估排名第一（具体详情请参见补充文件），尽管只占总数据的$11 \%$。这一观察表明，在百万规模的数据中，数据质量可能比数量更为重要。此外，VBench的评估维度可能对在特定质量维度中清理数据集具有潜在的帮助。

组合性：T2I 与 T2V。如图 6 (a) 所示，T2V 模型在多个对象和空间关系方面的表现明显逊色于 T2I 模型（尤其是 SDXL [81]），这突显了增强组合性的必要性（即在同一帧中正确组合多个对象）。我们相信可能的解决方案包括：1）整理包含多个对象的训练数据，并附上明确描绘这种组合性的相应说明；2）在视频合成过程中添加中间空间控制模块或模态。此外，文本编码器的差异也可能导致性能差距。由于 T2I 模型使用更大（SD2.1 的 OpenCLIP ViT-H [84]）或更复杂（SDXL 的 CLIP ViT-L 和 OpenCLIP ViT-G [81]）的文本编码器，而 T2V 模型仅使用 CLIP ViT-L（例如 LaVie），因此更具代表性的文本嵌入可能具有更准确的对象组合理解能力。

# 6. 结论

随着对视频生成的关注日益增加，对这些模型进行全面评估对于了解当前进展和指导未来研究至关重要。在这项工作中，我们迈出了第一步，提出了 VBench，这是一个用于评估视频生成模型的综合基准套件。凭借其多维、人性化和富有洞察力的特性，VBench 在评估未来的视频生成模型和激励进一步的进展方面可能发挥重要作用。我们相信，VBench 是对视频生成和评估社区的重要贡献。局限性与未来工作。我们计划在更多模型可用时扩展 VBench，并将评估范围扩展到其他视频生成任务，例如图像到视频。潜在的社会负面影响。我们也认识到在 VBench 的未来版本中考虑伦理方面的重要性。虽然 VBench 当前并未评估安全性和公平性维度，但我们建议用户在使用开源视频生成模型时保持谨慎。致谢。我们要感谢周尚琛、王剑怡和冯瑞城提供的有益建议。

# References

[1] Gen-2. Accessed September 25, 2023 [Online] ht tps : //research.runwayml.com/gen2,2023.27   
[2] Morph studio. Accessed September 25, 2023 [Online] https://www.morphstudio.com/,2023.   
[3] Pika labs. Accessed September 25, 2023 [Online] ht tps : //www.pika.art/,2023.   
[4] Zeroscope-xl. Accessed September 25, 2023 [Online] https:/ /huggingface. co/cerspense/ zeroscope_v2_XL,2023.27   
[5] Max Bain, Arsha Nagrani, Gül Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval. In ICCV, 2021. 6, 8, 25, 26   
[6] Eslam Mohamed Bakr, Pengzhan Sun, Xiaoqian Shen, Faizan Farooq Khan, Li Erran Li, and Mohamed Elhoseiny. Hrs-bench: Holistic, reliable and scalable benchmark for text-to-image models, 2023. 3   
[7] Samyadeep Basu, Mehrdad Saberi, Shweta Bhardwaj, Atoosa Malemir Chegini, Daniela Massiceti, Maziar Sanjabi, Shell Xu Hu, and Soheil Feizi. Editval: Benchmarking diffusion based text-guided image editing methods. arXiv preprint arXiv:2310.02426, 2023. 3   
[8] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In CVPR, 2023. 2, 3, 27   
[9] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity natural image synthesis. arXiv preprint arXiv:1809.11096, 2018. 2   
10] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In ICCV, 2021. 4, 14   
11] Duygu Ceylan, Chun-Hao P Huang, and Niloy J Mitra. Pix2video: Video editing using image diffusion. In ICCV, 2023. 27   
[12] Wenhao Chai, Xun Guo, Gaoang Wang, and Yan Lu. Stablevideo: Text-driven consistency-aware diffusion video editing. arXiv preprint arXiv:2308.09592, 2023. 3, 27   
[13] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter1: Open diffusion models for high-quality video generation. arXiv preprint arXiv:2310.19512, 2023. 3   
[14] Tsai-Shien Chen, Chieh Hubert Lin, Hung-Yu Tseng, Tsung-Yi Lin, and Ming-Hsuan Yang. Motion-conditioned diffusion model for controllable video synthesis. arXiv preprint arXiv:2304.14404, 2023. 3, 27   
[15] Weifeng Chen, Jie Wu, Pan Xie, Hefeng Wu, Jiashi Li, Xin Xia, Xuefeng Xiao, and Liang Lin. Control-a-video: Controllable text-to-video generation with diffusion models, 2023. 3, 27   
[16] Xinyuan Chen, Yaohui Wang, Lingjun Zhang, Shaobin Zhuang, Xin Ma, Jiashuo Yu, Yali Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. Seine: Short-to-long video diffusion model for generative transition and prediction. arXiv preprint arXiv:2310.20700, 2023. 3, 27   
[17] Ernie Chu, Shuo-Yen Lin, and Jun-Cheng Chen. Video controlnet: Towards temporally consistent synthetic-to-real video translation using conditional image diffusion models, 2023.27   
[18] Paul Couairon, Clément Rambour, Jean-Emmanuel Haugeard, and Nicolas Thome. Videdit: Zero-shot and spatially aware text-driven video editing. arXiv preprint arXiv:2306.08707, 2023. 27   
[19] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat GANs on image synthesis. In NeurIPS, 2021. 3   
[20] Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang. Cogview2: Faster and better text-to-image generation via hierarchical transformers. In NeurIPS, 2022. 2, 25   
[21] Patrick Esser, Robin Rombach, and Björn Ommer. A note on data biases in generative models. In NeurIPS Workshop, 2020.27   
[22] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In CVPR, 2021. 2   
[23] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis. Structure and content-guided video synthesis with diffusion models, 2023. 3, 27   
[24] Yuming Fang, Hanwei Zhu, Yan Zeng, Kede Ma, and Zhou Wang. Perceptual quality assessment of smartphone photography. In CVPR, 2020. 4, 17   
[25] Jianglin Fu, Shikai Li, Yuming Jiang, Kwan-Yee Lin, Chen Qian, Chen Change Loy, Wayne Wu, and Ziwei Liu. Stylegan-human: A data-centric odyssey of human generation. In ECCV, 2022. 2   
[26] Jianglin Fu, Shikai Li, Yuming Jiang, Kwan-Yee Lin, Wayne Wu, and Ziwei Liu. Unitedhuman: Harnessing multi-source data for high-resolution human generation. In ICCV 2023 2   
[27] Tsu-Jui Fu, Licheng Yu, Ning Zhang, Cheng-Yang Fu, Jong-Chyi Su, William Yang Wang, and Sean Bell. Tell me what happened: Unifying text-guided video completion via multimodal masked video generation, 2023. 27   
[28] Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, Andrew Tao, Bryan Catanzaro, David Jacobs, Jia-Bin Huang, Ming-Yu Liu, and Yogesh Balaji. Preserve your own correlation: A noise prior for video diffusion models. In ICCV, 2023. 3   
[29] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel. Tokenflow: Consistent diffusion features for consistent video editing. arXiv preprint arxiv:2307.10373, 2023. 27   
[30] Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C Courville, and Yoshua Bengio. Generative adversarial nets. In NeurIPS, 2014. 2   
[31] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vector quantized diffusion model for text-to-image synthesis. In CVPR, 2022. 3   
[32] Xianfan Gu, Chuan Wen, Jiaming Song, and Yang Gao. Seer: Language instructed video prediction with latent diffusion models. arXiv preprint arXiv:2303.14897, 2023. 27   
[33] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. 3, 27   
[34] William Harvey, Saeid Naderiparizi, Vaden Masrani, Christian Weilbach, and Frank Wood. Flexible diffusion modeling of long videos. arXiv preprint arXiv:2205.11495, 2022.   
[35] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for highfidelity video generation with arbitrary lengths. arXiv preprint arXiv:2211.13221, 2022. 2, 3, 5, 6, 15, 24, 25, 27   
[36] Yingqing He, Menghan Xia, Haoxin Chen, Xiaodong Cun, Yuan Gong, Jinbo Xing, Yong Zhang, Xintao Wang, Chao Weng, Ying Shan, et al. Animate-a-story: Storytelling with retrieval-augmented video generation. arXiv preprint arXiv:2307.06940, 2023. 27   
[37] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a local nash equilibrium. In NeurIPS, 2017. 2, 3   
[38] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. 2, 3   
[39] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. 3   
[40] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video diffusion models. arXiv preprint arXiv:2204.03458, 2022. 3   
[41] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. CogVideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. 2, 5, 6, 15, 24, 25, 27   
[42] Zhihao Hu and Dong Xu. Videocontrolnet: A motion-guided video-to-video translation framework by using diffusion model with controlnet. arXiv preprint arXiv:2307.14073, 2023. 27   
[43] Jiahui Huang, Leonid Sigal, Kwang Moo Yi, Oliver Wang, and Joon-Young Lee. Inve: Interactive neural video editing, 2023.27   
[44] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: A comprehensive benchmark for open-world compositional text-to-image generation. arXiv preprint arXiv: 2307.06350, 2023. 3, 4, 18   
[45] Xinyu Huang, Youcai Zhang, Jinyu Ma, Weiwei Tian, Rui Feng, Yuejie Zhang, Yaqian Li, Yandong Guo, and Lei Zhang. Tag2text: Guiding vision-language model via image tagging. arXiv preprint arXiv:2303.05657, 2023. 4, 18   
[46] Ziqi Huang, Kelvin C.K. Chan, Yuming Jiang, and Ziwei Liu. Collaborative diffusion for multi-modal face generation and editing. In CVPR, 2023. 3   
[47] Ziqi Huang, Tianxing Wu, Yuming Jiang, Kelvin C.K. Chan, and Ziwei Liu. ReVersion: Diffusion-based relation inversion from images. arXiv preprint arXiv:2303.13495, 2023. 3   
[48] Yuming Jiang, Ziqi Huang, Xingang Pan, Chen Change Loy, and Ziwei Liu. Talk-to-Edit: Fine-grained facial editing via dialog. In ICCV, 2021. 2   
[49] Yuming Jiang, Shuai Yang, Haonan Qju, Wayne Wu, Chen Change Loy, and Ziwei Liu. Text2human: Textdriven controllable human image generation. ACM TOG, 2022. 2   
[50] Yuming Jiang, Shuai Yang, Tong Liang Koh, Wayne Wu, Chen Change Loy, and Ziwei Liu. Text2Performer: Textdriven human video generation. In ICCV, 2023. 3   
[51] Johanna Karras, Aleksander Holynski, Ting-Chun Wang, and Ira Kemelmacher-Shlizerman. Dreampose: Fashion image-to-video synthesis via stable diffusion. arXiv preprint arXiv:2304.06025, 2023. 3, 27   
[52] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for improved quality, stability, and variation. In ICLR, 2018. 2   
[53] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In CVPR, 2019.   
[54] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of StyleGAN. In CVPR, 2020.   
[55] Tero Karras, Miika Aittala, Samuli Laine, Erik Härkönen, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-free generative adversarial networks. In NeurIPS, 2021. 2   
[56] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv preprint 7rVi.1705 06050, 2017 5,18, 20   
[57] Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and FeaMUIQ:ulcal uali. CoRR, abs/2108.05997, 2021. 4, 16   
[58] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Textto-image diffusion models are zero-shot video generators. arXiv preprint arXiv:2303.13439, 2023. 3, 27   
[59] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 2   
[60] LAION-AI. aesthetic-predictor. https : / / github. com/LAION-AI/aesthetic-predictor, 2022. 4, 16   
[61] Tony Lee, Michihiro Yasunaga, Chenlin Meng, Yifan Mai, Joon Sung Park, Agrim Gupta, Yunzhi Zhang, Deepak Narayanan, Hannah Benita Teufel, Marco Bellagente, et al. Holistic evaluation of text-to-image models. arXiv preprint arXiv:2311.04287, 2023. 3   
[62] Yao-Chih Lee, Ji-Ze Genevieve Jang Jang, Yi-Ting Chen, Elizabeth Qiu, and Jia-Bin Huang. Shape-aware textdriven layered video editing demo. arXiv preprint arXiv:2301.13173, 2023. 27   
[63] Dingquan Li, Tingting Jiang, and Ming Jiang. Quality assessment of in-the-wild videos. In ACM MM, 2019. 2   
[64] Kunchang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023. 6, 22   
[65] Kunchang Li, Yali Wang, Yizhuo Li, Yi Wang, Yinan He, Limin Wang, and Yu Qiao. Unmasked teacher: Towards training-effcient video foundation models. arXiv preprint arXiv:2303.16058, 2023. 4, 18   
[66] Zhen Li, Zuo-Liang Zhu, Ling-Hao Han, Qibin Hou, ChunLe Guo, and Ming-Ming Cheng. Amt: All-pairs multi-field transforms for efficient frame interpolation. In CVPR, 2023. 4,16   
[67] Jun Hao Liew, Hanshu Yan, Jianfeng Zhang, Zhongcong Xu, and Jiashi Feng. Magicedit: High-fidelity and temporally coherent video editing. arXiv preprint arXiv:2308.14749, 2023. 3, 27   
[68] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014. 19, 20   
[69] Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya Jia. Video-p2p: Video editing with cross-attention control, 2023. 27   
[70] Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang, Yong Zhang, Haoxin Chen, Yang Liu, Tieyong Zeng, Raymond Chan, and Ying Shan. Evalcrafter: Benchmarking and evaluating large video generation models. arXiv preprint arXiv:2310.11440, 2023. 3   
[71] Yuanxin Liu, Lei Li, Shuhuai Ren, Rundong Gao, Shicheng Li, Sishuo Chen, Xu Sun, and Lu Hou. Fetv: A benchmark for fine-grained evaluation of open-domain text-tovideo generation. In NeurIPS. 2023. 3 [72] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, and Tieniu Tan. VideoFusion: Decomposed diffusion models for high-quality video generation. In CVPR, 2023. 2, 3, 6,   
15, 24, 25, 27 [73] Yue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Ying Shan, Xiu Li, and Qifeng Chen. Follow your pose: Poseguided text-to-video generation using pose-free videos. arXiv preprint arXiv:2304.01186, 2023. 3, 27 [74] Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784, 2014. 2 [75] Guillaume Le Moing, Jean Ponce, and Cordelia Schmid. WALDO: Future video synthesis using object layer decomposition and parametric flow prediction. In ICCV, 2023.   
27 [76] Eyal Molad, Eliahu Horwitz, Dani Valevski, Alex Rav Acha, Yossi Matias, Yael Pritch, Yaniv Leviathan, and Yedid Hoshen. Dreamix: Video diffusion models are general video editors. arXiv preprint arXiv:2302.01329, 2023.   
27 [77] Haomiao Ni, Changhao Shi, Kai Li, Sharon X Huang, and Martin Renqiang Min. Conditional image-to-video generation with latent flow diffusion models. In CVPR, 2023.   
27 [78] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. GLIDE: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. 3 [79] Mayu Otani, Riku Togashi, Yu Sawai, Ryosuke Ishigami, Yuta Nakashima, Esa Rahtu, Janne Heikkilä, and Shin'ichi Satoh. Toward verifiable and reproducible human evaluation for text-to-image generation. In CVPR, 2023. 2 [80] Hao Ouyang, Qiuyu Wang, Yuxi Xiao, Qingyan Bai, Juntao Zhang, Kecheng Zheng, Xiaowei Zhou, Qifeng Chen, and Yujun Shen. Codef: Content deformation fields for temporally consistent video processing. arXiv preprint arXiv:2308.07926, 2023. 3, 27 [81] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. 3, 7, 8, 16, 27 [82] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, and Qifeng Chen. Fatezero: Fusing attentions for zero-shot text-based video editing. arXiv preprint arXiv:2303.09535, 2023. 3, 27 [83] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 2, 3, 4, 6, 14, 19 [84] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 3,   
7, 8, 25, 27 [85] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image airrusion moaeis ror surject-ariven generation. In CVPR, 2023. 14   
[86] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487, 2022. 3   
[87] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen, and Xi Chen. Improved techniques for training gans. In NeurIPS, 2016. 2, 3   
[88] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. 2, 3, 27   
[89] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In ICML, 2015. 2, 3   
[90] Xue Song, Jingjing Chen, Bin Zhu, and Yu-Gang Jiang. Text-driven video prediction. arXiv preprint arXiv:2210.02872, 2022. 27   
[91] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Scorebased generative modeling through stochastic differential equations. In ICLR, 2021. 2, 3   
[92] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012. 3   
[93] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In ECCV, 2020. 4, 15, 16   
[94] Zhengzhong Tu, Yilin Wang, Neil Birkbeck, Balu Adsumilli, and Alan C. Bovik. Ugc-vqa: Benchmarking blind video quality assessment for user generated content. IEEE TIP, 30:44494464, 2021. 2   
[95] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: A new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. 2, 3   
[96] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphaël Marinier, Marcin Michalski, and Sylvain Gelly. FVD: A new metric for video generation. In ICLRW, 2019. 2   
[97] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. In NeurIPS, 2017. 2   
[98] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023. 2, 3, 5, 6, 15, 24, 25, 27   
[99] Su Wang, Chitwan Saharia, Ceslee Montgomery, Jordi Pont-Tuset, Shai Noy, Stefano Pellegrini, Yasumasa Onoe, Sarah Laszlo, David J Fleet, Radu Soricut, et al. Imagen editor and EditBench: Advancing and evaluating text-guided image inpainting. arXiv preprint arXiv:2212.06909, 2022. 3   
[1u] lall wlg, Le Ll, Nevil L, Cu-Cig Lill, Zhengyuan Yang, Hanwang Zhang, Zicheng Liu, and Lijuan Wang. Disco: Disentangled control for referring human dance generation in real world. arXiv preprint arXiv:2307.00040, 2023. 27   
[101] Wen Wang, kangyang Xie, Zide Liu, Hao Chen, Yue Cao, Xinlong Wang, and Chunhua Shen. Zero-shot video editing using off-the-shelf image diffusion models. arXiv preprint arXiv:2303.17599, 2023. 27   
[102] Xiaodong Wang, Chenfei Wu, Shengming Yin, Minheng Ni, Jia Wag Lie  Zhean ng, an g, Lijuan Wang, Zicheng Liu, Yuejian Fang, and Nan Duan. Learning 3d photography videos via self-supervised diffusion on single images, 2023. 27   
[103] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jingren Zhou. Videocomposer: Compositional video synthesis with motion controllability. arXiv preprint arXiv:2306.02018, 2023. 3, 27   
[104] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, et al. Lavie: High-quality video generation with cascaded latent diffusion models. arXiv preprint arXiv:2309.15103, 2023. 2, 3, 5, 6, 15, 24, 25, 27   
[105] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinyuan Chen, Yaohui Wang, Ping Luo, Ziwei Liu, Yali Wang, Limin Wang, and Yu Qiao. Internvid: A large-scale video-text dataset for multimodal understanding and generation. arXiv preprint arXiv:2307.06942, 2023. 4, 19   
[106] Yaohui Wang, Xin Ma, Xinyuan Chen, Antitza Dantcheva, Bo Dai, and Yu Qiao. Leo: Generative latent image animator for human video synthesis. arXiv preprint arXiv:2305.03989, 2023. 27   
[107] Chenei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejan Fang, Daxin Jiang, and Nan Duan. Nüwa: Visual synthesis pretraining for neural visual world creation. In ECCV, 2022. 27   
[108] Haoning Wu, Chaofeng Chen, Jingwen Hou, Liang Liao, Annan Wang, Wenxiu Sun, Qiong Yan, and Weisi Lin. Fastvqa: Efficient end-to-end video quality assessment with fragment sampling. In ECCV, 2022. 2   
[109] Haoning Wu, Chaofeng Chen, Liang Liao, Jingwen Hou, Wenxiu Sun, Qiong Yan, Jinwei Gu, and Weisi Lin. Neighbourhood representative sampling for efficient end-to-end video quality assessment. arXiv preprint arXiv:2210.05357, 2022.   
[110] Haoning Wu, Chaofeng Chen, Liang Liao, Jingwen Hou, Wenxiu Sun, Qiong Yan, and Weisi Lin. Discovqa: Temporal distortion-content transformers for video quality assessment. IEEE Transactions on Circuits and Systems for Video Technology, pages 11, 2023.   
[111] Haoning Wu, Liang Liao, Chaofeng Chen, Jingwen Hou Hou, Erli Zhang, Annan Wang, Wenxiu Sun Sun, Qiong Yan, and Weisi Lin. Exploring opinion-unaware video quality assessment with semantic affinity criterion. In ICME, 2023   
[112] Haoning Wu, Liang Liao, Annan Wang, Chaofeng Chen, Jingwen Hou Hou, Erli Zhang, Wenxiu Sun Sun, Qiong Yan, and Weisi Lin. Towards robust text-prompted semantic criterion for in-the-wild video quality assessment. arXiv preprint arXiv:2304.14672, 2023.   
[113] Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jingwen Hou Hou, Annan Wang, Wenxiu Sun Sun, Qiong Yan, and Weisi Lin. Exploring video quality assessment on user generated contents from aesthetic and technical perspectives. In ICCV, 2023.   
[114] Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jingwen Hou Hou, Annan Wang, Wenxiu Sun Sun, Qiong Yan, and Weisi Lin. Towards explainable video quality assessment: A database and a language-prompted approach. In ACM MM, 2023. 2   
[115] Jialian Wu, Jianfeng Wang, Zhengyuan Yang, Zhe Gan, Zicheng Liu, Junsong Yuan, and Lijuan Wang. Grit:A generative region-to-text transformer for object understanding. arXiv preprint arXiv:2212.00280, 2022. 4, 17   
[116] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. arXiv preprint arXiv:2212.11565, 2022. 2, 27   
[117] Jinbo Xing, Menghan Xia, Yuxin Liu, Yuechen Zhang, Yong Zhang, Yingqing He, Hanyuan Liu, Haoxin Chen, Xiaodong Cun, Xintao Wang, Ying Shan, and Tien-Tsin Wong. Make-your-video: Customized video generation using textual and structural guidance. arXiv preprint arXiv:2306.00943, 2023. 27   
[118] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Xintao Wang, Tien-Tsin Wong, and Ying Shan. Dynamicrafter: Animating open-domain images with video diffusion priors. arXiv preprint arXiv:2310.12190, 2023. 3   
[119] Zhen Xing, Qi Dai, Han Hu, Zuxuan Wu, and Yu-Gang Jiang. Simda: Simple diffusion adapter for efficient video generation. arXiv preprint arXiv:2308.09710, 2023. 27   
[120 Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt:A large video description dataset for bridging video and language. In CVPR, 2016. 3   
[121] Shuai Yang, Yifan Zhou, Ziwei Liu, and Chen Change Loy. Rerender a video: Zero-shot text-guided video-tovideo translation. arXiv preprint arXiv:2306.07954, 2023. 3, 27   
[122] Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang Li, Gong Ming, and Nan Duan. Dragnuwa: Fine-grained control in video generation by integrating text, image, and trajectory, 2023. 3, 27   
[123] Lijun Yu, Yong Cheng, Kihyuk Sohn, José Lezama, Han Zhang, Huiwen Chang, Alexander G. Hauptmann, MingHsuan Yang, Yuan Hao, Iran Essa, and Lu Jiang. Magvit: Masked generative video transformer, 2023. 27   
[124] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and Mike Zheng Shou. Show-1: Marrying pixel and latent diffusion models for text-to-video generation. arXiv preprint arXiv:2309.15818, 2023. 2, 3, 27   
[125] Jianfeng Zhang, Hanshu Yan, Zhongcong Xu, Jiashi Feng, and Jun Hao Liew. Magicavatar: Multi-modal avatar generation and animation. In arXiv, 2023. 3, 27   
[126] Yabo Zhang, Yuxiang Wei, Dongsheng Jiang, Xiaopeng Zhang, Wangmeng Zuo, and Qi Tian. Controlvideo: Training-free controllable text-to-video generation. arXiv preprint arXiv:2305.13077, 2023. 3, 27   
[127] Zicheng Zhang, Bonan Li, Xuecheng Nie, Congying Han, Tiande Guo, and Luoqi Liu. Towards consistent video editing with text-to-image diffusion models, 2023. 27   
[128] Min Zhao, Rongzhen Wang, Fan Bao, Chongxuan Li, and Jun Zhu. Controlvideo: Adding conditional control for one shot text-to-video editing. arXiv preprint arXiv:2305.17098, 2023.   
[129] Yuyang Zhao, Enze Xie, Lanqing Hong, Zhenguo Li, and Gim Hee Lee. Make-a-protagonist: Generic video editing with an ensemble of experts. arXiv preprint arXiv:2305.08850, 2023. 27   
[130] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-ajudge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685, 2023. 5, 20   
[131] Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba, and Aude Oliva. Learning deep features for scene recognition using places database. In NeurIPS, 2014. 19   
[132] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video generation with latent diffusion models. arXiv preprint arXiv:2211.11018, 2022. 3   
[133] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video generation with latent diffusion models, 2023. 3

# VBench: Comprehensive Benchmark Suite for Video Generative Models

Supplementary Material

In this supplementary file, we provide more details on Evaluation Dimension Suite and Evaluation Method Suite in Section G, and elaborate on Prompt Suite details in Section H. We then provide further explanations on Human Preference Annotations in Section I, and more implementation details on our experiments and visualizations in Section J. The potential societal impacts of our work are discussed in Section K. We also discuss our limitations in Section L. Finally, in Section M, we provide additional experimental results used to support the visualizations and insights in the main paper.

A demo video is also provided along with this supplementary file to illustrate VBench and show video examples of each dimension.

# G. More Details on Evaluation Dimension and Method Suite

# G.1. Video Quality

![](images/8.jpg)  
Figure A8. Visualization of Subject Consistency. We demonstrate different degrees of subject consistency, as indicated by our Subject Consistency score (the larger the better) (a) The cow has a relatively consistent look throughout across different frames. (b) The cow shows inconsistency in its appearance over time. The red boxes indicate areas of subject inconsistency.

Subject Consistency. When there is a subject (e.g., a cow, a person, a car, or a cat) in the video, it is important that the subject looks consistent throughout the video (i.e., whether it is still the same thing or the same person). For example, in Figure A8, the cow in the top row remains consistent across different frames, while the cow in the bottom row shows changes in appearance between frames. To evaluate subject consistency, we employ DINO [10] to extract features from each frame to represent the subject. Since DINO is not trained to disregard the differences within subjects of the same class [85], its feature extraction is particularly sensitive to the identity variations of the subject within the video, thereby making it a suitable tool for evaluating subject consistency. Specifically, for each video, the subject consistency score is calculated as:

$$
S _ { s u b j e c t } = \frac { 1 } { T - 1 } \sum _ { t = 2 } ^ { T } \frac { 1 } { 2 } ( \langle d _ { 1 } \cdot d _ { t } \rangle + \langle d _ { t - 1 } \cdot d _ { t } \rangle ) ,
$$

where $d _ { i }$ is the DINO image feature of the $i ^ { t h }$ frame, normalized to unit length, and $\langle \cdot \rangle$ is the dot product operation for calculating cosine similarity. For each frame, we calculate the cosine similarity with the first frame and its preceding frame, take the average, and then compute the mean over all the non-starting video frames. We average the score $S _ { s u b j e c t }$ for all the videos generated by one model as the final score of the model.

![](images/9.jpg)  
Figure A9. Visualization of Background Consistency. We showcase varying levels of background consistency, as indicated by our Background Consistency metrics (larger values denote better consistency) (a) The background scene maintains a high degree of consistency (i.e., still the same scene) across different frames. (b) The background exhibits noticeable distortion and abrupt changes over time.

Background Consistency. Beyond the focus on the foreground subject, maintaining a consistent background scene across different frames is equally important. For example, in Figure A9, in the top row, the scene maintains a consistent appearance as the camera moves, while in the bottom row, the entire scene undergoes significant changes within a few frames. For each video frame, we employ the CLIP [83] image encoder to extract its feature vector. We then compute the background consistency metric, which is similar to the method used for Subject Consistency:

$$
S _ { b a c k g r o u n d } = \frac { 1 } { T - 1 } \sum _ { t = 2 } ^ { T } \frac { 1 } { { \mathcal { Q } } } ( \langle c _ { 1 } \cdot c _ { t } \rangle + \langle c _ { t - 1 } \cdot c _ { t } \rangle ) ,
$$

where $c _ { i }$ represents the CLIP image feature of the $i ^ { t h }$ frame, normalized to unit length.

![](images/10.jpg)  
Figure A10. Visualization of Temporal Flickering. We demonstrate different degrees of temporal flickering, with a mild occurrence in (a), and a severe occurrence in (b), both reflected by our flicker score metrics (the larger the better). To visualize temporal flickering, given a generated video (top row), we extract a small segment of pixels (marked as the red segment) from each frame at the same location and stack them in frame order (bottom row). (a) Pixel values do not vary abruptly, and the video suffers less from flickering. (b) Pixel values vary abruptly and frequently across different frames, showing strong temporal flickering. Our evaluation metrics also give a lower score.

Table A2. Dynamic Degree on Three Benchmarks. We report the Dynamic Degree metrics on three Temporal Flickering benchmarks. We use videos from the Subject Consistency dimension as the "Dynamic Benchmark", videos from the Background Consistency dimension as the "Semi-Dynamic Benchmark", and videos from the temporal flickering dimension as the "Static Benchmark".   

<table><tr><td>Models</td><td>Static Benchmark</td><td>Semi-Dynamic Benchmark</td><td>Dynamic Benchmark</td></tr><tr><td>LaVie [104]</td><td>0.00%</td><td>6.51%</td><td>49.72%</td></tr><tr><td>ModelScope [72, 98]</td><td>0.00%</td><td>33.72%</td><td>66.39%</td></tr><tr><td>VideoCrafter [35]</td><td>0.00%</td><td>51.63%</td><td>89.72%</td></tr><tr><td>CogVideo [41]</td><td>0.00%</td><td>14.19%</td><td>42.22%</td></tr></table>

Temporal Flickering. For real videos, temporal flickering is usually a result of frequent lighting variation, or shaky camera motions during the video capture process. However, for generated videos, temporal flickering is an intrinsic property of the video generation model, usually caused by imperfect temporal consistency at local and high-frequency details. In generated videos, temporal inconsistency can be attributed to various types of issues, including temporal flickering, unnatural motions, subject inconsistency etc. To disentangle the evaluation of temporal flickering from other aspects, we use static video scenes (i.e., no apparent motions) as the test cases (We use carefully designed prompts to generate static scenes for video sampling. To further ensure that the evaluation is conducted on static videos without apparent motions, we employ an optical flow estimator [93] to filter out videos and only keep the static videos). We calculate the frame-by-frame temporal flickering degree

![](images/11.jpg)  
Figure A11. Temporal Flickering Human Preference across Different Dynamic Degrees. In each plot, a dot represents the human preference win ratio, where the horizontal and vertical axes correspond to two different benchmarks with different dynamic degrees. We linearly fit a straight line to visualize the correlation and calculate the correlation $( \rho )$ for each dimension. We observe that the human preferences in terms of temporal fickering on these three benchmarks have high mutual correlations of around $9 9 \%$ .

with the following formula:

$$
S _ { f l i c k e r } = \frac { 1 } { N } \sum _ { i = 1 } ^ { N } ( \frac { 1 } { T - 1 } \sum _ { t = 1 } ^ { T - 1 } M A E ( f _ { i } ^ { t } , f _ { i } ^ { t + 1 } ) ) ,
$$

where $\mathbf { N }$ is the number of videos generated by a model, $\mathrm { T }$ is the number of frames per video, $f _ { i } ^ { t }$ is the frame $t$ in video $i$ , and MAE is the Mean Absolute Error between two consecutive frames over all pixel locations. We then normalize the temporal flickering degree to [0, 1] as follows:

$$
S _ { f l i c k e r - n o r m } = \frac { 2 5 5 - S _ { f l i c k e r } } { 2 5 5 } ,
$$

where a higher score implies less flickering, and thus better video perceptual quality in terms of temporal flickering.

To verify that the strength of motions (i.e., large motion or small motion) in videos does not significantly impact the model's ranking in terms of temporal flickering, we conduct separate human evaluations for the level of temporal flickering on videos with different dynamic degrees, and show in Figure A11 that model ranking in terms of temporal flickering does not vary based on the dynamic degree of test videos. For videos of high dynamic degrees, we use videos from the Subject Consistency dimension's prompt suite, and term as the "Dynamic Benchmark". For videos that exhibit lower dynamic degrees but remain non-static, we use videos sampled from the Background Consistency dimension's prompt suite, and label them as the "Semi-Dynamic Benchmark". Additionally, the "Static Benchmark" refers to the videos sampled from the prompt suite for the Temporal Flickering dimension. We show the dynamic degree of videos in these three benchmarks in Table A2. In Figure A11, we show that the human win ratio in terms of temporal flickering on three benchmarks is almost perfectly correlated with each other, with a correlation of around $9 9 \%$ between any two benchmarks. Therefore, we believe the degree of motion is disentangled with the temporal flickering ranking in video generative models, and we use the "Static Benchmark" for easier and more focused evaluation on Temporal Flickering.

![](images/12.jpg)  
Figure A12. Visualization of Motion Smoothness. We investigate various levels of motion smoothness, ranging from being smooth as depicted in (a) to highly erratic as depicted in (b), as indicated by our motion score metrics (larger values denote better smoothness). The red boxes indicate areas of discontinuous motion.

Motion Smoothness. To evaluate whether the motion in the generated video is smooth and follows the physical law of the real world, we make use of the frame-by-frame motion prior to video frame interpolation models. Specifically, video frame interpolation models usually assume realworld motions within a very short time period (i.e., a few consecutive frames) to be linear or quadratic and synthesize the non-existing intermediate frames based on this assumption. Given a generated video consisting of frames $[ f _ { 0 } , f _ { 1 } , f _ { 2 } , f _ { 3 } , f _ { 4 } . . . , f _ { 2 n - 2 } , f _ { 2 n - 1 } , f _ { 2 n } ]$ , we manually drop the odd-number frames to obtain a lower-frame-rate video $[ f _ { 0 } , f _ { 2 } , f _ { 4 } . . . , f _ { 2 n - 2 } , f _ { 2 n } ]$ , and use video frame interpolation [66] to infer the dropped frames $[ \hat { f } _ { 1 } , \hat { f } _ { 3 } , . . . , \hat { f } _ { 2 n - 1 } ]$ We then compute the Mean Absolute Error (MAE) between the reconstructed frames and the original dropped frames. The calculated MAE is normalized in the same way as Equation 4, so that the final score falls into $[ 0 , 1 ]$ , with a larger number implying smoother motion.

Dynamic Degree. Based on our observations, some models tend to generate static videos even when the prompt includes descriptions of movement. This results in a noticeable advantage for these models in evaluations of other temporal consistency dimensions, leading to unfair comparisons. This dimension is designed to assess the extent to which models tend to generate non-static videos. We use RAFT [93] to estimate optical flow strengths between consecutive frames of a generated video. We then take the average of the largest $5 \%$ optical flows (considering the movement of small objects in the video) as the basis to determine whether the video is static. The final dynamic degree score is calculated by measuring the proportion of nonstatic videos generated by the model.

Aesthetic Quality. Aesthetic Quality takes photographic layout rules, the richness and harmonies of colors, the artistic quality of the subjects, etc into account. We adopt an image aesthetic quality predictor to evaluate the generated videos frame by frame. We use the LAION aesthetic predictor [60] to give a 0-10 rating for each frame, linearly normalize the score to 0-1, and calculate the average score of all synthetic frames as the final video aesthetic score.

![](images/13.jpg)  
Figure A13. Visualization of Dynamic Degree. We present generated examples of different degrees of motion. (a) In the video, there is obvious motion of the camera and the object, which is identified as dynamic. (b) The video remains almost unchanged from the start to the end and is identified as static.

![](images/14.jpg)  
: b  pal-a   
Figure A14. Visualization of Aesthetic Quality. We demonstrate video frames with varying degrees of aesthetic quality in (a), (b), and (c), which are effectively reflected by our aesthetic score metrics (higher indicating better). In (d), we showcase images with high aesthetic scores sampled from SDXL [81].

Imaging Quality. Imaging quality mainly considers the low-level distortions presented in the generated video frames (e.g., over-exposure, noise, blur). We use the MUSIQ [57] image quality predictor trained on the

SPAQ [24] dataset, which is capable of handling variablesized aspect ratios and resolutions. The frame-wise score is linearly normalized to [0, 1] by dividing 100, and the final score is then calculated by averaging the frame-wise scores across the entire video sequence.

![](images/15.jpg)  
Figure A15. Visualization of Imaging Quality. We present examples of generated videos with high imaging quality scores in (a), and low imaging quality scores (where the video is blurry and over-exposed) in (b).

# G.2. Video-Condition Consistency

![](images/16.jpg)  
Figure A16. Visualization of Object Class. We demonstrate generation examples for the target object at varying degrees, as reflected by the object score metrics (where 1 represents success, and 0 represents failure). (a) The target object "person" is successfully generated in the video. (b) The synthesized video does not contain the target object.

Object Class. When a user specifies a certain type of object in the text prompt, we aim to evaluate whether the model can generate an object of the specified type. To this end, we use GRiT [115] to detect objects in each frame of the generated video and check whether the specified object class is successfully detected in these frames. Subsequently, we report the proportion of frames in which the corresponding object class has been successfully detected. We employ GRiT for this dimension, as well as several other semantics dimensions such as Multiple Objects, Color, and Spatial Relationship for two reasons: 1) GRiT is a versatile framework that can handle both detection and captioning tasks, predicting diverse object attributes, so that the VBench can use the same framework across different dimensions and save users from installing multiple frameworks or downloading multiple pre-trained models. 2) GRiT demonstrates reliable performance in evaluating our designated dimensions, with comparable performance with the state-of-the-art object detectors [115], and good alignment with human perception in terms of "correct detection" as validated by the human preference results in main paper Figure 5.

![](images/17.jpg)  
Figure A17. Visualization of Multiple Objects. We showcase instances of generating multiple objects within a video simultaneously at different levels, as indicated by our multiple objects score metrics (where 1 signifies success, and 0 denotes failure). (a) The video effectively generates multiple required objects (i.e., dog and horse). (b) The video fails to produce the dog and horse at the same time.

Multiple Objects. Other than generating a single object, compositionality is also an essential aspect of video generation. Suppose the user requires generating multiple objects, we use GRiT for frame-wise object detection. For each frame, we check whether all the user-requested objects simultaneously appear in each frame. We then report the proportion of frames in which all the required objects have been successfully detected.

![](images/18.jpg)  
Figure A18. Visualization of Human Action. We showcase examples of generating the target action at different levels, as indicated by our action score metrics (where 1 denotes success, and 0 denotes failure). (a) The video successfully generates the barbequing action. (b) The video does not generate the target action.

Human Action. In the process of video synthesis from textual prompts, both the mentioned subjects in the prompt and the corresponding actions they engage in are important.

Given the remarkable emergence of high-quality humancentric generated videos, we believe it is necessary to ensure that human subjects depicted in videos accurately execute the specific actions described by the textual prompts. To this end, we use the Kinetics-400 dataset [56] as a reference due to its comprehensive characterization of diverse human actions. To evaluate the accuracy of the generated videos, we uniformly sample 16 frames from each video and apply UMT [65], which achieves the state-of-the-art classification performance on the Kinetics-400 dataset among opensourced models to classify the action. The top 5 results with logits bigger than 0.85 are preserved as ground-truth candidates, and we check whether the actions mentioned in the text prompt appear in the ground-truth candidates. The average percentage of all classification results is reported to assess whether the generated videos have human actions aligned with the text prompts.

![](images/19.jpg)  
Figure A19. Visualization of Color. We present examples of generating the target color within videos, depicting various levels of success through our color score metrics (larger denotes better). (a) The video accurately generates the target color. (b) The video only generated the target color in certain parts.

Color. To evaluate whether the color of an object is consistent with the specified condition, we use GRiT's captioning ability to describe colors, with slight modification to the GRiT pipeline. To remove the influence of the Object Class dimension's ability, we only consider videos where the object has been successfully generated. Specifically, GRiT identifies the bounding boxes of objects, which are then fed to two text decoders: one for predicting categories and the other for generating dense captions on the synthesized video frame. We then verify if the corresponding object's color is successfully captioned in all frames. Among the frames where the corresponding object is generated and the caption contains color information, we compute the percentage of frames where the color required by the text prompt is successfully captioned.

Spatial Relationship. We focus on left-right and topbottom relationships and evaluate whether the video content adheres to the spatial relationship specified by the text prompts. Inspired by the T2I-CompBench [44] evaluation, we compute the spatial relationship accuracy based on the horizontal and vertical positioning of object pairs. During evaluation, distances on the designated axis (e.g., left-right) are expected to be greater than those on the other orientation (e.g., top-bottom). Under this condition, we observe the intersection over the union metric (IoU) of two objects to obtain the final score, where IoU values that fall below a specified threshold result in a score of $100 \%$ , and the values exceeding the threshold are multiplied by a coefficient based on the IoU to determine the final score. We use GRiT to detect the objects and their locations within the generated video frames, and we also calculate the Intersection over Union (IoU) of the two objects' bounding boxes as the final spatial relationship score coefficient.

![](images/20.jpg)  
Figure A20. Visualization of Spatial Relationship. We show examples of generating the spatial relationships mentioned in the prompt within videos. (a) The video successfully captures the spatial relationship and objects described in the prompt. (b) The generated video does not contain the intended relationship.

![](images/21.jpg)  
Figure A21. Visualization of Scene. We present examples of generating the required scene (where 1 represents success, and 0 indicates failure). (a) The required scene is generated successfully. (b) The video does not show the scene as required.

Scene. For a scenario described by the text prompt, we need to evaluate whether the synthesized video is consistent with the intended scene. For example, when prompted to "ocean", the generated video should be "ocean" instead of "river". We use Tag2Text [45] to caption the generated scenes, and then check the correspondence with scene descriptions in the text prompt. Specifically, each word related to the scene in the text prompt needs to appear in the predicted caption, but the word order can be different. We then report the proportion of frames in which the corresponding scene has been successfully generated.

![](images/22.jpg)  
Figure A22. Visualization of Appearance Style. We demonstrate examples of generating the required appearance style within videos, showcasing different levels of success as assessed by our appearance style score metrics. (a) The generated video follows the requested Van Gogh style. (b) The video does not show the desired appearance style.

Appearance Style. For stylized video generation, we first extract the style description in the text prompt, then evaluate the video-text feature similarity to assess appearance style consistency. Specifically, We use CLIP [83] to extract features from each frame and the text, and then compute the mean cosine similarity of the normalized features. CLIP demonstrates robust zero-shot performance in perceiving textual descriptions of styles, aiding our evaluation of style consistency.

![](images/23.jpg)  
Figure A23. Visualization of Temporal Style. We demonstrate two different generated videos to show the consistency of their temporal style with the prompt at various degrees, measured by our temporal style score. (a) The generated video follows the "zoom in" temporal style. (b) The video's temporal style does not align with the prompt.

Temporal Style. In videos, style is not only spatially narrated in individual frames, but also temporally revealed in different types of object motions and camera motions. For example, we are interested in whether the text prompt specifies "zoom in" or "zoom out", "pan left" or "pan right", and whether the generated video can show such kind of camera motion. Additionally, there are different types of other temporal styles like "super slow motion", "camera shaking", and "racking focus". In terms of temporal awareness, ViCLIP [105] is pre-trained on a diverse 10M videotext dataset, which shows strong zero-shot learning capabilities in video-text retrieval tasks. When a video is generated based on a specified temporal style, we use ViCLIP to calculate the video-text feature similarity to reflect temporal style consistency.

![](images/24.jpg)  
Figure A24. Visualization of Overall Consistency. We demonstrate different examples that illustrate the extent to which they align with the prompt, as measured by our overall score metrics (larger values denote better consistency). (a) The video aligns closely with the prompt. (b) The video lacks alignment with the target concept.

Overall Consistency. We also use overall video-text consistency computed by ViCLIP as an aiding metric to reflect both semantics and style consistency, where the text prompts contain different semantics and styles.

# H. More Details on Prompt Suite

# H.1. Prompt Suite per Evaluation Dimension

For each VBench dimension, we carefully designed around 100 prompts as the test cases. For semantics-related prompt suites, we provide clear semantics labels to each prompt in the prompt suites to facilitate efficient and accurate evaluation. For example, we provide the object class labels for prompt suites of Object Class, Multiple Objects, and Spatial Relationship. We also provide color labels for Color prompts, relationship tags for Spatial Relationship prompts, and style labels for Appearance Style. We detail the prompt suite for each dimension as follows.

Subject Consistency. We choose 19 representative living or movable object categories from the COCO [68] dataset's 80 object categories. These categories encompass animals and transportation-related items. Each object category is associated with a set of carefully crafted actions or movements, ensuring logical coherence between the actions and their respective objects. A list of distinct prompts used for evaluating subject consistency is therefore created.

Background Consistency. We carefully select a list of distinct and representative scenes from the Places365 [131] dataset, aiming to include a diverse set of scenes within a limited number of prompts. The selected scenes contain indoor, modern, rural, and various other settings, thereby ensuring the representation of a wide range of environmental contexts. This prompt suite is applied to both the Background Consistency dimension and the Scene dimension.

Temporal Flickering. To more effectively evaluate temporal flickering, it is essential to eliminate interference from other temporal dimensions. According to observations in Section G, whether the scene is static does not affect the temporal flickering ranking among models. Ultimately, we selected a set of prompts, covering various topics, scenarios, and prompt lengths. Each prompt is accompanied by a prefix instructing the model to generate a static scene.

Motion Smoothness. Since Subject Consistency's prompt suite involves movements performed by different subjects, they serve as a good benchmark for Motion Smoothness as well. To minimize the number of videos needed to be sampled for each model in evaluation, we share the same prompt suite for both dimensions.

Dynamic Degree. Considering the issue of the model tending to generate static videos even when prompted with descriptions of motion, we use the same prompt suite as Subject Consistency's, which includes a variety of motion descriptions.

Object Class. We use the COCO dataset [68] and drop the object mouse, due to the potential confusion as it can be interpreted as both device and animal. We then append articles to the rest of the 79 objects and create a list of prompts related to different object classes.

Multiple Objects. We categorize COCO objects into various groups so that it will be reasonable for them to appear together. These categories include animals, indoor items, dining objects, bathroom items, and outdoor items. We then generate a list of prompts by composing objects within each category.

Human Action. From the Kinetics-400 dataset [56], we carefully extract a subset of 100 actions by considering both diversity and minimal overlaps in their meanings. Our approach involves selecting only the actions that are unique. For instance, within the category of actions related to playing musical instruments, we only keep those actions that are considered dissimilar in terms of human posture and actions. The resulting selection contains a wide spectrum of actions. Subsequently, we integrate each action in the form of "a person is doing something", and craft a list of humancentric action prompts.

Color. We select representative classes from COCO objects and establish the color scope of our prompt suite. On the selection of objects, we select objects that are unique in shape and similar objects. For example, "skateboard" and "surfboard" are excluded due to their similar shapes and potential wrong detection results by detection models. A similar criterion is applied to the color selection, we aim to select colors to include a broad spectrum while avoiding closely related colors. For example, "gold" and "yellow" are considered similar, therefore we only include "yellow" in our color scope. Our prompts are generated by combining each object with a few of their typical colors, and we only keep objects with more than three typical colors.

Spatial Relationship. We organize COCO objects into different groups so that it is natural for them to be composed in the same scene with each other. Some examples of the categories include personal items, animals, and sports-related items. Additionally, we define relationship categories to be "left and right" and "top and bottom". We then select relationships that are reasonable for the objects within each category, resulting in a list of prompts designed to describe spatial relationships between objects.

Scene. We use the same prompt suite as Background Consistency, as both requires prompts describing different general scenes.

Appearance Style. We select a list of sentences covering a wide range of scenarios and themes and also define our list of appearance styles. The styles are carefully crafted to ensure diversity. For example, we include the representative "Van Gogh style" and traditional "Ukiyo style" for the clear contrast in their color schemes, brushwork techniques, and overall aesthetic expressions. Each scenario description is then composed with a list of appearance styles to form the prompts.

Temporal Style. We carefully curate a diverse list of representative temporal styles to represent a broad spectrum of camera movement and temporal effects commonly employed in video production. Our selected temporal styles include variations in motion speed, camera perspective, and dynamic effects, aiming to present a comprehensive range of cinematic techniques. Each sentence for a scenario is then composed with a list of temporal styles.

Overall Consistency. We create a range of prompts, covering different content categories and scenarios such as "natural scenery", "fantasy and sci-f", "character and fictional beings" etc., these prompts are of varied length, and we include both general and specific descriptions in our prompts.

# H.2. Prompt Suite per Category

In Section 3.2 of the main paper on Prompt Suite Per Category, we employ LLM [130] as the first step to categorize the collection of human-curated prompts into eight content categories. The input template for the language model is shown in Table A3. The accuracy of classification is around $9 5 \%$ , and we manually go through each classified prompt to filter out 100 prompts for each content category.

Animal. These prompts focus on various animals and their behaviors in different environments, such as "a frog eating an ant", "a harbour seal swimming near the shore", and "a squirrel eating nuts". This prompt suite captures diverse

The assistant gives helpful, detailed, and polite answers to the user's questions. Please act as a language expert, able to choose one or more suitable categories from [Animal, Architecture, Food, Human, Lifestyle, Plant, Scenery, Vehicles] for the given text. Given the input text, you should return the answer without explanation. For example, if the input is [A man eats hamburgers.], the output tag format should be [Food, Human]. The given text is Input text.

species from domestic pets to wild animals in various activities, such as feeding, playing, or simply existing in their natural or adapted environments.

Architecture. We keep prompts that include various types of architecture, including the different types of buildings and structures, such as "the view of the Sydney opera house from the other side of the harbor", "illuminated tower in Berlin", and "a tree house in the woods".

Food. These prompts are diverse and all revolve around food and beverages. They range from specific dishes and preparation methods to more conceptual food art and eating scenarios. Examples include "Freshly baked finger-licking cookies", "A person slicing a vegetable", and "Close-up video of Japanese food".

Human. These prompts describe a wide range of human activities, interactions, and scenes, each focusing on specific individuals or groups engaged in various actions. Here are some examples: "A family wearing paper bag masks", "Boy sitting on grass petting a dog", "Group of people protesting", and "Father and son holding hands". Each of these prompts paints a vivid picture of human life, capturing diverse moments from daily activities to special events, professional settings to personal interactions.

ronment. Here are some examples: "View of the sea from an abandoned building", "Aerial footage of a city at night", and "Scenery of desert landscape". Each prompt can be of natural settings like beaches and mountains, the structured scenery of agricultural lands, or urban environments.

Vehicles. These prompts depict various forms of transportation and related scenes, including various vehicles like trains, cars, buses, motorcycles, and boats in diverse settings ranging from urban streets to natural landscapes. Here are some examples: "A modern railway station in Malaysia used for public transportation", "Train arriving at a station", "Elderly couple checking engine of automobile", and "Helicopter landing on the street".

Lifestyle. These prompts describe various indoor scenes and activities, covering a wide range of settings and situations. For instance, "Interior design of the bar section" and "Dog on floor in room" are simple everyday indoor scenes. Each prompt captures a specific aspect of indoor life, ranging from personal moments and family interactions to professional and leisure activities, reflecting the diversity of experiences within indoor lifestyles.

Plant. These prompts mainly focus on plants and trees. Here are some examples: "Video of an indoor green plant", "A coconut tree by the house", and "Variety of trees and plants in a botanical garden".

Scenery. These prompts describe various natural and urban landscapes, each capturing a distinct aspect of the envi

# I. Human Preference Annotation

# I.1. Human Annotation Procedures

Labeling Instructions. To systematically communicate with human annotators about labeling rules, we prepare a labeling instruction document for each of the 16 dimensions. Each labeling instruction document consists of several important elements. First, we introduce the labeling user interface (shown in Figure 4 of the main paper), including the two videos in comparison, the location of prompts and questions, the control for video playback and stop, and the three choices to make (i.e., "A is better", "B is better", or "Same quality"). Second, we explain the dimension of interest. Since we want to verify the human alignment of VBench in each fine-grained dimension, we conduct the labeling of different dimensions separately. In each document, we elaborate on the definition of the current dimension, including aspects to consider or discard. For instance, for the Subject Consistency dimension, annotators are asked to only focus on the look of the main subject, and not to consider the degree of temporal flickering, or the video alignment with the text prompt, and many other irrelevant dimensions. Each aspect to consider or discard is illustrated by both text descriptions and examples of synthesized videos. Third, we categorize various scenarios that annotators may encounter while annotating this dimension (e.g., what is considered as "better", and what is considered as "same quality"). For each scenario, we provide explanatory examples.

Quality Assurance in Preference Annotations. To guarantee the accuracy of human preference annotations, we implement a systematic five-step approach: 1) Labeling Instructions Preparation: For each evaluation dimension, we provide clear and well-organized labeling instructions with examples. 2) Pre-Labeling Trial: Prior to the main annotation task, we conduct a pre-labeling trial, where annotators are assigned to annotate only 60 samples. We go through all 60 annotations and communicate with annotators about each wrong label, and clarify any misunderstanding or potential doubts in the labeling instructions. 3) Labeling Instructions Update: We update the labeling instructions according to feedback from the human annotators, and supplement the wrongly labeled samples into the labeling instructions. 4) Post-Labeling Checks by Annotators: Upon labeling all samples for a particular dimension, the samples are grouped as 60 samples per package. In each package of 60 samples, human annotators go through $20 \%$ of randomly selected samples for quality checking. If for any package the error rate exceeds $10 \%$ , the entire package is sent back for re-labeling conducted by a different annotator. 5) PostLabeling Checks by Authors: Upon labeling and possible re-labelings, we conduct the same post-labeling checks procedure similar to step 4. For any labeling errors spotted, we communicate with the human annotator for correction, and ask them to go through the entire package again. If any package reports an error rate higher than $10 \%$ , the entire labeled samples (all packages) for this dimension are considered invalid. We communicate with human annotators regarding possible problems encountered during annotation, and go back to step 1 to conduct annotation for this dimension all over again.

# I.2. VLM Tuning

VLM Tuning. Our human preference annotations are mainly used to validate the alignment of VBench evaluations with human perception in each dimension. We show that these annotations have the potential to be used for broader applications, such as fine-tuning VLM models to align more closely with human preferences. VideoChat [64] uses a pre-trained video encoder and a large language model to align and train video VLM with instruction data. We organize data from VBench for fine-tuning, enhancing the model's ability to assess video generation. By utilizing instruction data of the VBench Dataset coupled with human preference tags, VLM, through its cognitive chain, is with its Chain of Thought to choose suitable evaluation metrics and furnishing scores in accordance. Upon giving the prompt into the VLM and asking: Give the appropriate evaluation metrics", we obtain the dimensions to be evaluated from Chain of Thought, as shown in Table A4. We feed the derived metrics along with the video into the VLM, and ask two additional questions: "Please observe the video and describe it based on the provided metrics." and "Predict the assessment score of the video within these metrics." We show examples of before and after VLM Tuning in Figure A25

Data Preparation. We organize human preference annotations and model scores across different dimensions into training datasets. For model scores, each video's ratings are mapped from 0-1 to a 0-10 scale and are coupled with questions used for human annotation to form our instruction data. Evaluations of different aspects of the same video are <SYSTEM>: You are an assistant with fair and objective standards for video assessment. Given the input statement for the video generation model: {input text}, please select appropriate evaluation dimensions from ['subject consistency', 'background consistency', 'temporal flickering', 'motion smoothness', 'dynamic degree', 'aesthetic quality', 'imaging quality', 'object class', 'multiple objects', 'human action', 'color', 'spatial relationship', 'scene', 'temporal style', 'appearance style', 'overall consistency'].

<USER>: a dilapidated phone booth stood as a relic of a bygone era on the sidewalk, frozen in time.   
<ASSISTANT $>$ : Based on the input statement, the following evaluation dimensions may be relevant: subject consistency, background consistency, temporal flickering, motion smoothness,   
dynamic degree, aesthetic   
quality, imaging quality,   
scene, temporal style, overall   
consistency

grouped to facilitate the VLM's ability to engage in multiturn dialogues. As for human preference annotations, pairs of videos along with corresponding questions and preference options are organized into instruction data, thereby somewhat enhancing the model with the capability to make evaluative judgments.

Implementation Details Our approach uses VideoChatembed [64] as the baseline model. The model undergoes fine-tuning on a set of 30,000 instruction pairs. We fine-tune the model for 3 epochs using a learning rate of 2e-5 and an overall batch size of 64. The training for our model takes about one hour when performed on 8 A100-80GB GPUs.

# J. More Implementation Details

# J.1. Video Generation Models in Evaluation

To evaluate our benchmark on recent advances, we adopt four models for comparison, and more will be added as they become open-sourced. Details of models and sampling

# Video to be Evaluated

Prompt: a dilapidated phone booth stood as a relic of a bygone era on the sidewalk, frozen in time

Before VLM Tuning The input video prompt is α dilapidated phone booth stood as a relic of a bygone era on the sidewalk, frozen in time", please observe the video and describe it based on the subject consistency. B User The video depicts an old and worn out phone booth standing alone, frozen in time, on the sidewalk. The VideoChat boxy structure is surrounded by a brick wall, creating a sense of isolation. It appears to be an outdated piece of technology, a relic from a bygone era, which is now abandoned and left to stand alone on its own. Pc h User VideoChat are measures of the coherence and continuity between the elements within an image or vido. The subject matter of an abandoned phone booth standing alone on a sidewalk is not consistent with the typical subject content that is typically seen within a video, such as landscapes, people, or events. As such, the cohesive subject material. After VLM Tuning The input video prompt is a dilapidated phone booth stood as a relic of a bygone era on the sidewalk, B frozen in time", please observe the video and describe it based on the subject consistency. User First, we see an old metal phone booth that has been left behind standing on the sidewalk. It looks like it VideoChat repaired with tape. There is a black door on the side of the booth, and a white door handle. Next, we see the same phone booth, but the glass window and door are covered in a dark tint, making it e onthe oo givs  ee pat eot na next to it. Finally, we see the same phone booth in the same state of disrepair, but it is sitting on the ground rather than standing. It has an old wooden door, a round dial on the front, and a round window. The glass in the i Teo  n x l The surrounding area appears to be abandoned and overgrown with weeds. Predict the assessment score(0-10) of the video within subject consistency metrics. 8 User Theassessment score of thevideo is 8 within subject consistency. It maintains a high levelof consistency in VideoChat its description throughout the duration of the video. The metal phone booth is the main object of focus and it remains in the same location and is described in a similar manner throughout the video. The only difference is that the glass window and door are covered in a dark tint, making it harder to see inside. Overall, the video does an excelent jo of capturing the subject of a dilapidated phone booth in the same location and in a similar state of disrepair throughout the video. improve VLM's ability in evaluating video generation capabilities in specific dimensions.

![](images/25.jpg)  
the VBench evaluation results across different models on the same content cateory.

T aneach modelOur xperiments showhat Benc evaluations acrossl dimensions closely matc human perceptions.   

<table><tr><td rowspan="2">Models</td><td rowspan="2">Subject Consistency</td><td rowspan="2">Background nsistency</td><td rowspan="2">Temporal Fickering</td><td rowspan="2">Motion Smoothness</td><td rowspan="2">Dynamic Degree</td><td rowspan="2">Aesthetic Quality</td><td rowspan="2">Imaging Quality</td><td rowspan="2">Object Class</td></tr><tr><td></td></tr><tr><td>LaVie [104]</td><td>67.87% / 69.95%</td><td>85.27% / 65.04%</td><td>73.42% / 87.96%</td><td>69.54% / 65.65%</td><td>41.81% / 53.10%</td><td>77.56% / 83.41%</td><td>77.20% / 79.46%</td><td>57.55% / 79.20%</td></tr><tr><td>ModelScope [72, 98]</td><td>49.07% / 56.30%</td><td>49.96% / 56.36%</td><td>65.42% / 62.44%</td><td>58.61% / 59.58%</td><td>52.92% / 53.84%</td><td>67.74% / 63.15%</td><td>60.00% / 68.53%</td><td>49.37% / 49.58%</td></tr><tr><td>VideoCrafter [35]</td><td>24.72% / 20.42%</td><td>15.89% / 27.21%</td><td>31.20% / 43.64%</td><td>10.00% / 13.80%</td><td>68.47% / 62.18%</td><td>35.34% / 32.33%</td><td>55.05% / 37.85%</td><td>54.18% / 41.77%</td></tr><tr><td>CogVideo [41]</td><td>58.33% / 53.33%</td><td>48.88% / 51.40%</td><td>29.96% / 5.96%</td><td>61.85% / 60.97%</td><td>36.81% / 30.88%</td><td>19.35% / 21.11%</td><td>7.74% / 14.16%</td><td>38.90% / 29.45%</td></tr><tr><td>Correlation</td><td>96.51%</td><td>94.80%</td><td>88.73%</td><td>99.80%</td><td>82.09%</td><td>98.65%</td><td>92.16%</td><td>80.37%</td></tr><tr><td rowspan="2">Models</td><td>Multiple</td><td>Human</td><td>Color</td><td>Spatial</td><td>Scene</td><td>Appearance</td><td>Temporal</td><td>Overall</td></tr><tr><td>Objects</td><td>Action</td><td></td><td>Relationship</td><td></td><td>Style</td><td>Style</td><td>Consistency</td></tr><tr><td>LaVie [104]</td><td>53.37% / 57.97%</td><td>54.43% / 58.13%</td><td>52.31% / 51.37%</td><td>52.30% / 49.81%</td><td>59.69% / 77.52%</td><td>61.85% / 58.22%</td><td>69.07% / 55.73%</td><td>70.82% / 77.35%</td></tr><tr><td>ModelScope [72, 98]</td><td>57.15% / 62.15%</td><td>51.10% / 53.07%</td><td>50.12% / 49.73%</td><td>53.25% / 53.15%</td><td>48.22% / 50.00%</td><td>57.48% / 54.93%</td><td>65.40% / 57.50%</td><td>66.31% / 60.07%</td></tr><tr><td>VideoCrafter [35]</td><td>48.74% / 49.63%</td><td>52.17% / 47.87% 42.30% / 40.93%</td><td>48.71% / 47.92%</td><td>56.11% / 54.66%</td><td>52.79% / 46.05%</td><td>36.67% / 40.07%</td><td>65.40% / 51.90% 0.13% / 34.87%</td><td>62.65% / 48.10%</td></tr><tr><td>CogVideo [41]</td><td>40.73% / 30.24%</td><td></td><td>48.86% / 50.98%</td><td>38.33% / 42.38%</td><td>39.30% / 26.43%</td><td>44.00% / 46.78%</td><td></td><td>0.22% / 14.48%</td></tr><tr><td>Correlation</td><td>98.98%</td><td>89.15%</td><td>60.73%</td><td>97.59%</td><td>94.07%</td><td>99.65%</td><td>97.53%</td><td>93.27%</td></tr></table>

Tou WebVid-Avg baseline. We provide results from other models and baselines as wellfor a comprehensive view.   

<table><tr><td>Models</td><td>Subject Consistency</td><td>Background Consistency</td><td>Motion Smoothness</td><td>Dynamic Degree</td><td>Aesthetic Quality</td><td>Imaging Quality</td><td>Appearance Style</td><td>Temporal Style</td><td>Overall Consistency</td></tr><tr><td>LaVie [104]</td><td>91.41%</td><td>97.47%</td><td>96.38%</td><td>49.72%</td><td>54.94%</td><td>61.90%</td><td>23.56%</td><td>25.93%</td><td>26.41%</td></tr><tr><td>ModelScope [72, 98]</td><td>89.87%</td><td>95.29%</td><td>95.79%</td><td>66.39%</td><td>52.06%</td><td>58.57%</td><td>23.39%</td><td>25.37%</td><td>25.67%</td></tr><tr><td>VideoCrafter [35]</td><td>86.24%</td><td>92.88%</td><td>91.79%</td><td>89.72%</td><td>44.41%</td><td>57.22%</td><td>21.57%</td><td>25.42%</td><td>25.21%</td></tr><tr><td>CogVideo [41]</td><td>92.19%</td><td>95.42%</td><td>96.47%</td><td>42.22%</td><td>38.18%</td><td>41.03%</td><td>22.01%</td><td>7.80%</td><td>7.70%</td></tr><tr><td>Empirical Min</td><td>14.62%</td><td>26.15%</td><td>70.60%</td><td>0.00%</td><td>0.00%</td><td>0.00%</td><td>0.09%</td><td>0.00%</td><td>0.00%</td></tr><tr><td>WebVid Avg</td><td>96.17%</td><td>96.59%</td><td>98.17%</td><td>44.13%</td><td>42.37%</td><td>58.22%</td><td>22.15%</td><td>25.77%</td><td>34.14%</td></tr><tr><td>Empirical Max</td><td>100.00%</td><td>100.00%</td><td>99.75%</td><td>100.00%</td><td>100.00%</td><td>100.00%</td><td>28.55%</td><td>36.40%</td><td>36.40%</td></tr></table>

u eight content categories, on various evaluation dimensions.   

<table><tr><td rowspan=1 colspan=1>Models</td><td rowspan=1 colspan=1>Categories</td><td rowspan=1 colspan=1>SubjectConsistency</td><td rowspan=1 colspan=1>BackgroundConsistency</td><td rowspan=1 colspan=1>MotionSmoothness</td><td rowspan=1 colspan=1>DynamicDegree</td><td rowspan=1 colspan=1>AestheticQuality</td><td rowspan=1 colspan=1>ImagingQuality</td><td rowspan=1 colspan=1>OverallConsistency</td></tr><tr><td rowspan=7 colspan=1>LaVie[104]</td><td rowspan=7 colspan=1>AnimalArchitectureFoodLifestyleSceneryHumanPlantVehicles</td><td rowspan=1 colspan=1>97.49%98.04%</td><td rowspan=1 colspan=1>97.18%97.38%</td><td rowspan=1 colspan=1>97.29%97.83%</td><td rowspan=1 colspan=1>15.20%5.20%</td><td rowspan=1 colspan=1>48.26%54.20%</td><td rowspan=1 colspan=1>68.81%69.30%</td><td rowspan=1 colspan=1>26.43%25.46%</td></tr><tr><td rowspan=1 colspan=1>97.11%</td><td rowspan=1 colspan=1>96.90%</td><td rowspan=1 colspan=1>98.18%</td><td rowspan=1 colspan=1>28.80%</td><td rowspan=1 colspan=1>54.15%</td><td rowspan=1 colspan=1>65.24%</td><td rowspan=1 colspan=1>24.88%</td></tr><tr><td rowspan=3 colspan=1>96.10%97.27%96.11%</td><td rowspan=2 colspan=1>96.19%97.06%</td><td rowspan=1 colspan=1>98.08%</td><td rowspan=1 colspan=1>33.60%</td><td rowspan=1 colspan=1>48.76%</td><td rowspan=1 colspan=1>64.02%</td><td rowspan=1 colspan=1>24.43%</td></tr><tr><td rowspan=1 colspan=1>97.58%</td><td rowspan=1 colspan=1>6.40%</td><td rowspan=1 colspan=1>51.76%</td><td rowspan=1 colspan=1>63.86%</td><td rowspan=1 colspan=1>24.56%</td></tr><tr><td rowspan=1 colspan=1>95.88%</td><td rowspan=1 colspan=1>97.57%</td><td rowspan=1 colspan=1>39.00 %</td><td rowspan=1 colspan=1>51.87%</td><td rowspan=1 colspan=1>64.07%</td><td rowspan=1 colspan=1>24.63%</td></tr><tr><td rowspan=2 colspan=1>97.52%95.23%</td><td rowspan=2 colspan=1>97.20%95.82%</td><td rowspan=1 colspan=1>96.73%</td><td rowspan=1 colspan=1>16.40%</td><td rowspan=1 colspan=1>52.68%</td><td rowspan=1 colspan=1>67.86%</td><td rowspan=1 colspan=1>24.50%</td></tr><tr><td rowspan=1 colspan=1>97.11%</td><td rowspan=1 colspan=1>34.00%</td><td rowspan=1 colspan=1>50.70%</td><td rowspan=1 colspan=1>61.02%</td><td rowspan=1 colspan=1>24.51%</td></tr><tr><td rowspan=8 colspan=1>ModelScope[72, 98]</td><td rowspan=8 colspan=1>AnimalArchitectureFoodLifestyleSceneryHumanPlantVehicles</td><td rowspan=2 colspan=1>94.08%95.77%</td><td rowspan=1 colspan=1>95.80%</td><td rowspan=1 colspan=1>96.40%</td><td rowspan=1 colspan=1>37.20%</td><td rowspan=1 colspan=1>47.32%</td><td rowspan=1 colspan=1>60.30%</td><td rowspan=1 colspan=1>26.58%</td></tr><tr><td rowspan=1 colspan=1>95.88%</td><td rowspan=1 colspan=1>97.20%</td><td rowspan=1 colspan=1>24.80%</td><td rowspan=1 colspan=1>52.10%</td><td rowspan=1 colspan=1>58.38%</td><td rowspan=1 colspan=1>24.89%</td></tr><tr><td rowspan=1 colspan=1>94.53%</td><td rowspan=1 colspan=1>95.53%</td><td rowspan=1 colspan=1>97.17%</td><td rowspan=1 colspan=1>40.80%</td><td rowspan=1 colspan=1>53.06%</td><td rowspan=1 colspan=1>64.39%</td><td rowspan=1 colspan=1>24.40%</td></tr><tr><td rowspan=1 colspan=1>94.36%</td><td rowspan=1 colspan=1>95.17%</td><td rowspan=1 colspan=1>97.18%</td><td rowspan=1 colspan=1>41.00%</td><td rowspan=1 colspan=1>45.77%</td><td rowspan=1 colspan=1>59.62%</td><td rowspan=1 colspan=1>23.51%</td></tr><tr><td rowspan=1 colspan=1>94.88%</td><td rowspan=1 colspan=1>95.57%</td><td rowspan=1 colspan=1>97.03%</td><td rowspan=1 colspan=1>26.00%</td><td rowspan=1 colspan=1>48.57%</td><td rowspan=1 colspan=1>57.49%</td><td rowspan=1 colspan=1>23.28%</td></tr><tr><td rowspan=1 colspan=1>93.37%</td><td rowspan=1 colspan=1>94.21%</td><td rowspan=1 colspan=1>96.45%</td><td rowspan=1 colspan=1>56.00%</td><td rowspan=1 colspan=1>48.14%</td><td rowspan=1 colspan=1>58.41%</td><td rowspan=1 colspan=1>22.84%</td></tr><tr><td rowspan=2 colspan=1>95.14%93.17%</td><td rowspan=1 colspan=1>96.26%</td><td rowspan=1 colspan=1>96.48%</td><td rowspan=1 colspan=1>26.40%</td><td rowspan=1 colspan=1>51.03%</td><td rowspan=1 colspan=1>63.83%</td><td rowspan=1 colspan=1>23.55%</td></tr><tr><td rowspan=1 colspan=1>94.61%</td><td rowspan=1 colspan=1>96.47%</td><td rowspan=1 colspan=1>50.20%</td><td rowspan=1 colspan=1>47.53%</td><td rowspan=1 colspan=1>55.75%</td><td rowspan=1 colspan=1>23.60%</td></tr><tr><td rowspan=8 colspan=1>VideoCrafter[35]</td><td rowspan=8 colspan=1>AnimalArchitectureFoodLifestyleSceneryHumanPlantVehicles</td><td rowspan=2 colspan=1>87.01%91.18%</td><td rowspan=1 colspan=1>92.40%</td><td rowspan=1 colspan=1>91.80%</td><td rowspan=1 colspan=1>79.60%</td><td rowspan=1 colspan=1>40.51%</td><td rowspan=1 colspan=1>59.79%</td><td rowspan=1 colspan=1>25.47 %</td></tr><tr><td rowspan=1 colspan=1>92.93%</td><td rowspan=1 colspan=1>94.83%</td><td rowspan=1 colspan=1>47.80%</td><td rowspan=1 colspan=1>43.71%</td><td rowspan=1 colspan=1>59.63%</td><td rowspan=1 colspan=1>24.27%</td></tr><tr><td rowspan=1 colspan=1>89.50%</td><td rowspan=1 colspan=1>92.87%</td><td rowspan=1 colspan=1>93.44%</td><td rowspan=1 colspan=1>75.00%</td><td rowspan=1 colspan=1>48.19%</td><td rowspan=1 colspan=1>63.47%</td><td rowspan=1 colspan=1>24.47%</td></tr><tr><td rowspan=1 colspan=1>89.51%</td><td rowspan=1 colspan=1>91.87%</td><td rowspan=1 colspan=1>93.63%</td><td rowspan=1 colspan=1>72.20%</td><td rowspan=1 colspan=1>39.84%</td><td rowspan=1 colspan=1>59.44%</td><td rowspan=1 colspan=1>24.01%</td></tr><tr><td rowspan=1 colspan=1>89.67%</td><td rowspan=1 colspan=1>92.86%</td><td rowspan=1 colspan=1>94.17%</td><td rowspan=1 colspan=1>51.80%</td><td rowspan=1 colspan=1>43.06%</td><td rowspan=1 colspan=1>58.98%</td><td rowspan=1 colspan=1>23.20%</td></tr><tr><td rowspan=3 colspan=1>88.50%89.86%88.38%</td><td rowspan=1 colspan=1>90.92%</td><td rowspan=1 colspan=1>92.35%</td><td rowspan=1 colspan=1>86.20%</td><td rowspan=1 colspan=1>42.62%</td><td rowspan=1 colspan=1>59.23%</td><td rowspan=1 colspan=1>23.31%</td></tr><tr><td rowspan=2 colspan=1>93.57%91.44%</td><td rowspan=1 colspan=1>93.72%</td><td rowspan=1 colspan=1>52.00%</td><td rowspan=1 colspan=1>41.81%</td><td rowspan=2 colspan=1>63.81%54.14%</td><td rowspan=2 colspan=1>23.41%23.39%</td></tr><tr><td rowspan=1 colspan=1>93.04%</td><td rowspan=1 colspan=1>70.60%</td><td rowspan=1 colspan=1>42.95%</td></tr><tr><td rowspan=7 colspan=1>CogVideo[41]</td><td rowspan=7 colspan=1>AnimalArchitectureFoodLifestyleSceneryHumanPlantVehicles</td><td rowspan=5 colspan=1>92.95%95.00%94.08%93.80%95.27%</td><td rowspan=2 colspan=1>94.69%94.65%</td><td rowspan=1 colspan=1>96.65%</td><td rowspan=1 colspan=1>30.20%</td><td rowspan=1 colspan=1>45.37%</td><td rowspan=2 colspan=1>48.45%45.33%</td><td rowspan=2 colspan=1>8.26%7.48%</td></tr><tr><td rowspan=1 colspan=1>97.39%</td><td rowspan=1 colspan=1>10.20%</td><td rowspan=1 colspan=1>46.29%</td></tr><tr><td rowspan=1 colspan=1>94.94%</td><td rowspan=1 colspan=1>96.99%</td><td rowspan=1 colspan=1>32.00%</td><td rowspan=1 colspan=1>52.79%</td><td rowspan=1 colspan=1>45.05%</td><td rowspan=1 colspan=1>7.01%</td></tr><tr><td rowspan=1 colspan=1>93.93%</td><td rowspan=1 colspan=1>96.93%</td><td rowspan=1 colspan=1>28.00%</td><td rowspan=1 colspan=1>41.57%</td><td rowspan=1 colspan=1>41.28%</td><td rowspan=1 colspan=1>7.85%</td></tr><tr><td rowspan=1 colspan=1>95.46%</td><td rowspan=1 colspan=1>97.58%</td><td rowspan=1 colspan=1>13.20%</td><td rowspan=1 colspan=1>46.72%</td><td rowspan=1 colspan=1>40.49%</td><td rowspan=1 colspan=1>7.66%</td></tr><tr><td rowspan=1 colspan=1>92.08%</td><td rowspan=1 colspan=1>92.29%</td><td rowspan=1 colspan=1>95.93%</td><td rowspan=1 colspan=1>46.80%</td><td rowspan=1 colspan=1>46.38%</td><td rowspan=1 colspan=1>43.81%</td><td rowspan=1 colspan=1>8.29%</td></tr><tr><td rowspan=1 colspan=1>94.86%93.11%</td><td rowspan=1 colspan=1>95.71%93.27%</td><td rowspan=1 colspan=1>97.05%96.80%</td><td rowspan=1 colspan=1>19.60%33.60%</td><td rowspan=1 colspan=1>48.63%44.18%</td><td rowspan=1 colspan=1>43.22%41.05%</td><td rowspan=1 colspan=1>6.65%8.34%</td></tr></table>

strategy are listed as follows.

LaVie. LaVie [104] is a high-quality video generation model that incorporates cascaded latent diffusion models. Specifically, a set of temporal modules is attached to the vanilla Stable Diffusion [84] model and the entire model is jointly trained on both images and videos to achieve video generation. For each prompt, we sample 16 continuous frames of size $5 1 2 \times 5 1 2$ at 8 frames per second (FPS). We use the DDPM sampling of 250 steps. The initial random seed is set to 2 and the classifier-free guidance is set to 7.

ModelScope. ModelScope [72, 98] is a diffusion-based text-to-video generation model. We adopt its official inference code and sample 16 frames of size $2 5 6 \times 2 5 6$ at 8 FPS.

VideoCrafter. VideoCrafter [35] is a toolkit for text-tovideo generation and editing. We adopt the VideoCrafter 0.9 version (a.k.a., LVDM) and utilize its base generic textto-video generation model. We use the official inference code to sample 16 frames of size $2 5 6 \times 2 5 6$ at 8 FPS. The initial random seed is set to 2 during sampling.

CogVideo. CogVideo [41] is a transformer-based text-tovideo generation model that inherits the pretrained textto-image model CogView2 [20]. Since the official inference code requires simplified Chinese input, we translate all prompts into Chinese. We sample 33 frames of size $4 8 0 \times 4 8 0$ at 10 FPS for each video, according to its default settings. During sampling, all stages are involved in the pipelines, including sequential generation, frame interpolation, and recursive interpolation. The initial random seed is also set to 2 for a fair comparison.

# J.2. Reference Baselines

In the main paper, we devise the Empirical Max and Empirical Min baselines to approximate the maximum / minimum scores that videos might be able to achieve. We also devise the WebVid-Avg baseline to reflect the average video quality of WebVid-10M dataset [5] as a reference. The numerical results are displayed in Table 1 in the main paper, and Table A6 in this Supplementary File. We provide additional details on approximating these values as follows.

Empirical Max. (1) WebVid-10M's Maximum. For dimensions where the $100 \%$ score is unlikely to be achieved by any video, we retrieve WebVid-10M's real videos and report the highest-scoring video's result. Examples of such dimensions include Motion Smoothness, Scene, Appearance Style, Temporal Style, and Overall Consistency. (2) Theoretical $100 \%$ For dimensions where there exist videos that can achieve $100 \%$ , we directly use $100 \%$ as the empirical maximum value. For temporal consistency dimensions Subject Consistency, Background Consistency, and Temporal Flickering, a completely static video corresponds to the $100 \%$ score. For Dynamic Degree, a set of highly dynamic videos can achieve the $100 \%$ ratio of dynamic degree. For the frame-wise quality dimensions Aesthetic Quality and Imaging Quality, a video consisting of $100 \%$ -scoring frames results in a final $100 \%$ score. For video-text semantics dimensions Object Class, Multiple Objects, Human Actions, Color, and Spatial Relationship, videos with the correct semantics specified in the text prompt can score $100 \%$ .

Empirical Min. (1) Gaussian Noise Videos. For videotext feature similarity dimensions Appearance Style, Temporal Style, and Overall Consistency, we use videos of i.i.d. Gaussian noise and the corresponding prompt suites to compute the corresponding score, and select the smallest value as the approximated empirical minimum (with some actually reaching $0 \%$ ). For Temporal Flickering and Motion Smoothness, we directly compute the score of the Gaussian noise videos and take the minimum scoring video's result. For Human Action, our method suite gives $0 \%$ on the Gaussian noise videos. (2) Composed Videos. For temporal consistency dimensions Subject Consistency and Background Consistency, we randomly sample frames from different WebVid-10M [5] videos to form a video with dynamically shifting content. This procedure is repeated 1000 times, and the minimum score among all videos obtained serves as the empirical minimum reference. (3) Theoretical $0 \%$ . For dimensions where there exist videos that can achieve $0 \%$ , we directly use $0 \%$ as the empirical minimum value. For Dynamic Degree, a set of static videos can achieve the $0 \%$ ratio of dynamic degree. For the frame-wise dimensions Aesthetic Quality and Imaging Quality, a video consisting of $0 \%$ -scoring frames results in a final $0 \%$ score. For video-text semantics dimensions Object Class, Multiple Objects, Color, Spatial Relationship, and Scene, videos with the incorrect semantics specified in the text prompt can score $100 \%$ .

WebVid-Avg. For dimensions where WebVid-10M videos can be retrieved with high confidence according to their captions, such as Subject Consistency, Background Consistency, Motion Smoothness, Dynamic Degree, Aesthetic

Quality, Imaging Quality, Appearance Style, Temporal Style, and Overall Consistency, we compute the average score for all retrieved videos in relation to the corresponding dimension. This average score serves as a reference value for the average of real videos. The results are visualized in the main paper Figure 6 (b), and detailed in Table A6 in this Supplementary File.

# J.3. Normalization for Radar Chart Visualization

In the radar charts, we perform normalization to clearly visualize the relative performance. We detail the normalization methods as follows:

•Main Paper Figure 2. VBench Evaluation Results of Video Generative Models - For each dimension, we map the maximum score achieved by one of the T2V models to 0.8, and the minimum score to 0.3, and linearly map the remaining models' scores to the radar chart axes. The radar chart axes have a range from 0.0 to 1.0.   
•Main Paper Figure 6 (a). T2V vs. T2I - For each dimension, we map the maximum score achieved by one of the models (including T2I and T2V models) to 0.8, and the minimum score to 0.3, and linearly map the remaining models' scores to the radar chart axes. The radar chart axes have a range from 0.0 to 1.0.   
Main Paper Figure 6 (b). T2V vs. WebV-Avg & Max - For each dimension, we map the maximum score achieved by one of the models (including the Empirical Max and WebVid-Avg baselines) to 0.8, and the minimum score to 0.3, and linearly map the remaining models' scores to the radar chart axes. The radar chart axes have a range from 0.0 to 1.0.   
Main Paper Figure 7. VBench Results across Eight Content Categories (by Model) - For each dimension, there are 32 numerical results corresponding to the four T2V models and eight content categories. We map the maximum score among the 32 results to 1.0, and the minimum score among the 32 results to 0.0, and linearly map the remaining 30 scores to respective radar charts' axes. The radar chart axes have a range from 0.0 to 1.0.   
Supp File Figure A26. VBench Results across Eight Content Categories (by Category) - Unlike Figure 7 in the main paper which put different categories of the same model in one radar chart, in Figure A26 we use an alternative visualization method, that is, collecting different models' results of the same category in one radar chart. For each dimension, there are 32 numerical results corresponding to the four T2V models and eight content categories. We map the maximum score among the 32 results to 0.8, and the minimum score among the 32 results to 0.3, and linearly map the remaining 30 scores to respective radar charts' axes. The radar chart axes have a range from 0.0 to 1.0.

Tva o Overall Consistency we replaced the ViCLIP approach by CLIP to enable evaluating image generation models.   

<table><tr><td rowspan="2">Models</td><td rowspan="2">Aesthetic Quality</td><td rowspan="2">Imaging Quality</td><td rowspan="2">Object Class</td><td rowspan="2">Multiple Objects</td><td rowspan="2">Human Action</td><td rowspan="2">Color</td><td rowspan="2">Spatial Relationship</td><td rowspan="2">Scene</td><td rowspan="2">Appearance Style</td><td rowspan="2">Overall Consistency</td></tr><tr><td></td></tr><tr><td>LaVie [104]</td><td>54.94%</td><td>61.90%</td><td>91.82%</td><td>33.32%</td><td>96.80%</td><td>86.39%</td><td>34.09%</td><td>52.69%</td><td>23.56%</td><td>32.96%</td></tr><tr><td>ModelScope [72, 98]</td><td>52.06%</td><td>58.57%</td><td>82.25%</td><td>38.98%</td><td>92.40%</td><td>81.72%</td><td>33.68%</td><td>39.26%</td><td>23.39%</td><td>31.99%</td></tr><tr><td>VideoCrafter [35]</td><td>44.41%</td><td>57.22%</td><td>87.34%</td><td>25.93%</td><td>93.00%</td><td>78.84%</td><td>36.74%</td><td>43.36%</td><td>21.57%</td><td>30.78%</td></tr><tr><td>CogVideo [41]</td><td>38.18%</td><td>41.03%</td><td>73.40%</td><td>18.11%</td><td>78.20%</td><td>79.57%</td><td>18.24%</td><td>28.24%</td><td>22.01%</td><td>27.80%</td></tr><tr><td>SD1.4 [84]</td><td>65.85%</td><td>69.86%</td><td>91.14%</td><td>34.39%</td><td>91.80%</td><td>90.57%</td><td>61.89%</td><td>52.33%</td><td>25.35%</td><td>32.59%</td></tr><tr><td>SD2.1 [84]</td><td>66.50%</td><td>69.10%</td><td>93.42%</td><td>51.22%</td><td>89.00%</td><td>91.15%</td><td>73.11%</td><td>58.14%</td><td>25.48%</td><td>33.08%</td></tr><tr><td>SDXL [81]</td><td>70.38%</td><td>68.79%</td><td>91.39%</td><td>69.51%</td><td>91.20%</td><td>88.92%</td><td>86.17%</td><td>54.65%</td><td>25.23%</td><td>33.77%</td></tr></table>

# K. Potential Negative Societal Impacts

Video generation models could be maliciously applied to generate fake content involving human figures. Moreover, generative models can potentially inherit biases from the training datasets [21]. Therefore, we recognize the importance of considering ethical and safety aspects when evaluating video generation models. We plan to include safety and equality dimensions in future iterations of VBench. We also urge users to apply video generation models with discretion.

# L. Limitations and Future Work

Limited Amount of Open-Sourced T2V Models: Currently, the number of open-sourced T2V models are still limited. We will open-source our VBench and encourage more T2V models to participate in the evaluation, including but not limited to [14, 8, 124], so that we can provide more informed insights into the current state of T2V, and provide more annotated data on T2V generation results generated by different models.

Evaluation of Other Video Generation Tasks: Text-tovideo (T2V) is a fundamental task in video generation, and there are other related video generation tasks such as videodriven (i.e., video editing) [11, 12, 18, 29, 43, 58, 62, 67, 69, 76, 80, 82, 101, 116, 119, 121, 127129], image-driven (i.e., image-to-video) [14, 16, 23, 27, 32, 76, 77, 88, 90, 100, 102, 106, 107, 122, 123], personalized video generation [33, 36, 51, 129], and other types of multi-modalcontrolled video synthesis [15, 17, 42, 51, 58, 73, 75, 100, 103, 116, 117, 125, 126]. We build our VBench towards T2V as the initial step, and plan to extend our benchmark suite to accommodate other modalities' controls by adding towards the "Video-Condition Consistency" dimensions. Our "Video Quality" dimensions are readily available for evaluating these video generation tasks.

# M. Additional Experimental Results

In this section, we provide additional numerical results that correspond to the main paper visualizations. We list the resulting tables and figures as follows:

•In Table A8, we show the VBench evaluation results of four video generation models and three image generation models, further illustrating through numerical results the significant differences that exist in certain dimensions between video generation models and image generation models (corresponding to main paper Figure 6 (a)). For Overall Consistency we replaced the ViCLIP approach by CLIP to enable evaluating image generation models.   
In Table A5, we show the win ratio on evaluation results predicted by VBench and Human across four models and all dimensions, along with the correlation $( \rho )$ between Human and VBench results (corresponding to main pa  
per Figure 5).   
• In Table A6, we show the results of WebVid-Avg and compare them with the results of four models and other reference baselines (corresponding to main paper Figure $\boldsymbol { 6 } \left( \boldsymbol { b } \right) )$ .   
• In Table A7, we show all the evaluation results of VBench across four models and eight different categories, providing numerical support for the relevant observations in the insights. (corresponding to main paper Figure 7). Additionally, for the Dynamic Degree dimension, intrinsic attributes of different categories naturally result in noticeable differences in the dynamic degrees among various categories. For instance, the Human category consistently exhibits the highest dynamic degree across different models. Conversely, the Architecture, Scenery, and Plant categories consistently showcase the lowest dynamic degree across various models, and the ascending order from lowest to highest remains consistent as Architecture, Scenery, and Plant. Due to this characteristic, the dynamic degree shows significant variability across different categories. Therefore, we isolate it as a supplementary dimension for additional analysis on top of other

![](images/26.jpg)  
Figure A27. WebVid-10M Dataset Categorical Distribution. We visualize the percentage of data amount of each of the eight content categories in the WebVid-10M dataset.

![](images/27.jpg)  
Figure A28. Aesthetic Quality for Eight Categories in WebVid10M dataset. We visualize the aesthetic score of each of the eight content categories in the WebVid-10M dataset.

dimensions.

• In Figure A27, we show the statistical distribution of data amount of each of the eight content categories in the WebVid-10M dataset (supporting observations and insights mentioned in the main paper Section 5).

•In Figure A28, we show the aesthetic scores of eight different categories within the WebVid-10M dataset (supporting observations and insights mentioned in the main paper Section 5).